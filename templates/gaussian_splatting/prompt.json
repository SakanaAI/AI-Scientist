{
    "system": "You are an ambitious AI researcher who is looking to publish a paper that will contribute significantly to the field.",
    "task_description": "You are given the following file to work with, which implements the method from the paper '3D Gaussian Splatting for Real-Time Radiance Field Rendering' by Kerbl et al. 2023 to create a gaussian splat when given a COLMAP dataset. Find a way to improve metrics such as L1 and psnr and ignoring metrics such as FPS, training speed and memory usage etc. The content of the paper is as follows.\n\n## 3D Gaussian Splatting for Real-Time Radiance Field Rendering\n\n### BERNHARD KERBL[âˆ—], Inria, UniversitÃ© CÃ´te dâ€™Azur, France GEORGIOS KOPANAS[âˆ—], Inria, UniversitÃ© CÃ´te dâ€™Azur, France THOMAS LEIMKÃœHLER, Max-Planck-Institut fÃ¼r Informatik, Germany GEORGE DRETTAKIS, Inria, UniversitÃ© CÃ´te dâ€™Azur, France\n\nInstantNGP (9.2 fps) Plenoxels (8.2 fps) Mip-NeRF360 (0.071 fps) **Ours (135** fps) **Ours (93 fps)**\nGround Truth\nTrain: 7min, PSNR: 22.1 Train: 26min, PSNR: 21.9 Train: 48h, PSNR: 24.3 Train: 6min, PSNR: 23.6 Train: 51min, PSNR: 25.2\n\nFig. 1. Our method achieves real-time rendering of radiance fields with quality that equals the previous method with the best quality [Barron et al. 2022],\nwhile only requiring optimization times competitive with the fastest previous methods [Fridovich-Keil and Yu et al. 2022; MÃ¼ller et al. 2022]. Key to this\nperformance is a novel 3D Gaussian scene representation coupled with a real-time differentiable renderer, which offers significant speedup to both scene\noptimization and novel view synthesis. Note that for comparable training times to InstantNGP [MÃ¼ller et al. 2022], we achieve similar quality to theirs; while\nthis is the maximum quality they reach, by training for 51min we achieve state-of-the-art quality, even slightly better than Mip-NeRF360 [Barron et al. 2022].\n\n\nMip-NeRF360 (0.071 fps)\nTrain: 48h, PSNR: 24.3\n\n\n**Ours (93 fps)**\nTrain: 51min, PSNR: 25.2\n\n\nPlenoxels (8.2 fps)\nTrain: 26min, PSNR: 21.9\n\n\nRadiance Field methods have recently revolutionized novel-view synthesis\nof scenes captured with multiple photos or videos. However, achieving high\nvisual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For\nunbounded and complete scenes (rather than isolated objects) and 1080p\nresolution rendering, no current method can achieve real-time display rates.\nWe introduce three key elements that allow us to achieve state-of-the-art\nvisual quality while maintaining competitive training times and importantly\nallow high-quality real-time (â‰¥ 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration,\nwe represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while\navoiding unnecessary computation in empty space; Second, we perform\ninterleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the\nscene; Third, we develop a fast visibility-aware rendering algorithm that\nsupports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time\nrendering on several established datasets.\n\nCCS Concepts: â€¢ Computing methodologies â†’ **Rendering; Point-based**\n**models; Rasterization; Machine learning approaches.**\n\nâˆ—Both authors contributed equally to the paper.\n\n[Authorsâ€™ addresses: Bernhard Kerbl, bernhard.kerbl@inria.fr, Inria, UniversitÃ© CÃ´te](https://orcid.org/0000-0002-5168-8648)\n[dâ€™Azur, France; Georgios Kopanas, georgios.kopanas@inria.fr, Inria, UniversitÃ© CÃ´te](https://orcid.org/0009-0002-5829-2192)\n[dâ€™Azur, France; Thomas LeimkÃ¼hler, thomas.leimkuehler@mpi-inf.mpg.de, Max-](https://orcid.org/0009-0006-7784-7957)\n[Planck-Institut fÃ¼r Informatik, Germany; George Drettakis, george.drettakis@inria.fr,](https://orcid.org/0000-0002-9254-4819)\nInria, UniversitÃ© CÃ´te dâ€™Azur, France.\n\nPublication rights licensed to ACM. ACM acknowledges that this contribution was\nauthored or co-authored by an employee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or\nreproduce this article, or to allow others to do so, for Government purposes only.\nÂ© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n0730-0301/2018/0-ART0 $15.00\n[https://doi.org/XXXXXXX.XXXXXXX](https://doi.org/XXXXXXX.XXXXXXX)\n\n\nGround Truth\n\n\nInstantNGP (9.2 fps)\nTrain: 7min, PSNR: 22.1\n\n\nAdditional Key Words and Phrases: novel view synthesis, radiance fields, 3D\ngaussians, real-time rendering\n\n**ACM Reference Format:**\nBernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis. 2018. 3D Gaussian Splatting for Real-Time Radiance Field Rendering.\n_[ACM Trans. Graph. 0, 0, Article 0 ( 2018), 14 pages. https://doi.org/XXXXXXX.](https://doi.org/XXXXXXX.XXXXXXX)_\n[XXXXXXX](https://doi.org/XXXXXXX.XXXXXXX)\n\n##### 1 INTRODUCTION\n\nMeshes and points are the most common 3D scene representations\nbecause they are explicit and are a good fit for fast GPU/CUDA-based\nrasterization. In contrast, recent Neural Radiance Field (NeRF) methods build on continuous scene representations, typically optimizing\na Multi-Layer Perceptron (MLP) using volumetric ray-marching for\nnovel-view synthesis of captured scenes. Similarly, the most efficient\nradiance field solutions to date build on continuous representations\nby interpolating values stored in, e.g., voxel [Fridovich-Keil and Yu\net al. 2022] or hash [MÃ¼ller et al. 2022] grids or points [Xu et al. 2022].\nWhile the continuous nature of these methods helps optimization,\nthe stochastic sampling required for rendering is costly and can\nresult in noise. We introduce a new approach that combines the best\nof both worlds: our 3D Gaussian representation allows optimization\nwith state-of-the-art (SOTA) visual quality and competitive training\ntimes, while our tile-based splatting solution ensures real-time rendering at SOTA quality for 1080p resolution on several previously\npublished datasets [Barron et al. 2022; Hedman et al. 2018; Knapitsch\net al. 2017] (see Fig. 1).\nOur goal is to allow real-time rendering for scenes captured with\nmultiple photos, and create the representations with optimization\ntimes as fast as the most efficient previous methods for typical\nreal scenes. Recent methods achieve fast training [Fridovich-Keil\n\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n\n\n**Ours (135** fps)\nTrain: 6min, PSNR: 23.6\n\n\n-----\n\n2 - Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis\n\nand Yu et al. 2022; MÃ¼ller et al. 2022], but struggle to achieve the\nvisual quality obtained by the current SOTA NeRF methods, i.e.,\nMip-NeRF360 [Barron et al. 2022], which requires up to 48 hours of\ntraining time. The fast â€“ but lower-quality â€“ radiance field methods\ncan achieve interactive rendering times depending on the scene\n(10-15 frames per second), but fall short of real-time rendering at\nhigh resolution.\nOur solution builds on three main components. We first introduce 3D Gaussians as a flexible and expressive scene representation.\nWe start with the same input as previous NeRF-like methods, i.e.,\ncameras calibrated with Structure-from-Motion (SfM) [Snavely et al.\n2006] and initialize the set of 3D Gaussians with the sparse point\ncloud produced for free as part of the SfM process. In contrast to\nmost point-based solutions that require Multi-View Stereo (MVS)\ndata [Aliev et al. 2020; Kopanas et al. 2021; RÃ¼ckert et al. 2022], we\nachieve high-quality results with only SfM points as input. Note\nthat for the NeRF-synthetic dataset, our method achieves high quality even with random initialization. We show that 3D Gaussians\nare an excellent choice, since they are a differentiable volumetric\nrepresentation, but they can also be rasterized very efficiently by\nprojecting them to 2D, and applying standard ğ›¼-blending, using an\nequivalent image formation model as NeRF. The second component\nof our method is optimization of the properties of the 3D Gaussians\nâ€“ 3D position, opacity ğ›¼, anisotropic covariance, and spherical harmonic (SH) coefficients â€“ interleaved with adaptive density control\nsteps, where we add and occasionally remove 3D Gaussians during\noptimization. The optimization procedure produces a reasonably\ncompact, unstructured, and precise representation of the scene (1-5\nmillion Gaussians for all scenes tested). The third and final element\nof our method is our real-time rendering solution that uses fast GPU\nsorting algorithms and is inspired by tile-based rasterization, following recent work [Lassner and Zollhofer 2021]. However, thanks\nto our 3D Gaussian representation, we can perform anisotropic\nsplatting that respects visibility ordering â€“ thanks to sorting and ğ›¼blending â€“ and enable a fast and accurate backward pass by tracking\nthe traversal of as many sorted splats as required.\nTo summarize, we provide the following contributions:\n\nThe introduction of anisotropic 3D Gaussians as a high-quality,\n\n  unstructured representation of radiance fields.\nAn optimization method of 3D Gaussian properties, inter\n  leaved with adaptive density control that creates high-quality\nrepresentations for captured scenes.\nA fast, differentiable rendering approach for the GPU, which\n\n  is visibility-aware, allows anisotropic splatting and fast backpropagation to achieve high-quality novel view synthesis.\n\nOur results on previously published datasets show that we can optimize our 3D Gaussians from multi-view captures and achieve equal\nor better quality than the best quality previous implicit radiance\nfield approaches. We also can achieve training speeds and quality\nsimilar to the fastest methods and importantly provide the first\n_real-time rendering with high quality for novel-view synthesis._\n\n##### 2 RELATED WORK\n\nWe first briefly overview traditional reconstruction, then discuss\npoint-based rendering and radiance field work, discussing their\n\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n\n\nsimilarity; radiance fields are a vast area, so we focus only on directly\nrelated work. For complete coverage of the field, please see the\nexcellent recent surveys [Tewari et al. 2022; Xie et al. 2022].\n\n##### 2.1 Traditional Scene Reconstruction and Rendering\n\nThe first novel-view synthesis approaches were based on light fields,\nfirst densely sampled [Gortler et al. 1996; Levoy and Hanrahan 1996]\nthen allowing unstructured capture [Buehler et al. 2001]. The advent\nof Structure-from-Motion (SfM) [Snavely et al. 2006] enabled an\nentire new domain where a collection of photos could be used to\nsynthesize novel views. SfM estimates a sparse point cloud during\ncamera calibration, that was initially used for simple visualization\nof 3D space. Subsequent multi-view stereo (MVS) produced impressive full 3D reconstruction algorithms over the years [Goesele\net al. 2007], enabling the development of several view synthesis\nalgorithms [Chaurasia et al. 2013; Eisemann et al. 2008; Hedman\net al. 2018; Kopanas et al. 2021]. All these methods re-project and\n_blend the input images into the novel view camera, and use the_\ngeometry to guide this re-projection. These methods produced excellent results in many cases, but typically cannot completely recover from unreconstructed regions, or from â€œover-reconstructionâ€,\nwhen MVS generates inexistent geometry. Recent neural rendering algorithms [Tewari et al. 2022] vastly reduce such artifacts and\navoid the overwhelming cost of storing all input images on the GPU,\noutperforming these methods on most fronts.\n\n##### 2.2 Neural Rendering and Radiance Fields\n\nDeep learning techniques were adopted early for novel-view synthesis [Flynn et al. 2016; Zhou et al. 2016]; CNNs were used to estimate\nblending weights [Hedman et al. 2018], or for texture-space solutions\n\n[Riegler and Koltun 2020; Thies et al. 2019]. The use of MVS-based\ngeometry is a major drawback of most of these methods; in addition,\nthe use of CNNs for final rendering frequently results in temporal\nflickering.\nVolumetric representations for novel-view synthesis were initiated by Soft3D [Penner and Zhang 2017]; deep-learning techniques coupled with volumetric ray-marching were subsequently\nproposed [Henzler et al. 2019; Sitzmann et al. 2019] building on a continuous differentiable density field to represent geometry. Rendering\nusing volumetric ray-marching has a significant cost due to the large\nnumber of samples required to query the volume. Neural Radiance\nFields (NeRFs) [Mildenhall et al. 2020] introduced importance sampling and positional encoding to improve quality, but used a large\nMulti-Layer Perceptron negatively affecting speed. The success of\nNeRF has resulted in an explosion of follow-up methods that address\nquality and speed, often by introducing regularization strategies; the\ncurrent state-of-the-art in image quality for novel-view synthesis is\nMip-NeRF360 [Barron et al. 2022]. While the rendering quality is\noutstanding, training and rendering times remain extremely high;\nwe are able to equal or in some cases surpass this quality while\nproviding fast training and real-time rendering.\nThe most recent methods have focused on faster training and/or\nrendering mostly by exploiting three design choices: the use of spatial data structures to store (neural) features that are subsequently\ninterpolated during volumetric ray-marching, different encodings,\n\n\n-----\n\nand MLP capacity. Such methods include different variants of space\ndiscretization [Chen et al. 2022b,a; Fridovich-Keil and Yu et al. 2022;\nGarbin et al. 2021; Hedman et al. 2021; Reiser et al. 2021; Takikawa\net al. 2021; Wu et al. 2022; Yu et al. 2021], codebooks [Takikawa\net al. 2022], and encodings such as hash tables [MÃ¼ller et al. 2022],\nallowing the use of a smaller MLP or foregoing neural networks\ncompletely [Fridovich-Keil and Yu et al. 2022; Sun et al. 2022].\nMost notable of these methods are InstantNGP [MÃ¼ller et al. 2022]\nwhich uses a hash grid and an occupancy grid to accelerate computation and a smaller MLP to represent density and appearance; and\nPlenoxels [Fridovich-Keil and Yu et al. 2022] that use a sparse voxel\ngrid to interpolate a continuous density field, and are able to forgo\nneural networks altogether. Both rely on Spherical Harmonics: the\nformer to represent directional effects directly, the latter to encode\nits inputs to the color network. While both provide outstanding\nresults, these methods can still struggle to represent empty space\neffectively, depending in part on the scene/capture type. In addition,\nimage quality is limited in large part by the choice of the structured\ngrids used for acceleration, and rendering speed is hindered by the\nneed to query many samples for a given ray-marching step. The unstructured, explicit GPU-friendly 3D Gaussians we use achieve faster\nrendering speed and better quality without neural components.\n\n##### 2.3 Point-Based Rendering and Radiance Fields\n\nPoint-based methods efficiently render disconnected and unstructured geometry samples (i.e., point clouds) [Gross and Pfister 2011].\nIn its simplest form, point sample rendering [Grossman and Dally\n1998] rasterizes an unstructured set of points with a fixed size, for\nwhich it may exploit natively supported point types of graphics APIs\n\n[Sainz and Pajarola 2004] or parallel software rasterization on the\nGPU [Laine and Karras 2011; SchÃ¼tz et al. 2022]. While true to the\nunderlying data, point sample rendering suffers from holes, causes\naliasing, and is strictly discontinuous. Seminal work on high-quality\npoint-based rendering addresses these issues by â€œsplattingâ€ point\nprimitives with an extent larger than a pixel, e.g., circular or elliptic\ndiscs, ellipsoids, or surfels [Botsch et al. 2005; Pfister et al. 2000; Ren\net al. 2002; Zwicker et al. 2001b].\nThere has been recent interest in differentiable point-based rendering techniques [Wiles et al. 2020; Yifan et al. 2019]. Points have been\naugmented with neural features and rendered using a CNN [Aliev\net al. 2020; RÃ¼ckert et al. 2022] resulting in fast or even real-time\nview synthesis; however they still depend on MVS for the initial\ngeometry, and as such inherit its artifacts, most notably over- or\nunder-reconstruction in hard cases such as featureless/shiny areas\nor thin structures.\nPoint-based ğ›¼-blending and NeRF-style volumetric rendering\nshare essentially the same image formation model. Specifically, the\ncolor ğ¶ is given by volumetric rendering along a ray:\n\n\n3D Gaussian Splatting for Real-Time Radiance Field Rendering        - 3\n\nwhere samples of density ğœ, transmittance ğ‘‡, and color c are taken\nalong the ray with intervals ğ›¿ğ‘– . This can be re-written as\n\n\n_ğ¶_ =\n\n\n_ğ‘_\nâˆ‘ï¸\n\n_ğ‘‡ğ‘–ğ›¼ğ‘–_ cğ‘–, (2)\n_ğ‘–=1_\n\n\nwith\n\n\n_ğ›¼ğ‘–_ = (1 âˆ’ exp(âˆ’ğœğ‘–ğ›¿ğ‘– )) and ğ‘‡ğ‘– =\n\n\nï¿½ğ‘– âˆ’1\n\n(1 âˆ’ _ğ›¼ğ‘–_ ).\n_ğ‘—=1_\n\n\nA typical neural point-based approach (e.g., [Kopanas et al. 2022,\n2021]) computes the color ğ¶ of a pixel by blending N ordered points\noverlapping the pixel:\n\n\nâˆ‘ï¸\n_ğ¶_ = _ğ‘ğ‘–ğ›¼ğ‘–_\n\n_ğ‘–_ âˆˆN\n\n\nï¿½ğ‘– âˆ’1\n\n(1 âˆ’ _ğ›¼_ _ğ‘—_ ), (3)\n_ğ‘—=1_\n\n\nwhere cğ‘– is the color of each point and ğ›¼ğ‘– is given by evaluating a\n2D Gaussian with covariance Î£ [Yifan et al. 2019] multiplied with a\nlearned per-point opacity.\nFrom Eq. 2 and Eq. 3, we can clearly see that the image formation\nmodel is the same. However, the rendering algorithm is very different. NeRFs are a continuous representation implicitly representing\nempty/occupied space; expensive random sampling is required to\nfind the samples in Eq. 2 with consequent noise and computational\nexpense. In contrast, points are an unstructured, discrete representation that is flexible enough to allow creation, destruction, and\ndisplacement of geometry similar to NeRF. This is achieved by optimizing opacity and positions, as shown by previous work [Kopanas\net al. 2021], while avoiding the shortcomings of a full volumetric\nrepresentation.\nPulsar [Lassner and Zollhofer 2021] achieves fast sphere rasterization which inspired our tile-based and sorting renderer. However,\ngiven the analysis above, we want to maintain (approximate) conventional ğ›¼-blending on sorted splats to have the advantages of volumetric representations: Our rasterization respects visibility order\nin contrast to their order-independent method. In addition, we backpropagate gradients on all splats in a pixel and rasterize anisotropic\nsplats. These elements all contribute to the high visual quality of\nour results (see Sec. 7.3). In addition, previous methods mentioned\nabove also use CNNs for rendering, which results in temporal instability. Nonetheless, the rendering speed of Pulsar [Lassner and\nZollhofer 2021] and ADOP [RÃ¼ckert et al. 2022] served as motivation\nto develop our fast rendering solution.\nWhile focusing on specular effects, the diffuse point-based rendering track of Neural Point Catacaustics [Kopanas et al. 2022]\novercomes this temporal instability by using an MLP, but still required MVS geometry as input. The most recent method [Zhang\net al. 2022] in this category does not require MVS, and also uses\nSH for directions; however, it can only handle scenes of one object\nand needs masks for initialization. While fast for small resolutions\nand low point counts, it is unclear how it can scale to scenes of\ntypical datasets [Barron et al. 2022; Hedman et al. 2018; Knapitsch\net al. 2017]. We use 3D Gaussians for a more flexible scene representation, avoiding the need for MVS geometry and achieving\nreal-time rendering thanks to our tile-based rendering algorithm\nfor the projected Gaussians.\n\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n\n\nâˆ‘ï¸ğ‘– âˆ’1\n\n_ğœ_ _ğ‘—_ _ğ›¿_ _ğ‘—_ [ï¿½] _,_ (1)\nï¿½\n_ğ‘—=1_\nï¿½\n\n\n_ğ¶_ =\n\n\n_ğ‘_\nâˆ‘ï¸\n\n_ğ‘‡ğ‘–_ (1 âˆ’ exp(âˆ’ğœğ‘–ğ›¿ğ‘– ))cğ‘– with ğ‘‡ğ‘– = exp [ï¿½]ï¿½âˆ’\n_ğ‘–=1_\nï¿½\n\n\n-----\n\n4 - Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis\n\nA recent approach [Xu et al. 2022] uses points to represent a\nradiance field with a radial basis function approach. They employ\npoint pruning and densification techniques during optimization, but\nuse volumetric ray-marching and cannot achieve real-time display\nrates.\nIn the domain of human performance capture, 3D Gaussians have\nbeen used to represent captured human bodies [Rhodin et al. 2015;\nStoll et al. 2011]; more recently they have been used with volumetric\nray-marching for vision tasks [Wang et al. 2023]. Neural volumetric\nprimitives have been proposed in a similar context [Lombardi et al.\n2021]. While these methods inspired the choice of 3D Gaussians as\nour scene representation, they focus on the specific case of reconstructing and rendering a single isolated object (a human body or\nface), resulting in scenes with small depth complexity. In contrast,\nour optimization of anisotropic covariance, our interleaved optimization/density control, and efficient depth sorting for rendering allow\nus to handle complete, complex scenes including background, both\nindoors and outdoors and with large depth complexity.\n\n##### 3 OVERVIEW\n\nThe input to our method is a set of images of a static scene, together\nwith the corresponding cameras calibrated by SfM [SchÃ¶nberger\nand Frahm 2016] which produces a sparse point cloud as a sideeffect. From these points we create a set of 3D Gaussians (Sec. 4),\ndefined by a position (mean), covariance matrix and opacity ğ›¼, that\nallows a very flexible optimization regime. This results in a reasonably compact representation of the 3D scene, in part because highly\nanisotropic volumetric splats can be used to represent fine structures\ncompactly. The directional appearance component (color) of the\nradiance field is represented via spherical harmonics (SH), following\nstandard practice [Fridovich-Keil and Yu et al. 2022; MÃ¼ller et al.\n2022]. Our algorithm proceeds to create the radiance field representation (Sec. 5) via a sequence of optimization steps of 3D Gaussian\nparameters, i.e., position, covariance, ğ›¼ and SH coefficients interleaved with operations for adaptive control of the Gaussian density.\nThe key to the efficiency of our method is our tile-based rasterizer\n(Sec. 6) that allows ğ›¼-blending of anisotropic splats, respecting visibility order thanks to fast sorting. Out fast rasterizer also includes\na fast backward pass by tracking accumulated ğ›¼ values, without a\nlimit on the number of Gaussians that can receive gradients. The\noverview of our method is illustrated in Fig. 2.\n\n##### 4 DIFFERENTIABLE 3D GAUSSIAN SPLATTING\n\nOur goal is to optimize a scene representation that allows highquality novel view synthesis, starting from a sparse set of (SfM)\npoints without normals. To do this, we need a primitive that inherits\nthe properties of differentiable volumetric representations, while\nat the same time being unstructured and explicit to allow very fast\nrendering. We choose 3D Gaussians, which are differentiable and\ncan be easily projected to 2D splats allowing fast ğ›¼-blending for\nrendering.\nOur representation has similarities to previous methods that use\n2D points [Kopanas et al. 2021; Yifan et al. 2019] and assume each\npoint is a small planar circle with a normal. Given the extreme\nsparsity of SfM points it is very hard to estimate normals. Similarly,\n\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n\n\noptimizing very noisy normals from such an estimation would be\nvery challenging. Instead, we model the geometry as a set of 3D\nGaussians that do not require normals. Our Gaussians are defined\nby a full 3D covariance matrix Î£ defined in world space [Zwicker\net al. 2001a] centered at point (mean) ğœ‡:\n\n_ğº_ (ğ‘¥) = ğ‘’ [âˆ’] 2[1] [(][ğ‘¥] [)][ğ‘‡] [Î£][âˆ’][1] [(][ğ‘¥] [)] (4)\n\n. This Gaussian is multiplied by ğ›¼ in our blending process.\nHowever, we need to project our 3D Gaussians to 2D for rendering.\nZwicker et al. [2001a] demonstrate how to do this projection to\nimage space. Given a viewing transformation ğ‘Š the covariance\nmatrix Î£[â€²] in camera coordinates is given as follows:\n\nÎ£[â€²] = ğ½ğ‘Š Î£ ğ‘Š _[ğ‘‡]_ _ğ½[ğ‘‡]_ (5)\n\nwhere ğ½ is the Jacobian of the affine approximation of the projective\ntransformation. Zwicker et al. [2001a] also show that if we skip the\nthird row and column of Î£[â€²], we obtain a 2Ã—2 variance matrix with\nthe same structure and properties as if we would start from planar\npoints with normals, as in previous work [Kopanas et al. 2021].\nAn obvious approach would be to directly optimize the covariance\nmatrix Î£ to obtain 3D Gaussians that represent the radiance field.\nHowever, covariance matrices have physical meaning only when\nthey are positive semi-definite. For our optimization of all our parameters, we use gradient descent that cannot be easily constrained\nto produce such valid matrices, and update steps and gradients can\nvery easily create invalid covariance matrices.\nAs a result, we opted for a more intuitive, yet equivalently expressive representation for optimization. The covariance matrix Î£\nof a 3D Gaussian is analogous to describing the configuration of an\nellipsoid. Given a scaling matrix ğ‘† and rotation matrix ğ‘…, we can\nfind the corresponding Î£:\n\nÎ£ = ğ‘…ğ‘†ğ‘†[ğ‘‡] _ğ‘…[ğ‘‡]_ (6)\n\nTo allow independent optimization of both factors, we store them\nseparately: a 3D vector ğ‘  for scaling and a quaternion ğ‘ to represent\nrotation. These can be trivially converted to their respective matrices\nand combined, making sure to normalize ğ‘ to obtain a valid unit\nquaternion.\nTo avoid significant overhead due to automatic differentiation\nduring training, we derive the gradients for all parameters explicitly.\nDetails of the exact derivative computations are in appendix A.\nThis representation of anisotropic covariance â€“ suitable for optimization â€“ allows us to optimize 3D Gaussians to adapt to the\ngeometry of different shapes in captured scenes, resulting in a fairly\ncompact representation. Fig. 3 illustrates such cases.\n\n##### 5 OPTIMIZATION WITH ADAPTIVE DENSITY CONTROL OF 3D GAUSSIANS\n\nThe core of our approach is the optimization step, which creates\na dense set of 3D Gaussians accurately representing the scene for\nfree-view synthesis. In addition to positions ğ‘, ğ›¼, and covariance\nÎ£, we also optimize SH coefficients representing color ğ‘ of each\nGaussian to correctly capture the view-dependent appearance of\nthe scene. The optimization of these parameters is interleaved with\nsteps that control the density of the Gaussians to better represent\nthe scene.\n\n\n-----\n\n3D Gaussian Splatting for Real-Time Radiance Field Rendering - 5\n\n\nCamera\n\n\nProjection\n\n\nInitialization\n\n\nDifferentiable\nTile Rasterizer\n\n\nImage\n\n\nSfM Points **3D Gaussians**\n\n\nAdaptive\nDensity Control\n\n\nFig. 2. Optimization starts with the sparse SfM point cloud and creates a set of 3D Gaussians. We then optimize and adaptively control the density of this set\nof Gaussians. During optimization we use our fast tile-based renderer, allowing competitive training times compared to SOTA fast radiance field methods.\nOnce trained, our renderer allows real-time navigation for a wide variety of scenes.\n\nWe use íœ† = 0.2 in all our tests. We provide details of the learning\nschedule and other elements in Sec. 7.1.\n\n##### 5.2 Adaptive Control of Gaussians\n\n\nShrunken\nOriginal\nGaussians\n\n\nFig. 3. We visualize the 3D Gaussians after optimization by shrinking them\n60% (far right). This clearly shows the anisotropic shapes of the 3D Gaussians\nthat compactly represent complex geometry after optimization. Left the\nactual rendered image.\n\n##### 5.1 Optimization\n\nThe optimization is based on successive iterations of rendering and\ncomparing the resulting image to the training views in the captured\ndataset. Inevitably, geometry may be incorrectly placed due to the\nambiguities of 3D to 2D projection. Our optimization thus needs to\nbe able to create geometry and also destroy or move geometry if it\nhas been incorrectly positioned. The quality of the parameters of the\ncovariances of the 3D Gaussians is critical for the compactness of\nthe representation since large homogeneous areas can be captured\nwith a small number of large anisotropic Gaussians.\nWe use Stochastic Gradient Descent techniques for optimization,\ntaking full advantage of standard GPU-accelerated frameworks,\nand the ability to add custom CUDA kernels for some operations,\nfollowing recent best practice [Fridovich-Keil and Yu et al. 2022;\nSun et al. 2022]. In particular, our fast rasterization (see Sec. 6) is\ncritical in the efficiency of our optimization, since it is the main\ncomputational bottleneck of the optimization.\nWe use a sigmoid activation function for í›¼ to constrain it in\nthe 0 1 range and obtain smooth gradients, and an exponential\n[ âˆ’ )\nactivation function for the scale of the covariance for similar reasons.\nWe estimate the initial covariance matrix as an isotropic Gaussian\nwith axes equal to the mean of the distance to the closest three points.\nWe use a standard exponential decay scheduling technique similar\nto Plenoxels [Fridovich-Keil and Yu et al. 2022], but for positions\nonly. The loss function is L1 combined with a D-SSIM term:\n\nL = (1 âˆ’ _íœ†)L1 + íœ†LD-SSIM_ (7)\n\n\nWe start with the initial set of sparse points from SfM and then apply\nour method to adaptively control the number of Gaussians and their\ndensity over unit volume[1], allowing us to go from an initial sparse\nset of Gaussians to a denser set that better represents the scene, and\nwith correct parameters. After optimization warm-up (see Sec. 7.1),\nwe densify every 100 iterations and remove any Gaussians that are\nessentially transparent, i.e., with í›¼ less than a threshold íœ–í›¼ .\nOur adaptive control of the Gaussians needs to populate empty\nareas. It focuses on regions with missing geometric features (â€œunderreconstructionâ€), but also in regions where Gaussians cover large\nareas in the scene (which often correspond to â€œover-reconstructionâ€).\nWe observe that both have large view-space positional gradients.\nIntuitively, this is likely because they correspond to regions that are\nnot yet well reconstructed, and the optimization tries to move the\nGaussians to correct this.\nSince both cases are good candidates for densification, we densify Gaussians with an average magnitude of view-space position\ngradients above a threshold íœpos, which we set to 0.0002 in our tests.\nWe next present details of this process, illustrated in Fig. 4.\nFor small Gaussians that are in under-reconstructed regions, we\nneed to cover the new geometry that must be created. For this, it is\npreferable to clone the Gaussians, by simply creating a copy of the\nsame size, and moving it in the direction of the positional gradient.\nOn the other hand, large Gaussians in regions with high variance\nneed to be split into smaller Gaussians. We replace such Gaussians\nby two new ones, and divide their scale by a factor of íœ™ = 1.6 which\nwe determined experimentally. We also initialize their position by\nusing the original 3D Gaussian as a PDF for sampling.\nIn the first case we detect and treat the need for increasing both\nthe total volume of the system and the number of Gaussians, while\nin the second case we conserve total volume but increase the number of Gaussians. Similar to other volumetric representations, our\noptimization can get stuck with floaters close to the input cameras;\nin our case this may result in an unjustified increase in the Gaussian\ndensity. An effective way to moderate the increase in the number\nof Gaussians is to set the í›¼ value close to zero every í‘ = 3000\n\n1Density of Gaussians should not be confused of course with density íœ in the NeRF\nliterature.\n\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n\n\n-----\n\n6 - Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis\n\n# â€¦\n\nClone OptimizationOptimization\n\nContinues\n\n# â€¦\n\n\nOptimization\n\nSplitSplitSplit\n\nContinuesContinues\n\nFig. 4. Our adaptive Gaussian densification scheme. Top row (under_reconstruction): When small-scale geometry (black outline) is insufficiently_\ncovered, we clone the respective Gaussian. Bottom row (over-reconstruction):\nIf small-scale geometry is represented by one large splat, we split it in two.\n\niterations. The optimization then increases the í›¼ for the Gaussians\nwhere this is needed while allowing our culling approach to remove\nGaussians with í›¼ less than íœ–í›¼ as described above. Gaussians may\nshrink or grow and considerably overlap with others, but we periodically remove Gaussians that are very large in worldspace and\nthose that have a big footprint in viewspace. This strategy results\nin overall good control over the total number of Gaussians. The\nGaussians in our model remain primitives in Euclidean space at all\ntimes; unlike other methods [Barron et al. 2022; Fridovich-Keil and\nYu et al. 2022], we do not require space compaction, warping or\nprojection strategies for distant or large Gaussians.\n\n##### 6 FAST DIFFERENTIABLE RASTERIZER FOR GAUSSIANS\n\nOur goals are to have fast overall rendering and fast sorting to allow\napproximate í›¼-blending â€“ including for anisotropic splats â€“ and to\navoid hard limits on the number of splats that can receive gradients\nthat exist in previous work [Lassner and Zollhofer 2021].\nTo achieve these goals, we design a tile-based rasterizer for Gaussian splats inspired by recent software rasterization approaches [Lassner and Zollhofer 2021] to pre-sort primitives for an entire image\nat a time, avoiding the expense of sorting per pixel that hindered\nprevious í›¼-blending solutions [Kopanas et al. 2022, 2021]. Our fast\nrasterizer allows efficient backpropagation over an arbitrary number of blended Gaussians with low additional memory consumption, requiring only a constant overhead per pixel. Our rasterization\npipeline is fully differentiable, and given the projection to 2D (Sec. 4)\ncan rasterize anisotropic splats similar to previous 2D splatting\nmethods [Kopanas et al. 2021].\nOur method starts by splitting the screen into 16 16 tiles, and\nÃ—\nthen proceeds to cull 3D Gaussians against the view frustum and\neach tile. Specifically, we only keep Gaussians with a 99% confidence interval intersecting the view frustum. Additionally, we use a\nguard band to trivially reject Gaussians at extreme positions (i.e.,\nthose with means close to the near plane and far outside the view\nfrustum), since computing their projected 2D covariance would\nbe unstable. We then instantiate each Gaussian according to the\n\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n\n\nnumber of tiles they overlap and assign each instance a key that\ncombines view space depth and tile ID. We then sort Gaussians\nbased on these keys using a single fast GPU Radix sort [Merrill\nand Grimshaw 2010]. Note that there is no additional per-pixel ordering of points, and blending is performed based on this initial\nsorting. As a consequence, our í›¼-blending can be approximate in\nsome configurations. However, these approximations become negligible as splats approach the size of individual pixels. We found that\nthis choice greatly enhances training and rendering performance\nwithout producing visible artifacts in converged scenes.\nAfter sorting Gaussians, we produce a list for each tile by identifying the first and last depth-sorted entry that splats to a given\ntile. For rasterization, we launch one thread block for each tile. Each\nblock first collaboratively loads packets of Gaussians into shared\nmemory and then, for a given pixel, accumulates color and í›¼ values\nby traversing the lists front-to-back, thus maximizing the gain in\nparallelism both for data loading/sharing and processing. When we\nreach a target saturation of í›¼ in a pixel, the corresponding thread\nstops. At regular intervals, threads in a tile are queried and the processing of the entire tile terminates when all pixels have saturated\n(i.e., í›¼ goes to 1). Details of sorting and a high-level overview of the\noverall rasterization approach are given in Appendix C.\nDuring rasterization, the saturation of í›¼ is the only stopping criterion. In contrast to previous work, we do not limit the number\nof blended primitives that receive gradient updates. We enforce\nthis property to allow our approach to handle scenes with an arbitrary, varying depth complexity and accurately learn them, without\nhaving to resort to scene-specific hyperparameter tuning. During\nthe backward pass, we must therefore recover the full sequence of\nblended points per-pixel in the forward pass. One solution would\nbe to store arbitrarily long lists of blended points per-pixel in global\nmemory [Kopanas et al. 2021]. To avoid the implied dynamic memory management overhead, we instead choose to traverse the pertile lists again; we can reuse the sorted array of Gaussians and tile\nranges from the forward pass. To facilitate gradient computation,\nwe now traverse them back-to-front.\nThe traversal starts from the last point that affected any pixel in\nthe tile, and loading of points into shared memory again happens\ncollaboratively. Additionally, each pixel will only start (expensive)\noverlap testing and processing of points if their depth is lower than\nor equal to the depth of the last point that contributed to its color\nduring the forward pass. Computation of the gradients described in\nSec. 4 requires the accumulated opacity values at each step during\nthe original blending process. Rather than trasversing an explicit\nlist of progressively shrinking opacities in the backward pass, we\ncan recover these intermediate opacities by storing only the total\naccumulated opacity at the end of the forward pass. Specifically, each\npoint stores the final accumulated opacity í›¼ in the forward process;\nwe divide this by each pointâ€™s í›¼ in our back-to-front traversal to\nobtain the required coefficients for gradient computation.\n\n##### 7 IMPLEMENTATION, RESULTS AND EVALUATION\n\nWe next discuss some details of implementation, present results and\nthe evaluation of our algorithm compared to previous work and\nablation studies.\n\n\nSplitSplitSplit\n\n\n-----\n\n3D Gaussian Splatting for Real-Time Radiance Field Rendering                                                     - 7\n\nGround Truth Ours Mip-NeRF360 InstantNGP Plenoxels\n\nFig. 5. We show comparisons of ours to previous methods and the corresponding ground truth images from held-out test views. The scenes are, from the top\ndown: Bicycle, Garden, Stump, Counter and Room from the Mip-NeRF360 dataset; Playroom, DrJohnson from the Deep Blending dataset [Hedman et al.\n2018] and Truck and Train from Tanks&Temples. Non-obvious differences in quality highlighted by arrows/insets.\n\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n\n\nOurs\n\n\nPlenoxels\n\n\n-----\n\n8 - Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis\n\nTable 1. Quantitative evaluation of our method compared to previous work, computed over three datasets. Results marked with dagger â€  have been directly\nadopted from the original paper, all others were obtained in our own experiments.\n\nDataset Mip-NeRF360 Tanks&Temples Deep Blending\nMethod|Metric _ğ‘†ğ‘†ğ¼ğ‘€_ [â†‘] _ğ‘ƒğ‘†ğ‘ğ‘…[â†‘]_ _ğ¿ğ‘ƒğ¼ğ‘ƒğ‘†_ [â†“] Train FPS Mem _ğ‘†ğ‘†ğ¼ğ‘€_ [â†‘] _ğ‘ƒğ‘†ğ‘ğ‘…[â†‘]_ _ğ¿ğ‘ƒğ¼ğ‘ƒğ‘†_ [â†“] Train FPS Mem _ğ‘†ğ‘†ğ¼ğ‘€_ [â†‘] _ğ‘ƒğ‘†ğ‘ğ‘…[â†‘]_ _ğ¿ğ‘ƒğ¼ğ‘ƒğ‘†_ [â†“] Train FPS Mem\n\nPlenoxels 0.626 23.08 0.463 25m49s 6.79 2.1GB 0.719 21.08 0.379 25m5s 13.0 2.3GB 0.795 23.06 0.510 27m49s 11.2 2.7GB\nINGP-Base 0.671 25.30 0.371 5m37s 11.7 13MB 0.723 21.72 0.330 5m26s 17.1 13MB 0.797 23.62 0.423 6m31s 3.26 13MB\nINGP-Big 0.699 25.59 0.331 7m30s 9.43 48MB 0.745 21.92 0.305 6m59s 14.4 48MB 0.817 24.96 0.390 8m 2.79 48MB\nM-NeRF360 0.792[â€ ] 27.69[â€ ] 0.237[â€ ] 48h 0.06 8.6MB 0.759 22.22 0.257 48h 0.14 8.6MB 0.901 29.40 0.245 48h 0.09 8.6MB\nOurs-7K 0.770 25.60 0.279 6m25s 160 523MB 0.767 21.20 0.280 6m55s 197 270MB 0.875 27.78 0.317 4m35s 172 386MB\nOurs-30K 0.815 27.21 0.214 41m33s 134 734MB 0.841 23.14 0.183 26m54s 154 411MB 0.903 29.41 0.243 36m2s 137 676MB\n\nis observed by photos taken in the entire hemisphere around it, the\noptimization works well. However, if the capture has angular regions\nmissing (e.g., when capturing the corner of a scene, or performing\nan â€œinside-outâ€ [Hedman et al. 2016] capture) completely incorrect\nvalues for the zero-order component of the SH (i.e., the base or\n\n#### 7K iterations 30K iterations diffuse color) can be produced by the optimization. To overcome\n\nthis problem we start by optimizing only the zero-order component,\nand then introduce one band of the SH after every 1000 iterations\nuntil all 4 bands of SH are represented.\n\n##### 7.2 Results and Evaluation\n\n\n#### 30K iterations\n\n\n#### 7K iterations 30K iterations\n\nFig. 6. For some scenes (above) we can see that even at 7K iterations (âˆ¼5min\nfor this scene), our method has captured the train quite well. At 30K iterations (âˆ¼35min) the background artifacts have been reduced significantly. For\nother scenes (below), the difference is barely visible; 7K iterations (âˆ¼8min)\nis already very high quality.\n\nTable 2. PSNR scores for Synthetic NeRF, we start with 100K randomly\ninitialized points. Competing metrics extracted from respective papers.\n\nMic Chair Ship Materials Lego Drums Ficus Hotdog Avg.\n\nPlenoxels 33.26 33.98 29.62 29.14 34.10 25.35 31.83 36.81 31.76\nINGP-Base 36.22 35.00 31.10 29.78 36.39 26.02 33.51 37.40 33.18\nMip-NeRF 36.51 35.14 30.41 30.71 35.70 25.48 33.29 37.48 33.09\nPoint-NeRF 35.95 35.40 30.97 29.61 35.04 26.06 36.13 37.30 33.30\nOurs-30K 35.36 35.83 30.80 30.00 35.78 26.15 34.87 37.72 33.32\n\n##### 7.1 Implementation\n\nWe implemented our method in Python using the PyTorch framework and wrote custom CUDA kernels for rasterization that are\nextended versions of previous methods [Kopanas et al. 2021], and\nuse the NVIDIA CUB sorting routines for the fast Radix sort [Merrill and Grimshaw 2010]. We also built an interactive viewer using\nthe open-source SIBR [Bonopera et al. 2020], used for interactive\nviewing. We used this implementation to measure our achieved\nframe rates. The source code and all our data are available at:\n[https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)\n\n_Optimization Details. For stability, we â€œwarm-upâ€ the computa-_\ntion in lower resolution. Specifically, we start the optimization using\n4 times smaller image resolution and we upsample twice after 250\nand 500 iterations.\nSH coefficient optimization is sensitive to the lack of angular\ninformation. For typical â€œNeRF-likeâ€ captures where a central object\n\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n\n\n#### 7K iterations\n\n\n_Results. We tested our algorithm on a total of 13 real scenes_\ntaken from previously published datasets and the synthetic Blender\ndataset [Mildenhall et al. 2020]. In particular, we tested our approach on the full set of scenes presented in Mip-Nerf360 [Barron\net al. 2022], which is the current state of the art in NeRF rendering\nquality, two scenes from the Tanks&Temples dataset [2017] and\ntwo scenes provided by Hedman et al. [Hedman et al. 2018]. The\nscenes we chose have very different capture styles, and cover both\nbounded indoor scenes and large unbounded outdoor environments.\nWe use the same hyperparameter configuration for all experiments\nin our evaluation. All results are reported running on an A6000 GPU,\nexcept for the Mip-NeRF360 method (see below).\nIn supplemental, we show a rendered video path for a selection\nof scenes that contain views far from the input photos.\n\n_Real-World Scenes. In terms of quality, the current state-of-the-_\nart is Mip-Nerf360 [Barron et al. 2021]. We compare against this\nmethod as a quality benchmark. We also compare against two of\nthe most recent fast NeRF methods: InstantNGP [MÃ¼ller et al. 2022]\nand Plenoxels [Fridovich-Keil and Yu et al. 2022].\nWe use a train/test split for datasets, using the methodology\nsuggested by Mip-NeRF360, taking every 8th photo for test, for consistent and meaningful comparisons to generate the error metrics,\nusing the standard PSNR, L-PIPS, and SSIM metrics used most frequently in the literature; please see Table 1. All numbers in the table\nare from our own runs of the authorâ€™s code for all previous methods, except for those of Mip-NeRF360 on their dataset, in which we\ncopied the numbers from the original publication to avoid confusion\nabout the current SOTA. For the images in our figures, we used our\nown run of Mip-NeRF360: the numbers for these runs are in Appendix D. We also show the average training time, rendering speed, and\nmemory used to store optimized parameters. We report results for a\nbasic configuration of InstantNGP (Base) that run for 35K iterations\nas well as a slightly larger network suggested by the authors (Big),\nand two configurations, 7K and 30K iterations for ours. We show\n\n\n#### 7K iterations\n\n\n-----\n\n3D Gaussian Splatting for Real-Time Radiance Field Rendering                                                     - 9\n\nTable 3. PSNR Score for ablation runs. For this experiment, we manually downsampled high-resolution versions of each sceneâ€™s input images to the established\nrendering resolution of our other experiments. Doing so reduces random artifacts (e.g., due to JPEG compression in the pre-downscaled Mip-NeRF360 inputs).\n\n\nthe difference in visual quality for our two configurations in Fig. 6.\nIn many cases, quality at 7K iterations is already quite good.\nThe training times vary over datasets and we report them separately. Note that image resolutions also vary over datasets. In the\nproject website, we provide all the renders of test views we used to\ncompute the statistics for all the methods (ours and previous work)\non all scenes. Note that we kept the native input resolution for all\nrenders.\nThe table shows that our fully converged model achieves quality that is on par and sometimes slightly better than the SOTA\nMip-NeRF360 method; note that on the same hardware, their average training time was 48 hours[2], compared to our 35-45min, and\ntheir rendering time is 10s/frame. We achieve comparable quality\nto InstantNGP and Plenoxels after 5-10m of training, but additional\ntraining time allows us to achieve SOTA quality which is not the\ncase for the other fast methods. For Tanks & Temples, we achieve\nsimilar quality as the basic InstantNGP at a similar training time\n( 7min in our case).\nâˆ¼\nWe also show visual results of this comparison for a left-out\ntest view for ours and the previous rendering methods selected\nfor comparison in Fig. 5; the results of our method are for 30K\niterations of training. We see that in some cases even Mip-NeRF360\nhas remaining artifacts that our method avoids (e.g., blurriness in\nvegetation â€“ in Bicycle, Stump â€“ or on the walls in Room). In the\nsupplemental video and web page we provide comparisons of paths\nfrom a distance. Our method tends to preserve visual detail of wellcovered regions even from far away, which is not always the case\nfor previous methods.\n\n_Synthetic Bounded Scenes. In addition to realistic scenes, we also_\nevaluate our approach on the synthetic Blender dataset [Mildenhall\net al. 2020]. The scenes in question provide an exhaustive set of\nviews, are limited in size, and provide exact camera parameters. In\nsuch scenarios, we can achieve state-of-the-art results even with\nrandom initialization: we start training from 100K uniformly random\nGaussians inside a volume that encloses the scene bounds. Our\napproach quickly and automatically prunes them to about 6â€“10K\nmeaningful Gaussians. The final size of the trained model after 30K\niterations reaches about 200â€“500K Gaussians per scene. We report\nand compare our achieved PSNR scores with previous methods in\nTable 2 using a white background for compatibility. Examples can\n\n2We trained Mip-NeRF360 on a 4-GPU A100 node for 12 hours, equivalent to 48 hours\non a single GPU. Note that A100â€™s are faster than A6000 GPUs.\n\n\nbe seen in Fig. 10 (second image from the left) and in supplemental\nmaterial. The trained synthetic scenes rendered at 180â€“300 FPS.\n\n_Compactness. In comparison to previous explicit scene representa-_\ntions, the anisotropic Gaussians used in our optimization are capable\nof modelling complex shapes with a lower number of parameters.\nWe showcase this by evaluating our approach against the highly\ncompact, point-based models obtained by [Zhang et al. 2022]. We\nstart from their initial point cloud which is obtained by space carving\nwith foreground masks and optimize until we break even with their\nreported PSNR scores. This usually happens within 2â€“4 minutes.\nWe surpass their reported metrics using approximately one-fourth\nof their point count, resulting in an average model size of 3.8 MB,\nas opposed to their 9 MB. We note that for this experiment, we only\nused two degrees of our spherical harmonics, similar to theirs.\n\n##### 7.3 Ablations\n\nWe isolated the different contributions and algorithmic choices\nwe made and constructed a set of experiments to measure their\neffect. Specifically we test the following aspects of our algorithm:\ninitialization from SfM, our densification strategies, anisotropic\ncovariance, the fact that we allow an unlimited number of splats\nto have gradients and use of spherical harmonics. The quantitative\neffect of each choice is summarized in Table 3.\n\n_Initialization from SfM. We also assess the importance of initializ-_\ning the 3D Gaussians from the SfM point cloud. For this ablation, we\nuniformly sample a cube with a size equal to three times the extent\nof the input cameraâ€™s bounding box. We observe that our method\nperforms relatively well, avoiding complete failure even without the\nSfM points. Instead, it degrades mainly in the background, see Fig. 7.\nAlso in areas not well covered from training views, the random\ninitialization method appears to have more floaters that cannot be\nremoved by optimization. On the other hand, the synthetic NeRF\ndataset does not have this behavior because it has no background\nand is well constrained by the input cameras (see discussion above).\n\n_Densification. We next evaluate our two densification methods,_\nmore specifically the clone and split strategy described in Sec. 5.\nWe disable each method separately and optimize using the rest of\nthe method unchanged. Results show that splitting big Gaussians\nis important to allow good reconstruction of the background as\nseen in Fig. 8, while cloning the small Gaussians instead of splitting\nthem allows for a better and faster convergence especially when\nthin structures appear in the scene.\n\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n\n\n-----\n\n10 - Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis\n\nRandom\n\nSfM\n\nFig. 7. Initialization with SfM points helps. Above: initialization with a\nrandom point cloud. Below: initialization using SfM points.\n\nNo Split-5k\n\nNo Clone-5k\n\nFull-5k\n\nFig. 8. Ablation of densification strategy for the two cases 'clone' and\n'split' (Sec. 5).\n\n_Unlimited depth complexity of splats with gradients. We evaluate_\nif skipping the gradient computation after the ğ‘ front-most points\n\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n\n\nFig. 9. If we limit the number of points that receive gradients, the effect on\nvisual quality is significant. Left: limit of 10 Gaussians that receive gradients.\nRight: our full method.\n\nwill give us speed without sacrificing quality, as suggested in Pulsar [Lassner and Zollhofer 2021]. In this test, we choose N=10, which\nis two times higher than the default value in Pulsar, but it led to\nunstable optimization because of the severe approximation in the\ngradient computation. For the Truck scene, quality degraded by\n11dB in PSNR (see Table 3, Limited-BW), and the visual outcome is\nshown in Fig. 9 for Garden.\n\n_Anisotropic Covariance. An important algorithmic choice in our_\nmethod is the optimization of the full covariance matrix for the 3D\nGaussians. To demonstrate the effect of this choice, we perform an\nablation where we remove anisotropy by optimizing a single scalar\nvalue that controls the radius of the 3D Gaussian on all three axes.\nThe results of this optimization are presented visually in Fig. 10.\nWe observe that the anisotropy significantly improves the quality\nof the 3D Gaussianâ€™s ability to align with surfaces, which in turn\nallows for much higher rendering quality while maintaining the\nsame number of points.\n\n_Spherical Harmonics. Finally, the use of spherical harmonics im-_\nproves our overall PSNR scores since they compensate for the viewdependent effects (Table 3).\n\n##### 7.4 Limitations\n\nOur method is not without limitations. In regions where the scene\nis not well observed we have artifacts; in such regions, other methods also struggle (e.g., Mip-NeRF360 in Fig. 11). Even though the\nanisotropic Gaussians have many advantages as described above,\nour method can create elongated artifacts or â€œsplotchyâ€ Gaussians\n(see Fig. 12); again previous methods also struggle in these cases.\nWe also occasionally have popping artifacts when our optimization creates large Gaussians; this tends to happen in regions with\nview-dependent appearance. One reason for these popping artifacts\nis the trivial rejection of Gaussians via a guard band in the rasterizer.\nA more principled culling approach would alleviate these artifacts.\nAnother factor is our simple visibility algorithm, which can lead to\nGaussians suddenly switching depth/blending order. This could be\naddressed by antialiasing, which we leave as future work. Also, we\ncurrently do not apply any regularization to our optimization; doing\nso would help with both the unseen region and popping artifacts.\nWhile we used the same hyperparameters for our full evaluation,\nearly experiments show that reducing the position learning rate can\nbe necessary to converge in very large scenes (e.g., urban datasets).\n\n\nNo Clone-5k\n\n\nSfM\n\n\nRandom\n\n\n-----\n\n3D Gaussian Splatting for Real-Time Radiance Field Rendering - 11\n\n\n### Ground Ground Ground\n Full Isotropic Full Isotropic Full Isotropic Truth Truth Truth\n\nFig. 10. We train scenes with Gaussian anisotropy disabled and enabled. The use of anisotropic volumetric splats enables modelling of fine structures and has\na significant impact on visual quality. Note that for illustrative purposes, we restricted Ficus to use no more than 5k Gaussians in both configurations.\n\n\n### Ground\n Truth\n\n\n### Full\n\n\n### Full\n\n\n### Ground\n Truth\n\n\n### Full\n\n\n### Full\n\n\nEven though we are very compact compared to previous pointbased approaches, our memory consumption is significantly higher\nthan NeRF-based solutions. During training of large scenes, peak\nGPU memory consumption can exceed 20 GB in our unoptimized\nprototype. However, this figure could be significantly reduced by a\ncareful low-level implementation of the optimization logic (similar\nto InstantNGP). Rendering the trained scene requires sufficient GPU\nmemory to store the full model (several hundred megabytes for\nlarge-scale scenes) and an additional 30â€“500 MB for the rasterizer,\ndepending on scene size and image resolution. We note that there\nare many opportunities to further reduce memory consumption\nof our method. Compression techniques for point clouds is a wellstudied field [De Queiroz and Chou 2016]; it would be interesting to\nsee how such approaches could be adapted to our representation.\n\nFig. 11. Comparison of failure artifacts: Mip-NeRF360 has â€œfloatersâ€ and\ngrainy appearance (left, foreground), while our method produces coarse,\nanisoptropic Gaussians resulting in low-detail visuals (right, background).\nTrain scene.\n\nFig. 12. In views that have little overlap with those seen during training,\nour method may produce artifacts (right). Again, Mip-NeRF360 also has\nartifacts in these cases (left). DrJohnson scene.\n\n\n### Isotropic\n\n\n### Ground\n Truth\n\n\n### Ground\n Truth\n\n\n##### 8 DISCUSSION AND CONCLUSIONS\n\nWe have presented the first approach that truly allows real-time,\nhigh-quality radiance field rendering, in a wide variety of scenes\nand capture styles, while requiring training times competitive with\nthe fastest previous methods.\nOur choice of a 3D Gaussian primitive preserves properties of\nvolumetric rendering for optimization while directly allowing fast\nsplat-based rasterization. Our work demonstrates that â€“ contrary to\nwidely accepted opinion â€“ a continuous representation is not strictly\nnecessary to allow fast and high-quality radiance field training.\nThe majority ( 80%) of our training time is spent in Python code,\nâˆ¼\nsince we built our solution in PyTorch to allow our method to be\neasily used by others. Only the rasterization routine is implemented\nas optimized CUDA kernels. We expect that porting the remaining\noptimization entirely to CUDA, as e.g., done in InstantNGP [MÃ¼ller\net al. 2022], could enable significant further speedup for applications\nwhere performance is essential.\nWe also demonstrated the importance of building on real-time\nrendering principles, exploiting the power of the GPU and speed of\nsoftware rasterization pipeline architecture. These design choices\nare the key to performance both for training and real-time rendering, providing a competitive edge in performance over previous\nvolumetric ray-marching.\nIt would be interesting to see if our Gaussians can be used to perform mesh reconstructions of the captured scene. Aside from practical implications given the widespread use of meshes, this would\nallow us to better understand where our method stands exactly in\nthe continuum between volumetric and surface representations.\nIn conclusion, we have presented the first real-time rendering\nsolution for radiance fields, with rendering quality that matches the\nbest expensive previous methods, with training times competitive\nwith the fastest existing solutions.\n\n##### ACKNOWLEDGMENTS\n\nThis research was funded by the ERC Advanced grant FUNGRAPH\n[No 788065 http://fungraph.inria.fr. The authors are grateful to Adobe](http://fungraph.inria.fr)\nfor generous donations, the OPAL infrastructure from UniversitÃ©\nCÃ´te dâ€™Azur and for the HPC resources from GENCIâ€“IDRIS (Grant\n2022-AD011013409). The authors thank the anonymous reviewers\nfor their valuable feedback, P. Hedman and A. Tewari for proofreading earlier drafts also T. MÃ¼ller, A. Yu and S. Fridovich-Keil for\nhelping with the comparisons.\n\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n\n\n### Isotropic\n\n\n### Isotropic\n\n\n-----\n\n12 - Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis\n\n##### REFERENCES\n\nKara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lempitsky. 2020. Neural Point-Based Graphics. In Computer Vision â€“ ECCV 2020: 16th\n_European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part XXII. 696â€“_\n712.\nJonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo MartinBrualla, and Pratul P Srinivasan. 2021. Mip-nerf: A multiscale representation for\nanti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International\n_Conference on Computer Vision. 5855â€“5864._\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman.\n2022. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. CVPR (2022).\nSebastien Bonopera, Jerome Esnault, Siddhant Prakash, Simon Rodriguez, Theo Thonat,\nMehdi Benadel, Gaurav Chaurasia, Julien Philip, and George Drettakis. 2020. sibr:\n[A System for Image Based Rendering. https://gitlab.inria.fr/sibr/sibr_core](https://gitlab.inria.fr/sibr/sibr_core)\nMario Botsch, Alexander Hornung, Matthias Zwicker, and Leif Kobbelt. 2005. HighQuality Surface Splatting on Todayâ€™s GPUs. In Proceedings of the Second Eurographics\n_/ IEEE VGTC Conference on Point-Based Graphics (New York, USA) (SPBGâ€™05). Euro-_\ngraphics Association, Goslar, DEU, 17â€“24.\nChris Buehler, Michael Bosse, Leonard McMillan, Steven Gortler, and Michael Cohen.\n2001. Unstructured lumigraph rendering. In Proc. SIGGRAPH.\nGaurav Chaurasia, Sylvain Duchene, Olga Sorkine-Hornung, and George Drettakis.\n2013. Depth synthesis and local warps for plausible image-based navigation. ACM\n_Transactions on Graphics (TOG) 32, 3 (2013), 1â€“12._\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. 2022b. TensoRF:\nTensorial Radiance Fields. In European Conference on Computer Vision (ECCV).\nZhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. 2022a.\nMobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field\nRendering on Mobile Architectures. arXiv preprint arXiv:2208.00277 (2022).\nRicardo L De Queiroz and Philip A Chou. 2016. Compression of 3D point clouds using\na region-adaptive hierarchical transform. IEEE Transactions on Image Processing 25,\n8 (2016), 3947â€“3956.\nMartin Eisemann, Bert De Decker, Marcus Magnor, Philippe Bekaert, Edilson De Aguiar,\nNaveed Ahmed, Christian Theobalt, and Anita Sellent. 2008. Floating textures. In\n_Computer graphics forum, Vol. 27. Wiley Online Library, 409â€“418._\nJohn Flynn, Ivan Neulander, James Philbin, and Noah Snavely. 2016. Deepstereo:\nLearning to predict new views from the worldâ€™s imagery. In CVPR.\nFridovich-Keil and Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo\nKanazawa. 2022. Plenoxels: Radiance Fields without Neural Networks. In CVPR.\nStephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien\nValentin. 2021. FastNeRF: High-Fidelity Neural Rendering at 200FPS. In Proceedings\n_of the IEEE/CVF International Conference on Computer Vision (ICCV). 14346â€“14355._\nMichael Goesele, Noah Snavely, Brian Curless, Hugues Hoppe, and Steven M Seitz.\n2007. Multi-view stereo for community photo collections. In ICCV.\nSteven J Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F Cohen. 1996. The\nlumigraph. In Proceedings of the 23rd annual conference on Computer graphics and\n_interactive techniques. 43â€“54._\nMarkus Gross and Hanspeter (Eds) Pfister. 2011. Point-based graphics. Elsevier.\nJeff P. Grossman and William J. Dally. 1998. Point Sample Rendering. In Rendering\n_Techniques._\nPeter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and\nGabriel Brostow. 2018. Deep blending for free-viewpoint image-based rendering.\n_ACM Trans. on Graphics (TOG) 37, 6 (2018)._\nPeter Hedman, Tobias Ritschel, George Drettakis, and Gabriel Brostow. 2016. Scalable\nInside-Out Image-Based Rendering. ACM Transactions on Graphics (SIGGRAPH\n_[Asia Conference Proceedings) 35, 6 (December 2016). http://www-sop.inria.fr/reves/](http://www-sop.inria.fr/reves/Basilic/2016/HRDB16)_\n[Basilic/2016/HRDB16](http://www-sop.inria.fr/reves/Basilic/2016/HRDB16)\nPeter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, and Paul\nDebevec. 2021. Baking Neural Radiance Fields for Real-Time View Synthesis. ICCV\n(2021).\nPhilipp Henzler, Niloy J Mitra, and Tobias Ritschel. 2019. Escaping platoâ€™s cave: 3d shape\nfrom adversarial rendering. In Proceedings of the IEEE/CVF International Conference\n_on Computer Vision. 9984â€“9993._\nArno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. 2017. Tanks and\ntemples: Benchmarking large-scale scene reconstruction. ACM Transactions on\n_Graphics (ToG) 36, 4 (2017), 1â€“13._\nGeorgios Kopanas, Thomas LeimkÃ¼hler, Gilles Rainer, ClÃ©ment Jambon, and George\nDrettakis. 2022. Neural Point Catacaustics for Novel-View Synthesis of Reflections.\n_ACM Transactions on Graphics (SIGGRAPH Asia Conference Proceedings) 41, 6 (2022),_\n[201. http://www-sop.inria.fr/reves/Basilic/2022/KLRJD22](http://www-sop.inria.fr/reves/Basilic/2022/KLRJD22)\nGeorgios Kopanas, Julien Philip, Thomas LeimkÃ¼hler, and George Drettakis. 2021. PointBased Neural Rendering with Per-View Optimization. Computer Graphics Forum 40,\n[4 (2021), 29â€“43. https://doi.org/10.1111/cgf.14339](https://doi.org/10.1111/cgf.14339)\nSamuli Laine and Tero Karras. 2011. High-performance software rasterization on GPUs.\nIn Proceedings of the ACM SIGGRAPH Symposium on High Performance Graphics.\n79â€“88.\n\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n\n\nChristoph Lassner and Michael Zollhofer. 2021. Pulsar: Efficient Sphere-Based Neural\nRendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n_Recognition (CVPR). 1440â€“1449._\nMarc Levoy and Pat Hanrahan. 1996. Light field rendering. In Proceedings of the 23rd\n_annual conference on Computer graphics and interactive techniques. 31â€“42._\nStephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh,\nand Jason Saragih. 2021. Mixture of volumetric primitives for efficient neural\nrendering. ACM Transactions on Graphics (TOG) 40, 4 (2021), 1â€“13.\nDuane G Merrill and Andrew S Grimshaw. 2010. Revisiting sorting for GPGPU stream\narchitectures. In Proceedings of the 19th international conference on Parallel architec_tures and compilation techniques. 545â€“546._\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields\nfor View Synthesis. In ECCV.\nThomas MÃ¼ller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant\nNeural Graphics Primitives with a Multiresolution Hash Encoding. ACM Trans.\n_Graph. 41, 4, Article 102 (July 2022), 15 pages._ [https://doi.org/10.1145/3528223.](https://doi.org/10.1145/3528223.3530127)\n[3530127](https://doi.org/10.1145/3528223.3530127)\nEric Penner and Li Zhang. 2017. Soft 3D reconstruction for view synthesis. ACM\n_Transactions on Graphics (TOG) 36, 6 (2017), 1â€“11._\nHanspeter Pfister, Matthias Zwicker, Jeroen van Baar, and Markus Gross. 2000. Surfels:\nSurface Elements as Rendering Primitives. In Proceedings of the 27th Annual Con_ference on Computer Graphics and Interactive Techniques (SIGGRAPH â€™00). ACM_\nPress/Addison-Wesley Publishing Co., USA, 335â€“342. [https://doi.org/10.1145/](https://doi.org/10.1145/344779.344936)\n[344779.344936](https://doi.org/10.1145/344779.344936)\nChristian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. 2021. KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs. In International\n_Conference on Computer Vision (ICCV)._\nLiu Ren, Hanspeter Pfister, and Matthias Zwicker. 2002. Object Space EWA Surface\nSplatting: A Hardware Accelerated Approach to High Quality Point Rendering.\n_Computer Graphics Forum 21 (2002)._\nHelge Rhodin, Nadia Robertini, Christian Richardt, Hans-Peter Seidel, and Christian\nTheobalt. 2015. A versatile scene model with differentiable visibility applied to\ngenerative pose estimation. In Proceedings of the IEEE International Conference on\n_Computer Vision. 765â€“773._\nGernot Riegler and Vladlen Koltun. 2020. Free view synthesis. In European Conference\n_on Computer Vision. Springer, 623â€“640._\nDarius RÃ¼ckert, Linus Franke, and Marc Stamminger. 2022. ADOP: Approximate\nDifferentiable One-Pixel Point Rendering. ACM Trans. Graph. 41, 4, Article 99 (jul\n[2022), 14 pages. https://doi.org/10.1145/3528223.3530122](https://doi.org/10.1145/3528223.3530122)\nMiguel Sainz and Renato Pajarola. 2004. Point-based rendering techniques. Computers\n_[and Graphics 28, 6 (2004), 869â€“879. https://doi.org/10.1016/j.cag.2004.08.014](https://doi.org/10.1016/j.cag.2004.08.014)_\nJohannes Lutz SchÃ¶nberger and Jan-Michael Frahm. 2016. Structure-from-Motion\nRevisited. In Conference on Computer Vision and Pattern Recognition (CVPR).\nMarkus SchÃ¼tz, Bernhard Kerbl, and Michael Wimmer. 2022. Software Rasterization of\n2 Billion Points in Real Time. Proc. ACM Comput. Graph. Interact. Tech. 5, 3, Article\n[24 (jul 2022), 17 pages. https://doi.org/10.1145/3543863](https://doi.org/10.1145/3543863)\nVincent Sitzmann, Justus Thies, Felix Heide, Matthias NieÃŸner, Gordon Wetzstein, and\nMichael Zollhofer. 2019. Deepvoxels: Learning persistent 3d feature embeddings. In\n_Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition._\n2437â€“2446.\nNoah Snavely, Steven M Seitz, and Richard Szeliski. 2006. Photo tourism: exploring\nphoto collections in 3D. In Proc. SIGGRAPH.\nCarsten Stoll, Nils Hasler, Juergen Gall, Hans-Peter Seidel, and Christian Theobalt. 2011.\nFast articulated motion tracking using a sums of gaussians body model. In 2011\n_International Conference on Computer Vision. IEEE, 951â€“958._\nCheng Sun, Min Sun, and Hwann-Tzong Chen. 2022. Direct Voxel Grid Optimization:\nSuper-fast Convergence for Radiance Fields Reconstruction. In CVPR.\nTowaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas MÃ¼ller, Morgan McGuire,\nAlec Jacobson, and Sanja Fidler. 2022. Variable bitrate neural fields. In ACM SIG_GRAPH 2022 Conference Proceedings. 1â€“9._\nTowaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek\nNowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. 2021. Neural\nGeometric Level of Detail: Real-time Rendering with Implicit 3D Shapes. (2021).\nAyush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, W Yifan,\nChristoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi,\net al. 2022. Advances in neural rendering. In Computer Graphics Forum, Vol. 41.\nWiley Online Library, 703â€“735.\nJustus Thies, Michael ZollhÃ¶fer, and Matthias NieÃŸner. 2019. Deferred neural rendering:\nImage synthesis using neural textures. ACM Transactions on Graphics (TOG) 38, 4\n(2019), 1â€“12.\nAngtian Wang, Peng Wang, Jian Sun, Adam Kortylewski, and Alan Yuille. 2023. VoGE: A\nDifferentiable Volume Renderer using Gaussian Ellipsoids for Analysis-by-Synthesis.\nIn The Eleventh International Conference on Learning Representations. [https://](https://openreview.net/forum?id=AdPJb9cud_Y)\n[openreview.net/forum?id=AdPJb9cud_Y](https://openreview.net/forum?id=AdPJb9cud_Y)\n\n\n-----\n\nOlivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. 2020. Synsin:\nEnd-to-end view synthesis from a single image. In Proceedings of the IEEE/CVF\n_Conference on Computer Vision and Pattern Recognition. 7467â€“7477._\nXiuchao Wu, Jiamin Xu, Zihan Zhu, Hujun Bao, Qixing Huang, James Tompkin, and\nWeiwei Xu. 2022. Scalable Neural Indoor Scene Rendering. ACM Transactions on\n_Graphics (TOG) (2022)._\nYiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan,\nFederico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. 2022.\nNeural fields in visual computing and beyond. In Computer Graphics Forum, Vol. 41.\nWiley Online Library, 641â€“676.\nQiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and\nUlrich Neumann. 2022. Point-nerf: Point-based neural radiance fields. In Proceedings\n_of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5438â€“5448._\nWang Yifan, Felice Serena, Shihao Wu, Cengiz Ã–ztireli, and Olga Sorkine-Hornung.\n2019. Differentiable surface splatting for point-based geometry processing. ACM\n_Transactions on Graphics (TOG) 38, 6 (2019), 1â€“14._\nAlex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. 2021.\nPlenOctrees for Real-time Rendering of Neural Radiance Fields. In ICCV.\nQiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and Felix Heide. 2022. Differentiable Point-Based Radiance Fields for Efficient View Synthesis. In SIGGRAPH\n_Asia 2022 Conference Papers (Daegu, Republic of Korea) (SA â€™22). Association for_\n[Computing Machinery, New York, NY, USA, Article 7, 12 pages. https://doi.org/10.](https://doi.org/10.1145/3550469.3555413)\n[1145/3550469.3555413](https://doi.org/10.1145/3550469.3555413)\nTinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A Efros.\n2016. View synthesis by appearance flow. In European conference on computer vision.\nSpringer, 286â€“301.\nMatthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. 2001a. EWA\nvolume splatting. In Proceedings Visualization, 2001. VISâ€™01. IEEE, 29â€“538.\nMatthias Zwicker, Hanspeter Pfister, Jeroen van Baar, and Markus Gross. 2001b. Surface\nSplatting. In Proceedings of the 28th Annual Conference on Computer Graphics and\n_Interactive Techniques (SIGGRAPH â€™01). Association for Computing Machinery, New_\n[York, NY, USA, 371â€“378. https://doi.org/10.1145/383259.383300](https://doi.org/10.1145/383259.383300)\n\n##### A DETAILS OF GRADIENT COMPUTATION\n\nRecall that Î£/Î£[â€²] are the world/view space covariance matrices of\nthe Gaussian, ğ‘ is the rotation, and ğ‘  the scaling, ğ‘Š is the viewing\ntransformation and ğ½ the Jacobian of the affine approximation of\nthe projective transformation. We can apply the chain rule to find\nthe derivatives w.r.t. scaling and rotation:\n\n_ğ‘‘Î£[â€²]_ _ğ‘‘Î£_\n\n(8)\n\n_ğ‘‘ğ‘ _ [=][ ğ‘‘]ğ‘‘[Î£]Î£[â€²] _ğ‘‘ğ‘ _\n\n\nand\n_ğ‘‘Î£[â€²]_ _ğ‘‘Î£_\n\n(9)\n\n_ğ‘‘ğ‘_ [=][ ğ‘‘]ğ‘‘[Î£]Î£[â€²] _ğ‘‘ğ‘_\n\nSimplifying Eq. 5 using ğ‘ˆ = ğ½ğ‘Š and Î£[â€²] being the (symmetric) upper\nleft 2 Ã— 2 matrix of _ğ‘ˆ_ Î£ğ‘ˆ _[ğ‘‡]_, denoting matrix elements with subscripts,\n\nwe can find the partial derivatives _[ğœ•][Î£][â€²]_ ï¿½ _ğ‘ˆ1,ğ‘–ğ‘ˆ1,ğ‘—_ _ğ‘ˆ1,ğ‘–ğ‘ˆ2,ğ‘—_ ï¿½.\n\n_ğœ•Î£ğ‘–ğ‘—_ [=] _ğ‘ˆ1,ğ‘—ğ‘ˆ2,ğ‘–_ _ğ‘ˆ2,ğ‘–ğ‘ˆ2,ğ‘—_\n\nNext, we seek the derivatives _[ğ‘‘]_ [Î£]\n\n_ğ‘‘ğ‘ _ [and][ ğ‘‘]ğ‘‘ğ‘[Î£] [. Since][ Î£][ =][ ğ‘…ğ‘†ğ‘†][ğ‘‡] _[ğ‘…][ğ‘‡]_ [,]\n\nwe can compute ğ‘€ = ğ‘…ğ‘† and rewrite Î£ = ğ‘€ğ‘€[ğ‘‡] . Thus, we can\nwrite _[ğ‘‘]_ [Î£] _ğ‘‘_ Î£ _ğ‘‘ğ‘€_ _ğ‘‘_ Î£ _ğ‘‘ğ‘€_\n\n_ğ‘‘ğ‘ _ [=] _ğ‘‘ğ‘€_ _ğ‘‘ğ‘ _ [and][ ğ‘‘]ğ‘‘ğ‘[Î£] [=] _ğ‘‘ğ‘€_ _ğ‘‘ğ‘_ [. Since the covariance ma-]\n\ntrix Î£ (and its gradient) is symmetric, the shared first part is compactly found byï¿½ _ğ‘…ğ‘–,ğ‘˜_ if j = kğ‘‘ğ‘€[ğ‘‘] [Î£] ï¿½[=]. To derive gradients for rotation, we recall the[ 2][ğ‘€][ğ‘‡] [. For scaling, we further have][ ğœ•ğ‘€]ğœ•ğ‘ ğ‘˜[ğ‘–,ğ‘—] =\n\n0 otherwise\n\nconversion from a unit quaternion ğ‘ with real part ğ‘ğ‘Ÿ and imaginary\nparts ğ‘ğ‘–,ğ‘ _ğ‘—_ _,ğ‘ğ‘˜_ to a rotation matrix ğ‘…:\n\n\nï¿½ï¿½ï¿½\nï¿½\n\n\n_ğ‘…(ğ‘) = 2_\nï¿½ï¿½ï¿½\nï¿½\n\n\n((21ğ‘ğ‘ğ‘–[âˆ’(]ğ‘–ğ‘ğ‘ğ‘˜ğ‘—[ğ‘]+âˆ’[2]ğ‘— ğ‘ğ‘[+]ğ‘Ÿğ‘Ÿ[ ğ‘]ğ‘ğ‘ğ‘˜ğ‘˜[2]ğ‘—)[)]) ((12ğ‘ğ‘ğ‘–[âˆ’(]ğ‘—ğ‘ğ‘ğ‘—ğ‘˜[ğ‘]âˆ’+ğ‘–[2] _ğ‘ğ‘[+]ğ‘Ÿğ‘Ÿ[ ğ‘]ğ‘ğ‘ğ‘˜ğ‘˜[2]ğ‘–_ )[)]) ((12ğ‘ğ‘ğ‘–[âˆ’(]ğ‘—ğ‘ğ‘ğ‘˜ğ‘˜[ğ‘]+âˆ’ğ‘–[2] ğ‘[+]ğ‘ğ‘Ÿğ‘Ÿ[ ğ‘]ğ‘ğ‘[2]ğ‘—ğ‘—ğ‘–[)]))\n\n\n(10)\n\n\n3D Gaussian Splatting for Real-Time Radiance Field Rendering        - 13\n\nAs a result, we find the following gradients for the components of ğ‘:\n\n_ğœ•ğ‘€_ ï¿½ 0 âˆ’ğ‘ ğ‘¦ğ‘ğ‘˜ _ğ‘ ğ‘§ğ‘_ _ğ‘—_ ï¿½ _ğœ•ğ‘€_ ï¿½ 0 _ğ‘ ğ‘¦ğ‘_ _ğ‘—_ _ğ‘ ğ‘§ğ‘ğ‘˜_ ï¿½\n= 2 _ğ‘ ğ‘¥_ _ğ‘ğ‘˜_ 0 âˆ’ğ‘ ğ‘§ğ‘ğ‘– _,_ = 2 _ğ‘ ğ‘¥_ _ğ‘_ _ğ‘—_ âˆ’2ğ‘ ğ‘¦ğ‘ğ‘– âˆ’ğ‘ ğ‘§ğ‘ğ‘Ÿ\n_ğœ•ğ‘ğ‘Ÿ_ âˆ’ğ‘ ğ‘¥ _ğ‘_ _ğ‘—_ _ğ‘ ğ‘¦ğ‘ğ‘–_ 0 _ğœ•ğ‘ğ‘–_ _ğ‘ ğ‘¥_ _ğ‘ğ‘˜_ _ğ‘ ğ‘¦ğ‘ğ‘Ÿ_ âˆ’2ğ‘ ğ‘§ğ‘ğ‘–\n\n_ğœ•ğ‘€_ ï¿½ âˆ’2ğ‘ ğ‘¥ _ğ‘_ _ğ‘—_ _ğ‘ ğ‘¦ğ‘ğ‘–_ _ğ‘ ğ‘§ğ‘ğ‘Ÿ_ ï¿½ _ğœ•ğ‘€_ ï¿½ âˆ’2ğ‘ ğ‘¥ _ğ‘ğ‘˜_ âˆ’ğ‘ ğ‘¦ğ‘ğ‘Ÿ _ğ‘ ğ‘§ğ‘ğ‘–_ ï¿½\n= 2 _ğ‘ ğ‘¥_ _ğ‘ğ‘–_ 0 _ğ‘ ğ‘§ğ‘ğ‘˜_ _,_ = 2 _ğ‘ ğ‘¥_ _ğ‘ğ‘Ÿ_ âˆ’2ğ‘ ğ‘¦ğ‘ğ‘˜ _ğ‘ ğ‘§ğ‘_ _ğ‘—_\n_ğœ•ğ‘_ _ğ‘—_ âˆ’ğ‘ ğ‘¥ _ğ‘ğ‘Ÿ_ _ğ‘ ğ‘¦ğ‘ğ‘˜_ âˆ’2ğ‘ ğ‘§ğ‘ _ğ‘—_ _ğœ•ğ‘ğ‘˜_ _ğ‘ ğ‘¥_ _ğ‘ğ‘–_ _ğ‘ ğ‘¦ğ‘_ _ğ‘—_ 0\n\n(11)\nDeriving gradients for quaternion normalization is straightforward.\n\n##### B OPTIMIZATION AND DENSIFICATION ALGORITHM\n\nOur optimization and densification algorithms are summarized in\nAlgorithm 1.\n\n**Algorithm 1 Optimization and Densification**\n_ğ‘¤, â„: width and height of the training images_\n\n_ğ‘€_ â† SfM Points _âŠ²_ Positions\n_ğ‘†,ğ¶,ğ´_ â† InitAttributes() _âŠ²_ Covariances, Colors, Opacities\n_ğ‘–_ â† 0 _âŠ²_ Iteration Count\n**while not converged do**\n_ğ‘‰,_ _ğ¼[Ë†]_ â† SampleTrainingView() _âŠ²_ Camera ğ‘‰ and Image\n_ğ¼_ â† Rasterize(ğ‘€, ğ‘†, ğ¶, ğ´, ğ‘‰ ) _âŠ²_ Alg. 2\n_ğ¿_ â† _ğ¿ğ‘œğ‘ ğ‘ _ (ğ¼, _ğ¼[Ë†])_ _âŠ²_ Loss\n_ğ‘€, ğ‘†, ğ¶, ğ´_ â† Adam(âˆ‡ğ¿) _âŠ²_ Backprop & Step\n**if IsRefinementIteration(ğ‘–) then**\n**for all Gaussians (ğœ‡, Î£,ğ‘, ğ›¼) in (ğ‘€,ğ‘†,ğ¶,ğ´) do**\n**if ğ›¼** _< ğœ–_ or IsTooLarge(ğœ‡, Î£) then _âŠ²_ Pruning\nRemoveGaussian()\n**end if**\n**if âˆ‡ğ‘ğ¿** _> ğœğ‘_ **then** _âŠ²_ Densification\n**if âˆ¥ğ‘†** âˆ¥ _> ğœğ‘†_ **then** _âŠ²_ Over-reconstruction\nSplitGaussian(ğœ‡, Î£,ğ‘, ğ›¼)\n**else** _âŠ²_ Under-reconstruction\nCloneGaussian(ğœ‡, Î£,ğ‘, ğ›¼)\n**end if**\n**end if**\n**end for**\n**end if**\n_ğ‘–_ â† _ğ‘–_ + 1\n**end while**\n\n##### C DETAILS OF THE RASTERIZER\n\n_Sorting. Our design is based on the assumption of a high load_\nof small splats, and we optimize for this by sorting splats once for\neach frame using radix sort at the beginning. We split the screen\ninto 16x16 pixel tiles (or bins). We create a list of splats per tile by\ninstantiating each splat in each 16 16 tile it overlaps. This results\nÃ—\nin a moderate increase in Gaussians to process which however is\namortized by simpler control flow and high parallelism of optimized\nGPU Radix sort [Merrill and Grimshaw 2010]. We assign a key for\neach splats instance with up to 64 bits where the lower 32 bits\nencode its projected depth and the higher bits encode the index of\nthe overlapped tile. The exact size of the index depends on how\nmany tiles fit the current resolution. Depth ordering is thus directly\nresolved for all splats in parallel with a single radix sort. After\n\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n\n\n-----\n\n14 - Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis\n\nsorting, we can efficiently produce per-tile lists of Gaussians to\nprocess by identifying the start and end of ranges in the sorted\narray with the same tile ID. This is done in parallel, launching\none thread per 64-bit array element to compare its higher 32 bits\nwith its two neighbors. Compared to [Lassner and Zollhofer 2021],\nour rasterization thus completely eliminates sequential primitive\nprocessing steps and produces more compact per-tile lists to traverse\nduring the forward pass. We show a high-level overview of the\nrasterization approach in Algorithm 2.\n\n**Algorithm 2 GPU software rasterization of 3D Gaussians**\n_ğ‘¤, â„: width and height of the image to rasterize_\n_ğ‘€, ğ‘†: Gaussian means and covariances in world space_\n_ğ¶, ğ´: Gaussian colors and opacities_\n_ğ‘‰_ : view configuration of current camera\n\n**function Rasterize(ğ‘¤, â„, ğ‘€, ğ‘†, ğ¶, ğ´, ğ‘‰** )\nCullGaussian(ğ‘, ğ‘‰ ) _âŠ²_ Frustum Culling\n_ğ‘€_ [â€²],ğ‘† [â€²] â† ScreenspaceGaussians(ğ‘€, ğ‘†, ğ‘‰ ) _âŠ²_ Transform\n_ğ‘‡_ â† CreateTiles(ğ‘¤, â„)\n_ğ¿, ğ¾_ â† DuplicateWithKeys(ğ‘€ [â€²], ğ‘‡ ) _âŠ²_ Indices and Keys\nSortByKeys(ğ¾, ğ¿) _âŠ²_ Globally Sort\n_ğ‘…_ â† IdentifyTileRanges(ğ‘‡, ğ¾)\n_ğ¼_ â† 0 _âŠ²_ Init Canvas\n**for all Tiles ğ‘¡** **in ğ¼** **do**\n**for all Pixels ğ‘–** **in ğ‘¡** **do**\n_ğ‘Ÿ_ â† GetTileRange(ğ‘…, ğ‘¡)\n_ğ¼_ [ğ‘–] â† BlendInOrder(ğ‘–, ğ¿, ğ‘Ÿ, ğ¾, ğ‘€ [â€²], ğ‘† [â€²], ğ¶, ğ´)\n**end for**\n**end for**\n**return ğ¼**\n**end function**\n\n_Numerical stability. During the backward pass, we reconstruct_\nthe intermediate opacity values needed for gradient computation by\nrepeatedly dividing the accumulated opacity from the forward pass\nby each Gaussianâ€™s ğ›¼. Implemented naÃ¯vely, this process is prone to\nnumerical instabilities (e.g., division by 0). To address this, both in\nthe forward and backward pass, we skip any blending updates with\n_ğ›¼_ _< ğœ–_ (we choose ğœ– as 2551 [) and also clamp][ ğ›¼] [with 0][.][99 from above.]\n\nFinally, before a Gaussian is included in the forward rasterization\npass, we compute the accumulated opacity if we were to include it\nand stop front-to-back blending before it can exceed 0.9999.\n\n##### D PER-SCENE ERROR METRICS\n\nTables 4â€“9 list the various collected error metrics for our evaluation\nover all considered techniques and real-world scenes. We list both\nthe copied Mip-NeRF360 numbers and those of our runs used to\ngenerate the images in the paper; averages for these over the full\nMip-NeRF360 dataset are PSNR 27.58, SSIM 0.790, and LPIPS 0.240.\n\nACM Trans. Graph., Vol. 42, No. 4, Article . Publication date: August 2023.\n\n\nTable 4. SSIM scores for Mip-NeRF360 scenes. â€  copied from original paper.\n\nbicycle flowers garden stump treehill room counter kitchen bonsai\n\nPlenoxels 0.496 0.431 0.6063 0.523 0.509 0.8417 0.759 0.648 0.814\nINGP-Base 0.491 0.450 0.649 0.574 0.518 0.855 0.798 0.818 0.890\nINGP-Big 0.512 0.486 0.701 0.594 0.542 0.871 0.817 0.858 0.906\nMip-NeRF360[â€ ] 0.685 0.583 0.813 0.744 0.632 0.913 0.894 0.920 **0.941**\nMip-NeRF360 0.685 0.584 0.809 0.745 0.631 0.910 0.892 0.917 0.938\nOurs-7k 0.675 0.525 0.836 0.728 0.598 0.884 0.873 0.900 0.910\nOurs-30k **0.771** **0.605** **0.868** **0.775** **0.638** **0.914** **0.905** **0.922** 0.938\n\nTable 5. PSNR scores for Mip-NeRF360 scenes. â€  copied from original paper.\n\nbicycle flowers garden stump treehill room counter kitchen bonsai\n\nPlenoxels 21.912 20.097 23.4947 20.661 22.248 27.594 23.624 23.420 24.669\nINGP-Base 22.193 20.348 24.599 23.626 22.364 29.269 26.439 28.548 30.337\nINGP-Big 22.171 20.652 25.069 23.466 22.373 29.690 26.691 29.479 30.685\nMip-NeRF360[â€ ] 24.37 **21.73** 26.98 26.40 22.87 **31.63** **29.55** **32.23** **33.46**\nMip-NeRF360 24.305 21.649 26.875 26.175 **22.929** 31.467 29.447 31.989 33.397\nOurs-7k 23.604 20.515 26.245 25.709 22.085 28.139 26.705 28.546 28.850\nOurs-30k **25.246** 21.520 **27.410** **26.550** 22.490 30.632 28.700 30.317 31.980\n\nTable 6. LPIPS scores for Mip-NeRF360 scenes. â€  copied from original paper.\n\nbicycle flowers garden stump treehill room counter kitchen bonsai\n\nPlenoxels 0.506 0.521 0.3864 0.503 0.540 0.4186 0.441 0.447 0.398\nINGP-Base 0.487 0.481 0.312 0.450 0.489 0.301 0.342 0.254 0.227\nINGP-Big 0.446 0.441 0.257 0.421 0.450 0.261 0.306 0.195 0.205\nMip-NeRF360[â€ ] 0.301 0.344 0.170 0.261 0.339 **0.211** **0.204** **0.127** **0.176**\nMip-NeRF360 0.305 0.346 0.171 0.265 0.347 0.213 0.207 0.128 0.179\nOurs-7k 0.318 0.417 0.153 0.287 0.404 0.272 0.254 0.161 0.244\nOurs-30k **0.205** **0.336** **0.103** **0.210** **0.317** 0.220 **0.204** 0.129 0.205\n\nTable 7. SSIM scores for Tanks&Temples and Deep Blending scenes.\n\nTruck Train Dr Johnson Playroom\n\nPlenoxels 0.774 0.663 0.787 0.802\nINGP-Base 0.779 0.666 0.839 0.754\nINGP-Big 0.800 0.689 0.854 0.779\nMip-NeRF360 0.857 0.660 **0.901** 0.900\nOurs-7k 0.840 0.694 0.853 0.896\nOurs-30k **0.879** **0.802** 0.899 **0.906**\n\nTable 8. PSNR scores for Tanks&Temples and Deep Blending scenes.\n\nTruck Train Dr Johnson Playroom\n\nPlenoxels 23.221 18.927 23.142 22.980\nINGP-Base 23.260 20.170 27.750 19.483\nINGP-Big 23.383 20.456 28.257 21.665\nMip-NeRF360 24.912 19.523 **29.140** 29.657\nOurs-7k 23.506 18.892 26.306 29.245\nOurs-30k **25.187** **21.097** 28.766 **30.044**\n\nTable 9. LPIPS scores for Tanks&Temples and Deep Blending scenes.\n\n\n-----\n\n"
}