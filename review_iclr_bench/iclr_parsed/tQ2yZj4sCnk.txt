# DIVERGENCE-REGULARIZED MULTI-AGENT ACTOR- CRITIC

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Entropy regularization is a popular method in reinforcement learning (RL). Although it has many advantages, it alters the RL objective and makes the converged
policy deviate from the optimal policy of the original Markov Decision Process.
Though divergence regularization has been proposed to settle this problem, it cannot be trivially applied to cooperative multi-agent reinforcement learning (MARL).
In this paper, we investigate divergence regularization in cooperative MARL and
propose a novel off-policy cooperative MARL framework, divergence-regularized
multi-agent actor-critic (DMAC). Mathematically, we derive the update rule of
DMAC which is naturally off-policy, guarantees a monotonic policy improvement
and is not biased by the regularization. DMAC is a flexible framework and can be
combined with many existing MARL algorithms. We evaluate DMAC in a didactic
stochastic game and StarCraft Multi-Agent Challenge and empirically show that
DMAC substantially improves the performance of existing MARL algorithms.

1 INTRODUCTION

Regularization is a common method for single-agent reinforcement learning (RL). The optimal
policy learned by traditional RL algorithm is always deterministic (Sutton and Barto, 2018). This
property may result in the inflexibility of the policy facing with unknown environments (Yang et al.,
2019). Entropy regularization is proposed to settle this problem by learning a policy according to the
maximum-entropy principle (Haarnoja et al., 2017). Moreover, entropy regularization is beneficial to
exploration and robustness for RL algorithms (Haarnoja et al., 2018). However, entropy regularization
is imperfect. Eysenbach and Levine (2019) pointed out maximum-entropy RL is a modification of the
original RL objective because of the entropy regularizer. Maximum-entropy RL is actually learning
an optimal policy for the entropy-regularized Markov Decision Process (MDP) rather than the original
MDP, i.e., the converged policy may be biased. Nachum et al. (2017) analysed a more general case for
regularization in RL and proposed what we call divergence regularization. Divergence regularization
can avoid the bias of the converged policy as well as be beneficial to exploration. Wang et al. (2019)
employed divergence regularizer and proposed a single-agent RL algorithm, DAPO, which prevents
the altering-objective drawback of entropy regularization.

Regularization can also be applied to cooperative multi-agent reinforcement learning (MARL)
(Agarwal et al., 2020; Zhang et al., 2021). However, most of cooperative MARL algorithms do not
use regularizer (Lowe et al., 2017; Foerster et al., 2018; Rashid et al., 2018; Son et al., 2019; Jiang
et al., 2020; Wang et al., 2021a). Only few cooperative MARL algorithms such as FOP (Zhang et al.,
2021) use entropy regularization, which may suffer from the drawback aforementioned. Divergence
regularization, on the other hand, could potentially benefit cooperative MARL. In addition to its
advantages mentioned above, divergence regularization can also help to control the step size of policy
update which is similar to conservative policy iteration (Kakade and Langford, 2002) in single-agent
RL. Conservative policy iteration and its successive methods such as TRPO (Schulman et al., 2015)
and PPO (Schulman et al., 2017) can stabilize policy improvement (Touati et al., 2020). These
methods use a surrogate objective for policy update, but decentralized policies in centralized training
with decentralized execution (CTDE) paradigm may not preserve the properties of the surrogate
objective. Moreover, DAPO (Wang et al., 2019) cannot be trivially extended to cooperative MARL
settings. Even with some tricks like V-trace (Espeholt et al., 2018) for off-policy correction, DAPO
is essentially an on-policy algorithm and thus may not be sample-efficient in cooperative MARL
settings.


-----

In the paper, we propose and analyze divergence policy iteration in general cooperative MARL
settings and a special case combined with value decomposition. Based on divergence policy iteration,
we derive the off-policy update rule for the critic, policy, and target policy and propose divergenceregularized multi-agent actor-critic (DMAC), a novel off-policy cooperative MARL framework. We
theoretically show that DMAC guarantees a monotonic policy improvement and is not biased by the
regularization. Besides, DMAC is beneficial to exploration and stable policy improvement by applying
our update rule of target policy. Moreover, DMAC is a flexible framework and can be combined with
many existing cooperative MARL algorithms to substantially improve their performance.

We empirically investigate DMAC in a didactic stochastic game and StarCraft Multi-Agent Challenge
(Samvelyan et al., 2019). We combine DMAC with five representative MARL methods, i.e., COMA
(Foerster et al., 2018) for on-policy multi-agent policy gradient, MAAC (Iqbal and Sha, 2019) for
off-policy multi-agent actor-critic, QMIX (Rashid et al., 2018) for value decomposition, DOP (Wang
et al., 2021b) for the combination of value decomposition and policy gradient, and FOP (Zhang
et al., 2021) for the combination of value decomposition and entropy regularization. Experimental
results show that DMAC indeed induces better performance, faster convergence, and better stability
in most tasks, which verifies the benefits of DMAC and demonstrates the advantages of divergence
regularization over entropy regularization in cooperative MARL.

2 RELATED WORK

**MARL. MARL has been a hot topic in the field of RL. In this paper, we focus on cooperative MARL.**
Cooperative MARL is usually modeled as Dec-POMDP (Oliehoek et al., 2016), where all agents share
a reward and aim to maximize the long-term return. Centralized training with decentralized execution
(CTDE) (Lowe et al., 2017) paradigm is widely used in cooperative MARL. CTDE usually utilizes a
centralized value function to address the non-stationarity for multi-agent settings and decentralized
policies for scalability. Many MARL algorithms adopt CTDE paradigm such as COMA, MAAC,
QMIX, DOP and FOP. COMA (Foerster et al., 2018) employs the counterfactual baseline which can
reduce the variance as well as settle the credit assignment problem. MAAC (Iqbal and Sha, 2019)
uses self-attention mechanism to integrate local observation and action of each agent and provides the
structured information for the centralized critic. Value decomposition (Sunehag et al., 2018; Rashid
et al., 2018; Son et al., 2019; Yang et al., 2020; Wang et al., 2021a;b; Zhang et al., 2021) is a popular
class of cooperative MARL algorithms. These methods express the global Q-function as a function
of individual Q-functions to satisfy Individual-Global-Max (IGM), which means the optimal actions
of individual Q-functions are corresponding to the optimal joint action of global Q-function. QMIX
(Rashid et al., 2018) is a representative of value decomposition methods. It uses a hypernet to ensure
the monotonicity of the global Q-function in terms of individual Q-functions, which is a sufficient
condition of IGM. DOP (Wang et al., 2021b) is a method that combines value decomposition with
policy gradient. DOP uses a linear value decomposition which is another sufficient condition of IGM
and the linear value decomposition helps the compute of policy gradient. FOP (Zhang et al., 2021) is
a method that combines value decomposition with entropy regularization and uses a more general
condition, Individual-Global-Optimal, to replace IGM. In this paper, we will combine DMAC with
these algorithms and show its improvement.

**Regularization. Entropy regularization was first proposed in single-agent RL. Nachum et al. (2017)**
analyzed the entropy-regularized MDP and revealed the properties about the optimal policy and
the corresponding Q-function and V-function. They also showed the equivalence of value-based
methods and policy-based methods in entropy-regularized MDP. Haarnoja et al. (2018) pointed
out maximum-entropy RL can achieve better exploration and stability facing with the model and
estimation error. Although entropy regularization has many advantages, Eysenbach and Levine (2019)
showed entropy regularization modifies the MDP and results in the bias of the convergent policy. Yang
et al. (2019) revealed the drawbacks of the convergent policy of general RL and maximum-entropy
RL. The former is usually a deterministic policy (Sutton and Barto, 2018) which is not flexible
enough for unknown situations, while the latter is a policy with non-zero probability for all actions
which may be dangerous in some scenarios. Neu et al. (2017) analyzed the entropy regularization
method from several views. They revealed a more general form of regularization which is actually
divergence regularization and showed entropy regularization is just a special case of divergence
regularization. Wang et al. (2019) absorbed previous result and proposed an on-policy algorithm, i.e.,
DAPO. However, DAPO cannot be trivially applied to MARL. Moreover, its on-policy learning is


-----

not sample-efficient for MARL settings and its off-policy correction trick V-trace (Espeholt et al.,
2018) is also intractable in MARL. There are some previous studies in single-agent RL which use
similar target policy to ours, but their purposes are quite different. Trust-PCL (Nachum et al., 2018)
introduces a target policy as a trust region constraint for maximum-entropy RL, but the policy is
still biased by entropy regularizer. MIRL (Grau-Moya et al., 2019) uses a distribution which is only
related to actions as the target policy to compute a mutual-information regularizer, but it still changes
the objective of the original RL.

3 PRELIMINARIES

**Dec-POMDP is a general model for cooperative MARL. A Dec-POMDP is a tuple M** =
_{S, A, P, Y, O, I, n, r, γ}. S is the state space, n is the number of agents, γ is the discount fac-_
tor, andspace where I = { A1,i 2 is the individual action space for agent · · · n} is the set of all agents. A = A1 × i A. P2 × · · · ×(s[′]|s, a) : A Sn represents the joint action × A × S → [0, 1] is the
transition function, and r(s, a) : S × A → R is the reward function of state s and joint action a. Y is
the observation space, and O(s, i) : S × _I →_ _Y is a mapping from state to observation for each agent._
The objective of Dec-POMDP is to maximize J(π) = Eπ [[P]t=0 _[γ][t][r][(][s][t][,][ a][t][)]][,][ and thus we need to]_

find the optimal joint policy π[∗] = arg maxπ J(π). To settle the partial observable problem, history
_τagenti ∈T ii has an individual policy = (Y × Ai)[∗]_ is often used to replace observation πi(ai _τi) and the joint policy o πi ∈ is the product of eachY . As for policies in CTDE, each πi. Though we_
_|_
calculate individual policy as πi(ai _τi) in practice, we will use πi(ai_ _s) in analysis and proofs for_
_|_ _|_
simplicity.

**Entropy regularization adds the logarithm of current policy to the reward function. It mod-**
ifies the optimization objective as Jent(π) = Eπ [[P]t=0 _[γ][t][ (][r][(][s][t][,][ a][t][)][ −]_ _[λ][ log][ π][(][a][t][|][s][t][))]][ .][ We]_

also have the corresponding Q-function Q[π]ent[(][s,][ a][) =][ r][(][s,][ a][) +][ γ][E][ [][V][ π]ent[(][s][′][)]][ and V-function]
_Vent[π]_ [(][s][) =][ E][ [][Q][π]ent[(][s,][ a][)][ −] _[λ][ log][ π][(][a][|][s][)]][. Given these definitions, we can deduce an interesting]_
property Vent[π] [(][s][) =][ E][ [][Q][π]ent[(][s,][ a][)] +][ λ][H][ (][π][(][·|][s][))][, where][ H][ (][π][(][·|][s][))][ represents the entropy of policy]
**_π(·|s). Vent[π]_** [(][s][)][ includes an entropy term which is the reason it is called][ entropy regularization][.]

4 METHOD

In this section, we first give the definition of divergence regularization. Then we propose and analyze
_divergence policy iteration. Finally, based on divergence policy iteration, we derive the update rules_
of the critic, policy, and target policy for divergence-regularized MARL.

4.1 DIVERGENCE REGULARIZATION

We maintain a target policy ρi for each agent i, which is different from the policy πi. Then we
have a joint target policy ρ = _i=1_ _[ρ][i][. This joint target policy][ ρ][ modifies the objective function]_

as Jρ(π) = Eπ _t=0_ _[γ][t][ ]r(st, at) −_ _λ log_ **_[π]ρ([(]a[a]t[t]|[|]s[s]t[t])[)]_** _. That is, a regularizer log_ **_[π]ρ([(]a[a]t[t]|[|]s[s]t[t])[)]_** [, which]

describes the discrepancyhP between policy[Q][n] **_π and target policyi_** **_ρ, is added to the reward function just_**
like entropy regularization.

Given ρ, we can define corresponding V-function and Q-function for divergence regularization as
follows,

_Vρ[π][(][s][) =][ E][π]_ _γ[t](r(st, at)_ _λ log_ **_[π][(][a][t][|][s][t][)]_** (1)

_t=0_ _−_ **_ρ(at_** _st) [)][|][s][0][ =][ s]#_

_|_

"X

_Q[π]ρ_ [(][s,][ a][) =][ r][(][s,][ a][) +][ γ][E]s[′] _P (_ _s,a)_ _Vρ[π][(][s][′][)]_ _._ (2)
_∼_ _·|_
 

Further, by simple deduction, we have


_Vρ[π][(][s][) =][ E]a_ **_π(_** _s)_ _Q[π]ρ_ [(][s,][ a][)][ −] _[λ][ log][ π][(][a][|][s][)]_ = Ea **_π(_** _s)_ _Q[π]ρ_ [(][s,][ a][)] _λDKL (π(_ _s)_ **_ρ(_** _s)) ._
_∼_ _·|_ **_ρ(a_** _s)_ _∼_ _·|_ _−_ _·|_ _∥_ _·|_

 _|_ 

 

_Vρ[π][(][s][)][ includes an extra term which is the KL divergence between][ π][ and][ ρ][, and thus this regularizer]_
is referred to as divergence regularization.


-----

4.2 DIVERGENCE POLICY ITERATION

From the perspective of policy evaluation, we can define an operator Γ[π]ρ [as]


Γ[π]ρ _[Q][(][s,][ a][) =][ r][(][s,][ a][) +][ γ][E]s[′]_ _P (_ _s,a),a[′]_ **_π(_** _s[′])_ _Q(s[′], a[′])_ _λ log_ **_[π][(][a][′][|][s][′][)]_**
_∼_ _·|_ _∼_ _·|_ _−_ **_ρ(a[′]_** _s[′])_

 _|_

and have the following lemma. Note that all the proofs are given in Appendix A.


(3)


**Lemma 1 (Divergence Policy Evaluation) For any initial Q-function Q[0](s, a) : S × A →** R, we
_define a sequence_ _Q[k]_ _given operator Γ[π]ρ_ _[as][ Q][k][+1][ = Γ]ρ[π][Q][k][. Then, the sequence will converge to]_
_Q[π]ρ_ _[as][ k][ →∞][.]_


After the evaluation of the policy, we need a method to improve the policy. We have the following
lemma about policy improvement.

**Lemma 2 (Divergence Policy Improvement) If we define πnew satisfying**

**_πnew(_** _s) = arg min_ (4)

_·|_ **_π_** _[D][KL][ (][π][(][·|][s][)][∥][u][(][·|][s][))][,]_

_where u(_ _s) = ρ(_ _s)_ exp(QZπρ[π]old[old]((s,s)·)/λ) _and Z_ **_[π][old](s) is a normalization term, then for all actions a_**

_·|_ _·|_

_and all states s we have Q[π]ρ_ [new] (s, a) _Q[π]ρ_ [old](s, a).
_≥_

Lemma 1 and 2 could be seen as corollaries of the conclusion of Haarnoja et al. (2018). Lemma 2
indicates that given a policy πold, if we find a policy πnew according to (4), then the policy πnew is
better than πold.

Lemma 2 does not make any assumption and is for general settings. Further, the policy improvement
can be established and simplified based on value decomposition. In the following, we give an example
for linear value decomposition like DOP (i.e., Q(s, a) = _i_ _[k][i][(][s][)][Q][i][(][s, a][i][) +][ b][(][s][)][) (][Wang et al.][,]_

2021b).

[P]

**Lemma 3 (Divergence Policy Improvement with Linear Value Decomposition) If Q-functions**
_satisfy Q[π]ρ_ [(][s,][ a][) =][ P]i _[k][i][(][s][)][Q]ρ[π][i]_ [(][s, a][i][) +][ b][(][s][)][ and we define][ π]new[i] _[satisfying]_

_πnew[i]_ [(][·|][s][) = arg min] _i_ _I,_
_πi_ _[D][KL][ (][π][i][(][·|][s][)][∥][u][i][(][·|][s][))]_ _∀_ _∈_

_where ui(_ _s) = ρi(_ _s)_ expki(s)Qπρold[i] (s,·)/λ _and Z_ _[π]old[i]_ (s) is a normalization term, then for all

_·|_ _·|_ _Z[πi]old_ (s)

_actions a and all states s we have Q[π]ρ_ [new] (s, a) _Q[π]ρ_ [old](s, a).
_≥_

Lemma 3 further tells us that if the MARL setting satisfies the linear value decomposition condition,
then each agent can optimize its individual policy with an objective of its own individual Q-function,
which immediately improves the joint policy. By combining divergence policy evaluation and
divergence policy improvement, we have the following theorem of divergence policy iteration.

**Theorem 1 (Divergence Policy Iteration) By iteratively using Divergence Policy Evaluation and**
_Divergence Policy Improvement, we will get a sequence_ _Q[k]_ _and this sequence will converge to the_
_optimal Q-function Q[∗]ρ_ _[and the corresponding policy sequence will converge to the optimal policy]_
**_πρ[∗][.]_** 

Theorem 1 shows that with repeated application of divergence policy improvement and divergence
policy evaluation, the policy can be monotonically improved and converge to the optimal policy. Let
**_πρ[∗][,][ V][ ∗]ρ_** [(][s][)][, and][ Q]ρ[∗] [(][s,][ a][)][ denote the optimal policy, Q-function, and V-function respectively, given a]
target policy ρ. We have the following proposition.

**Proposition 1 If πρ[∗]** [= arg max][π] _[J][ρ][(][π][)][, and][ V][ ∗]ρ_ [(][s][) =][ V]ρπρ[∗] [(][s][)][ and][ Q][∗]ρ[(][s,][ a][) =][ Q]πρ **_ρ[∗]_** [(][s,][ a][)][ are]
_respectively the corresponding Q-function and V-function of πρ[∗][, then they satisfy the following]_


-----

_properties:_


**_πρ[∗][(][a][|][s][)][ ∝]_** **_[ρ][(][a][|][s][) exp]_** _r(s, a) + γEs′∼P (·|s,a)_ _Vρ[∗][(][s][′][)]_ _/λ_ (5)

_Vρ[∗][(][s][) =][ λ][ log]_ **_ρ(a|  s) exp_** _r(s, a) + γEs′∼P (·|s,a)_ Vρ[∗][(][s][′][)] _/λ_ (6)

**_a_**

X      

_Q[∗]ρ[(][s,][ a][) =][ r][(][s,][ a][) +][ γλ][E]s[′]_ _P (_ _s,a)_ log **_ρ(a_** _s) exp_ _Q[∗]ρ[(][s][′][,][ a][′][)][/λ]_ _._ (7)
_∼_ _·|_ _|_

**_a[′]_**

h X    [i]


With all these results above, we have enough tools to obtain the practical update rule of the critic,
policy and target policy of DMAC.

4.3 DIVERGENCE-REGULARIZED CRITIC

From Proposition 1, we can obtain

**_πρ[∗][(][a][|][s][) =]_** **_ρ(a|s) exp_** _r(s, a) + γEs′∼P (·|s,a)_ _Vρ[∗][(][s][′][)]_ _/λ_ = **_[ρ][(][a][|][s][) exp]_** _Q[∗]ρ[(][s,][ a][)][/λ]_

**_b_** **_[ρ][(][b][|][s][) exp]  _** _r(s, b) + γEs′∼P (·|s,b)_ _Vρ[∗][(][s][′][)]_ _/λ_ exp _V ρ[∗][(][s][)][/λ]_ 

= ρP(a _s) exp_ _Q  [∗]ρ[(][s,][ a][)][ −]_ _[V][ ∗]ρ_ [(][s][)] _/λ_ _._       (8)
_|_

By rearranging the equation, we have

    

**_ρ[(][a][|][s][)]_**
_Vρ[∗][(][s][) =][ Q]ρ[∗]_ [(][s,][ a][)][ −] _[λ][ log][ π]ρ[∗](a_ _s)_ _[,]_ (9)

_|_

which is tenable for all actions a ∈ _A. Therefore, we have the following corollary._
**Corollary 1**


**_ρ[(][a][′][|][s][′][)]_**
_Q[∗]ρ[(][s][′][,][ a][′][)][ −]_ _[λ][ log][ π][∗]_

**_ρ(a[′]|s[′])_**


_Q[∗]ρ[(][s,][ a][) =][ r][(][s,][ a][) +][ γ][E]s[′]∼P (·|s,a),a[′]∼πρ[∗][(][·|][s][′][)]_

_is tenable for all actions a[′]_ _∈_ _A._


Corollary 1 gives an iterative formula for Q[∗]ρ[(][s,][ a][)][, with which we can design a loss function and]
update rule for learning the critic,

_Q = E_ (Qφ(s, a) _y)[2][i]_ _, where y = r(s, a) + γ_ _Q ˜φ_ [(][s][′][,][ a][′][)][ −] _[λ][ log][ π][ (][a][′][|][s][′][)]_ _,_ (10)
_L_ _−_ **_ρ (a[′]_** _s[′])_
h  _|_ 

where φ and _φ[˜] are respectively the weights of Q-function and target Q-function.The update of_
Q-function is similar to that in general MDP, except that the action for next state could be chosen
_arbitrarily while it must be the action that maximizes Q-function for next state in general MDP. This_
property greatly enhances the flexibility of learning Q-function, e.g., we can easily extend it to TD(λ).

4.4 DIVERGENCE-REGULARIZED ACTORS

DAPO (Wang et al., 2019) analyzes the divergence-regularized MDP from the perspective of policy
gradient theorem (Sutton et al., 2000) and gives an on-policy update rule for single-agent RL. Unlike
existing work, we focus on a different perspective and derive an off-policy update rule by taking into
consideration the characteristics of MARL.

From Lemma 2, we can obtain an optimization target for policy improvement,

_Q[π]ρ_ [(][s,][ ·][)][/λ]

arg min **_π(_** _s)_ **_ρ(_** _s)_ [exp] = arg max **_π(a_** _s)_ _Q[π]ρ_ [(][s,][ a][)][ −] _[λ][ log][ π][(][a][|][s][)]_
**_π_** _[D][KL]_ _·|_ _∥_ _·|_  Z **_[π](s)_**  ! **_π_** **_a_** _|_  **_ρ(a|s)_**

Then, we can define the objective of the actors, X

**_π = Es_** **_π(a_** _s)_ _Q[π]ρ_ [(][s,][ a][)][ −] _[λ][ log][ π][(][a][|][s][)]_ _,_ (11)
_L_ _∼D_ _|_ **_ρ(a_** _s)_

**_a_**  _|_ [#]

"X

where D is the replay buffer. Suppose each individual policy πi has a corresponding parameterization
_θi. We can obtain the following policy gradient for each agent with some derivation and the detail is_
given in Appendix A.6,

_θi_ **_π = Es_** _,a_ **_π_** _θi log πi(ai_ _s)_ _Q[π]ρ_ [(][s,][ a][)][ −] _[λ][ log][ π][(][a][|][s][)]_ _._ (12)
_∇_ _L_ _∼D_ _∼_ _∇_ _|_ **_ρ(a_** _s)_

  _|_ _[−]_ _[λ]_


-----

We need to point out that the key to off-policy update is that Lemma 2 does not limit the state
distribution. It only requires the condition is satisfied for each state. Therefore, we can maintain a
replay buffer to cover different states as much as possible, which is a common practice in off-policy
learning. DAPO uses a similar formula to ours, but it obtains the formula from policy gradient
theorem, which requires the state distribution of the current policy.

Further, we can add a counterfactual baseline to the gradient. First, we have the following equation
about the counterfactual baseline (Foerster et al., 2018),

Es _,a_ **_π [_** _θi log πi(ai_ _s)b(s, a_ _i)] = 0,_ (13)
_∼D_ _∼_ _∇_ _|_ _−_

where a _i denotes the joint action of all agents except agent i. Next, we take the baseline as_
_−_

_b(s, a_ _i) = Eai_ _πi_ [(Q[π]ρ [(][s,][ a][)][ −] _[λ][ log][ π][(][a][|][s][)]_ (14)
_−_ _∼_ **_ρ(a|s)_** _[−]_ _[λ][)]][.]_

Then, the gradient for each agent i can be modified as follows,

_θi_ **_π = E[_** _θi log πi(ai_ _s)(Q[π]ρ_ [(][s,][ a][)][ −] _[λ][ log][ π][(][a][|][s][)]_
_∇_ _L_ _∇_ _|_ **_ρ(a_** _s)_

_|_ _[−]_ _[λ][ −]_ _[b][(][s, a][−][i][))]]_

= E[ _θi log πi(ai_ _s)(Q[π]ρ_ [(][s,][ a][)][ −] _[λ][ log][ π][i][(][a][i][|][s][)]_ **_ρ_** [(][s,][ a][)] +][ λD][KL][(][π][i][(][·|][s][)][∥][ρ][i][(][·|][s][)))]][.]
_∇_ _|_ _ρi(ai_ _s)_

_|_ _[−]_ [E][a][i][∼][π][i] [[][Q][π]

In addition to variance reduction and credit assignment, this counterfactual baseline eliminates the
policies of other agents from the gradient. This property makes it convenient to calculate the gradient
and easy to select the target policy for each agent. Moreover, if the linear value decomposition
condition is satisfied, we have the following gradient formula,

_θi_ **_π = E[_** _θi log πi(ai_ _s)(ki(s)A[π]ρ[i]_ [(][s, a][i][)][ −] _[λ][ log][ π][i][(][a][i][|][s][)]_
_∇_ _L_ _∇_ _|_ _ρi(ai_ _s) [+][ λD][KL][(][π][i][(][·|][s][)][∥][ρ][i][(][·|][s][)))]][,][ (15)]_

_|_

where A[π]ρ[i] [(][s, a][i][) =][ Q]ρ[π][i] [(][s, a][i][)][ −] [E]a[˜]i _πi_ _Q[π]ρ[i]_ [(][s,][ ˜]ai) _._
_∼_
 

4.5 TARGET POLICY

We have discussed the update rule of the critic and actors, and now we focus on the selection and
update rule of the target policy. With the update rules above, we can obtain divergence policy iteration
given a fixed target policy ρ. Then we need to devise the update rule of ρ to prevent the bias of
regularization and benefit the learning procedure.

Intuitively, this regularizer log **_[π]ρ([(]a[a]t[t][|]s[s]t[t])[)]_** [could help to balance exploration and exploitation. For]

_|_
example, for some action a, if ρ(a|s) > π(a|s), then the regularizer is equivalent to adding a
positive value to the reward and vice versa. Therefore, if we choose previous policy as target
policy, the regularizer will encourage agents to take actions whose probability has decreased and
discourage agents to take actions whose probability has increased. Additionally, the regularizer
actually controls the discrepancy between current policy and previous policy, which could stabilize
the policy improvement (Kakade and Langford, 2002; Schulman et al., 2015; 2017).

To derive the update rule of target policy, we need to further analyze the regularizer theoretically. Let
_µπ denote the state-action distribution given a policy π. That is, µπ(s, a) = d[π](s)π(a|s), where_
_d[π](s) =_ _t=0_ _[γ][t][ Pr(][s][t][ =][ s][|][π][)][ is the stationary distribution of states given][ π][. With][ µ][π][, we can]_

rewrite the optimization objective Jρ(π) as follows,

[P]

_Jρ(π) =_ _µπ(s, a)r(s, a)_ _λ_ _µπ(s, a) log_ **_[π][(][a][|][s][)]_**

_−_ **_ρ(a_** _s)_
_s,a_ _s,a_

X X _|_ (16)

= _µπ(s, a)r(s, a) −_ _λDC (µπ∥µρ),_

_s,a_

X

where DC (µπ∥µρ) = _s,a_ _[µ][π][(][s,][ a][) log][ π]ρ([(]a[a]|[|]s[s])[)]_ [is a Bregman divergence (][Neu et al.][,][ 2017][). There-]

fore, the objective of the divergence regularized MDP can be represented as (17)

[P]

**_π[∗]_** = arg max _µπ(s, a)r(s, a)_ _λDC (µπ_ _µρ)_ (17)
**_π_** _−_ _∥_

_s,a_

X


-----

With this property, similar to Neu et al. (2017) and Wang et al. (2019), we can use the following
iterative process,
**_π[t][+1]_** = arg max _µπ(s, a)r(s, a)_ _λDC (µπ_ _µπt_ ) (18)
**_π_** _−_ _∥_

_s,a_

X

This iteration is a mirror descent process (Neu et al., 2017), so the convergence of the policy is
guaranteed. This process also guarantees that when the policy converges, DC (µπt+1 _∥µπt_ ) → 0; i.e.,
the regularizer will vanish. Moreover, we can obtain the following inequalities:

_J(π[t][+1]) ≥_ _J(π[t][+1]) −_ _λDC (µπt+1_ _∥µπt_ ) ≥ _J(π[t]) −_ _λDC (µπt_ _∥µπt_ ) = J(π[t]),

The first inequality is from DC (µπ∥µρ) ≥ 0 and the second inequality is from the definition of π[t][+1].
This conclusion means the policy sequence obtained by this iteration improves monotonically in the
original MDP. With these deductions, we actually obtain the following theorem.

**Theorem 2 By iteratively applying the divergence policy iteration and taking ρ[k]** _as π[k],the policy_
_sequence {π[k]} will converge and improve monotonically in the original MDP._

However, it is intractable to perform this update rule in practice because every iteration in (18) needs
a convergent policy. Thus, we propose an alternative approximate method. For each agent, we update
the policy πi and the target policy ρi as θi = θi + β∇θi _Lπ and_ _θ[˜]i = (1 −_ _τ_ )θ[˜]i + τθi, where β is
the learning rate, _θ[˜]i is the weights of ρi, and τ is the hyperparameter for soft update. Here we use_
one gradient step to replace the max operator in (18). From Theorem 1 and previous discussion, we
know that optimizing Lπ can maximize Jρ(π), so we use ∇θi _Lπ in the gradient step for off-policy_
training instead of the gradient step directly optimizing Jρ(π) in (16). Moreover, as the convergence
of (17) is guaranteed only if the target policy ρ is fixed, we softly update the target policy as the
moving average of the policy to prevent the instability caused by the large change of the target policy
and hence obtain stable policy improvement.

Now we have all the update rules of DMAC. The training of DMAC is a typical off-policy learning
process, which is given in Appendix B for completeness.

5 EXPERIMENTS

In this section, we first empirically study the benefits of DMAC and investigate how DMAC improves
the performance of existing MARL algorithms in a didactic stochastic game and five SMAC tasks.
Then, we demonstrate the advantages of divergence regularizer over entropy regularizer in cooperative
MARL.

5.1 IMPROVEMENTS OF EXISTING METHODS

DMAC is a flexible framework and can be combined with many existing MARL algorithms. In
the experiments, we choose four representative algorithms for different types of methods: COMA
(Foerster et al., 2018) for on-policy multi-agent policy gradients, MAAC (Iqbal and Sha, 2019) for
off-policy multi-agent actor-critic, QMIX (Rashid et al., 2018) for value decomposition, DOP (Wang
et al., 2021b) for the combination of value decomposition and policy gradient. These algorithms
need minor modifications to fit the framework of DMAC. We denote these modified algorithms as
COMA+DMAC, MAAC+DMAC, QMIX+DMAC, and DOP+DMAC. Generally, our modification
is limited and tries to keep the original architecture so as to fairly demonstrate the improvement
of DMAC. The details of the modifications are included in Appendix C.1. More details about
hyperparameters are available in Appendix C. All the curves in our plots correspond to the mean
value of five training runs with different random seeds, and shaded regions indicate 95% confidence
interval.

5.1.1 A DIDACTIC EXAMPLE

We first test the four groups of methods in a stochastic game where agents share the reward. The
stochastic game is generated randomly for the reward function and transition probabilities with
30 states, 3 agents and 5 actions for each agent. Each episode contains 30 steps in this game.
The performance of these methods is illustrated in Figure 1. We could find that DMAC performs


-----

Stochastic Game Stochastic Game Stochastic Game Stochastic Game

125 125 100

100 100 100 80

75 75 75 60

50 50 50 40

episode rewards 250 COMA+DMACCOMA 250 QMIX+DMACQMIX 250 MAAC+DMACMAAC 200 DOP+DMACDOP

0 20000 40000 60000 80000 100000 0 20000 40000 60000 80000 100000 0 20000 40000 60000 80000 100000 0 50000 100000 150000 200000

step step step step

Figure 1: The learning curves in terms of episode rewards of COMA, MAAC, QMIX and DOP
groups in the randomly generated stochastic game.

better than the baseline at the end of the training in all the four groups. Moreover, COMA+DMAC,
QMIX+DMAC and MAAC+DMAC learn faster than their baselines. Though DOP learns faster than
DOP+DMAC at the start, it falls into a sub-optimal policy and DOP+DMAC finds a better policy in
the end.


We also show the benefit of exploration in this stochastic game
for the convenience of statistics. We evaluate the exploration
in terms of the cover rate of all state-action pairs, i.e., the
ratio of the explored state-action pairs to all state-action pairs.
The cover rates of COMA and COMA+DMAC are illustrated
in Figure 2. We use COMA here as a representation of the
traditional policy gradient method in cooperative MARL. We
could find that the cover rate of COMA+DMAC is higher
than COMA, which can be an evidence for the benefit of
exploration of DMAC. The cover rates of other three groups
of algorithms are available in Appendix D.

5.1.2 SMAC


Stochastic Game

0.8

0.6

0.4

cover rate

0.2

COMA+DMAC

0.0 COMA

0 20000 40000 60000 80000 100000

step

Figure 2: The learning curves in
terms of cover rates of COMA and
COMA+DMAC in the randomly
generated stochastic game.


We test all the methods in five tasks of StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al.,
2019). The introduction of the SMAC environment and the training details are included in Appendix
C. The learning curves in terms of win rate of all the methods in the five SMAC tasks are illustrated
in Figure 3 (four columns for four groups of algorithms and five rows for five different maps in
SMAC). In addition, the learning curves in terms of rewards are available in Appendix D. We show
the empirical result of DAPO in the map of 3m in the first figure of the second column in Figure 3. It
can be seen that DAPO cannot obtain a good performance in the simple task of SMAC, so we skip
it in other SMAC tasks. The reason for the low performance of DAPO may be that DAPO omits
the correction foruses V-trace as off-policy correction which however is biased. These drawbacks may be magnified dπ(s)/dπt (s) in policy update which introduces bias in the gradient of policy, and






3m 3m 3m 3m

1.00 1.00 1.00 1.00

0.75 0.75 0.75 0.75

0.50 0.50 0.50 0.50

win_rates 0.25 0.25 COMA+DMAC DAPO 0.25 0.25

0.00 MAAC+DMAC MAAC 0.00 COMA 0.00 QMIX+DMAC QMIX 0.00 DOP+DMAC DOP

0.0 0.2 0.4 2s3z 0.6 0.8 1.01e6 0.0 0.2 0.4 2s3z 0.6 0.8 1.01e6 0.0 0.2 0.4 2s3z 0.6 0.8 1.01e6 0.0 0.2 0.4 2s3z 0.6 0.8 1.01e6

1.00 0.8 1.00 1.00

0.75 0.6 0.75 0.75

0.50 0.4 0.50 0.50

win_rates 0.25 0.2 0.25 0.25

0.00 0.0 0.00 0.00

0.0 0.2 0.4 3s5z 0.6 0.8 1.01e6 0.0 0.2 0.4 3s5z 0.6 0.8 1.01e6 1.00 0.0 0.2 0.4 3s5z 0.6 0.8 1.01e6 1.00 0.0 0.2 0.4 3s5z 0.6 0.8 1.01e6

0.4 0.06 0.75 0.75

win_rates 0.2 0.040.02 0.500.25 0.500.25

0.0 0.00 0.00 0.00

0.0 0.2 0.4 8m 0.6 0.8 1.01e6 0.0 0.2 0.4 8m 0.6 0.8 1.01e6 0.0 0.2 0.4 8m 0.6 0.8 1.01e6 0.0 0.2 0.4 8m 0.6 0.8 1e61.0

1.00 1.00 1.00 1.00

0.75 0.75 0.75 0.75

0.50 0.50 0.50 0.50

win_rates 0.25 0.25 0.25 0.25

0.00 0.00 0.00 0.00

1.00 0.0 0.2 0.4 1c3s5z 0.6 0.8 1.01e6 1.00 0.0 0.2 0.4 1c3s5z 0.6 0.8 1.01e6 1.00 0.0 0.2 0.4 1c3s5z 0.6 0.8 1.01e6 1.00 0.0 0.2 0.4 1c3s5z 0.6 0.8 1.01e6

0.75 0.75 0.75 0.75

0.50 0.50 0.50 0.50

win_rates 0.25 0.25 0.25 0.25

0.00 0.00 0.00 0.00

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

step 1e6 step 1e6 step 1e6 step 1e6

Figure 3: Learning curves in terms of win rates of COMA, MAAC, QMIX and DOP groups in five
SMAC maps (each row corresponds to a map and each column corresponds to a group).


-----

3s_vs_3z


2c_vs_64zg


MMM2


1.0

0.8

0.6

0.4

0.2

0.0


1.0

0.8

0.6

0.4

0.2

0.0


0.8

0.6

0.4

0.2

0.0

|Col1|FOP+DMAC FOP|
|---|---|

|Col1|FOP+DMAC FOP|
|---|---|

|Col1|FOP+DMAC FOP|
|---|---|


0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0

FOP+DMAC
FOP

FOP+DMAC
FOP

FOP+DMAC
FOP

step 1e6 step 1e6 step 1e6

Figure 4: The learning curves of win rates of FOP+DMAC and FOP in three SMAC maps.

in MARL settings. The superiority of our naturally off-policy method over the biased off-policy
correction method can be partly seen from the large performance gap between COMA+DMAC and
DAPO.

In all the five tasks, MAAC+DMAC outperforms MAAC significantly, but MAAC+DMAC does not
change the network architecture of MAAC, which shows the benefits of divergence regularizer. As
for the result of COMA and COMA+DMAC. We find that COMA+DMAC has higher win rates than
COMA in most cases at the end of the training, which can be attributed to the benefits of off-policy
training and exploration of divergence regularizer. Though in some cases COMA learns faster than
COMA+DMAC, it falls into sub-optimal in the end. This phenomenon can be observed more clearly
in the plots of episode rewards in Appendix D, especially in the hard tasks like 3s5z. This can be an
evidence for the advantage of divergence regularizer which helps the agents find a better policy.

The stable policy improvement of divergence regularizer can be showed in the variance of the learning
curves especially in the comparison between QMIX and QMIX+DMAC. In most tasks, we find
that QMIX+DMAC learns substantially faster than QMIX and gets higher win rates in harder tasks.
The results of DOP groups are illustrated in the fourth column of Figure 3. DOP+DMAC learns
faster than DOP in most cases and finally obtains a better performance. The difference of DOP and
DOP+DMAC can also partly show the advantage of naturally off-policy method to the off-policy
correction method, as DOP+DMAC replaces the tree backup loss with off-policy TD(λ).

DMAC improves the performance and/or convergence speed of the evaluated algorithms in most tasks.
This empirically demonstrates the benefits of divergence regularizer. Moreover, the superiority of our
naturally off-policy learning over the biased off-policy correction method can be partly witnessed
from the empirical results.


5.2 COMPARISON WITH ENTROPY REGULARIZATION

FOP (Zhang et al., 2021) combines the value decomposition with entropy regularization, which
obtained the state-of-the-art performance in SMAC tasks. FOP has a well tuned scheme for the
temperature parameter of the entropy, so we take FOP as a strong baseline for entropy-regularized
methods in cooperative MARL. We compare FOP and FOP+DMAC in three SMAC tasks, 3s_vs_3z,
2c_vs_64zg, and MMM2, which respectively correspond to the three levels of difficulties (i.e.,
easy, hard, and super hard) for SMAC tasks. These tasks are taken from the original paper of FOP.
The modifications of FOP+DMAC are also included in Appendix C.1. The win rates of FOP and
FOP+DMAC are illustrated in Figure 4. We could find that FOP+DMAC learns much faster than
FOP in 3s_vs_3z, while it performs better than FOP in other two harder tasks. These results could be
an evidence for the advantages of DMAC and the bias of entropy regularization.


6 CONCLUSION

We propose a multi-agent actor-critic framework, DMAC, for cooperative MARL. We investigate
divergence regularization, derive divergence policy iteration, and deduce the update rules for the
critic, policy, and target policy in multi-agent settings. DMAC is a naturally off-policy framework and
the divergence regularizer is beneficial to exploration and stable policy improvement. Unlike entropy
regularizer, the divergence regularizer will not bias the converged policy. DMAC is also a flexible
framework and can be combined with many existing MARL algorithms with limited modification. It
is empirically demonstrated that combining DMAC with existing MARL algorithms can improve the
performance and convergence speed in a stochastic game and SMAC tasks.


-----

REFERENCES

Akshat Agarwal, Sumit Kumar, Katia Sycara, and Michael Lewis. Learning transferable cooperative
behavior in multi-agent team. In International Conference on Autonomous Agents and Multiagent
_Systems (AAMAS), 2020._

Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam
Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with
importance weighted actor-learner architectures. In International Conference on Machine Learning
_(ICML), 2018._

Benjamin Eysenbach and Sergey Levine. If maxent rl is the answer, what is the question? arXiv
_preprint arXiv:1910.01913, 2019._

Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In AAAI Conference on Artificial Intelligence (AAAI),
2018.

Jordi Grau-Moya, Felix Leibfried, and Peter Vrancx. Soft q-learning with mutual-information
regularization. In International Conference on Learning Representations (ICLR), 2019.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In International Conference on Machine Learning (ICML), 2017.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Conference
_on Machine Learning (ICML), 2018._

Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In Interna_tional Conference on Machine Learning (ICML), 2019._

Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. Graph convolutional reinforcement
learning. In International Conference on Learning Representations (ICLR), 2020.

Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
_International Conference on Machine Learning (ICML), 2002._

Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
_Processing Systems (NeurIPS), 2017._

Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. In Advances in Neural Information Processing
_Systems (NeurIPS), 2017._

Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Trust-pcl: An off-policy trust
region method for continuous control. In International Conference on Learning Representations
_(ICLR), 2018._

Gergely Neu, Anders Jonsson, and Vicenç Gómez. A unified view of entropy-regularized markov
decision processes. arXiv preprint arXiv:1705.07798, 2017.

Frans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs,
volume 1. Springer, 2016.

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: monotonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on Machine Learning (ICML), 2018.

Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning (ICML), 2015.


-----

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to
factorize with transformation for cooperative multi-agent reinforcement learning. In International
_Conference on Machine Learning (ICML), 2019._

Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinícius Flores Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Valuedecomposition networks for cooperative multi-agent learning based on team reward. In Interna_tional Conference on Autonomous Agents and MultiAgent Systems (AAMAS), 2018._

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information
_Processing Systems (NeurIPS), 2000._

Ahmed Touati, Amy Zhang, Joelle Pineau, and Pascal Vincent. Stable policy optimization via
off-policy divergence regularization. arXiv preprint arXiv:2003.04108, 2020.

Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agent q-learning. In International Conference on Learning Representations (ICLR), 2021a.

Qing Wang, Yingru Li, Jiechao Xiong, and Tong Zhang. Divergence-augmented policy optimization.
In Advances in Neural Information Processing Systems (NeurIPS), 2019.

Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Dop: Off-policy multiagent decomposed policy gradients. In International Conference on Learning Representations
_(ICLR), 2021b._

Wenhao Yang, Xiang Li, and Zhihua Zhang. A regularized approach to sparse optimal policy in
reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019.

Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang.
Qatten: A general framework for cooperative multiagent reinforcement learning. arXiv preprint
_arXiv:2002.03939, 2020._

Tianhao Zhang, Yueheng Li, Chen Wang, Guangming Xie, and Zongqing Lu. Fop: Factorizing
optimal joint policy of maximum-entropy multi-agent reinforcement learning. In International
_Conference on Machine Learning, pages 12491–12500. PMLR, 2021._


-----

A PROOFS

A.1 LEMMA 1

**_Proof._** We define a new reward function

_rρ[π][(][s,][ a][) =][ r][(][s,][ a][)][ −]_ _[λ][E]s[′]_ _P (_ _s,a)_ [[][D][KL] [(][π][(][·|][s][′][)][∥][ρ][(][·|][s][′][))]][,]
_∼_ _·|_

then we can rewrite the definition of operator Γ[π]ρ [as]


Γ[π]ρ _[Q][(][s,][ a][) =][ r]ρ[π][(][s,][ a][) +][ γ][E]s[′]_ _P (_ _s,a),a[′]_ **_π(_** _s[′])_ [[][Q][(][s][′][,][ a][′][)]][ .]
_∼_ _·|_ _∼_ _·|_

With this formula, we can apply the traditional convergence result of policy evaluation in Sutton and
Barto (2018). □

A.2 LEMMA 2

For the proof of Lemma 2, we need the following lemma (Haarnoja et al., 2018) about improving
policy in entropy-regularized MDP.

**Lemma 4 If we have a new policy πnew and**


ent [(][s,][ ·][)][/λ][)]
**_π(_** _s)_

_·|_ _∥_ [exp (][Q]Z[π][π][old][old](s)


**_πnew = arg min_**
**_π_** _[D][KL]_


_where Z_ **_[π][old](s) represents the normalization term, then we have_**

_Q[π]ent[new]_ [(][s, a][)][ ≥] _[Q]ent[π][old][(][s, a][)][,]_ _∀s ∈_ _S, a ∈_ _A._

With Lemma 4, we have the following proof of Lemma 2.

**_Proof._** Let _Q[ˆ] be the same as the definition in Proof A.5. Then we have_ _Q[ˆ][π](s, a) = Q[π]ρ_ [(][s,][ a][) +]
_λ log ρ(a|s), ∀π._

According to Lemma 4,


exp _Qˆ[π][old](s, ·)/λ_

**_πˆ_** new(s, ) = arg min **_π(_** _s)_
_·_ **_π_** _[D][KL]_  _·|_ _∥_ Z **_[π][old](s)_**

_Qˆπ[ˆ]_ new (s, a) ≥ _Qˆ[π][old](s, a),_ _∀a ∈_ _A._

With the definition, we have


_Q[π][old](s,_ )/λ)
**_π(_** _s)_ _·_

_·|_ _∥_ [exp( ˆ]Z **_[π][old](s)_**


**_ρ_** (s, )/λ)
**_π(_** _s)_ **_ρ(_** _s)_ [exp(][Q][π][old] _·_

_·|_ _∥_ _·|_ _Z_ **_[π][old](s)_**


_DKL_


= DKL


**_πnew = ˆπnew_**

_Q[π]ρ_ [new] (s, a) = Q[ˆ][π][new] (s, a) _λ log ρ(a_ _s)_ _Q[π][old](s, a)_ _λ log ρ(a_ _s) = Q[π]ρ_ [old](s, a).
_−_ _|_ _≥_ [ˆ] _−_ _|_

A.3 LEMMA 3

**_Proof._** From the equation


_πi(ai_ _s)_ _ki(s)Q[π]ρ_ old[i] (s, ai) _λ log_ _[π][i][(][a][i][|][s][)]_
_|_ _−_ _ρi(ai_ _s)_
_ai_  _|_

X


_πnew[i]_ [= arg max]πi


we can obtain


_πnew[i]_ [(][a][i][|][s][)] _ki(s)Q[π]ρ_ old[i] (s, ai) _λ log_ _[π]new[i]_ [(][a][i][|][s][)]
_−_ _ρi(ai_ _s)_
_ai_  _|_ 

X

old old[(][a][i][|][s][)]
_πold[i]_ [(][a][i][|][s][)] _ki(s)Q[π]ρ_ _[i]_ (s, ai) _λ log_ _[π][i]_

_≥_ _−_ _ρi(ai_ _s)_

_ai_  _|_

X


(19)


-----

By taking expectation on the both side of (19), we can obtain the followings.


_πnew[i]_ [(][a][i][|][s][)] _ki(s)Q[π]ρ_ old[i] (s, ai) _λ log_ _[π]new[i]_ [(][a][i][|][s][)]
_−_ _ρi(ai_ _s)_
_ai_  _|_

X


_π˜−i(a−i|s)_

Xa−i


old old[(][a][i][|][s][)]
_πold[i]_ [(][a][i][|][s][)] _ki(s)Q[π]ρ_ _[i]_ (s, ai) _λ log_ _[π][i]_
_−_ _ρi(ai_ _s)_
_ai_  _|_

X


(20)

(21)

(22)


_≥_ _π˜−i(a−i|s)_
Xa−i

_∀i ∈_ _I_ _∀π˜−i_


Moreover, we can easily have the following derivation,

_πnew[i]_ [(][a][i][|][s][)] _ki(s)Q[π]ρ_ old[i] (s, ai) + b(s) _λ log_ _[π]new[i]_ [(][a][i][|][s][)]
_−_ _ρi(ai_ _s)_
_ai_  _|_ 

X

= _u1(a_ _i_ _s)_ _πnew[i]_ [(][a][i][|][s][)] _ki(s)Q[π]ρ_ old[i] (s, ai) + b(s) _λ log_ _[π]new[i]_ [(][a][i][|][s][)]

_−_ _|_ _−_ _ρi(ai_ _s)_

Xa−i Xai  _|_

= _u2(a_ _i_ _s)_ _πnew[i]_ [(][a][i][|][s][)] _ki(s)Q[π]ρ_ old[i] (s, ai) + b(s) _λ log_ _[π]new[i]_ [(][a][i][|][s][)]

_−_ _|_ _−_ _ρi(ai_ _s)_

Xa−i Xai  _|_

_u1, u2._
_∀_

Then we have


Ea∼πnew _Q[π]ρ_ [old](s, a) − log **_[π][new]ρ(a[(][a]s)[|][s][)]_**

 _|_ 

= **_πnew(a_** _s)_ _ki(s)Q[π]ρ_ old[i] (s, ai) + b(s) _λ log_ _[π]new[i]_ [(][a][i][|][s][)]

_|_ _−_ _ρi(ai_ _s)_
**_a_** _i_  _|_ 

X X

= **_πnew(a_** _s)_ _ki(s)Q[π]ρ_ old[i] (s, ai) + b(s) _λ log_ _[π]new[i]_ [(][a][i][|][s][)]

_|_ _−_ _ρi(ai_ _s)_
**_a_** _i_  _|_ 

X X

= _πnew[−][i]_ [(][a][−][i][|][s][)] _πnew[i]_ [(][a][i][|][s][)] _ki(s)Q[π]ρ_ old[i] (s, ai) + b(s) _λ log_ _[π]new[i]_ [(][a][i][|][s][)]

_−_ _ρi(ai_ _s)_

Xi Xa−i Xai  _|_

= _πold[−][i]_ [(][a][−][i][|][s][)] _πnew[i]_ [(][a][i][|][s][)] _ki(s)Q[π]ρ_ old[i] (s, ai) + b(s) _λ log_ _[π]new[i]_ [(][a][i][|][s][)]

_−_ _ρi(ai_ _s)_

Xi Xa−i Xai  _|_

old old[(][a][i][|][s][)]

_πold[−][i]_ [(][a][−][i][|][s][)] _πold[i]_ [(][a][i][|][s][)] _ki(s)Q[π]ρ_ _[i]_ (s, ai) + b(s) _λ log_ _[π][i]_

_≥_ _−_ _ρi(ai_ _s)_
Xi Xa−i Xai  _|_ 

= Vρ[π][old](s)


The fourth equation is from (21) and the fifth inequality is from (20).

By repeatedly applying (22) and the relation Q[π]ρ [old](s, a) = r(s, a) + γEs′ _Vρ[π][old](s[′])_, we can
complete the proof as followings.

[] 

_Qρ[π][old](s, a) = r(s, a) + γEs′_ _Vρ[π][old](s[′])_


_≤_ _r(s, a) + γEs′_ [] Ea′∼πnew _Q[π]ρ_ [old](s[′],, a[′]) − log **_[π][new]ρ(a[(][′][a]s[′][|][′][s])[′][)]_**
  _|_ 

= r(s, a) + γEs[′] Ea[′]∼πnew _r(s[′], a[′]) + γEs[′′]_ _Vρ[π][old](s[′′])_ _−_ log **_[π][new]ρ(a[(][′][a]s[′][′][|])[s][′][)]_**
  _|_ 

_· · ·_ [] 
_Q[π]ρ_ [new] (s, a)
_≤_


-----

A.4 THEOREM 1

**_Proof._** First, we will show that Divergence Policy Iteration will monotonically improve the policy.
From Lemma 2, we know that


_Q[π]ρ_ [new] (s, a) − _λ log_ **_[π][new]ρ(a[(][a]s)[|][s][)]_**
 _|_

_Q[π]ρ_ [old](s, a) − _λ log_ **_[π][new]ρ(a[(][a]s)[|][s][)]_**
 _|_

_Q[π]ρ_ [old](s, a) − _λ log_ **_[π]ρ[old](a[(][a]s[|])[s][)]_**

_|_ 


_Vρ[π][new]_ (s) = Ea∼πnew(·|s)

Ea **_πnew(_** _s)_
_≥_ _∼_ _·|_

Ea **_πold(_** _s)_
_≥_ _∼_ _·|_

= Vρ[π][old](s).


The first inequality is from the conclusion of Lemma 2 that

_Q[π]ρ_ [new] (s, a) _Q[π]ρ_ [old](s, a) **_a_** _A,_
_≥_ _∀_ _∈_

and the second inequality is from the definition of πnew that


_Q[π]ρ_ [old](s, )/λ
**_π(_** _s)_ _·_

_·|_ _∥_ [exp] _Z_ **_[π][old](s)_**
 


**_πnew = arg min_**
**_π_** _[D][KL]_


Here we have V **_[π][new]_** (s) _V_ **_[π][old](s),_** _s_ _S, and thus Jρ(πnew)_ _Jρ(πold). So, Divergence_
_≥_ _∀_ _∈_ _≥_
Policy Iteration will monotonically improve the policy.

Since the Q[π]ρ [is bounded above (the reward function is bounded), the sequence of Q-function] _Q[k]_

of Divergence Policy Iteration will converge and the corresponding policy sequence will also converge
to some policy πconv. We need to show πconv = πρ[∗][.] 


_s)_ _Q[π]ρ_ [conv] (s, a) − _λ log_ **_[π][conv]ρ(a[(][a]s)[|][s][)]_**

 _|_ 

_Q[π]ρ_ [conv] (s, a) − _λ log_ **_[π]ρ([(]a[a][|]s[s])[)]_**

_|_ 

_r(s, a) + γEa′∼π(·|s′)_ _Q[π]ρ_ [conv] (s[′], a[′]) − _λ log_ **_[π]ρ([(]a[a][′][′][|]s[s][′][′])[)]_**

 _|_


_Vρ[π][conv]_ (s) = Ea∼πconv(·|s)


Ea **_π(_** _s)_
_≥_ _∼_ _·|_

Ea **_π(_** _s)_
_≥_ _∼_ _·|_

_· · ·_
_≥_ _Vρ[π][(][s][)][.]_


_λ log_ **_[π][(][a][|][s][)]_**
_−_ **_ρ(a_** _s)_

_|_


The first inequality is from the definition of πconv that


_Q[π]ρ_ [conv] (s, )/λ
**_π(_** _s)_ _·_

_·|_ _∥_ [exp] _Z_ **_[π][conv]_** (s)
 


**_πconv = arg min_**
**_π_** _[D][KL]_


and all the other inequalities are just iteratively using the first inequality and the relation of Q[π]ρ [and]
_Vρ[π][. With iterations, we replace all the][ π][conv]_ [with][ π][ in the expression of][ V][ π]ρ [conv] (s) and finally we
get Vρ[π][(][s][)][. Therefore, we have]

_Vρ[π][conv]_ (s) ≥ _Vρ[π][(][s][)]_ _∀s ∈_ _S_ _∀π ∈_ Π

_Jρ(πconv)_ _Jρ(π)_ **_π_** Π
_≥_ _∀_ _∈_
**_πconv = πρ[∗][.]_**


A.5 PROPOSITION 1

Before the proof of Proposition 1, we need some results about the optimal Q-function Q[∗]ent[, the]
optimal V-function Vent[∗] [, and the optimal policy][ π]ent[∗] [in entropy-regularized MDP. We have the]
following lemma (Nachum et al., 2017).


-----

**Lemma 5**


**_πent[∗]_** [(][s, a][)][ ∝] [exp] _r(s, a) + γEs′∼P (·|s,a) [Vent[∗]_ [(][s][′][)]] _/λ_

_Vent[∗]_ [(][s][) =][ λ][ log]    exp _r(s, a) + γEs′∼P (·|s,a) [Vent[∗]_ [(][s][′][)]] _/λ_

_a_

X    


_Q[∗]ent[(][s, a][) =][ r][(][s, a][) +][ γλ][E]s[′]_ _P (_ _s,a)_
_∼_ _·|_


exp (Q[∗]ent[(][s][′][, a][′][)][/λ][)]
_a[′]_

X


log


With Lemma 5, we can complete the proof of Proposition 1.

**_Proof._** Let ˆr(s, a) = r(s, a) + λ log ρ(a|s), we consider the objective function

_Jˆ(π) = Eπ_ _t=0_ _γ[t]_ (ˆr(st, at) − _λ log π(at|st))#_ _._

"X


Let ˆπ[∗](a|s),V[ˆ] _[∗](s) and_ _Q[ˆ][∗](s, a) be the corresponding optimal policy, V-function and Q-function of_
_Jˆ(π). By definition we can obtain_

**_πˆ_** _[∗](a|s) = πρ[∗][(][a][|][s][)]_

_Vˆ_ _[∗](s) = E a∼πˆ_ _[∗](·|s),s[′]∼P (·|s,a)_ _rˆ(s, a) + γV[ˆ]_ _[∗](s[′]) −_ _λ log ˆπ[∗](a|s)_
h i

**_ρ[(][a][|][s][)]_**

= E a∼πρ∗[(][·|][s][)][,s][′][∼][P][ (][·|][s,a][)] _r(s, a) + γV[ˆ]_ _[∗](s[′]) −_ _λ log_ **_[π]ρ[∗](a_** _s)_

 _|_ 

= Vρ[∗][(][s][)]


_Qˆ[∗](s, a) = ˆr(s, a) + Es′∼P (·|s,a)_ _Vˆ_ _[∗](s[′])_

= r(s, a) + Es′∼P (·|s,a) hVρ[∗][(][s][′][)] i+ λ log ρ(a|s)

= Q[∗]ρ[(][s,][ a][) +][ λ][ log][ ρ][(][a][|][s][)][.] 

According to Lemma 5, we have


**_πρ[∗][(][a][|][s][) = ˆ]π[∗](a|s) ∝_** exp _rˆ(s, a) + γEs′∼P (·|s,a)_ _Vˆ_ _[∗](s[′])_ _/λ_

= ρ(a|s) exp _r(s, a) + γEs′∼P (·|s,a)_ _Vρ[∗][(]h[s][′][)]_ _/λi_ 

_Vρ[∗][(][s][) = ˆ]V_ _[∗](s)_      

= λ log exp _rˆ(s, a) + γEs′∼P (·|s,a)_ _Vˆ_ _[∗](s[′])_ _/λ_

**_a_**

X  h i 

= λ log **_ρ(a|s) exp_** _r(s, a) + γEs′∼P (·|s,a)_ _Vρ[∗][(][s][′][)]_ _/λ_

**_a_**

X      

_Q[∗]ρ[(][s,][ a][) = ˆ]Q[∗](s, a) −_ _λ log ρ(a|s)_

= ˆr(s, a) + γλEs′∼P (·|s,a) "log **_a[′]_** exp _Qˆ[∗](s[′], a[′])/λ_ _−_ _λ log ρ(a|s)_

X  [#]

= r(s, a) + γλEs′∼P (·|s,a) "log **_a[′]_** **_ρ(a|s) exp_** _Q[∗]ρ[(][s][′][,][ a][′][)][/λ]_ # _._

X   


-----

A.6 DERIVATION OF GRADIENT

_θi_ **_π = Es_** _θi_ **_π(a_** _s)_ _Q[π]ρ_ [(][s,][ a][)][ −] _[λ][ log][ π][(][a][|][s][)]_ + π(a _s)_ _θi_ ( _λ log_ **_[π][(][a][|][s][)]_**
_∇_ _L_ _∼D_ **_a_** _∇_ _|_  **_ρ(a|s)_**  _|_ _∇_ _−_ **_ρ(a|s) [)]#_**

"X

= Es **_π(a_** _s)_ _θi log πi(ai_ _s)_ _Q[π]ρ_ [(][s,][ a][)][ −] _[λ][ log][ π][(][a][|][s][)]_ _λπ(a_ _s)_ _θi log πi(ai_ _s)_
_∼D_ _|_ _∇_ _|_ **_ρ(a_** _s)_ _−_ _|_ _∇_ _|_

**_a_**  _|_ 

"X

= Es _,a_ **_π_** _θi log πi(ai_ _s)_ _Q[π]ρ_ [(][s,][ a][)][ −] _[λ][ log][ π][(][a][|][s][)]_
_∼D_ _∼_ _∇_ _|_ **_ρ(a_** _s)_

  _|_ _[−]_ _[λ]_


B ALGORITHM

Algorithm 1 gives the training procedure of DMAC.

**Algorithm 1 DMAC**

1: for episode = 1 to m do
2: Initialize the environment and receive initial state s

3: **for t = 1 to max-episode-length do**

4: For each agent i, select action ai _πi(_ _s)_

5: Execute joint-action a = (a1, a2 ∼, _, a·|n) and observe reward r and next state s[′]_
_· · ·_

6: Store (s, a, r, s[′]) in replay buffer D

7: **end for**

8: Sample a random minibatch of K samples from, (sk, ak, rk, s[′]k[)][}][K]
_D_ _{_

9: **for agent i = 1 to n do**

10: Update policy πi: θi = θi + β∇θi _Lπ_

11: Update target policy ρi: _θ[˜]i = (1 −_ _τ_ )θ[˜]i + τθi

12: **end for**

13: Update critic: φ = φ _α_ _φ_ _Q_
_−_ _∇_ _L_

14: Update target critic: _φ[˜] = (1 −_ _τ_ )φ[˜] + τφ

15: end for


C IMPLEMENTATION DETAILS

SMAC is a MARL environment based on the game StarCraft II (SC2). Agents control different units
in SC2 and can attack, move or take other actions. The general mode of SMAC tasks is that agents
control a team of units to counter another team controlled by built-in AI. The target of agents is to
wipe out all the enemy units and agents will gain rewards when hurting or killing enemy units. Agents
have an observation range and can only observe information of units in this range, but the information
of all the units can be accessed in training. We test all the methods in totally 8 tasks/maps: 3m, 2s3z,
3s5z, 8m,1c3s5z,3s_vs_3z,2c_vs_64zg, and MMM2.

In SMAC tasks, we evaluate 20 episodes in every 10000 training steps in the one million steps
training procedure for COMA, MAAC, QMIX and in every 20000 time steps for DOP and FOP. In
evaluation, we select greedy actions for QMIX and FOP (following the setting in the FOP paper)
and sample actions according to action distribution for stochastic policy (COMA, MAAC, DOP and
divergence-regularized methods).

C.1 MODIFICATIONS OF THE BASELINE METHODS

The modifications of the baseline methods, COMA,MAAC,QMIX,DOP and FOP, are as follows.

-  COMA. We keep the original critic and actor networks and add a target policy network with the
same architecture as the actor. As COMA is on-policy but COMA+DMAC is off-policy, we add a
replay buffer for experience replay.

-  MAAC already has a target policy for stability, so we do not need to modify the network architecture.
We only change the update rule for the critic and actors.


-----

Stochastic Game Stochastic Game Stochastic Game Stochastic Game

0.8 0.8 0.8

0.6 0.4 0.6 0.6

0.4 0.4 0.4

cover rate 0.2

0.2 0.2 0.2

COMA+DMAC QMIX+DMAC MAAC+DMAC DOP+DMAC

0.0 COMA 0.0 QMIX 0.0 MAAC 0.0 DOP

0 20000 40000 60000 80000 100000 0 20000 40000 60000 80000 100000 0 20000 40000 60000 80000 100000 0 50000 100000 150000 200000

step step step step

Figure 5: Learning curves in terms of cover rates of COMA, QMIX, MAAC and DOP groups in the
randomly generated stochastic game.

-  QMIX is a value-based method, so we need to add a policy network and a target policy network for
each agent. We keep the original individual Q-functions to learn the critic in QMIX+DMAC. In
divergence-regularized MDP, the max operator is not needed in the critic update, so we abandon
the hypernet and use an MLP, which takes individual Q-values and state as input and produces the
joint Q-value. This architecture is simple and its expressive capability is not limited by QMIX’s
IGM condition.

-  DOP. We keep the original critic and actor networks and add a target policy network with the same
architecture as the actor. We keep the value decomposition structure and use off-policy TD(λ) for
all samples in training to replace the tree backup loss and on-policy TD(λ) loss.

-  FOP. We replace the entropy regularizers with divergence regularizers in FOP and use the update
rules of DMAC. We keep the original architecture of FOP.


C.2 HYPERPARAMETERS

As all tasks in our experiments are cooperative with shared reward, so we use parameter-sharing
policy network and critic network for MAAC and MAAC+DMAC to accelerate training. Besides, we
add a RNN layer to the policy network and critic network in MAAC and MAAC+DMAC to settle the
partial observability.

All the policy networks are the same as two linear layers and one GRUCell layer with ReLU activation
and the number of hidden units is 64. The individual Q-networks for QMIX group is the same as the
policy network mentioned before. The critic network for COMA group is a MLP with three 128-unit
hidden layers and ReLU activation. The attention dimension in the critic networks of MAAC group
is 32. The number of hidden units of mixer network in QMIX group is 32. The learning rate for
critic is 10[−][3] and the learning rate for actor is 10[−][4]. We train all networks with RMSprop optimizer.
The discouted factor is γ = 0.99. The coefficient of regularizer is λ = 0.01. The td_lambda factor
used in COMA group is 0.8. The parameter used for soft updating target policy is τ = 0.01. Our






3m 3m 3m 3m

20 20 20 20

15 15 15 15

10 10 10 10

episode_rewards 5 5 COMA+DMAC DAPO 5 5

0 MAAC+DMAC MAAC 0 COMA 0 QMIX+DMAC QMIX DOP+DMAC DOP

20 0.0 0.2 0.4 2s3z 0.6 0.8 1.01e6 20 0.0 0.2 0.4 2s3z 0.6 0.8 1.01e6 20 0.0 0.2 0.4 2s3z 0.6 0.8 1.01e6 20 0.0 0.2 0.4 2s3z 0.6 0.8 1.01e6

15 15 15 15

10 10 10 10

episode_rewards 5 5 5 5

0.0 0.2 0.4 3s5z 0.6 0.8 1.01e6 0.0 0.2 0.4 3s5z 0.6 0.8 1.01e6 0.0 0.2 0.4 3s5z 0.6 0.8 1.01e6 0.0 0.2 0.4 3s5z 0.6 0.8 1.01e6

12.5 20 20

15

10.0 15 15

10 7.5 10 10

episode_rewards 5 5.0 5 5

0.0 0.2 0.4 8m 0.6 0.8 1.01e6 0.0 0.2 0.4 8m 0.6 0.8 1.01e6 0.0 0.2 0.4 8m 0.6 0.8 1.01e6 0.0 0.2 0.4 8m 0.6 0.8 1e61.0

20 20 20 20

15 15 15 15

10 10 10 10

episode_rewards 5 5 5 5

0 0

0.0 0.2 0.4 1c3s5z 0.6 0.8 1.01e6 0.0 0.2 0.4 1c3s5z 0.6 0.8 1.01e6 0.0 0.2 0.4 1c3s5z 0.6 0.8 1.01e6 0.0 0.2 0.4 1c3s5z 0.6 0.8 1.01e6

20 20 20 20

15 15 15 15

10 10

episode_rewards 10 5 5 10

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

step 1e6 step 1e6 step 1e6 step 1e6

Figure 6: Learning curves in terms of mean episode reward of COMA, MAAC, QMIX, and DOP
groups in five SMAC maps (each row corresponds to a map and each column corresponds to a group).


-----

3s_vs_3z 2c_vs_64zg MMM2

20 20.0 20

15 17.5 15

15.0

10 10

12.5

episode rewards 5 FOP+DMAC 10.0 FOP+DMAC 5 FOP+DMAC

0 FOP 7.5 FOP FOP

0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0

step 1e6 step 1e6 step 1e6

Figure 7: Learning curves in terms of mean episode rewards of FOP+DMAC and FOP in three SMAC
maps (each column corresponds to a map).




CDM CDM

10 10

5 5

0 0

episode rewards 5 5

COMA+DMAC DOP+DMAC

10 COMA 10 DOP

0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000

step step

Figure 8: Learning curves in terms of mean episode rewards of COMA and DOP groups in the CDM
environment used by the DOP paper.

code is based on the implementation of PyMARL (Samvelyan et al., 2019), MAAC (Iqbal and Sha,
2019), DOP (Wang et al., 2021b), FOP (Zhang et al., 2021) and an open source code for algorithms
[in SMAC (https://github.com/starry-sky6688/StarCraft).](https://github.com/starry-sky6688/StarCraft)


C.3 EXPERIMENT SETTINGS

We trained each algorithms for five runs with different random seeds. In SMAC tasks, we train each
algorithm for one million time steps in each run for COMA, QMIX, MAAC, and DOP groups and
two million timesteps for FOP groups. We do all the experiments by a server with 2 NVIDIA A100
GPUs.


ADDITIONAL RESULTS





MMM2 MMM2 MMM2 MMM2

1.0 10 10

0.8 0.6 8 8

0.6 0.4 6 6

win rates 0.40.20.0 QMIX+DMACQMIX 0.20.0 DOP+DMACDOP episode rewards 42 COMA+DMACCOMA 420 MAAC+DMACMAAC

0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0

step 1e6 step 1e6 step 1e6 step 1e6

Figure 9: Learning curves of QMIX, DOP, COMA and MAAC groups on the map MMM2 in SMAC.

Figure 5 shows the learning curves in terms of cover rates of COMA, QMIX, MAAC and DOP
groups in the randomly generated stochastic game.

Figure 6 shows the learning curves of COMA, MAAC, QMIX and DOP groups in terms of mean
episode rewards in each SMAC map.

Figure 7 shows the learning curves of FOP+DMAC and FOP in terms of mean episode rewards in
3s_vs_3z, 2c_vs_64zg and MMM2.

Figure 8 shows the learning curves in terms of mean episode rewards of COMA and DOP groups in
the CDM environment used by the DOP paper.

Figure 9 shows the learning curves of QMIX and DOP groups in terms of win rate, and COMA and
MAAC groups in terms of episode reward on the map MMM2 in SMAC.


-----

