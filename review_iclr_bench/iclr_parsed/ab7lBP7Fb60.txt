# ENFORCING FAIRNESS IN PRIVATE FEDERATED LEARN## ING VIA THE MODIFIED METHOD OF DIFFERENTIAL MULTIPLIERS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Federated learning with differential privacy, or private federated learning, provides a strategy to train machine learning models while respecting users’ privacy.
However, differential privacy can disproportionately degrade the performance of
the models on under-represented groups, as these parts of the distribution are difficult to learn in the presence of noise. Existing approaches for enforcing fairness
in machine learning models have considered the centralized setting, in which the
algorithm has access to the users’ data. This paper introduces an algorithm to
enforce group fairness in private federated learning, where users’ data does not
leave their devices. First, the paper extends the modified method of differential
multipliers to empirical risk minimization with fairness constraints, thus providing an algorithm to enforce fairness in the central setting. Then, this algorithm is
extended to the private federated learning setting. The proposed algorithm, FPFL,
is tested on a federated version of the Adult dataset and an “unfair” version of the
FEMNIST dataset. The experiments on these datasets show how private federated
learning accentuates unfairness in the trained models, and how FPFL is able to
mitigate such unfairness.

1 INTRODUCTION

Machine learning requires data to build models. This data is often private and resides on users’
devices. Federated learning (FL) (McMahan et al., 2017) is a strategy where multiple users (or
other entities) collaborate in training a model under the coordination of a central server or service
provider. In FL, devices share only data statistics (e.g. gradients of the model computed from their
data) with the server, and therefore the users’ data never leaves their devices.

However, these statistics can still leak information about the users (see Shokri et al. (2017); Fredrikson et al. (2015) for practical attacks recovering the users’ identity and reconstructing the users’
face images). To protect against this leakage, differential privacy (DP) can be provided. Differential
privacy offers a mathematical guarantee on the maximal amount of information any attacker can
obtain about a user after observing the released model. In practice, often (Bhowmick et al., 2018;
McMahan et al., 2018a;b; Truex et al., 2019; Granqvist et al., 2020) this guarantee is achieved by
adding a certain amount of noise to the individuals’ statistics. In this paper, federated learning with
differential privacy will be referred to as private federated learning (PFL).

The models trained with PFL are often neural networks, since they work well in many use cases, and
turn out to be resistant to the noise added for DP. These models have been successfully trained with
PFL for tasks like next word prediction or speaker verification (McMahan et al., 2017; Granqvist
et al., 2020). However, they are prone to perpetuating societal biases existing in the data (Caliskan
et al., 2017) or to discriminate against certain groups even when the data is balanced (Buolamwini &
Gebru, 2018). Moreover, when the training is differentially private, degradation in the performance
of these models disproportionately impacts under-represented groups (Bagdasaryan et al., 2019).
More specifically, the accuracy of the model on minority groups is deteriorated to a larger extent
than the accuracy for the majority groups.

In the realm of FL, there has been some research studying how to achieve that the performance of the
model is similar among devices (Hu et al., 2020; Huang et al., 2020; Li et al., 2019; 2021). However,


-----

this notion of fairness falls short in terms of protecting users from under-represented groups falling
foul of disproportionate treatment. For example, Castelnovo et al. (2021, Section 3.6) show that a
model that performs well on the majority of the users will have a good score on individual fairness
metrics, even if all the users suffering from bad performance belong to the same group. Conversely,
there is little work proposing solutions to enforce group fairness, i.e. that the performance is similar
among users from different groups. Current work in this area is either limited to reducing demographic parity in logistic regression (Du et al., 2021) or to minimizing the largest loss across the
different groups with a minimax optimization (Mohri et al., 2019). Concurrently to this paper, Cui
et al. (2021) proposed an approach to enforce group fairness in FL for general models and metrics.
However, this approach does not consider DP. Moreover, most prior work does not consider the
adverse effects that DP has on the models trained with private federated learning, or is restricted to
logistic regression models (Abay et al., 2020).

On the other hand, work studying the trade-offs between privacy and group fairness proposes solutions that are either limited to simple models such as linear logistic regression (Ding et al., 2020),
require an oracle (Cummings et al., 2019), scale poorly for large hypothesis classes (Jagielski et al.,
2019), or only offer privacy protection for the variable determining the group (Tran et al., 2021;
Rodr´ıguez-G´alvez et al., 2021). Furthermore, the aforementioned work only considers the central
learning paradigm, and the techniques are not directly adaptable to FL.

For all the above reasons, in this paper, we propose an algorithm to train neural networks with
PFL while enforcing group fairness. We pose the problem as an optimization problem with fairness
constraints and extend the modified method of differential multipliers (MMDM) (Platt & Barr, 1987)
to solve such a problem with FL and DP. Hence, the resulting algorithm (i) is applicable to any model
that can be learned using stochastic gradient descent (SGD) or any of its variants, (ii) can be tailored
to enforce the majority of the group fairness metrics, (iii) can consider any number of attributes
determining the groups, and (iv) can consider both classification and regression tasks.

The paper is structured as follows: in Section 2, we review the background on differential privacy,
private federated learning, the MMDM algorithm, and group fairness; in Section 3, we present our
approach for fair private federated learning; in Section 4, we describe our experimental results; and
in Section 5, we conclude with a summary and interpretation of our findings.

2 BACKGROUND

In this section, we review several aspects on the theory of private federated learning, the MMDM
algorithm, and group fairness that are necessary to develop and understand the proposed algorithm.

2.1 FEDERATED LEARNING

The federated learning setting focuses on learning a model that minimizes the expected value of
a loss function ℓ using a dataset d = (z1, z2, . . ., zn) of n samples distributed across K
_∈D_
users. This paper will consider models parametrized by a fixed number of parameters w ∈ R[p] and
differentiable loss functions ℓ, which includes neural networks.

Since the distribution of the data samples Z is unknown and each user k is the only owner of their
private dataset d[k], the goal of FL is to find the parameters w[⋆] that minimize the loss function across
the samples of the users. That is, defining L(d[k], w) := **_z_** _d[k][ ℓ][(][z][,][ w][)][, to solve]_

_∈_

1 _K_
**_w[⋆]_** = arg min (1)
**_w∈R[p]_** _n_ _k=1[P][L][(][d][k][,][ w][)][.]_

X

This way, the parameters w can be learned with an approximation of SGD. McMahan et al.
(2017) suggest that the central server iteratively samples m users; they compute and send back
to the server an approximation of the model’s gradient **_wL(d[k], w); and finally the server up-_**
_∇_
dates the parameters as dictated by gradient descent w ← **_w −_** _n[1]_ _k_ _[η][∇][w][L][(][d][k][,][ w][)][, where][ η][ is the]_

learning rate. If the users send back the exact gradient the algorithm is known as FederatedSGD.
A generalisation of this, FederatedAveraging, involves users running several local epochs ofP
mini-batch SGD and sending up the difference. However, the method in this paper considers an
optimization with fairness constraints, requiring the communication of extra information in each
iteration, and therefore will extend FederatedSGD.


-----

2.2 PRIVATE FEDERATED LEARNING

Private federated learning combines federated learning with differential privacy. Differential privacy
(Dwork et al., 2006; 2014) provides a mathematical guarantee by restricting the amount of information that a randomized function m of a dataset d leaks about any individual. More precisely, it
ensures that the probability that the output of the function m on the dataset d belongs to any set of
outcomes O is close to the probability that the output of the function m applied to a neighbouring
dataset d[′] (i.e. equal to d but where all samples from any individual are removed or replaced) belongs to said set of outcomes. Formally, a randomized function m is (ϵ, δ)-differentially private if

P _m(d) ∈_ _O_ _≤_ _e[ϵ]P_ _m(d′) ∈_ _O_ + δ, (2)

where ϵ and δ are parameters that determine the strength of the guarantee. Two useful properties of   
DP are that that it composes (i.e. the combination of DP functions is DP) and that post-processing
does not undo its privacy guarantees (Dwork et al., 2014). These properties ensure that the modifications that we will make to FederatedSGD will keep the algorithm differentially private.

Typically, the functions of interest are not differentially private. For this reason, they are privatized
with “privacy mechanisms”, which apply a random transformation to the output of the function on
the dataset. A standard mechanism is the Gaussian mechanism, which adds Gaussian noise to the
function’s output, calibrating the noise variance with the privacy parameters (ϵ, δ) and the maximum
individual’s contribution to the function’s output measured by the ℓ2 norm.

In the context of FederatedSGD, the functions to privatize are the gradient estimates sent to the
server. The individual’s contribution is the gradient computed on their data. This gradient is “normclipped” (Abadi et al., 2016), i.e. it is scaled down so its ℓ2 norm is at most a predetermined value C.
Then, Gaussian noise is added to the sum of contributions, which is sent to the server.

Applying the privacy mechanism to a sum of contributions is a form of central DP. An alternative
is local DP (Dwork et al., 2014), where the privacy mechanism is applied to the statistics before
leaving the device. However, models trained with local DP often suffer from low utility (Granqvist
et al., 2020), and are thus not considered in this paper. Central DP can involve trust in the server
that updates the model, or a trusted third party separated from the model, which is how DP was
originally formulated. This paper instead assumes the noisy sum is computed through secure multiparty computation (Goryczka & Xiong, 2015), such as secure aggregation (Bonawitz et al., 2017).
This is a cryptographic method that ensures the server only sees the sum of contributions. A crucial
limitation of secure aggregation is that it can only compute sums, so the novel method introduced
in Section 3 is restricted to working only on sums of individuals’ statistics.

2.3 THE MODIFIED METHOD OF DIFFERENTIAL MULTIPLIERS

Ultimately, our objective is to find a parametric model that minimizes a differentiable loss function
and respects some fairness constraints. Thus, this subsection reviews a constrained differential optimization algorithm, the modified method of differential multipliers (MMDM) (Platt & Barr, 1987).

This algorithm tries to find a solution to the following constrained optimization problem:

**_w[⋆]_** = arg min s.t. g(w) = 0, (P1)
**_w_** R[p][ φ][(][w][)]
_∈_

where φ : R[p] _→_ R is the function to minimize, g(w) = (g1(w), g2(w), . . ., gr(w)) is the concatenation of r constraint functions, and {w ∈ R[p] : g(w) = 0} is the solution subspace.

The algorithm consists of solving the following set of differential equations resulting from the Lagrangian of (P1) with an additional quadratic penalty cg(w)[2] using gradient descent/ascent. This
results in an algorithm that applies these updates iteratively:
**_λ ←_** **_λ + γg(w)_**

_,_ (3)

(w **_w_** _η_ **_wφ(w) + λ[T]_** **_wg(w) + cg(w)[T]_** **_wg(w)_**

_←_ _−_ _∇_ _∇_ _∇_
 

where λ ∈ R[r] is a Lagrange (or dual) multiplier, c ∈ R+ is a damping parameter, η is the learning
rate of the model parameters, and γ is the learning rate of the Lagrange multiplier.


-----

Intuitively, these updates gradually fulfill (P1). Updating the parameters **_wφ(w) and_** **_wg(w)_**
_∇_ _∇_
respectively enforce function minimization and constraint’s satisfaction. Then, the multiplier λ and
the multiplicative factor cg(w) control how strongly the constraints’ violations are penalized.

A desirable property of MMDM is that for small enough learning rates η, γ and large enough damping parameter c, there is a region comprising the vicinity of each constrained minimum such that if
the parameters’ initialization is in that region and the parameters remain bounded, then the algorithm
converges to a constrained minimum (Platt & Barr, 1987). Intuitively, the term cg(w) **_wg(w)_**
_∇_
enforces a quadratic shape on the optimization search space to the neighbourhood of the solution
subspace, and this local behavior is stronger the larger the damping parameter c. Thus, the algorithm
converges to a minimum if the parameters’ initialization is in this locally quadratic region.

2.4 GROUP FAIRNESS

In this subsection, we mathematically formalize what group fairness means. To ease the exposition,
we describe this notion in the central setting, i.e. where the data is not distributed across users.

Consider a dataset d = (z1, z2, . . ., zn) of n samples. The dataset is partitioned into groups from
_A such that each sample zi belongs to group ai. Group fairness considers how differently a model_
treats the samples belonging to each group. Many fairness metrics can be written in terms of the
similarity of the expected value of a function of interest f of the model evaluated on the general
Fioretto et al., 2020). That is, if we consider a supervised learning problem, wherepopulation d with that on the population of each group da = {zi ∈ _d : ai = a} (Agarwal et al., 2018; zi = (xi, yi, ai)_
and where the output of the model is an approximation ˆyi of yi, we say a model is fair if

E[f (X, Y, **_Y[ˆ] ) | A = a] = E[f_** (X, Y, **_Y[ˆ] )] for all a ∈A._** (4)

Most of the group fairness literature focuses on the case of binary classifiers, i.e. ˆyi 0, 1,
and on the binary group case, i.e. A = {0, 1}. However, many of the fairness metrics can be ∈{ _}_
the indicator functionextended to general output spaces 1 of some logical relationship between the random variables, thus turning (4) Y and categorical groups A. It is common that the function f is
to an equality between probabilities. As an example, we describe two common fairness metrics that
will be used later in the paper. For a comprehensive survey of different fairness metrics and their
inter-relationships, please refer to Verma & Rubin (2018); Castelnovo et al. (2021).

**False negative rate (FNR) parity (or equal opportunity) (Hardt et al., 2016)** This fairness
metric is designed for binary classification and binary groups. It was originally defined as equal true
positive rate between the groups, which is equivalent to an equal FNR between each group and the
overall population. That is, if we let f (X, Y, _Y[ˆ] ) = E[1( Y[ˆ] = 0) | Y = 1] then (4) reduces to_

P[ Y[ˆ] = 0 | Y = 1, A = a] = P[ Y[ˆ] = 0 | Y = 1] for all a ∈A. (5)

This is usually a good metric when the target variable Y is something positive such as being granted
a loan or hired for a job, since we want to minimize the group disparity of misclassification among
the individuals deserving such a loan or such a job (Hardt et al., 2016; Castelnovo et al., 2021).

**Accuracy parity (or overall misclassification rate) (Zafar et al., 2017)** This fairness metric is
also designed for binary classification and binary groups. Nonetheless, it applies well to general
tasks and categorical groups. In this case, if we let f (X, Y, _Y[ˆ] ) = 1( Y[ˆ] = Y ) then (4) reduces to_

P[ Y[ˆ] = Y | A = a] = P[ Y[ˆ] = Y ] for all a ∈A. (6)

This is usually a good metric when there is not a clear positive or negative semantic meaning to the
target variable, and also when this variable is not binary.

3 AN ALGORITHM FOR FAIR AND PRIVATE FEDERATED LEARNING (FPFL)

In this section, we describe the proposed algorithm to enforce fairness in PFL of parametric models
learned with SGD. First, we describe an adaptation of the MMDM algorithm to enforce fairness in
standard central learning. Then, we extend that algorithm to PFL.


-----

3.1 ADAPTING THE MMDM ALGORITHM TO ENFORCE FAIRNESS

Consider a dataset d = (z1, z2, . . ., zn) of n samples. The dataset is partitioned into groups from
_A_
such that each sample zi belongs to group ai. Then, let us consider also a supervised learning setting
where zi = (xi, yi, ai), the model’s output ˆyi approximates yi, and the model is parametrized by
the parameters w ∈ R[p]. Finally, assume we have no information about the variables’ (X, Y, A)
distribution apart from the available samples.

We concern ourselves with the task of finding the model’s parameters w[⋆] that minimize a loss
function ℓ across the data samples and enforce a measure of fairness on the model. That is, we
substitute the expected values of the fairness constraints in (4) with empirical averages:

_L(d, w)_ _F_ (d[′], w) _a[,][ w][)]_
**_w[⋆]_** = arg min s.t. (P2)
**_w∈R[p]_** _n_ _n[′]_ _−_ _[F]_ [(][d]n[′] _[′]a_ _[≤]_ _[α,][ for all][ a][ ∈A][,]_

where L(d, w) := **_z_** _d_ _[ℓ][(][z][,][ w][)][,][ F]_ [(][d][′][,][ w][) :=][ P]z _d[′][ f]_ [(][z][,][ w][)][,][ d][′][ is a subset of][ d][ that varies among]

_∈_ _∈_

different fairness metrics, n[′] is the number of samples in d[′], d[′]a [is the set of samples in][ d][′][ such that]
_ai = a, n[′]a_ [is the number of samples in][P] _[ d]a[′]_ [,][ f][ is the function employed for the fairness metric]
definition, and α is a tolerance threshold. The subset d[′] is chosen based on the function f used
for the fairness metric: if the fairness function does not involve any conditional expectation (e.g.
accuracy parity), then d[′] = d; if, on the other hand, the subset involves a conditional expectation,
then the d[′] is the subset of d where that condition holds, e.g. when the fairness metric is FNR parity
_d[′]_ = {(x, y, a) ∈ _d : y = 1}. We exclude the case where the conditioning in the expectation is_ _Y[ˆ],_
thus excluding predictive parity as a target fairness metric.

Note that the constraints are not strict, meaning that there is a tolerance α for how much the function
_f can vary between certain groups and the overall population. The reason for this choice is twofold._
First, it facilitates the training since the solution subspace is larger. Second, it is known that some
fairness metrics, such as FNR parity, are incompatible with DP and non-trivial accuracy. However,
if the fairness metric is relaxed, fairness, accuracy, and privacy can coexist (Jagielski et al., 2019;
Cummings et al., 2019).

This way, we may re-write (P2) in the form of (P1) to solve the problem with MMDM. To do so we
let φ(w) = L(d, w)/n and g(w) = (g0(w), g1(w), . . ., g|A|−1(w)), where

_ha(w)_ if ha(w) 0 _a[,][ w][)]_
_ga(w) =_ _≥_ and _ha(w) :=_ _α._ (7)
0 otherwise _n[′]_ _−_ _[F]_ [(][d]n[′] _[′]a_ _−_


Therefore, the parameters are updated according to _[F]_ [(][d][′][,][ w][)]
**_λ ←_** **_λ + γg(w)_** _,_ (8)
(w **_w_** _η_ 1/n **_wL(d, w) + λ[T]_** **_wg(w) + cg(w)[T]_** **_wg(w)_**

_←_ _−_ _∇_ _∇_ _∇_

where we note that ∇wg(w) = (  _∇wg0(w), ∇wg1(w), . . ., ∇wg|A|−1(w)) and_ 

**_wga(w) =_** sign _F (dn[′][′],w)_ _−_ _[F][ (][d]na[′][′]a[,][w][)]_ _∇wFn ([′]d[′],w)_ _−_ _[∇][w][F]n[ (][′]a[d]a[′]_ _[,][w][)]_ if ha(w) ≥ 0 _._ (9)
_∇_ ( 0    otherwise

Now, the fairness-enforcing problem (P2) can be solved with gradient descent/ascent or mini-batch
stochastic gradient descent/ascent, where instead of the full dataset d, d[′], and d[′]a[, one considers]
batches b, b[′], and b[′]a [(or subsets) of that dataset. Moreover, it can be learned with DP adapting the]
DP-SGD algorithm from (Abadi et al., 2016), where a clipping bound and Gaussian noise is included
in both the network parameters’ and multipliers’ individual updates. Nonetheless, there are a series
of caveats of doing so. First, the batch sizesamples of each group a ∈A so that the difference |b[′]| should be large enough to, on average, have enough F (|bb′[′],|w) _−_ _[F][ (]|[b]ba[′][′]a[,][|][w][)]_ is well estimated. Second,

in many situations of interest such as when we want to enforce FNR parity or accuracy parity, the

  

function f employed for the fairness metric is not differentiable and thus **_wF_** (d[′], w) does not exist.
_∇_
To solve this issue, we resort to estimate the gradient **_wF_** (d[′], w) using a differentiable estimation
_∇_
of the function aggregate F (d[′], w); see Appendix A.1 for the details on these estimates in the above
situations of interest.


-----

We conclude this section noting the similarities and differences of this work and (Tran et al., 2021).
Even though their algorithm is derived from first principles on Lagrangian duality, it is ultimately
equivalent to an application of the basic method of differential multipliers (BMDM) to solve a problem equivalent to (P2) when α = 0. Nonetheless, the two algorithms differ in three main aspects:

1. BMDM vs MMDM: BMDM is equivalent to MMDM when c = 0, that is, when the effect
of making the neighbourhood of the solution subspace quadratic is not present. Moreover,
the guarantee of achieving a local minimum that respects the constraints does not hold for
_c = 0 unless the problem is simple (e.g. quadratic programming)._

2. How they deal with impossibilities in the goal of achieving perfect fairness together with
privacy and accuracy. Tran et al. (2021) include a limit λmax to the Lagrange multiplier to
avoid floating point errors and reaching trivial solutions, which in our case is taken care by
the tolerance α, which, in contrast to λmax, is an interpretable parameter.

3. This paper uses the exact expression in (9) for the gradient ∇w _[F][ (][d]n[′][′][,][w][)]_ _−_ _[F][ (][d]na[′][′]a[,][w][)]_, while

Tran et al. (2021) ignore the sign of the difference and use _[∇][w][F]n[ (][′][d][′][,][w][)]_ _−_ _[∇][w][F]n[ (][′]a[d]a[′]_ _[,][w][)]_ .

In the next subsection, we extend the algorithm to PFL, which introduces two new differences
with (Tran et al., 2021). Firstly, the privacy guarantees will be provided for the individuals, and
not only to the group to which they belong; and secondly, the algorithm will be tailored to FL.

3.2 EXTENDING THE ALGORITHM TO PRIVATE FEDERATED LEARNING

In the federated learning setting we now consider that the dataset d is distributed across K users
such that each user maintains a local dataset d[k] with n[k] samples. This setting applies to both crossdevice and cross-silo FL, since each sample contains the group information ai. Nonetheless, as in
the central setting, the task is to find the model’s parameters w[⋆] that minimize the loss function
across the data samples of the users while enforcing a measure of fairness to the model. That is, to
solve (P2).

To achieve this goal, we might first combine the ideas from FederatedSGD (McMahan et al.,
2017) and the previous section to extend the developed adaptation of MMDM to FL. Our adaptation
of MMDM cannot be combined with FederatedAveraging; for more details see Appendix A.2.
In order to perform the model updates from (8), the central server requires the following statistics:
**_wL(d, w); F_** (d[′], w); _F_ (d[′]a[,][ w][)] _a_ **_wF_** (d[′]a[,][ w][)] _a_ _n[′]a_ _a_
_∇_ _∈A[;][ ∇][w][F]_ [(][d][′][,][ w][);] _∇_ _∈A[;][ n][′][;]_ _∈A[.]_

(10)

     

However, some of these statistics can be obtained from the others: F (d[′], w) = _a_ _[F]_ [(][d]a[′] _[,][ w][)][,]_

_∈A_
_∇section, one might use a sufficiently large batchwF_ (d[′], w) = _a∈A_ _[∇][w][F]_ [(][d]a[′] _[,][ w][)][, and][ n][′][ =][ P] ba of the data instead of the full dataset for each∈A_ _[n]a[′]_ [. Moreover, as mentioned in the previous]

[P]

update. Therefore, we consider an iterative algorithm where, at each iteration, the central server

[P]

samples a cohort of m users S that report a vector with the sufficient statistics for the update, that is

**_v[k]_** = **_wL(d[k], w),_** _F_ ((d[k])[′]a[,][ w][)] _a_ **_wF_** ((d[k])[′]a[,][ w][)] _a_ (n[k])[′]a _a_ _._ (11)
_∇_ _∈A[,]_ _∇_ _∈A[,]_ _∈A_

This way, if we define the batchh  b as the sum of the  m users’ local datasets  b :=  _k_ _Si[d][k][ and the]_

_∈_
batch b[′] analogously, then the aggregation of each user’s vectors results in

**_v =_** _k_ _S_ **_[v][k][ =]_** **_wL(b, w),_** _F_ (b[′]a[,][ w][)] _a_ **_wF_** (b[′]a[,][ w][)] _a_ _b[′]a[P][|]_ _a_ _,_ (12)

_∈_ _∇_ _∈A[,]_ _∇_ _∈A[,]_ _|_ _∈A_

which contains all the sufficient statistics for the parameters’ update.[P] h       i

Finally, the resulting algorithm, termed Fair PFL or FPFL and described in Algorithm 1, inspired by
the ideas from (McMahan et al., 2018b; Truex et al., 2019; Granqvist et al., 2020; Bonawitz et al.,
2017) guarantees the users’ privacy by (i) making sure the aggregation of the users’ sent vectors is
done securely with secure aggregation (Bonawitz et al., 2017), and (ii) clipping each users’ vectors
with a clipping bound C and employing the Gaussian mechanism to the sum of the clipped vectors
(lines 13 and 15). The variance of the Gaussian noise is calculated according to the refined moments
accountant privacy analysis from (Wang et al., 2019), taking into account the number of iterations
_T_, the cohort size m, the total number of users (or population size) K, and the privacy parameters ϵ
and δ. Note that the post-processing property of DP keeps the algorithm private even if the received
noisy vector v is processed to extract the relevant information for the model’s update.


-----

**Batch size.** This algorithm can also be employed using only a fraction of the user’s data in each
update, i.e. using a batch b[k] of their local dataset d[k]. Nonetheless, it is desirable (i) to delegate as
much computation to the user as possible and (ii) to use as much users’ data as possible to have a
good approximation of the performance metrics F (d[′]a[,][ w][)][, which are needed to enforce fairness.]

4 EXPERIMENTAL RESULTS


We study the performance of the algorithm in two classification tasks. The first task is a binary classification based on some demographic data from the publicly
available Adult dataset (Dua & Graff, 2017). The fairness metric considered for this task is FNR parity. The
second task is a multi-class classification where there are
three different attributes. This task considers accuracy
parity as the fairness metric and uses a modification of
the publicly available FEMNIST dataset (Caldas et al.,
2018), where only digits are considered and the classes Figure 1: Samples from the Unfair FEMare digits written with a black pen on a white sheet, dig- NIST dataset. Digits written on a white
its written with a blue pen on a white sheet, and digits sheet with black pen (top), on a white
written with white chalk on a blackboard (see Figure 1). sheet with blue pen (middle), and on a

blackboard with white chalk (bottom).

For the first task, we first compare the performance of
the MMDM algorithm with BMDM, Tran et al. (2021), and vanilla SGD centrally and without
privacy. After that, for both tasks, we confirm how FederatedSGD deteriorates the performance
of the model for the under-represented classes when clipping and noise (DP) are introduced. Finally,
we demonstrate how FPFL can, under the appropriate circumstances, level the performance of the
model across the groups without largely decreasing the overall performance of the model.

In all our experiments, the fairness metrics are defined as the maximum difference of the value
of a performance measure between the general testing data and each of the groups described by
that sensitive attribute. Moreover, the privacy parameters are (ϵ, δ) = (2, 1/K), where K is the
population size. All experimental details and additional experiments are in Appendix B.

4.1 RESULTS ON THE ADULT DATASET

We start our experiments comparing the performance and fairness in the central, non-federated, nonprivate setting. We trained a shallow network (consisting of one hidden layer with 10 hidden units
and a ReLU activation function) with these algorithms, where we tried to enforce FNR parity with
a tolerance α = 0.02. For MMDM, the damping parameter is c = 2. For Tran et al. (2021), the
multiplier threshold is λmax = 0.05. The results after 1,000 iterations are displayed in Table 1,
which also shows the gap in other common measures of fairness such as the equalized odds, the
demographic parity, or the predictive parity, see e.g. Castelnovo et al. (2021).

Compared to Federated SGD, the MMDM algorithm reduces the FNR gap from 7% to 0.5%, while
hardly reducing the accuracy of the model. Similarly, the gap in the equalized odds (EO), which is a
stronger fairness notion than the FNR parity, also decreases from around 7% to 3%. Moreover, the
demographic parity (DemP) gap, which considers the probability of predicting one or the other target
class, also improves. In terms of predictive parity (PP), which uses the precision as the performance
function, the MMDM algorithm did not improve the parity among groups. The BMDM algorithm,
which lacks a quadratic term of the fairness function, performs even slightly better in the central
scenario. However, the algorithm from Tran et al. (2021), an approximation to BMDM as discussed
in Section 3.1, gives only a small improvement. Therefore, we continue our analysis to the federated
and private settings only with the BMDM and MMDM algorithms.

The second experiment is to study how federated learning, clipping, and DP decrease the performance for the under-represented groups, thus increasing the fairness gap on the different fairness
metrics. For that, we trained the same network with FederatedSGD and versions of this algorithm where only clipping was performed and where both clipping and DP where included. They
were trained with a cohort size of m = 200 for T = 1, 000 iterations and the model with best
training cohort accuracy was selected. The privacy parameters are (ϵ, δ) = (2, 5 · 10[−][5]). The results


-----

Table 1: Performance in the central (i.e. non-federated) setting on the Adult dataset. The fairness
metric optimized is FNR parity. The tolerance for BMDM and MMDM is α = 0.02.

|Algorithm Accuracy|FNR gap EO gap DemP gap PP gap|
|---|---|


|SGD 0.857 Tran et al. (2021) without privacy 0.856 BMDM 0.857 MMDM (this paper) 0.856|0.071 0.071 0.113 0.016 0.048 0.048 0.108 0.036 0.001 0.030 0.094 0.046 0.005 0.028 0.091 0.063|
|---|---|



Table 2: Performance of a neural network on the Adult dataset when trained with FL. The fairness
metric considered for BMDM-FL and FPFL is FNR parity and the tolerance is α = 0.02.

|Algorithm Accuracy|FNR gap EO gap DemP gap PP gap|
|---|---|


|FL 0.851 BMDM-FL 0.854 Fair FL 0.855|0.121 0.121 0.122 0.037 0.043 0.043 0.101 0.016 0.036 0.039 0.108 0.033|
|---|---|


|FL + Clip 0.844 BMDM-FL + Clip 0.851 Fair FL + Clip 0.853|0.169 0.169 0.129 0.051 0.052 0.052 0.105 0.008 0.018 0.029 0.090 0.016|
|---|---|


|Private FL 0.847 BMDM-PFL 0.850 Fair Private FL 0.851|0.148 0.148 0.126 0.041 0.023 0.035 0.097 0.009 0.001 0.027 0.087 0.042|
|---|---|



are displayed in Table 2. The network becomes less fair under all the metrics considered, compared
with the central setting. Moreover, the introduction of clipping largely increases the unfairness of
the models reaching more than a 16% gap in FNR. The addition of DP requires an increase in cohort
size from 200 to 1, 000, but does not have a larger effect on the unfairness of the models (see Table 5
in Appendix B.1 for the details). These observations are in line with (Bagdasaryan et al., 2019),
where they note that the under-represented groups usually have the higher loss gradients and thus
clipping affects them more than the majority groups.

After that, we repeated the above experiment with FPFL and BMDM with DP. FPFL and BMDM
with DP converged to a solution faster than FederatedSGD, and thus the models were trained for
only T = 250 iterations. Here the model with the best training cohort accuracy while respecting
the fairness condition on the training cohort data was selected. Note that the fairness condition is
evaluated with noisy statistics, so a model may be deemed as fair while slightly violating the desired
constraints. The results are also included in Table 2 to aid the comparison. We note how, similarly
to the central case, the model trained with FPFL manages to enforce the fairness constraints while
keeping a similar accuracy. In contrast, BMDM seems to not be able to find network weights that
respect the fairness conditions, as expected by the less favorable (“less convex”) loss surface around
these solutions. Moreover, clipping does not seem to affect largely the performance of FPFL since
it compensates the gradient loss clipping with the fairness enforcement.

4.2 RESULTS ON THE UNFAIR FEMNIST DATASET

We start our experiments confirming again the hypothesis and findings from Bagdasaryan et al.
(2019) that clipping and DP disproportionately affect under-represented groups. For that, we trained
a convolutional network (consisting of 2 convolution layers with kernel of size 5 × 5, stride of 2,
ReLU activation function, and 32 and 64 filters respectively) with FederatedSGD and versions of
this algorithm where only clipping was performed and where both clipping and DP were included.
For the FEMNIST experiments, the privacy parameters are (ϵ, δ) = (2, 2.5 · 10[−][4]) and the damping
parameter is c = 20. They were trained with a cohort size of m = 100 for T = 2, 000 iterations
and the model with best training cohort accuracy was selected. The results are displayed in Table 3.
Similarly to before, clipping increases the accuracy gap from 13% to almost 17%. In this case, since
the number of users K is small, the DP noise is large compared to the users’ statistics. Therefore, the


-----

accuracy drops from more than 94% with clipping to 80.7% when DP is also used, and the accuracy
gap increases to more than 40%.
Table 3: Performance of a convolutional net
The second experiment tests whether FPFL can work on the Unfair FEMNIST when trained with
remedy the unfairness without decreasing ac- different algorithms: like Table 2, starting with
curacy too much. We trained the same con- FederatedSGD and ending with FPFL. The
volutional network, again for T = 2, 000 it- fairness metric considered for FPFL is accuracy
erations, and selected the model with the best parity and the tolerance is α = 0.04.
training cohort accuracy that respected the fairness condition on the training cohort. When DP

|Algo. Population|Accuracy Acc. gap|
|---|---|

noise is not included, FPFL reduces the accu- _m = 100_
racy gap with FederatedSGD by around 9% FL _K_ 0.960 0.134
while keeping the accuracy within 1%. We note FFL _K_ 0.950 0.047
how, as before, clipping largely does not affect

|FL K FFL K|0.960 0.134 0.950 0.047|
|---|---|

the ability of FPFL to enforce fairness. How- FL + Clip _K_ 0.946 0.166
ever, note that since the data is more non-i.i.d. FFL + Clip _K_ 0.954 0.053
than before (i.e. there are more differences be- PFL _K_ 0.807 0.409

|FL + Clip K FFL + Clip K|0.946 0.166 0.954 0.053|
|---|---|

tween the distribution of each user) the models FPFL _K_ 0.093 0.015
that are deemed fair in the training cohort may

|PFL K FPFL K|0.807 0.409 0.093 0.015|
|---|---|

not be as fair in the general population, and _m = 2, 000_
now we see a larger gap between the desired PFL 100K 0.951 0.157
tolerance α = 0.04 and the obtained accuracy FPFL 100K 0.903 0.074
gap from FPFL without noise (0.047 and 0.053 PFL 1, 000K 0.951 0.153
without and with clipping). FPFL 1, 000K 0.927 0.073

|PFL 100K FPFL 100K PFL 1, 000K FPFL 1, 000K|0.951 0.157 0.903 0.074 0.951 0.153 0.927 0.073|
|---|---|


When DP is used, the noise is too large for
FPFL to function properly and often the sign of the constraints’ gradient, see (9), flips. Note that
in the estimation of the performance function, i.e. F (da, w)/na, both the numerator and denominator are obtained from a noisy vector, thus making the estimators more sensitive to noise than the
estimators for plain FederatedSGD. This is due to two main factors: (i) the larger the model, the
larger the aggregation of the gradients for each weight, and thus more noise needs to be added; and
(ii) the smaller the amount of users, the larger the noise that needs to be added.

For this reason, we considered the hypothetical scenario where the population, used for calculating the DP noise, is 100 and 1,000 times larger, which is a conservative assumption for federated
learning deployments (Apple, 2017). Then, we repeated the experiment with DP FederatedSGD
and FPFL and increased the cohort size to m = 2, 000. In this scenario, DP FederatedSGD
maintained an accuracy gap of more than 15% while FPFL reduced this gap to less than a half in
both cases. Nonetheless, the accuracy still decreases slightly, with a reduction of around 5% and 2%
respectively compared with DP FederatedSGD.

5 CONCLUSION

In this paper, we studied and proposed a solution to the problem of group fairness in private federated
learning. For this purpose, we adapt the modified method of multipliers (MMDM) (Platt & Barr,
1987) to empirical loss minimization with fairness constraints, producing an algorithm for enforcing
fairness in central learning. Then, we extend this algorithm to private federated learning.

Through experiments in the Adult (Dua & Graff, 2017) and a modified version of the FEMNIST (Caldas et al., 2018) datasets, we first confirm previous knowledge that DP disproportionately
affects performance on under-represented groups (Bagdasaryan et al., 2019), with the further observation that this is true for many different fairness metrics, and not just for accuracy parity. The
proposed FPFL algorithm is able to remedy this unfairness even in the presence of DP.

**Limitations and future work.** The FPFL algorithm is more sensitive to DP noise than other algorithms for PFL. This requires increasing the cohort size or ensuring that enough users take part in
training. Still, for the experiments, the number of users required is still lower (more than an order
of magnitude) than the usual amount of users available in deployments of federated learning settings (Apple, 2017). In the future, we intend to scale the algorithm to larger models and empirically
integrate it with faster training methods like Federated Averaging (McMahan et al., 2017).


-----

REFERENCES

Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
_conference on computer and communications security (CCS), pp. 308–318, 2016._

Annie Abay, Yi Zhou, Nathalie Baracaldo, Shashank Rajamoni, Ebube Chuba, and Heiko Ludwig.
Mitigating bias in federated learning. arXiv preprint arXiv:2012.02447, 2020.

Alekh Agarwal, Alina Beygelzimer, Miroslav Dud´ık, John Langford, and Hanna Wallach. A reductions approach to fair classification. In International Conference on Machine Learning, pp.
60–69. PMLR, 2018.

Differential Privacy Team Apple. Learning with privacy at scale. Technical report, Apple, 2017.

Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate
impact on model accuracy. Advances in Neural Information Processing Systems (NeurIPS), 32:
15479–15488, 2019.

Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, and Ryan Rogers. Protection against reconstruction and its applications in private federated learning. _arXiv preprint_
_arXiv:1812.00984, 2018._

Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacypreserving machine learning. In proceedings of the 2017 ACM SIGSAC Conference on Computer
_and Communications Security, pp. 1175–1191, 2017._

Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency, pp. 77–91.
PMLR, 2018.

Sebastian Caldas, Peter Wu, Tian Li, Jakub Koneˇcn`y, H Brendan McMahan, Virginia Smith, and
Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097,
[2018. URL https://arxiv.org/abs/1812.01097.](https://arxiv.org/abs/1812.01097)

Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from
language corpora contain human-like biases. Science, 356(6334):183–186, 2017.

Alessandro Castelnovo, Riccardo Crupi, Greta Greco, and Daniele Regoli. The zoo of fairness
metrics in machine learning. arXiv preprint arXiv:2106.00467, 2021.

Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist
to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp.
2921–2926. IEEE, 2017.

Sen Cui, Weishen Pan, Jian Liang, Changshui Zhang, and Fei Wang. Addressing algorithmic disparity and performance inconsistency in federated learning, 2021.

Rachel Cummings, Varun Gupta, Dhamma Kimpara, and Jamie Morgenstern. On the compatibility of privacy and fairness. In Adjunct Publication of the 27th Conference on User Modeling,
_Adaptation and Personalization, pp. 309–315, 2019._

Jiahao Ding, Xinyue Zhang, Xiaohuan Li, Junyi Wang, Rong Yu, and Miao Pan. Differentially
private and fair classification via calibrated functional mechanism. In Proceedings of the AAAI
_Conference on Artificial Intelligence, volume 34, pp. 622–629, 2020._

Wei Du, Depeng Xu, Xintao Wu, and Hanghang Tong. Fairness-aware agnostic federated learning.
In Proceedings of the 2021 SIAM International Conference on Data Mining (SDM), pp. 181–189.
SIAM, 2021.

[Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.](http://archive.ics.uci.edu/ml)
[ics.uci.edu/ml.](http://archive.ics.uci.edu/ml)


-----

Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Theory of cryptography conference, pp. 265–284. Springer, 2006.

Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found.
_Trends Theor. Comput. Sci., 9(3-4):211–407, 2014._

Ferdinando Fioretto, Pascal Van Hentenryck, Terrence WK Mak, Cuong Tran, Federico Baldo,
and Michele Lombardi. Lagrangian duality for constrained deep learning. _arXiv preprint_
_arXiv:2001.09394, 2020._

Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC confer_ence on computer and communications security, pp. 1322–1333, 2015._

Slawomir Goryczka and Li Xiong. A comprehensive comparison of multiparty secure additions
with differential privacy. IEEE Transactions on Dependable and Secure Computing, 2015. doi:
10.1109/TDSC.2015.2484326.

Filip Granqvist, Matt Seigel, Rogier van Dalen, Aine Cahill, Stephen Shum, and Matthias Paulik.[´]
Improving on-device speaker verification using federated learning with privacy. Interspeech, pp.
4328–4332, 2020.

Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances
_in neural information processing systems, 29:3315–3323, 2016._

Zeou Hu, Kiarash Shaloudegi, Guojun Zhang, and Yaoliang Yu. FedMGDA+: Federated learning
meets multi-objective optimization. arXiv preprint arXiv:2006.11489, 2020.

Wei Huang, Tianrui Li, Dexian Wang, Shengdong Du, and Junbo Zhang. Fairness and accuracy in
federated learning. arXiv preprint arXiv:2012.10069, 2020.

Matthew Jagielski, Michael Kearns, Jieming Mao, Alina Oprea, Aaron Roth, Saeed SharifiMalvajerdi, and Jonathan Ullman. Differentially private fair learning. In International Conference
_on Machine Learning (ICML), pp. 3000–3008. PMLR, 2019._

Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated
learning. arXiv preprint arXiv:1905.10497, 2019.

Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated
learning through personalization. In Proceedings of the 38th International Conference on Ma_chine Learning (ICML), pp. 6357–6368. PMLR, 2021._

Jingcheng Liu and Kunal Talwar. Private selection from private candidates. In Proceedings of the
_ACM SIGACT Symposium on Theory of Computing, 2019._

Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag¨uera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Proceedings of
_the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 1273–_
1282. PMLR, 2017.

H Brendan McMahan, Galen Andrew, Ulfar Erlingsson, Steve Chien, Ilya Mironov, Nicolas Papernot, and Peter Kairouz. A general approach to adding differential privacy to iterative training
procedures. arXiv preprint arXiv:1812.06210, 2018a.

H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private
recurrent language models. In International Conference on Learning Representations (ICLR),
2018b.

Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Interna_tional Conference on Machine Learning, pp. 4615–4625. PMLR, 2019._

John C Platt and Alan H Barr. Constrained differential optimization. In Proceedings of the 1987
_International Conference on Neural Information Processing Systems, pp. 612–621, 1987._


-----

Borja Rodr´ıguez-G´alvez, Ragnar Thobaben, and Mikael Skoglund. A variational approach to privacy and fairness. In 2nd Privacy Preserving Artificial Intelligence Workshop (PPAI) of the
_[AAAI Conference on Artificial Intelligence, 2021. URL https://arxiv.org/abs/2006.](https://arxiv.org/abs/2006.06332)_
[06332.](https://arxiv.org/abs/2006.06332)

Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP),
pp. 3–18. IEEE, 2017.

Cuong Tran, Ferdinando Fioretto, and Pascal Van Hentenryck. Differentially private and fair deep
learning: A Lagrangian dual approach. In Proceedings of the AAAI Conference on Artificial
_Intelligence, volume 35, pp. 9932–9939, 2021._

Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, and
Yi Zhou. A hybrid approach to privacy-preserving federated learning. In Proceedings of the 12th
_ACM Workshop on Artificial Intelligence and Security, pp. 1–11, 2019._

Sahil Verma and Julia Rubin. Fairness definitions explained. In 2018 IEEE/ACM International
_Workshop on Software Fairness (FairWare), pp. 1–7. IEEE, 2018._

Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St´efan J. van der
Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, [˙]Ilhan Polat, Yu Feng, Eric W. Moore,
Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,
Charles R. Harris, Anne M. Archibald, Antˆonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing
in Python. Nature Methods, 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2.

Yu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled R´enyi differential
privacy and analytical moments accountant. In Proceedings of the 22nd International Conference
_on Artificial Intelligence and Statistics (AISTATS), pp. 1226–1235. PMLR, 2019._

Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate
mistreatment. In Proceedings of the 26th international conference on world wide web, pp. 1171–
1180, 2017.


-----

A ADDITIONAL DETAILS ABOUT FPFL

A.1 DIFFERENTIABLE ESTIMATES OF THE FUNCTION AGGREGATES

As mentioned in Section 3.1, there are many situations of interest when the function f employed
to describe the fairness metric is not differentiable. In these cases, the gradient **_wF_** (d[′], w) does
_∇_
not exist and we resort to estimate the gradient **_wF_** (d[′], w) using a differentiable estimation of the
_∇_
function aggregate F (d[′], w). As an example, when trying to enforce FNR parity or accuracy parity
(as is the case in our experiments from Section 4 and Appendix B) the employed estimates are:

-  Enforcing FNR parity on a neural network ψw (with a Sigmoid output activation function)
in a binary classification task. We note that given an input xi, the raw output of the network
_ψw(xi) is an estimate of the probability that ˆyi = 1. Hence, the function aggregate can be_
estimated as


1

_n[′][ F]_ [(][d][′][,][ w][)][ ≈] _n[1][′]_


(1 − _ψw(xi)) ≈_ P[ Y[ˆ] = 0|Y = 1]. (13)
**_xXi∈d[′]_**



-  Enforcing accuracy parity on a neural network ψw (with softmax output activation function) in a multi-class classification task. We note that given an input xi, the raw jth output
of the network ψw(xi)j is an estimate of the probability that ˆyi = j. Hence, if yi the
one-hot encoding vector of yi, the function aggregate can be estimated as


1

_n[′][ F]_ [(][d][′][,][ w][)][ ≈] _n[1][′]_


**_xXi∈d[′]_** **_ψw(xi)[T]_** **_yi ≈_** P[ Y[ˆ] = Y ]. (14)


A.2 WHY IS FEDERATEDAVERAG I N G INCOMPATIBLE WITH FPFL?

The proposed algorithm extends the MMDM algorithm from Section 3.1 to FL adapting
FederatedSGD, where each user uses all their data, computes the necessary statistics for a model
update, and sends them to the third-party.

A natural question could be why is not FederatedAveraging adapted instead. That is, to
perform several stochastic gradient descent/ascent updates of the model’s parameters w and the
Lagrange multipliers λ, and send the difference of the updated and the original version, i.e. to send
the vector v[k] := [w[k] _−w, λ[k]_ _−λ]. This way, the size of the communicated vector would be reduced_
from (|A| + 1)p + 2|A| to just p + |A| and a larger part of the computation would be done locally,
increasing the convergence speed. Moreover, the clipping bound could be reduced, thus decreasing
the noise necessary for the DP guarantees.

Unfortunately, for the proposed MMDM algorithm, this option could lead to catastrophic effects.
Imagine for instance a situation where each user only has data points belonging to one group, say
_a = 0 or a = 1. Then in their local dataset the general population is equivalent to the population_
of their group and thus F (d[k][′], w) = F (d[k]a′, w), implying that locally g(w) = 0. Therefore, the
Lagrange multipliers will never be locally updated and the weights updates will be equivalent to
those updates without considering the fairness constraints. That is, using this approach one would
(i) recover the same algorithm than standard PFL and (ii) communicate a vector of size p + |A|
instead of size p.

A.3 THE ALGORITHM

The employed algorithm is detailed in algorithm 1. It is separated into three main parts:

1. Server: The server initializes the network w0 and Lagrange multiplier λ0 parameters, and
also calculates the noise scale σ using Wang et al. (2019)’s privacy analysis before training.
Then, for each iteration t it samples m users St and asks for the noisy aggregation of
their statistics vt. Then it updates the Lagrange multiplier λt and the network weights wt
following (8).

2. Virtual secure third-party (obtained via secure multi-party computation): The third
party asks for each users’ statistics v[k] and clips them with the clipping bound C. After


-----

that, it aggregates the clipped statistics and privatizes the aggregation with the Gaussian
mechanism using the previously calculated by the server noise scale σ.

3. Users: Each user calculates the relevant statistics, see (11), and sends them to the secure
virtual third-party.

**Algorithm 1: FPFL. The K users are indexed by k, m is the cohort size, T is the number of**
iterations, C is the clipping bound, and (ε, δ) are the DP parameters.
**Server Executes:**

**_w0, λ0_** InitializeParameters()
_σ ←_ CalculateNoiseScale ← (K, m, ε, δ, T )
**for each iteration t = 1, 2, . . ., T do**

_St_ (random set of m users)
**_wvλttt ← ← ←_** SecureAggregationUpdateMultiplierUpdateParameters(v((vtw,t w,t λ−t1−t, S)1)t, C, σ)
_←_
**end**


**SecureAggregation(w, S, C, σ):**

// Performed by the virtual secure third-party
**for each user k in S do**

**_v[k]_** _←_ UserStatistics(k, w)
**_v[k]_** _←_ **_v[k]_** _· min_ 1, _[C]/∥v[k]∥2_

**end**



**_v ←_** [P]k∈S **_[v][k][ +][ N]_** 0, C [2] _· σ[2][]_

**return v**

 

**UserStatistics(k, w):**


|Col1|// Performed by the user v ←h ∇wL(w, dk),F(w, d′k a) a∈A, ∇wF(w, d′k a) a∈A,n′k a a∈Ai return v|
|---|---|


B EXPERIMENTAL DETAILS AND ADDITIONAL EXPERIMENTS

B.1 DETAILS OF THE EXPERIMENTS AND ADDITIONAL EXPERIMENTS ON ADULT

B.1.1 DETAILS OF THE EXPERIMENTS

**Adult dataset, from the UCI Machine Learning Repository (Dua & Graff, 2017).** This dataset
consists of 32,561 training and 16,281 testing samples of demographic data from the US census.
Each datapoint contains various demographic attributes. Though the particular task, predicting individuals’ salary ranges, is not itself of interest, this dataset serves as a proxy for tasks with inequalities
in the data. A reason this dataset is often used in the literature on ML fairness is that the fraction
of individuals in the higher salary range is 30% for the men and only 10% for the women. The
experiments in this paper will aim to stop this imbalance from entering into the model by balancing
the false negative rate (Castelnovo et al., 2021).

**Federated Adult dataset.** To generate a version of the Adult dataset suitable for federated learning, it must be partitioned into individual contributions. In the experiments, differential privacy will
be guaranteed per contribution. In this paper, the number of datapoints per contribution is Poissondistributed with mean of 2. Therefore, the number of users is K ≈ _n/2 ≈_ 16, 280.5.

**Privacy and fairness parameters.** For all our experiments, we considered the privacy parameters
_ϵ = 2 and δ = 5 · 10[−][5]_ _< 1/K and the fairness tolerance α = 0.02._

**Data pre-processing.** The 7 categorical variables were one-hot encoded and the 6 numerical variables where normalized with the training mean and variance. There is an underlying assumption


-----

that these means and variances can be learned at a low privacy cost. Hence, to be precise, the private
models are (ϵ0 + 2, 5 · 10[−][5])-DP, where ϵ0 is a small constant representing the privacy budget for
learning said parameters for the normalization.

**Models considered.** We experimented with two different fully connected networks. The first network, from now on the shallow network, has one hidden layer with 10 hidden units and a ReLU
activation function. The second network, henceforth the deep network, has three hidden layers with
16, 8, and 8 hidden units respectively and all with ReLU activation functions. Both networks ended
with a fully connected layer to a final output unit with a Sigmoid activation function.

**Hyperparameters.** For all the experiments, the learning rate was η = 0.1 for the network parameters and γ = 0.01 for the Lagrange multipliers. The damping parameter was c = 2. The batch
size for the experiments learned centrally was nbatch = 400 and the cohort sized studied for the
federated experiments were m = 200 and m = 1, 000. Finally, the clipping bounds for the shallow
and the deep networks were, respectively and depending if the training algorithm was PFL or FPFL,
_C = 1.3 or C = 2 and C = 2 or C = 2.4. These hyper-parameters where not selected with a pri-_
vate hyper-parameter search and were just set as an exemplary configuration. For Tran et al. (2021),
we chose the multiplier threshold λmax = 0.05 by sweeping over multiple orders of magnitude and
finding the optimal accuracy and FNR gap. If one desires to find the best hyper-parameters, one can
do so at an additional privacy budget cost following e.g. Abadi et al. (2016, Appendix D), Liu &
Talwar (2019).

B.1.2 ADDITIONAL EXPERIMENTS

We replicated all the experiments with the shallow network from Section 4.1 with the deep network instead. The results for these experiments on the central and federated setting are displayed
in Table 4 and Table 5, respectively.

The results obtained with the deep network are almost identical to those of the shallow network in
the central setting without DP noise.

The first difference with the results for the shallow network is that the performance and fairness
of the deep network does not change much when going from the central to the federated setting
without DP noise nor clipping. The shallow network, however, becomes less fair under all the
metrics considered.

The results for federated learning with clipping do not differ much between the shallow and deep networks. In both cases we see how FederatedAveraging with clipping deteriorates the fairness
of the model with all the measures considered. Moreover, they also suggest how FPFL can mitigate
this problem, maintaining similar levels of accuracy (in this case, even higher) while keeping the
fairness below the fairness tolerance.

Another contrast with the shallow network appears in the comparison of algorithms with and without
differential privacy. With DP, training does not reliably converge. This is partly due to the fact that
the noise is large enough so that sign of the constraints’ gradient, see (9), is sometimes mistaken.

For this reason, we repeat the experiments with PFL and FPFL with a larger cohort size, m = 1, 000,
to see if a smaller relative noise would aid the training with PFL or with FPFL. The results with PFL
were almost identical, with similar levels of accuracy and unfairness. On the other hand, the larger
signal-to-DP noise ratio helped the models trained with FPFL to keep models with the desired levels
of FNR gap and lower unfairness measured with any other metric. Moreover, the accuracy of the
models, that now work better for the under-represented group, is in fact slightly higher than for the
models trained with PFL.

B.2 DETAILS OF THE EXPERIMENTS ON THE UNFAIR FEMNIST

**FEMNIST dataset (Caldas et al., 2018).** This dataset is an adaptation of the Extended MNIST
dataset (Cohen et al., 2017), which collects more than 800,000 samples of digits and letters distributed across 3,550 users. The task considered is to predict which of the 10 digits or 26 letters
(upper or lower case) is depicted in the image, so it is a multi-class classification with 62 possible
classes.


-----

Table 4: Performance of a deep and a shallow network on the Adult dataset when trained with
SGD and the MMDM algorithm. The fairness metric considered for MMDM is FNR parity and the
tolerance is α = 0.02. Tran et al. (2021) without privacy, and with λmax = 0.05.

|Model Algorithm|Accuracy FNR gap EO gap DemP gap PP gap|
|---|---|


|Shallow SGD Shallow Tran et al. (2021)* Shallow BMDM Shallow MMDM|0.857 0.071 0.071 0.113 0.016 0.856 0.048 0.048 0.108 0.036 0.857 0.001 0.030 0.094 0.046 0.856 0.005 0.028 0.091 0.063|
|---|---|


|Deep SGD Deep Tran et al. (2021)* Deep BMDM Deep MMDM|0.858 0.070 0.070 0.117 0.006 0.853 0.054 0.054 0.111 0.035 0.853 0.000 0.027 0.088 0.050 0.855 0.003 0.027 0.090 0.066|
|---|---|



Table 5: Performance of a deep and a shallow network on the Adult dataset when trained with
different algorithms: FederatedSGD without privacy, with norm clipping, and with DP, denoted
as FL, FL + Clip, and PFL respectively; and FPFL without privacy nor norm clipping, with norm
clipping only, and with DP, denoted as FFL, FFL + Clip, and FPFL respectively. The fairness metric
considered for FPFL is FNR parity and the tolerance is α = 0.02.

|Model Algorithm|Accuracy FNR gap EO gap DemP gap PP gap|
|---|---|



_m = 200_

|Shallow FL Shallow BMDM-FL Shallow FFL Deep FL Deep BMDM-FL Deep FFL|0.851 0.121 0.121 0.122 0.037 0.854 0.043 0.043 0.101 0.016 0.855 0.036 0.039 0.108 0.033 0.853 0.078 0.078 0.125 0.015 0.850 0.051 0.051 0.116 0.012 0.854 0.009 0.030 0.093 0.049|
|---|---|


|Shallow FL + Clip Shallow BMDM-FL + Clip Shallow FFL + Clip Deep FL + Clip Deep BMDM-FL + Clip Deep FFL + Clip|0.844 0.169 0.169 0.129 0.051 0.851 0.052 0.052 0.105 0.008 0.853 0.018 0.029 0.090 0.016 0.848 0.160 0.160 0.131 0.056 0.844 0.130 0.130 0.112 0.041 0.852 0.008 0.023 0.081 0.031|
|---|---|


|Shallow PFL Shallow BMDM-PFL Shallow FPFL Deep PFL Deep BMDM-PFL Deep FPFL|0.828 0.171 0.171 0.093 0.038 0.803 0.002 0.005 0.044 0.209 0.793 0.060 0.060 0.051 0.167 0.804 0.174 0.174 0.073 0.031 0.792 0.303 0.303 0.164 0.067 0.281 0.036 0.036 0.014 0.127|
|---|---|



_m = 1, 000_

|Shallow PFL Shallow BMDM-PFL Shallow FPFL Deep PFL Deep BMDM-PFL Deep FPFL|0.847 0.148 0.148 0.126 0.041 0.850 0.023 0.035 0.097 0.009 0.851 0.001 0.027 0.087 0.042 0.847 0.167 0.167 0.132 0.043 0.837 0.167 0.167 0.115 0.085 0.848 0.027 0.027 0.080 0.026|
|---|---|



**Unfair FEMNIST dataset.** We considered the FEMNIST dataset with only the digit samples.
This restriction consists of 3,383 users spanning 343,099 training and 39,606 testing samples. The
task now is to predict which of the 10 digits is depicted in the image, so it is a multi-class classification with 10 possible classes. Since the dataset does not contain clear sensitive groups, we artificially
create three classes (see Figure 1):


-----

-  Users that write with a black pen in a white sheet. These users represent the first (lexicographical) 45% of the users, i.e. ⌊0.45·3, 383⌋ = 1, 522 users. These users contain 146,554
(42.7%) training and 16,689 (42.1%) testing samples.
The images belonging to this group are unchanged.

-  Users that write with a blue pen in a white sheet. These users represent the second (lexicographical) 45% of the users, i.e. 1,522 users as well. These users contain 159,902 (46.6%)
training and 18,672 (47.1%) testing samples.
The images belonging to this group are modified making sure that the digit strokes are blue
instead of black.

-  Users that write with white chalk in a blackboard. These users represent the last remaining
10% of the users, i.e. 339 users. These users contain 36,643 (10.7%) training and 4,245
(10.7%) testing samples.
The images belonging to this group are modified making sure that the digit strokes are
white and the background is black. Moreover, to make the task more unfair, we simulated
the blurry effect that chalk leaves in a blackboard. With this purpose, we added Gaussian
blurred noise to the image, and then we blended them with further Gaussian blur. To be
precise, if x is the image normalized to [0, 1], the blackboard effect is the following.

_x ←_ (x + ξ ⊛ _κ2) ⊛_ _κ1,_ (15)

kernelswhere ξ[1]1 ∼Nwith standard deviation 1 and 2, respectively, and(0, I) is Gaussian noise of the size of the image, ⊛ represents the convolution κ1 and κ2 are Gaussian
operation. Moreover, the images are rotated 90 degrees, simulating how the pictures were
taken with the device in horizontal mode due to the usual shape of the blackboards.

**Privacy and fairness parameters.** For all our experiments, we considered the privacy parameters
_ϵ = 2 and δ = 2.5 · 10[−][4]_ _< 1/K. However, for the last experiment, we consider the hypothetical_
scenario where we had a larger number of users K ← 100K and K ← 1, 000K, thus decreasing
the privacy parameter to δ ← _δ/100 and δ ←_ _δ/1, 000 and reducing the added noise in the analysis_
from (Wang et al., 2019).

**Model considered.** We experimented with a network with 2 convolution layers with kernel of size
5 × 5, stride of 2, ReLU activation function, and 32 and 64 filters respectively. These layers are
followed by a fully connected layer with 100 hidden units and a ReLU activation function, and a
fully connected output layer with 10 hidden units and a Softmax activation function. From now on,
this model will be referred as the convolutional network.

**Hyperparameters.** For all the experiments the learning rate was η = 0.1 for the network parameters and γ = 0.05 for the Lagrange multipliers. The damping parameter was c = 20. Note that
we set a larger damping parameter to increase the strength to which we want to enforce the constraints, given that the task is harder than before. The cohort sizes considered were m = 100 and
_m = 2, 000. Finally, the clipping bound for the convolutional network was C = 250 if the training_
algorithm was PFL and C = 350 if it was FPFL. As previously, these hyper-parameters where not
selected with a private hyper-parameter search and were just set as an exemplary configuration. If
one desires to find the best hyper-parameters, one can do so at an additional privacy budget cost
following e.g. (Abadi et al., 2016, Appendix D).

1We used the Gaussian filter implementation from SciPy (Virtanen et al., 2020).


-----

