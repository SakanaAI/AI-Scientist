# f -DIVERGENCE THERMODYNAMIC VARIATIONAL OB## JECTIVE: A DEFORMED GEOMETRY PERSPECTIVE

**Anonymous authors**
Paper under double-blind review

ABSTRACT

In this paper, we propose a f -divergence Thermodynamic Variational Objective
(f -TVO). f -TVO generalizes the Thermodynamic Variational Objective (TVO)
by replacing Kullback–Leibler (KL) divergence with arbitary differeitiable f - 
divergence. In particular, f -TVO approximates dual function of model evidence
_f_ _[∗](p(x)) rather than the log model evidence log p(x) in TVO. f_ -TVO is derived
from a deformed χ-geometry perspective. By defining χ-exponential family exponential, we are able to integral f -TVO along the χ-path, which is the deformed
geodesic between variational posterior distribution and true posterior distribution.
Optimizing scheme of f -TVO includes reparameterization trick and Monte Carlo
approximation. Experiments on VAE and Bayesian neural network show that
the proposed f -TVO performs better than cooresponding baseline f -divergence
variational inference.

1 INTRODUCTION

Variational inference (VI) is a core technique in probabilistic machine learning (Mnih & Gregor,
2014; Mnih & Rezende, 2016; Tucker et al., 2017; Maddison et al., 2017; Paisley et al., 2012;
Salimans et al., 2013; Ranganath et al., 2014; Kucukelbir et al., 2015). Normal Variational inference
tries to perform a tractable posterior distribution approximation for log model evidence estimation
log p(x). Commonly, this is done by introducing a divergence D[q(z|x)||p(z|x)] between variational
distribution q(z|x) and true posterior p(z|x) as a regularization term. Then the evidence lower bound
(ELBO): log p(x) − _D[q(z|x)||p(z|x)] is used as a surrogate optimization objective for maximum_
likelihood parameter estimation (Hoffman & Johnson, 2016; Duan et al., 2017; Yang, 2017).

Recently, there are serveral directions that generalize normal variational inference. Interested readers
can refer to VI’s background and progression in (Blei et al., 2017; Zhang et al., 2018). One direction
is to explore tighter variational bounds (Burda et al., 2016; Chen et al., 2018; Masrani et al., 2019;
Brekelmans et al., 2020). Among these efforts, the recent Thermodynamic Variational Objective
(TVO) approach connects variational inference with thermodynamic integration. In paricular, TVO
interprets the log model evidence estimation as 1D integral along the geodesic path between joint
distribution of data and hidden variable p(x, z) and variational hidden variable distribution q(z|x).
By properly choosing intergral intervals of Riemannian sum approximations, the TVO approximates
the log likelihood with a series of upper and lower bounds, which are tighter surrogates than the
normal variational inference objectives for model parameter likelihood maximization (Masrani et al.,
2019).

Another direction is trying to extend the normal VI framework by replacing KL divergence with
other statistical divergences. Among these efforts, Renyi’s´ _α-divergence and χ-divergence can be_
regarded as the root divergences, which makes many existing divergence as special cases including
KL-divergence. Renyi’s´ _α-VI (Li & Turner, 2016) and χ-VI (Dieng et al., 2017) are stochastic_
VI that generalized from stochastic VI (Hoffman et al., 2013) and black-box VI (Ranganath et al.,
2014), and outperform the classical KL-VI in Bayesian regressions and image reconstruction on
some benchmarks. More generally, recent work f -divergence variational inference (f -VI) (Wan
et al., 2020) gives a general framework that includes all divergences from f -divergence family. The
variational objective of f -divergence VI can be regarded as the surrogate bound of dual function of
model evidence f _[∗](p(x))._


-----

Divergences _f_ (t) _f_ _[∗](t)_ Variational Bound

KL divergence _t log t_ _−_ log t ELBO
KL divergence _−_ log t _t log t_ EUBO
General χ[n] divergence _t[1][−][n]_ _−_ _t, n ∈_ R \ {0, 1} _t[n]_ _−_ 1 CUBOn
Total Varaition divergence _|t −_ 1| _|t −_ 1| TVD
R´enyi α divergence _Dα(q_ _p) = (α_ 1)[−][1] log[1 + (α 1) _α(q_ _p)]_ RVB
_||_ _−_ _−_ _H_ _||_

Table 1: Difference divergences and their corresponding f function. Renyi´ _α-divergence is not_
_f_ -divergence, but it can be formulated with Hellinger divergence _α(q_ _p), which is an f_ -divergence.
_H_ _||_

In this paper, we combine the above two directions together by proposing f -TVO that generalizes the
original TVO to arbitary f -divergence in a unified framework. f -TVO approximates dual function
of model evidence f _[∗](p(x)) rather than the log model evidence log p(x) in TVO. The estimation of_
_f_ -TVO is established from a deformed χ-geometry prospective. By defining χ-exponential family
distribution, the computation of f -TVO is integrated over the χ-path, which is the geodesic between
_p(x, z) and q(z|x) under χ-geometry. The proposed f_ -TVO is a tighter bound of dual function of
model evidence f _[∗](p(x)) rather than the normal variational lower bound in TVO._

Our contributions include the following:

– We enrich the TVO by proposing a unified f-TVO framework that compatible with an amount
of existing divergence, such as KL, α divergence, χ divergence;

– We derive a unified f-TVO bound equiped with the upper/lower bound criteria that improves
many existing bounds, such as ELBO, CUBO, RVB;

– Experiments show that our proposed f-TVO is a better optimization surrogate than corresponding f-VI.
The rest of the paper is organized as follows: Sec. 2 reviews preliminary knowledges for f -divergence
and Thermodynamic Variational Objective, Sec. 3 presents our f -TVO, Sec. 4 extends our approach
to more variants of f -TVO, and Sec. 5 presents our experiment results.

2 PRELIMINARY

In this section, we will review the preliminary knowledge for f -divergence and Thermodynamic
Variational Objective.

2.1 _f_ -DIVERGENCE

_f_ -divergence is a measures of difference between two probability distributions. It can be defined as
follows (Sason & Verd´u, 2016).

**Definition 1. Given a convex function f with f** (1) = 0, then f **-divergence from probability distri-**
_bution q(z) to p(z) is defined as:_

_Df_ (p _q) =_ _f_ ( _[q][(][z][)]_ (1)
_||_ _p(z)_ [)][p][(][z][)][dz][ =][ E][p][[][f] [(] _p[q][(]([z]z[)])_ [)]]
Z


Many existing well-know divergence can be regarded as special cases of f -divergence by properly
choose the generation function f . Table 1 shows the relationship between the sample divergences
and their corresponding generation function f .

Wan et al.(Wan et al., 2020) proposed a f -divergence variational inference (f -VI) that generalizes
normal variational inference to all f-divergences in a unified framework. f -VI framework is primarily
based on reverse f -divergence (Wan et al., 2020). One can connect forward f -divergence Df (p _q)_
_||_
with the reverse f -divergence Df (q _p) via dual function f_ as follows.
_||_ _[∗]_


-----

TVO log p(x)

ELBO


Figure 1: Thermodynamic Variational Objective (TVO). Left:evidence lower bound (ELBO); middle:
TVO is a Riemannian sum approximation of log p(x); right: log model evidence log p(x). TVO is a
tighter bound of log p(x) than ELBO.

**Definition 2. Given a function f, its dual funtion[1]** _f_ _[∗]_ _is defined as:_

_f_ _[∗](t) = t · f_ (1/t) (2)

The dual function f _[∗]_ has the following properties: (1)(f _[∗])[∗]_ = f, (2)f _[∗]_ is convex iff f is convex,
(3)f _[∗](1) = 0 iff f_ (1) = 0. f -divergence in equation 1 can be reformulated with the dual function
_f_ _[∗]:_

_Df_ (p _q) =_ _f_ ( _[q][(][z][)]_ _f_ ( _[p][(][z][)]_ (3)
_||_ _p(z)_ [)][p][(][z][)][dz][ =] _[∗]_ _q(z)_ [)][q][(][z][)][dz][ =][ D][f][ ∗] [(][q][||][p][)]
Z Z


As shown in the theorem below, f -variational bound is an approximation of the dual function f _[∗]_ of
the model evidence p(x):
**Theorem 1. The dual function of model evidence f** _[∗](p(x)) is bounded by the f_ _-variational bound_
_f_ (q, x)
_L_

_Lf_ (q, x) = Eq(z|x)[f _[∗](_ _[p]q[(]([z, x]z_ _x) [)]_ [)]][ ≥] _[f][ ∗][(][p][(][x][))]_ (4)

_|_

_where equality is obtained when q(z|x) = p(z|x)._

The proof of the theorem is straightforward as f _[∗]_ is convex (Wan et al., 2020).

Many well-known variation bounds can be represented in the f -VI framework. For example, f -VI
recovers to ELBO when f = t log t, and f -VI recovers to CUBOn when f = t[1][−][n] _−_ _t. More details_
are shown in the Table 1.

2.2 THERMODYNAMIC VARIATIONAL OBJECTIVE

The evidence lower bound (ELBO) is a lower bound of the log evidence of a generative model p(x, z).
It can be formulated as the difference between log evidence of model evidence log p(x) and KL
divergence of variational posterior distribution q(z|x) and true posterior distribution p(z|x):

ELBO = log p(x) _DKL(q(z_ _x)_ _p(z_ _x))_ (5)
_−_ _|_ _||_ _|_

The Thermodynamic Variational Objective (TVO) gives a tighter bound of log p(x) than ELBO. As
shown in Fig. 1, TVO is a Riemannian sum of integral along the geometric path between the model
and inference network:


1 _K−1_ 1

_K_ ELBO + _k=1_ Eπβk [log _[p]q[(]([x, z]z|x) [)]_ []] _≤_ Z0 Eπβ [log _[p]q[(]([x, z]z|x) [)]_ []][dβ][ = log][ p][(][x][)] (6)
  X 

TVO

1Note:dual function here is different from Fenchel conjugate function:| {z } _f c(y) = supx∈dom f_ (y⊤x − _f_ (x)).


-----

where β = [β0, β1, · · ·, βK] is a partition between [0,1], where β0 = 0, and βK = 1, πβ is
the normalized geometric combination of p(x, z) and q(z|x), i.e. ˜πβ = q(z|x)[1][−][β]p(x, z)[β] and
_πβ = ˜πβ/Zβ, where Zβ is a normalization constant for each β. One can verify that πβ is a member_
of exponential family distribution, no matter what q(z|x) and p(x, z) are:

_πβ(z_ _x) = π0(z_ _x) exp_ _β_ _T_ (x, z) _φ(x; β)_ (7)
_|_ _|_ _{_ _·_ _−_ _}_

where T (x, z) = log _[p]q([(]z[x,z]x)[)]_ [,][ φ][(][x][;][ β][)][ is the log partition function that normalize over][ z][.]

_|_

3 _χ-THERMODYNAMIC VARIATIONAL OBJECTIVE_

In this section, we establish our theory for f -Thermodynamic Variational Objective (f -TVO). We
first connect f -divergence with χ-geometry. Then, we present f -TVO as an integration along the
_χ-path. Lastly, we specify the computation process of f_ -TVO and its gradients.

3.1 _χ-LOGARITHM AND χ-EXPONENTIAL_

We first define a generalized χ-logarithm function for a positive non-increasing function χ:

_u_
logχ(u) = _χ(v)dv_ (8)

1

Z

which is different from the definition in (Naudts, 2011; Amari, 2016). Notice that equation 8 gives
normal log function log u when χ(v) = _v[1]_ [. The inverse of the][ χ][-logarithm is called][ χ][-exponential:]

_u_
expχ(u) = 1 + _ψ(v)dv_ (9)

0

Z

where ψ is an auxiliary function. One can determine ψ for each logχ(u), e.g. ψ(v) = exp(v) when
logχ(u) = log u. χ-Logarithm and χ-Exponential have the following properties:


(1) _du[d]_ [exp][χ][(][u][) =][ ψ][(][u][)][,] (2) u = expχ(logχ(u)), (3)


_χ(u) [=][ ψ][(log][χ][(][u][))][.]_


For any divergence with convex diffrentiable generation function f with f (0) = 1, there exists a
continuous positive non-increasing function χ that connects the dual function f _[∗]:_

_u_
_f_ _[∗](u) = −_ logχ(u) = − 1 _χ(v)dv_ (10)
Z

then f -divergence can be represented as

_Df_ (q _p) = Df ∗_ (p _q) =_ _p(z)_ logχ( _[q][(][z][)]_ (11)
_||_ _||_ _−_ _·_ _p(z)_ [)][dz]
Z


3.2 THERMODYNAMIC INTEGRATION

Using the χ-logarithm function and χ-exponential function defined in equation 8 and equation 9, we
can define a exponential family distribution under χ-geometry:

**Definition 3. χ-Exponential Family Distribution:**


_p(x, θ)_
logχ _c(x)_ = θ · x − Φ(θ) (12)

_p(x, θ) = c(x) expχ_ _θ_ **x** Φ(θ) (13)
_{_ _·_ _−_ _}_


_or equivalently:_


_where x is random variable, θ is parameter, Φ(θ) is the partition function, c(x) is the normalization_
_term._


-----

Given two unnormalized distribution ˜π0(z) and ˜π1(z), we define an unnormalized distribution that is
the log χ interpolation between ˜π0(z) and ˜π1(z):

logχ(˜πβ(z)) = (1 _β) logχ(˜π0(z)) + β logχ(˜π1(z))_ (14)
_−_

Its normalized distribution is defined as:

_πβ(z)_
_πβ(z) = [˜]_ (15)

_Zβ_

where Zβ = _π˜β(z)dz. It’s easy to show that: ˜πβ(z) is on the log χ geodesic between ˜π0(z) and_
_π˜1(z) with χ-Exponential Family Distribution as:_
R
**Theorem 2. πβ is also an χ-exponential family distribution, no matter ˜π0(z) and ˜π1(z).**

_Proof._

_π˜β(z) = expχ((1 −_ _β) logχ ˜π0(z) + β logχ ˜π1(z))_

= expχ(logχ ˜π0(z) + β(logχ ˜π1(z) − logχ ˜π0(z))) (16)


By defining the potential energy function: Uβ(z) = logχ ˜πβ(z), then we can estimate the logχ of the
ratio of the normalizing constants via thermodynamic integration:

1

_χ(Zβ)_

logχ Z1 logχ Z0 = _π0(z)_ logχ ˜π1(z))dzdβ (17)
_−_ 0 _χ(˜πβ(z))_ [(log][χ][ ˜] _−_
Z Z

Notice that variational inference can be connected with thermodynamic integration if we set:

_π˜0(z) = q(z|x),_ _Z0 =_ _q(z|x)dz = 1_
Z

_π˜1(z) = p(x, z),_ _Z1 =_ _p(x, z)dz = p(x)_ (18)
Z

then the logχ of model evidence is the thermodynamic integration (TI):

1
logχ p(x) = _Sβdβ_ (19)

0

Z

where Sβ = _χχ(˜π(βZ(βz)))_ [(log][χ][ q][(][z][|][x][)][ −] [log][χ][ p][(][x, z][)))][dz]

Recall the relation betweenR _f_ -divergence and χ function in equation 10, then logχ p(x) = f _[∗](p(x))_
is dual function f of the model evidence, which corresponds to the f -VI in (Wan et al., 2020).

To approximate f -TVO defined in equation 17, we pick the a list of ordered partitions β =

[Riemannian approximation of TI:β0, β1, · · ·, βK], where β0 ≤ _β1 ≤· · · ≤_ _βK, β0 = 0, βK = 1, then our f_ -TVO is the left


_f_ -TVO[L](K) =


∆kSβk−1 _,_ (20)
_k=1_

X


where ∆k = βk _βk_ 1.
_−_ _−_

3.3 COMPUTATION

In this section, we show how to efficiently compute the f -TVO.

We first rewrite Sβ in equation 17 into

_χ(Zβ)_
_Sβ =_ _π0(z)_ logχ ˜π1(z))dz

_χ(˜πβ(z))_ [(log][χ][ ˜] _−_

Z


= Eq(z)[ _[χ][(][Z][β][)]_ _π0(z)_ logχ ˜π1(z))/q(z _x)]_ (21)

_χ(˜πβ(z))_ [(log][χ][ ˜] _−_ _|_


-----

We apply the reparameterization trick to estimate Sβ. We assume there is a mapping gφ(ϵ) that
satisfies: z = gφ(ϵ). Then the expectation of arbitrary function F (z) over distribution q(z) can
be computed as Eq(z)[F (z)] = Eϵ[F (gφ(ϵ))]. In particular, we use the prevalent Gaussian repa1
rameterization: z ∼N (µ, Σ), i.e. z = µ + Σ 2 ϵ, ϵ ∼N (0, I). Then Sβ can be reformulated
as:

_χ(Zβ)_
_Sβ = Eϵ[_ (22)

_p(ϵ)χ(˜πβ(gφ(ϵ)))_ [(log][χ][ p][(][ϵ][)][ −] [log][χ][ p][(][x, g][φ][(][ϵ][)))]]

where we define q(z _x) = p(ϵ) and p(x, z) = p(x, gφ(ϵ)) in equation 19. Empirically, we can_
_|_
compute Sβ as:

_N_

_Sˆβ = [1]_ [wn(logχ p(ϵn) logχ p(x, gφ(ϵn)))] (23)

_N_ _−_

_n=1_

X

where wn = χ([P][N]n=1 _π[˜]β(gφ(ϵn)))/p(ϵn)χ(˜πβ(gφ(ϵn)))._

In practice, we split the computation of _S[ˆ]β into two parts:_ _S[ˆ]β = L −_ _R:_


_L = [1]_

_N_

_R = [1]_



[wn logχ p(ϵn)]
_n=1_

X

_N_

[wn logχ p(x, gφ(ϵn))] (24)
_n=1_

X


This is because we use log p(x, gφ(ϵn)) and log p(ϵn)) during our computation to avoid numerical
overflow or underflow. All the intermediate sum operations are performed by logsumexp operation
until we get log L and log R, then we use exponential operation to complete the computation.

4 GENERALIZING THERMODYNAMIC OBJECTIVES

Recall f -TVO in equation 20 is left Riemannian approximation of Thermodynamic integration.
Similarly, We can also have right Riemannian approximation of f -TI:


_f_ -TVO[R](K) =


∆kSβk (25)
_k=1_

X


Both left and right Riemannian approximation corresponds to existing variational bounds. For
example, left single Riemannian approximation of the KL-TVO equals the ELBO, while the right
single Riemannian approximation equals to the EUBO. Also, it is easy to verify if the dual function
_f_ _[∗]_ is increasing (decreasing), then left Riemannian approximation is an upper (lower) bound of
Thermodynamic integration, while right Riemannian approximation is a lower (upper) bound of
Thermodynamic integration.

In addition to the left or right Riemannian approximation of Thermodynamic integration, we can
also define a “zig-zag” (z) approximation of Thermodynamic integration, which combines left and
right Riemannian approximation. As shown in FIg. 2, the “zig-zag” Thermodynamic objective can be
formulated as:


_f_ -TVO[Z] =


∆k[1(k%2 = 1)Sβk−1 + 1(k%2 = 0)Sβk ] (26)
_k=1_

X


where 1(A) is indicator function, i.e. 1(A) = 1 when condition A is true, 1(A) = 0 when condition
_A is false._

5 EXPERIMENTS

In this section, we evaluate the effectiveness and the wide applicability of f -TVO on two tasks.
_f_ -TVO is first used as a surrogate optimization objective of f -VI in a Bayesian neural network for
linear regression, then used in a VAE for image reconstruction and generation. For both tasks, we use
Adam as our optimizer with recommended parameters in (Da, 2014). For all experiments, we use
even partition of the temperature for Riemannian integration.


-----

(a) !-TVO[%] (b) !-TVO[&] (c) !-TVO[']


Figure 2: Variants of f -Thermodynamic Variational Objective (f -TVO). (a)Left Riemannian approximation; (b) Right Riemannian approximation; (c)zig-zag Riemannian approximation.

Dataset KL-VI KL-TVO _χ-VI_ _χ-TVO_ _α-VI_ _α-TVO_ _fc1-VI_ _fc1-TVO_

Caltech 101 73.72 **73.51** 73.84 73.61 74.95 74.23 74.90 74.62
Frey Face 160.85 160.58 160.61 **160.32** 161.06 160.85 160.73 160.35
MNIST 59.03 **58.89** 62.15 61.77 61.88 61.42 59.47 59.21
Omniglot 109.65 109.52 110.57 109.89 110.75 110.12 108.31 **108.09**

Table 2: Test reconstruction errors of f -TVO VAEs. Lower is better. In most cases, f -TVO
outperform corresponding f -VI.

5.1 BAYESIAN VARIATIONAL AUTOENCODER

_f_ -TVO is used in Bayesian VAE for image reconstruction task on datasets including Caltech 101
Silhouettes (Cal), Frey Face (Fre), MNIST (MNI), and Omniglot (Omn). We have replaced the
conventional ELBO loss function of VAE with the proposed f -TVOs that correspond to flexible f - 
divergences. We test and compare the f -TVO VAEs associated with three well-known f -divergences
(KL-divergence, Renyi’s α-divergence with α = 3, and χ-divergence with n = 2). We also try a
customized divergence which is defined by its dual function fc[∗]1[(][t][) = log][2][ t][ + log][ t][.]

We use the left Riemannian approximation with 5 partitions, i.e. f -TVO[L](5) in our setting. As shown
in Table 2, the proposed f -TVOs outperforms the corresponding baselines in most cases.

**Effects of numbers of partitions. Here, we study the effects of different numbers of partitions on**
Bayesian variational autoencoder in our experiments. We try partition numbers K = 1, 2, 5, 10 in
the f -TVO, respectively. As shown in Fig. 3, we can see that even partition number is K = 2 can
improve the baseline performance.

**Effects of f** **-TVO variants. We also study the effects of f** -TVO variants defined in Sec. 4. We try all
3 variants f -TVO[L],f -TVO[R] and f -TVO[Z] on Bayesian variational autoencoder in our experiments.
As shown in Fig. 4, three f -TVO variants achieve similar performance.

5.2 BAYESIAN NEURAL NETWORKS

Our experiments of Bayesian neural network are performed on regression tasks. We generally follow
the experiments settings as in (Wan et al., 2020). In particular, We use twelve datasets that are
collected from the UCI dataset repository, where each dataset is randomly split into 90%/10% for
training and testing. The model is a single-layer neural network with 50 hidden units (ReLUs). We
use θ ∼N (θ; 0, I) as a Gaussian prior of the network weights and Gaussian approximation to the
true posterior.

In the experiments, we try different f -TVOs, including corresponds to KL TVO, α-divergence
TVO, where α = 3, χ-square divergence TVO. Following (Wan et al., 2020), we also define a
custom divergence, which is defined by its dual function fc2 = f[˜][∗](t) − _f[˜][∗](1), where_ _f[˜][∗](t) =_


-----

(a) (b)

(c) (d)

Figure 3: Effect of partition numbers. We evaluate f -TVO with partition numbers [1,2,5,10] on
datasets (a)Caltech (b) Frey Face (c) MNIST (d) Omniglot.

Dataset KL-VI KL-TVO _χ-VI_ _χ-TVO_ _α-VI_ _α-TVO_ _fc2-VI_ _fc2-TVO_

Airfoil 2.18 2.07 2.36 **1.96** 2.43 2.14 2.35 2.20
Aquatic 1.17 1.11 1.25 1.12 1.16 1.11 1.14 **1.05**
Boston 2.88 **2.69** 2.99 2.80 2.89 2.73 2.86 2.73
Building 1.56 **1.38** 2.81 2.36 1.87 1.64 1.85 1.65
CCPP 4.19 3.96 4.26 3.96 4.20 **3.86** 4.35 4.08
Concrete 5.40 5.38 3.51 **3.37** 5.40 5.24 5.26 5.11
Fish Toxicity 0.95 0.88 0.92 0.89 0.91 0.86 0.89 **0.85**
Protein 1.96 1.91 2.46 2.24 1.85 **1.74** 2.00 1.89
Real Estate 7.53 7.39 7.51 **7.33** 7.50 7.40 7.62 7.45
Stock 3.95 3.78 3.93 **3.74** 3.99 3.84 3.95 3.81
Wine .658 .648 .647 .639 .643 **.635** .665 .649
Yacht 0.79 **0.76** 1.23 1.23 1.05 0.95 1.22 1.18

Table 3: Test RMSE of Bayesian Neural Network for f -VIs and f -TVOs. Lower is better. In most
cases, f -TVOs outperform corresponding f -VIs.

1/6 (log t + t0)[3] 1/2 (log t + t0) 1. We use the left Riemannian approximation with 5
_−_ _·_ _−_ _·_ _−_
partitions, i.e. f -TVO[L](5) in our setting. As shown in Table 3, the proposed f -TVOs outperform the
corresponding baselines.

6 CONCLUSION

In this paper, we have proposed a general f -TVO framework that is a tighter evidence bounds
than f -VI. f -TVO unifies all differential f -divergence into TVO from a χ-geometry perspective.
By connecting f -divergence with χ-geometry, f -TVO is the Riemanian sum of thermodynamic


-----

(a) (b)

(c) (d)

Figure 4: Effect of f -TVO variants. We evaluate f -TVO with all 3 variants: left riemannian
approximation, right Riemannian approximation, “zig-zag” Riemannian approximation, on datasets
(a)Caltech (b) Frey Face (c) MNIST (d) Omniglot.

Dataset KL-VI KL-TVO _χ-VI_ _χ-TVO_ _α-VI_ _α-TVO_ _fc2-VI_ _fc2-TVO_

Airfoil 2.22 2.14 2.30 **2.02** 2.40 2.21 2.29 2.16
Aquatic 1.57 1.52 1.59 1.47 1.54 1.48 1.54 **1.39**
Boston 2.49 **2.32** 2.54 2.40 2.48 2.36 2.49 2.38
Building 6.90 6.71 6.85 6.75 6.79 **6.56** 6.74 6.58
CCPP 2.84 2.65 2.93 2.76 2.84 **2.57** 2.99 2.80
Concrete 3.23 3.02 2.75 **2.51** 3.12 2.87 3.10 2.83
Fish Toxicity 1.35 1.36 1.27 **1.25** 1.31 1.26 1.32 1.27
Protein 2.12 1.91 2.11 1.97 2.14 **1.90** 2.25 2.06
Real Estate 3.55 3.40 3.69 3.45 3.59 **3.38** 3.65 3.51
Stock -1.08 -1.12 -1.08 -1.06 -1.10 -1.08 -1.14 **-1.18**
Wine .975 .974 .970 .966 .971 **.965** .978 .970
Yacht 1.72 **1.68** 1.88 1.82 1.87 1.87 2.02 1.95

Table 4: Test negative log-likelihood of Bayesian Neural Network for f -VIs and f -TVOs. Lower is
better. In most cases, f -TVOs outperform corresponding f -VIs.

integration along the geodesic between q(z|x) and p(x, z) under χ-geometry. Empirical experiments
of some instances of f -TVO on the popular benchmarks show the superior performance than stateof-the art results, which imply the flexibility and effectiveness of f -TVO. Future work on f -TVO
may include more efficient f -TVO optimization methods, and more explorations on the properties of
_χ-geometry._


-----

REFERENCES

Caltech 101 silhouettes dataset. [https://people.cs.umass.edu/˜marlin/data.](https://people.cs.umass.edu/~marlin/data.shtml)
[shtml.](https://people.cs.umass.edu/~marlin/data.shtml)

[Frey face dataset. https://cs.nyu.edu/˜roweis/data.html.](https://cs.nyu.edu/~roweis/data.html)

[Mnist dataset. http://yann.lecun.com/exdb/mnist/.](http://yann.lecun.com/exdb/mnist/)

Omniglot dataset. [https://github.com/yburda/iwae/tree/master/datasets/](https://github.com/yburda/iwae/tree/master/datasets/OMNIGLOT)
[OMNIGLOT.](https://github.com/yburda/iwae/tree/master/datasets/OMNIGLOT)

Shun-ichi Amari. Information geometry and its applications, volume 194. Springer, 2016.

David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.
_Journal of the American statistical Association, 112(518):859–877, 2017._

Rob Brekelmans, Vaden Masrani, Frank Wood, Greg Ver Steeg, and Aram Galstyan. All in the
exponential family: Bregman duality in thermodynamic variational inference. In International
_Conference on Machine Learning, pp. 1111–1122. PMLR, 2020._

Yuri Burda, Roger B Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In ICLR
_(Poster), 2016._

Liqun Chen, Chenyang Tao, Ruiyi Zhang, Ricardo Henao, and Lawrence Carin Duke. Variational
inference and model selection with generalized evidence bounds. In International conference on
_machine learning, pp. 893–902. PMLR, 2018._

Kingma Da. A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

Adji B Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, and David M Blei. Variational inference
via x upper bound minimization. In Proceedings of the 31st International Conference on Neural
_Information Processing Systems, pp. 2729–2738, 2017._

Huiping Duan, Linxiao Yang, Jun Fang, and Hongbin Li. Fast inverse-free sparse bayesian learning
via relaxed evidence lower bound maximization. IEEE Signal Processing Letters, 24(6):774–778,
2017.

Matthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another way to carve up the
variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference,
_NIPS, volume 1, pp. 2, 2016._

Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference.
_Journal of Machine Learning Research, 14(5), 2013._

Alp Kucukelbir, Rajesh Ranganath, Andrew Gelman, and David M Blei. Automatic variational
inference in stan. arXiv preprint arXiv:1506.03431, 2015.

Yingzhen Li and Richard E Turner. R\’enyi divergence variational inference. arXiv preprint
_arXiv:1602.02311, 2016._

Chris J Maddison, Dieterich Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy
Mnih, Arnaud Doucet, and Yee Whye Teh. Filtering variational objectives. arXiv preprint
_arXiv:1705.09279, 2017._

Vaden W Masrani, Tuan Anh Le, and Frank Wood. The thermodynamic variational objective. 2019.

Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In
_International Conference on Machine Learning, pp. 1791–1799. PMLR, 2014._

Andriy Mnih and Danilo Rezende. Variational inference for monte carlo objectives. In International
_Conference on Machine Learning, pp. 2188–2196. PMLR, 2016._

Jan Naudts. Generalised thermostatistics. Springer Science & Business Media, 2011.


-----

John Paisley, David Blei, and Michael Jordan. Variational bayesian inference with stochastic search.
_arXiv preprint arXiv:1206.6430, 2012._

Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial
_intelligence and statistics, pp. 814–822. PMLR, 2014._

Tim Salimans, David A Knowles, et al. Fixed-form variational posterior approximation through
stochastic linear regression. Bayesian Analysis, 8(4):837–882, 2013.

Igal Sason and Sergio Verd´u. f -divergence inequalities. IEEE Transactions on Information Theory,
62(11):5973–6006, 2016.

George Tucker, Andriy Mnih, Chris J Maddison, Dieterich Lawson, and Jascha Sohl-Dickstein. Rebar:
Low-variance, unbiased gradient estimates for discrete latent variable models. arXiv preprint
_arXiv:1703.07370, 2017._

Neng Wan, Dapeng Li, and Naira Hovakimyan. f-divergence variational inference. Advances in
_Neural Information Processing Systems, 33, 2020._

Xitong Yang. Understanding the variational lower bound, 2017.

Cheng Zhang, Judith Butepage, Hedvig Kjellstr¨ om, and Stephan Mandt. Advances in variational¨
inference. IEEE transactions on pattern analysis and machine intelligence, 41(8):2008–2026,
2018.


-----

