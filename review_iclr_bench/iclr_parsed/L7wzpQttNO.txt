# BDDM: BILATERAL DENOISING DIFFUSION MODELS
## FOR FAST AND HIGH-QUALITY SPEECH SYNTHESIS


**Max W. Y. Lam, Jun Wang, Dan Su**
Tencent AI Lab
Shenzhen, China
_{maxwylam, joinerwang, dansu}@tencent.com_

ABSTRACT


**Dong Yu**
Tencent AI Lab
Bellevue WA, USA
dyu@tencent.com


Diffusion probabilistic models (DPMs) and their extensions have emerged as competitive generative models yet confront challenges of efficient sampling. We
propose a new bilateral denoising diffusion model (BDDM) that parameterizes
both the forward and reverse processes with a schedule network and a score
network, which can train with a novel bilateral modeling objective. We show
that the new surrogate objective can achieve a lower bound of the log marginal
likelihood tighter than a conventional surrogate. We also find that BDDM allows inheriting pre-trained score network parameters from any DPMs and consequently enables speedy and stable learning of the schedule network and optimization of a noise schedule for sampling. Our experiments demonstrate that
BDDMs can generate high-fidelity audio samples with as few as three sampling steps. Moreover, compared to other state-of-the-art diffusion-based neural vocoders, BDDMs produce comparable or higher quality samples indistinguishable from human speech, notably with only seven sampling steps (143x
faster than WaveGrad and 28.6x faster than DiffWave). We release our code at
[https://github.com/tencent-ailab/bddm.](https://github.com/tencent-ailab/bddm)

1 INTRODUCTION

Deep generative models have shown a tremendous advancement in speech synthesis (van den Oord
et al., 2016; Kalchbrenner et al., 2018; Prenger et al., 2019; Kumar et al., 2019; Kong et al., 2020b;
Chen et al., 2020; Kong et al., 2021). Successful generative models can be mainly divided into two
categories: generative adversarial network (GAN) (Goodfellow et al., 2014) based and likelihoodbased. The former is based on adversarial learning, where the objective is to generate data indistinguishable from the training data. Yet, the training GANs can be very unstable, and the relevant training objectives are not suitable to compare against different GANs. The latter uses log-likelihood or
surrogate objectives for training, but they also have intrinsic limitations regarding generation speed
or quality. For example, the autoregressive models (van den Oord et al., 2016; Kalchbrenner et al.,
2018), while being capable of generating high-fidelity data, are limited by their inherently slow
sampling process and the poor scaling properties on high-dimensional data. Likewise, the flowbased models (Dinh et al., 2016; Kingma & Dhariwal, 2018; Chen et al., 2018; Papamakarios et al.,
2021) rely on specialized architectures to build a normalized probability model, whose training is
less parameter-efficient. Other prior works use surrogate objectives, such as the evidence lower
bound in variational auto-encoders (Kingma & Welling, 2014; Rezende et al., 2014; MaalÃ¸e et al.,
2019) and the contrastive divergence in energy-based models (Hinton, 2002; Carreira-Perpinan &
Hinton, 2005). These models, despite showing improved speed, typically only work well for lowdimensional data, and, in general, the sample qualities are not competitive to the GAN-based and
the autoregressive models (Bond-Taylor et al., 2021).

An up-and-coming class of likelihood-based models is the diffusion probabilistic models (DPMs)
(Sohl-Dickstein et al., 2015), which introduces the idea of using a forward diffusion process to
sequentially corrupt a given distribution and learning the reversal of such diffusion process to restore
the data distribution for sampling. From a similar perspective, Song & Ermon (2019) proposed
the score-based generative models by applying the score matching technique (Hyvarinen & Dayan,


-----

2005) to train a neural network such that samples can be generated via Langevin dynamics. Along
these two lines of research, Ho et al. (2020) proposed the denoising diffusion probabilistic models
(DDPMs) for high-quality image syntheses. Dhariwal & Nichol (2021) demonstrated that improved
DDPMs Nichol & Dhariwal (2021) are capable of generating high-quality images of comparable
or even superior quality to the state-of-the-art (SOTA) GAN-based models. For speech syntheses,
DDPMs were also applied in Wavegrad (Chen et al., 2020) and DiffWave (Kong et al., 2021) to
produce higher-fidelity audio samples than the conventional non-autoregressive models (Yamamoto
et al., 2020; Kumar et al., 2019; Yang et al., 2021; BiÂ´nkowski et al., 2020) and matched the quality
of the SOTA autoregressive methods (Chen et al., 2020).

Despite the compelling results, the diffusion generative models are two to three orders of magnitude
slower than other generative models such as GANs and VAEs. Their primary limitation is that they
require up to thousands of diffusion steps during training to learn the target distribution. Therefore a
large number of reverse steps are often required at sampling time. Recently, extensive investigations
have been conducted to reduce the sampling steps for efficiently generating high-quality samples,
which we will discuss in the related work in Section 2. Distinctively, we conceived that we might
train a neural network to efficiently and adaptively estimate a much shorter noise schedule for sampling while achieving generation performances comparable or superior to the conventional DPMs.
With such an incentive, after introducing the conventional DPMs as our background in Section 3, we
propose in Section 4 bilateral denoising diffusion models (BDDMs), named after a bilateral modeling perspective â€“ parameterizing the forward and reverse processes with a schedule network and
a score network, respectively. We theoretically derive that the schedule network should be trained
after the score network is optimized. For training the schedule network, we propose a novel objective to minimize the gap between a newly derived lower bound and the log marginal likelihood. We
describe the training algorithm as well as the fast and high-quality sampling algorithm in Section 5.
The training of the schedule network converges very fast using our newly derived objective, and its
training only adds negligible overhead to DDPMâ€™s. In Section 6, our neural vocoding experiments
demonstrated that BDDMs could generate high-fidelity samples with as few as three sampling steps.
Moreover, our method can produce speech samples indistinguishable from human speech with only
seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave).

2 RELATED WORK

Prior works showed that noise scheduling is crucial for efficient and high-fidelity data generation in
DPMs. DDPMs (Ho et al., 2020) used a shared linear noise schedule for both training and sampling,
which, however, requires thousands of sampling iterations to obtain competitive results. To speed
up the sampling process, one class of related work, including (Chen et al., 2020; Kong et al., 2021;
Nichol & Dhariwal, 2021), attempts to use a different, shorter noise schedule for sampling. For
clarity, we thereafter denote the training noise schedule as Î² âˆˆ R[T] and the sampling noise schedule
as **_Î²[Ë†] âˆˆ_** R[N] with N < T . In particular, Chen et al. (2020) applied a grid search (GS) algorithm to
select **_Î²[Ë†]. Unfortunately, GS becomes prohibitively slow when N grows large, e.g., N = 6 took more_**
than a day on a single NVIDIA Tesla P40 GPU. This is because the time costs of GS algorithm grow
exponentially with N, i.e., O(9[N] ) with 9 bins as the default setting in Chen et al. (2020). Instead
of searching, Kong et al. (2021) devised a fast sampling (FS) algorithm based on an expert-defined
6-step noise schedule for their score network. However, this specifically tuned noise schedule is
hard to generalize to other score networks, tasks, or datasets.

Another class of noise scheduling methods searches for a subsequence of time indices of the training
noise schedule, which we call the time schedule. DDIMs (Song et al., 2021) introduced an accelerated reverse process that relies on a pre-specified time schedule. A linear and a quadratic time
schedule were used in DDIMs and showed superior generation quality over DDPMs within 10 to
100 sampling steps. Nichol & Dhariwal (2021) proposed a re-scaled noise schedule for fast sampling, but this also requires pre-specifying the time schedule and the training noise schedule. Nichol
& Dhariwal (2021) also proposed learning variances for the reverse processes, whereas the variances
of the forward processes, i.e., the noise schedule, which affected both the means and variances of
the reverse processes, were not learnable. According to the results of (Song et al., 2021; Nichol &
Dhariwal, 2021), using a linear or quadratic time schedule resulted in quite different performances
in different datasets, implying that the optimal choice of schedule varies with the datasets. So, there
remains a challenge in finding a short and effective schedule for fast sampling on different datasets.


-----

Notably, Kong & Ping (2021) proposed a method to map a noise schedule to a time schedule for
fast sampling. In this sense, searching for a time schedule becomes a sub-set of the noise scheduling
problem, which resembles the above category of methods.

Although DPMs (Sohl-Dickstein et al., 2015) and DDPMs (Ho et al., 2019) mentioned that the noise
schedule could be learned by re-parameterization, the approach was not investigated in their works.
Closely related works that learn a noise schedule emerged until very recently. San-Roman et al.
(2021) proposed a noise estimation (NE) method, which trained a neural net with a regression loss
to estimate the noise scale from the noisy sample at each time point, and then predicted the next
noise scale. However, NE requires a prior assumption of the noise schedule following a linear or
Fibonacci rule. Most recently, a concurrent work to ours by Kingma et al. (2021) jointly trained a
neural net to predict the signal-to-noise ratio (SNR) by maximizing the variational lower bound. The
SNR was then used for noise scheduling. Different from ours, this scheduling neural net only took
_t as input and is independent of the noisy sample generated during the loop of sampling process._
Intrinsically, with limited information about the sampled data, the predicted SNR could deviate from
the actual SNR of the noisy data during sampling.

3 BACKGROUND

3.1 DIFFUSION PROBABILISTIC MODELS (DPMS)

abilistic models (DPMs) (Sohl-Dickstein et al., 2015) define a forward processGiven i.i.d. samples {x0 âˆˆ R[D]} from an unknown data distribution pdata(x0), diffusion prob- q(x1:T **_x0) =_**
_T_ _|_
_t=1_ _[q][(][x][t][|][x][t][âˆ’][1][)][ that converts any complex data distribution into a simple, tractable distribution af-]_
terQ _T steps of diffusion. A reverse process pÎ¸(xtâˆ’1|xt) parameterized by Î¸ is used to model the data_
distribution: pÎ¸(x0) = _Ï€(xT )_ _t=1_ _[p][Î¸][(][x][t][âˆ’][1][|][x][t][)][d][x][1:][T][, where][ Ï€][(][x][T][ )][ is the prior distribution for]_
starting the reverse process. Then, the variational parameters Î¸ can be learned by maximizing the
R
standard log evidence lower bound (ELBO):

[Q][T]


_Felbo := Eq_


log pÎ¸(x0 **_x1)_**
_|_ _âˆ’_


_DKL (q(xt_ 1 **_xt, x0)_** _pÎ¸(xt_ 1 **_xt))_** _DKL (q(xT_ **_x0)_** _Ï€(xT ))_
_t=2_ _âˆ’_ _|_ _||_ _âˆ’_ _|_ _âˆ’_ _|_ _||_

X

(1)


3.2 DENOISING DIFFUSION PROBABILISTIC MODELS (DDPMS)

As an extension to DPMs, denoising diffusion probabilistic models (DDPMs) (Ho et al., 2020)
applied the score matching technique (Hyvarinen & Dayan, 2005; Song & Ermon, 2019) to define
the reverse process. In particular, DDPMs considered a Gaussian diffusion process parameterized
by a noise schedule Î² âˆˆ R[T] with 0 < Î²1, . . ., Î²T < 1:


_qÎ²t_ (xt|xtâˆ’1), where _qÎ²t_ (xt|xtâˆ’1) := N (
_t=1_

Y


1 âˆ’ _Î²txtâˆ’1, Î²tI)._ (2)


_qÎ²(x1:T_ **_x0) :=_**
_|_


Based on the nice property of isotropic Gaussians, one can express xt directly conditioned on x0:


_qÎ²(xt|x0) = N_ (Î±tx0, (1 âˆ’ _Î±t[2][)][I][)][,]_ where _Î±t =_


1 _Î²i._ (3)
_âˆ’_


_i=1_


To revert this forward process, DDPMs employ a score network[1] **_ÏµÎ¸(xt, Î±t) to define_**


_Î²t_
**_xt_** **_ÏµÎ¸ (xt, Î±t)_**
_âˆ’_ 1 _Î±t[2]_

_âˆ’_

p


(4)


_pÎ¸(xtâˆ’1|xt) := N_


_âˆš1_ _Î²t_
_âˆ’_


_, Î£t_


1Here, ÏµÎ¸(xt, Î±t) is conditioned on the continuous noise scale Î±t, as in (Song et al., 2020b; Chen et al.,
2020). Alternatively, the score network can also be conditioned on a discrete time index ÏµÎ¸(xt, t), as in (Song
et al., 2021; Ho et al., 2020). An approximate mapping of a noise schedule to a time schedule (Kong & Ping,
2021) exists, therefore we consider conditioning on noise scales as the general case.


-----

Figure 1: A bilateral denoising diffusion model (BDDM) introduces a junctional variable xt and a
schedule network Ï†. The schedule network can optimize the shortened noise schedule _Î²[Ë†]n(Ï†) if we_
know the score of the distribution at the junctional step, using the KL divergence to directly compare
_pÎ¸âˆ—_ (Ë†xnâˆ’1|xË†n = xt) against the re-parameterized forward process posteriors.

where Î£t is the co-variance matrix defined for the reverse process. Ho et al. (2020) showed that

1 _Î±[2]t_ 1
setting Î£t = Î²[Ëœ]tI = _âˆ’1âˆ’Î±âˆ’[2]t_ _[Î²][t][I][ is optimal for a deterministic][ x][0][, while setting][ Î£][t][ =][ Î²][t][I][ is optimal]_

for a white noise x0 (0, I). Alternatively, Nichol & Dhariwal (2021) proposed learnable
variances by interpolating the two optimals with a jointly trained neural network, i.e., âˆ¼N Î£t,Î¸(x) :=
diag(exp(vÎ¸(x) log Î²t + (1 âˆ’ **_vÎ¸(x)) log Î²[Ëœ]t)), where vÎ¸(x) âˆˆ_** R[D] is a trainable network.

Note that the calculation of the complete ELBO in Eq. (1) requires T forward passes of the score
network, which would make the training computationally prohibitive for a large T . To feasibly
train the score network, instead of computing the complete ELBO, Ho et al. (2020) proposed an
efficient training mechanism by sampling from a discrete uniform distribution: t âˆ¼U{1, ..., T _},_
**_x0 âˆ¼_** _pdata(x0), Ïµt âˆ¼N_ (0, I) at each training iteration to compute the training loss:

2

ddpm[(][Î¸][) :=] _Î±tx0 +_ 1 _Î±t[2][Ïµ][t][, Î±][t]_ _,_ (5)
_L[(][t][)]_ _âˆ’_ 2

 q 

which is a re-weighted form of DKL (q[Ïµ]Î²[t][ âˆ’](xt[Ïµ]âˆ’[Î¸]1|xt, x0)||pÎ¸(xtâˆ’1|xt)). Ho et al. (2020) reported that
the re-weighting worked effectively for learning Î¸. Yet, we demonstrate it is deficient for learning
the noise schedule Î² in our ablation experiment in Section 6.2.

4 BILATERAL DENOISING DIFFUSION MODELS (BDDMS)

4.1 PROBLEM FORMULATION


For fast sampling with DPMs, we strive for a noise schedule **_Î²[Ë†] for sampling that is much shorter_**
than the noise schedule Î² for training. As shown in Fig. 1, we define two separate diffusion
processes corresponding to the noise schedules, Î² and **_Î²[Ë†], respectively. The upper diffusion pro-_**
cess parameterized by Î² is the same as in Eq. (2), whereas the lower process is defined as
_q Ë†Î²[(Ë†]x1:N_ _|xË†0) =_ _n=1_ _[q][ Ë†]Î²n_ [(Ë†]xn|xË†nâˆ’1) with much fewer diffusion steps (N â‰ª _T_ ). In our prob
lem formulation, Î² is given, but **_Î²[Ë†] is unknown. The goal is to find a_** **_Î²[Ë†] for the reverse process_**
_pÎ¸(Ë†xn_ 1 **_xË†n; Î²[Ë†]n)[Q] such that[N]_** Ë†x0 can be effectively recovered from Ë†xN with N reverse steps.
_âˆ’_ _|_

4.2 MODEL DESCRIPTION

Although many prior arts (Ho et al., 2020; Chen et al., 2020; Song et al., 2021; San-Roman et al.,
2021) directly applied a shortened linear or Fibonacci noise schedule to the reverse process, we
argue that these are sub-optimal solutions. Theoretically, the diffusion process specified by a new
shortened noise schedule is essentially different from the one used to train the score network Î¸.
Therefore, Î¸ is not guaranteed suitable for reverting the shortened diffusion process. This issue
motivated a novel modeling perspective to establish a link between the shortened schedule **_Î²[Ë†] and_**
the score network Î¸, i.e., to have **_Î²[Ë†] optimized according to Î¸._**


-----

As a starting point, we consider an N = âŒŠT/Ï„ _âŒ‹, where 1 â‰¤_ _Ï„ < T is a hyperparameter controlling_
the step size such that each diffusion step between two consecutive variables in the shorter diffusion
process corresponds to Ï„ diffusion steps in the longer one. Based on Eq. (2), we define the following:


_Î±t[2]+Ï„_ **_xt,_** 1 _t+Ï„_

_Î±t[2]_ _âˆ’_ _[Î±]Î±[2]_ _t[2]_



_q Ë†Î²n+1_ [(Ë†]xn+1 **_xË†n = xt) := qÎ²(xt+Ï„_** **_xt) =_** _Î±t+Ï„_ **_xt,_** 1 _t+Ï„_ **_I_** _,_ (6)
_|_ _|_ _N_ ï£«s _Î±t[2]_ _âˆ’_ _[Î±]Î±t[2]_ ï£¶

 

ï£­ ï£¸

where xt is an intermediate diffused variable we introduced to link the two differently indexed
diffusion sequences. We call it a junctional variable, which can be easily generated given x0 and Î²
during training: xt = Î±tx0 + 1 âˆ’ _Î±t[2][Ïµ][n][.]_

Unfortunately, for the reverse process whenp **_x0 is not given, the junctional variable is intractable._**
However, our key observation is that while using the score by a score network Î¸[âˆ—] trained for the
long Î²-parameterized diffusion process, a short noise schedule **_Î²[Ë†](Ï†) can be optimized accordingly_**
by introducing a schedule network Ï†. We provide its mathematical derivations in Appendix A.3.
Next, we present a formal definition of BDDM and derive its training objectives, Lscore[(][n][)] [(][Î¸][)][ and]
_Lstep[(][n][)][(][Ï†][;][ Î¸][âˆ—][)][, for the score network and the schedule network, respectively, in more detail.]_

4.3 SCORE NETWORK

Recall that a DDPM starts the reverse process with a white noise xT (0, I) and takes T steps
to recover the data distribution: _âˆ¼N_

DDPM
_pÎ¸(x0)_ := EN (0,I) EpÎ¸(x1:T âˆ’1|xT ) [pÎ¸(x0|x1:T )] _._ (7)

A BDDM, in contrast, starts from the junctional variable xt, and reverts a shorter sequence of
diffusion random variables with only n steps:

BDDM
_pÎ¸(Ë†x0)_ := Eq Ë†Î²[(Ë†]xnâˆ’1;xt,Ïµn) EpÎ¸(Ë†x1:nâˆ’2|xË†nâˆ’1) [[][p]Î¸[(Ë†]x0|xË†1:nâˆ’1)] _,_ 2 â‰¤ _n â‰¤_ _N,_ (8)

where q Ë†Î²[(Ë†]xn 1; xt, Ïµn) is defined as a re-parameterization on the posterior: 
_âˆ’_

1 _Î±Ë†n[2]_ **_[Ïµ][n]_**

_q Ë†Î²[(Ë†]xn_ 1; xt, Ïµn) :=q Ë†Î² **_xË†n_** 1 **_xn = xt, Ë†x0 =_** **_[x][t][ âˆ’]_** _âˆ’_ (9)
_âˆ’_ _âˆ’_ pÎ±Ë†n !

= 1 [Ë†] **_xt_** _Î²Ë†n_ **_Ïµn,_** [1][ âˆ’] _Î±[Ë†]n[2]_ _âˆ’1_ _Î²Ë†nI_ _,_ (10)
_N_ ï£« _âˆ’_ 1 _Î±Ë†n[2]_ ï£¶

1 âˆ’ _Î²[Ë†]n_ (1 âˆ’ _Î²[Ë†]n)(1 âˆ’_ _Î±Ë†n[2]_ [)] _âˆ’_

ï£­ q q ï£¸

where Ë†Î±n = _i=1_ 1 _Î²i, xt = Î±tx0 +_ 1 _Î±t[2][Ïµ][n]_ [is the][ junctional][ variable that maps][ x][t]

_âˆ’_ [Ë†] _âˆ’_

to Ë†xn given an approximate indexq _t_ (n 1)Ï„, ..., nÏ„ 1, nÏ„ and a sampled white noise
_âˆ¼U{_ p âˆ’ _âˆ’_ _}_
**_Ïµn_** (0, I)[Q]. Detailed derivation from Eq. (9) to (10) is provided in Appendix A.2.[n]
_âˆ¼N_

4.3.1 TRAINING OBJECTIVE FOR SCORE NETWORK

With the above definition, a new form of lower bound to the log marginal likelihood can be derived
such that log pÎ¸(Ë†x0) â‰¥Fscore[(][n][)] [(][Î¸][) :=][ âˆ’L][(]score[n][)] [(][Î¸][)][ âˆ’R]Î¸[(Ë†]x0, xt), where

score[(][Î¸][) :=][D][KL] _pÎ¸(Ë†xn_ 1 **_xË†n = xt)_** _q Ë†Î²[(Ë†]xn_ 1; xt, Ïµn) _,_ (11)
_L[(][n][)]_ _âˆ’_ _|_ _||_ _âˆ’_

_Î¸(Ë†x0, xt) :=_ EpÎ¸(Ë†x1 **_xË†n=xt)_** [[log][ p]Î¸[(Ë†]x0 **_xË†1)] ._**  (12)
_R_ _âˆ’_ _|_ _|_

See detailed derivation in Proposition 1 in Appendix A.2. In the following Proposition 2, we prove
that via the junctional variable xt, the solution Î¸[âˆ—] for optimizing the objective L[(]ddpm[t][)] [(][Î¸][)][,][ âˆ€][t][ âˆˆ]

_{1, ..., T_ _} is also the solution for optimizing Lscore[(][n][)]_ [(][Î¸][)][,][ âˆ€][n][ âˆˆ{][2][, ..., N] _[}][. Thereby, we show that the]_
score network Î¸ can be trained with Lddpm[(][t][)] [(][Î¸][)][ and re-used for reverting the short diffusion process]
over Ë†xN :0. Although the newly derived lower bound result in the same objective as the conventional
score network, it for the first time establishes a link between the score network Î¸ and Ë†xN :0. The
connection is essential for learning **_Î²[Ë†], which we will describe next._**


**_x[Ë†]n = xt, Ë†x0 =_** **_[x][t][ âˆ’]_**


1 _Î±Ë†n[2]_ **_[Ïµ][n]_**
_âˆ’_

_Î±Ë†n_

p


-----

4.4 SCHEDULE NETWORK

In BDDMs, a schedule network is introduced to the forward process by re-parameterizing _Î²[Ë†]n as_
_Î²Ë†n(Ï†) = fÏ†_ **_xt; Î²[Ë†]n+1_**, and recall that during training, we can use xt = Î±tx0 + 1 _Î±t[2][Ïµ][n]_ [and]

_âˆ’_

_Î²Ë†n+1 = 1 âˆ’Î±Î±[2]t+[2]t_ _Ï„_ [. Through the re-parameterization, the task of noise scheduling, i.e., searching] p

for **_Î²[Ë†], can now be reformulated as training a schedule network fÏ† that ancestrally estimates data-_**
dependent variances. The schedule network learns to predict _Î²[Ë†]n based on the current noisy sample_
**_xt â€“ this makes our method fundamentally different from existing and concurrent work, includ-_**
ing Kingma et al. (2021) â€“ as we reveal that, aside from _Î²[Ë†]n+1, t, or n that reflects diffusion step_
information, xt is also essential for noise scheduling from a reverse direction at inference time.

Specifically, we adopt the ancestral step information (Î²[Ë†]n+1) to derive an upper bound for the current
step while leaving the schedule network only to take the current noisy sample xt as input to predict
a relative change of noise scales against the ancestral step. First, we derive an upper bound of _Î²[Ë†]n_
by proving 0 < _Î²[Ë†]n < min_ 1 âˆ’ 1âˆ’Î±Ë†[2]nÎ²[Ë†]+1n+1 _[,][ Ë†]Î²n+1_ in Appendix A.1. Then, by multiplying the upper

bound by a ratio estimated by a neural networkn _ÏƒoÏ† : R[D]_ _7â†’_ (0, 1), we define

_fÏ†(xt; Î²[Ë†]n+1) := min_ 1 _Î±Ë†n[2]_ +1 _,_ _Î²[Ë†]n+1_ _ÏƒÏ†(xt),_ (13)
_âˆ’_ 1 _Î²n+1_
 _âˆ’_ [Ë†] 

where the network parameter set Ï† is learned to estimate the ratio between two consecutive noise
scales (Î²[Ë†]n and _Î²[Ë†]n+1) from the current noisy input xt._

Finally, at inference time for noise scheduling, starting from a maximum reverse steps (N ) and two
hyperparameters (Ë†Î±N _,_ _Î²[Ë†]N_ ), we ancestrally predict the noise scale _Î²[Ë†]n(Ï†) = fÏ†_ **_xË†n; Î²[Ë†]n+1_**, for n

_Î±Ë†n+1_
from N to 1, and cumulatively update the product Ë†Î±n =  
_âˆš1âˆ’Î²[Ë†]n+1_ [.]

4.4.1 TRAINING OBJECTIVE FOR SCHEDULE NETWORK

Here we describe how to learn the network parameters Ï† effectively. First, we demonstrated that
_Ï† should be trained after Î¸ is well-optimized, referring to Proposition 3 in Appendix A.3. The_
Proposition also shows that we are minimizing the gap between the lower bound Fscore[(][n][)] [(][Î¸][âˆ—][)][ and]
log pÎ¸âˆ— (Ë†x0), i.e., log pÎ¸âˆ— (Ë†x0) score[(][Î¸][âˆ—][)][, by minimizing the following objective]
_âˆ’F_ [(][n][)]

_Lstep[(][n][)][(][Ï†][;][ Î¸][âˆ—][) :=][D][KL]_ _pÎ¸âˆ—_ (Ë†xnâˆ’1|xË†n = xt)||q Ë†Î²n(Ï†)[(Ë†]xnâˆ’1; x0, Î±t) _,_ (14)
 

which is defined as a KL divergence to directly compare pÎ¸âˆ— (Ë†xnâˆ’1|xË†n = xt) against the reparameterized forward process posteriors, which are tractable when conditioned on the junctional
noise scale Î±t and x0.

The detailed derivation of Eq. (14) is also provided in the proof of Proposition 3 to get its concrete
formulas as shown in Step (8-10) in Alg. 2.

5 ALGORITHMS: TRAINING, NOISE SCHEDULING, AND SAMPLING

5.1 TRAINING SCORE AND SCHEDULE NETWORKS

Following the theoretical result in Appendix A.3, Î¸ should be optimized before learning Ï†. Thereby
first, to train the score network ÏµÎ¸, we refer to the settings in (Ho et al., 2020; Chen et al., 2020; Song
et al., 2021) to define Î² as a linear noise schedule: Î²t = Î²start + _T[t]_ [(][Î²][end][ âˆ’] _[Î²][start][)][,]_ for 1 â‰¤ _t â‰¤_ _T,_

where Î²start and Î²end are two hyperparameter that specifies the start value and the end value. This
results in Algorithm 1, which resembles the training algorithm in (Ho et al., 2020).

Next, based on the converged score network Î¸[âˆ—], we train the schedule network Ï†. We draw an
_n âˆ¼U{2, . . ., N_ _} at each training step, and then draw a t âˆ¼U{(n âˆ’_ 1)Ï„, ..., nÏ„ _}. These together_
can be re-formulated as directly drawing t âˆ¼U{Ï„, ..., T âˆ’ _Ï„_ _} for a finer-scale time step. Then, we_


-----

**Algorithm 1 Training Score Network (Î¸)**

1: Given T, _Î²t_ _t=1_
_{_ _}[T]_ _T_

2: _Î±t_ _t=1_ [=][ {][Q][t]i=1 _âˆš1_ _Î²t_ _t=1_
_{_ _}[T]_ _âˆ’_ _}_

3: repeat
4:5: **_xt âˆ¼U{0 âˆ¼_** _p1data, . . ., T(x0)_ _}_

6: **_Ïµt âˆ¼N_** (0, I)

7: **_xt = Î±tx0 +_** 1 _Î±t[2][Ïµ][t]_

_âˆ’_

8: _L[(]ddpm[t][)]_ [=][ âˆ¥][Ïµ][t][ âˆ’]p[Ïµ][Î¸][(][x][t][, Î±][t][)][âˆ¥]2[2]

9: Take a gradient descent step on _Î¸_ ddpm
_âˆ‡_ _L[(][t][)]_

10: until converged

**Algorithm 3 Noise Scheduling**


**Algorithm 2 Training Schedule Network (Ï†)**


1: Given Î¸[âˆ—], Ï„, T, _Î±t, Î²t_ _t=1_
_{_ _}[T]_

2: repeat
3:4: **_xt âˆ¼U{0 âˆ¼_** _pÏ„, . . ., Tdata(x0)_ _âˆ’_ _Ï„_ _}_

5: _Î´t = 1 âˆ’_ _Î±t[2]_

6: **_Ïµn âˆ¼N_** (0, I)

7: **_xt = Î±tx0 +_** _Î´tÏµn_

8: _Î²Ë†n = min_ _Î´t[âˆš], 1 âˆ’_ _Î±Î±[2]t+[2]t_ _Ï„_



_ÏƒÏ†(xt)_


10:9: _CL = 4step[(][n][)]_ [=][âˆ’][1]2(log(Î´tÎ´âˆ’t _Î²Î´[Ë†]nt/)Î²[Ë†]nÏµ) + 2n âˆ’_ _Î²[âˆ’]Ë†Î´nt[1][Ïµ]D[Î¸][âˆ—][(]Î²[x]Ë†n[t]/Î´[, Î±]t[t] âˆ’[)]_ 221[+][ C]

11: Take a gradient descent step on _Ï†_ step
_âˆ‡_ _L[(][n][)]_

12: until converged
**Algorithm 4 Sampling**

1: Given Î¸[âˆ—], {Î²[Ë†]n}n[N]=1[s] _[,][ Ë†]xNs âˆ¼N_ (0N, Is )

2: {Î±Ë†n}n[N]=1[s] [=] _ni=1_ 1 âˆ’ _Î²[Ë†]n_ _n=1_

q 

3: for n = Ns toQ 1 do
4:5: end forxË†nâˆ’1 âˆ¼ _pÎ¸[âˆ—]_ (Ë†xnâˆ’1|xË†n; Ë†Î±n, _Î²[Ë†]n)_

6: return Ë†x0


2:1: Given for n = Î¸[âˆ—] N, Ë†Î± toN 2, _Î²[Ë†] doN_ _, xN âˆ¼N_ (0, I)
3:4: **_xÎ±Ë†Ë†nnâˆ’11 = âˆ¼_** _pÎ¸[âˆ—]Î±Ë†(Ë†nxnâˆ’1|xË†n; Ë†Î±n,_ _Î²[Ë†]n)_

_âˆ’_ _âˆš1âˆ’Î²[Ë†]n_

5: _Î²Ë†n_ 1 = min 1 _Î±Ë†n[2]_ 1[,][ Ë†]Î²n _ÏƒÏ†(Ë†xn_ 1)
_âˆ’_ _{_ _âˆ’_ _âˆ’_ _}_ _âˆ’_

6: **if** _Î²[Ë†]nâˆ’1 < Î²1 then_

7: **return** _Î²[Ë†]n, . . .,_ _Î²[Ë†]N_

8: **end if**

9: end for

10: return _Î²[Ë†]1, . . .,_ _Î²[Ë†]N_


sequentially compute the variables needed for calculating Lstep[(][n][)][(][Ï†][;][ Î¸][âˆ—][)][, as presented in Algorithm 2.]
We observed that, although a linear schedule is used to define Î², the noise schedule of **_Î²[Ë†] predicted_**
by fÏ† is not limited to but rather different from a linear one.

5.2 NOISE SCHEDULING FOR FAST AND HIGH-QUALITY SAMPLING

After the score network and the schedule network are trained, the inference procedure can divide
into two phases: (1) the noise scheduling phase and (2) the sampling phase.

First, we run the noise scheduling process similarly to a sampling process with N iterations maximum. Different from training, where Î±t is forward-computed, Ë†Î±n is instead a backward-computed
variable (from N to 1) that may deviate from the forward one because _Î²i_ _i_ are unknown in the
_{_ [Ë†] _}[n][âˆ’][1]_
noise scheduling phase during inference. To start noise scheduling, we first set two hyperparameters: Ë†Î±N and _Î²[Ë†]N_ . We use Î²1, the smallest noise scale seen in training, as a threshold to early stop
the noise scheduling process so that we can ignore small noise scales (< Î²1) that were never seen
by the score network. Overall, the noise scheduling process presents in Algorithm 3.

In practice, we apply a grid search algorithm of M bins to Algorithm 3, which takes O(M [2]) time, to
find proper values for (Ë†Î±N _,_ _Î²[Ë†]N_ ). We used M = 9 as in (Chen et al., 2020). The grid search for our
noise scheduling algorithm can be evaluated on a small subset of the training samples. Empirically,
even as few as 1 sample for evaluation works well in our algorithm. Finally, given the predicted
noise schedule **_Î²[Ë†] âˆˆ_** R[N][s], we generate samples with Ns sampling steps, as shown in Algorithm 4.

6 EXPERIMENTS

We conducted a series of experiments on neural vocoding tasks to evaluate the proposed BDDMs.
First, we compared BDDMs against several strongest models that have been published: the mixture
of logistics (MoL) WaveNet (Oord et al., 2018) implemented in (Yamamoto, 2020), the WaveGlow
(Prenger et al., 2019) implemented in (Valle, 2020), the MelGAN (Kumar et al., 2019) implemented
in (Kumar, 2019), the HiFi-GAN (Kong et al., 2020b) implemented in (Kong et al., 2020a) and
the two most recently proposed diffusion-based vocoders, i.e., WaveGrad (Chen et al., 2020) and
DiffWave (Kong et al., 2021), both re-implemented in our code. The hyperparameter settings of
BDDMs and all these models are detailed in Appendix B.

In addition, we also compared BDDMs to a variety of scheduling and acceleration techniques applicable to DDPMs, including the grid search (GS) approach in WaveGrad, the fast sampling (FS)


-----

Table 1: Comparison of neural vocoders in terms of MOS with 95% confidence intervals, real-time
factor (RTF) and model size in megabytes (MB) for inference. The highest score and the scores that
are not significantly different from the highest score (p-values â‰¥ 0.05) are bold-faced.

**Neural Vocoder** **MOS** **RTF** **Size**

Ground-truth 4.64 Â± 0.08 â€” â€”

WaveNet (MoL) (Oord et al., 2018) 3.52 Â± 0.16 318.6 282MB
WaveGlow (Prenger et al., 2019) 3.03 Â± 0.15 0.0198 645MB
MelGAN (Kumar et al., 2019) 3.48 Â± 0.14 0.00396 17MB
HiFi-GAN (Kong et al., 2020b) 4.33 Â± 0.12 0.0134 54MB
WaveGrad - 1000 steps (Chen et al., 2020) 4.36 Â± 0.13 38.2 183MB
DiffWave - 200 steps (Kong et al., 2021) **4.49 Â± 0.13** 7.30 27MB

BDDM - 3 steps (Î±Ë†N = 0.68, _Î²[Ë†]N = 0.53)_ 3.64 Â± 0.13 0.110 27MB
BDDM - 7 steps (Î±Ë†N = 0.62, _Î²[Ë†]N = 0.42)_ **4.43 Â± 0.11** 0.256 27MB
BDDM - 12 steps (Î±Ë†N = 0.67, _Î²[Ë†]N = 0.12)_ **4.48 Â± 0.12** 0.438 27MB


Table 2: Comparison of sampling acceleration methods with the same score network and the same
number of steps. The highest score and the scores that are not significantly different from the highest
score (p-values â‰¥ 0.05) are bold-faced.

**Steps** **Acceleration Method** **STOI** **PESQ** **MOS**

GS (Chen et al., 2020) **0.965 Â± 0.009** **3.66 Â± 0.20** **3.61 Â± 0.12**
FS (Kong et al., 2021) 0.939 Â± 0.023 3.09 Â± 0.23 3.10 Â± 0.12

3 DDIM (Song et al., 2021) 0.943 Â± 0.015 3.42 Â± 0.27 3.25 Â± 0.13

NE (San-Roman et al., 2021) **0.966 Â± 0.010** **3.62 Â± 0.18** 3.55 Â± 0.12
BDDM **0.966 Â± 0.011** **3.63 Â± 0.24** **3.64 Â± 0.13**

FS (Kong et al., 2021) **0.981 Â± 0.006** 3.68 Â± 0.24 3.70 Â± 0.14
DDIM (Song et al., 2021) 0.974 0.008 3.85 0.12 3.94 0.12

7 _Â±_ _Â±_ _Â±_

NE (San-Roman et al., 2021) 0.978 Â± 0.007 3.75 Â± 0.18 4.02 Â± 0.11
BDDM **0.983 Â± 0.006** **3.96 Â± 0.09** **4.43 Â± 0.11**

DDIM (Song et al., 2021) 0.979 Â± 0.006 3.90 Â± 0.10 4.16 Â± 0.12
12 NE (San-Roman et al., 2021) 0.981 Â± 0.007 3.82 Â± 0.13 3.98 Â± 0.14
BDDM **0.987 Â± 0.006** **3.98 Â± 0.12** **4.48 Â± 0.12**


approach based on a user-defined 6-step schedule in DiffWave, the DDIMs (Song et al., 2021) and
a noise estimation (NE) approach (San-Roman et al., 2021). For fair and reproducible comparison
with other models and approaches, we used the LJSpeech dataset (Ito & Johnson, 2017), which
consists of 13,100 22kHz audio clips of a female speaker. All diffusion models were trained on the
same training split as in (Chen et al., 2020). We also replicated the comparative experiment of neural
vocoding using a multi-speaker VCTK dataset (Yamagishi et al., 2019) as presented in Appendix C
and obtained a result consistent with that obtained from the LJSpeech dataset.

6.1 SAMPLING QUALITY IN OBJECTIVE AND SUBJECTIVE METRICS

To assess the quality of each generated audio sample, we used both objective and subjective measures for comparing different neural vocoders given the same ground-truth spectrogram s as the
condition, i.e., ÏµÎ¸(x, s, Î±t). Specifically, we used two scale-invariant metrics: the perceptual evaluation of speech quality (PESQ) (Rix et al., 2001) and the short-time objective intelligibility (STOI)
(Taal et al., 2010) to measure the noisiness and the distortion of the generated speech relative to the
reference speech. Mean opinion score (MOS) was also used as a subjective metric for evaluating the
naturalness of the generated speech. The assessment scheme of MOS is included in Appendix B.

In Table 1, we compared BDDMs against the state-of-the-art (SOTA) vocoders. To predict noise
schedules with different sampling steps (3, 7, and 12), we set three pairs of _Î±Ë†N_ _,_ _Î²[Ë†]N_ for BDDMs
_{_ _}_
by running on Algorithm 3 a quick hyperparameter grid search, which is detailed in Appendix B.
Among the 9 evaluated vocoders, only our proposed BDDMs with 7 and 12 steps and DiffWave with
200 steps showed no statistic-significant difference from the ground-truth in terms of MOS. Moreover, BDDMs significantly outspeeded DiffWave in terms of RTFs. Notably, previous diffusion

-----

Figure 2: Different training losses for ÏƒÏ† Figure 3: Different lower bounds to log pÎ¸(x0)

based vocoders achieved high MOS scores at the cost of an unacceptable RTF for industrial deployment. In contrast, BDDMs managed to achieve a high standard of generation quality with only 7
sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave).

In Table 2, we evaluated BDDMs and alternative accelerated sampling methods, which used the
same score network for a pair-to-pair comparison. The GS method performed stably when the
step number was small (i.e., N â‰¤ 6) but not scalable to more step numbers, which were therefore
bypassed in the comparisons of 7 and 12 steps. The FS method by Song et al. (2021) was linearly
interpolated to 3 and 7 steps for a fair comparison. Comparing its 7-step and 3-step results, we
observed that the FS performance degraded drastically. Both the DDIM and the NE methods were
stable across all the steps but were not performing competitively enough. In comparison, BDDMs
consistently attained the leading scores across all the steps. This evaluation confirmed that BDDM
was superior to other acceleration methods for DPMs in terms of both stability and quality.

6.2 ABLATION STUDY AND ANALYSIS

We attribute the primary advantage of BDDMs to the newly derived objective Lstep[(][n][)] [for learning][ Ï†][.]
To better reason about this, we performed an ablation study, where we substituted the proposed loss
with the standard negative ELBO for learning Ï† as mentioned by Sohl-Dickstein et al. (2015). We
plotted the network outputs with different training losses in Fig. 2. It turned out that, when using
_Lelbo[(][n][)]_ [to learn][ Ï†][, the network output rapidly collapsed to zero within several training steps; whereas,]
the network trained with Lstep[(][n][)] [produced fluctuating outputs. The fluctuation is a desirable property]
showing the network properly predicts t-dependent noise scales, as t is a random time step drawn
from a uniform distribution in training.

By settingvalues at t âˆˆÎ²[Ë†] =[20 Î²,, we empirically validated that 180] using the same optimized F Î¸bddm[(][t][âˆ—][)]. Each value is provided with 95% confidence[:=][ F]score[(][t][)] [+][L][(]step[t][)] _[â‰¥F]elbo[(][t][)]_ [with their respective]
intervals, as shown in Fig. 3. In this experiment, we used the LJ speech dataset and set T = 200 and
_Ï„ = 20. Notably, we dropped their common entropy term_ _Î¸(Ë†x0, xt) < 0 to mainly compare their_
_R_
KL divergences. This explains those positive lower bound values in the plot. The graph shows that
our proposed bound Fbddm[(][t][)] [is always a tighter lower bound than the standard one across all examined]
_t. Moreover, we found that Fbddm[(][t][)]_ [attained low values with a relatively much lower variance for]
_t â‰¤_ 50, where Felbo[(][t][)] [was highly volatile. This implies that][ F]bddm[(][t][)] [better tackles the difficult training]
part, i.e., when the score becomes more challenging to estimate as t â†’ 0.

7 CONCLUSIONS

BDDMs parameterize the forward and reverse processes with a schedule network and a score network, of which the formerâ€™s optimization is tied with the latter by introducing a junctional variable.
We derived a new lower bound that leads to the same training loss for the score network as in DDPMs
(Ho et al., 2020), which thus enables inheriting any pre-trained score networks in DDPMs. We also
showed that training the schedule network after a well-optimized score network can be viewed as
tightening the lower bound. Followed from the theoretical results, an efficient training algorithm and
a noise scheduling algorithm were respectively designed for BDDMs. Finally, in our experiments,
BDDMs showed a clear edge over the previous diffusion-based vocoders.


-----

REFERENCES

MikoÅ‚aj BiÂ´nkowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman
Casagrande, Luis C. Cobo, and Karen Simonyan. High fidelity speech synthesis with adversarial
networks. International conference on learning representations, 2020.

S Bond-Taylor, A Leach, Y Long, and CG Willcocks. Deep generative modelling: A comparative
review of vaes, gans, normalizing flows, energy-based and autoregressive models. IEEE Transac_tions on Pattern Analysis and Machine Intelligence, 2021._

Miguel A Carreira-Perpinan and Geoffrey E Hinton. On contrastive divergence learning. AISTATS,
pp. 33â€“40, 2005.

Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In International conference on learning
_representations, 2020._

Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, pp. 6571â€“6583, 2018.

Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances
_in neural information processing systems, 34, 2021._

Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
_preprint arXiv:1605.08803, 2016._

Brendan J Frey. Local probability propagation for factor analysis. Advances in neural information
_processing systems, 12:442â€“448, 1999._

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information
_processing systems, 27, 2014._

G. E. Hinton. Training products of experts by minimizing contrastive divergence. neural computation. Neural computation, pp. 14(8):1771â€“1800, 2002.

Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flowbased generative models with variational dequantization and architecture design. ICML, 2019.

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
_neural information processing systems, 33:6840â€“6851, 2020._

Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, pp. 6(4), 2005.

Keith Ito and Linda Johnson. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017.

Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural
audio synthesis. In International Conference on Machine Learning, pp. 2410â€“2419. PMLR, 2018.

Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.
_Advances in neural information processing systems, pp. 10215â€“10224, 2018._

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. stat, 1050:1, 2014.

Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In
_Advances in neural information processing systems, 2021._

Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for
[efficient and high fidelity speech synthesis. https://github.com/jik876/hifi-gan,](https://github.com/jik876/hifi-gan)
2020a.

Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for
efficient and high fidelity speech synthesis. Advances in neural information processing systems,
33, 2020b.


-----

Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. In ICML Workshop
_on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models, 2021._

Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile
diffusion model for audio synthesis. International conference on learning representations, 2021.

Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo,
Alexandre de BrÂ´ebisson, Yoshua Bengio, and Aaron C Courville. Melgan: Generative adversarial
networks for conditional waveform synthesis. Advances in neural information processing systems,
32, 2019.

Rithesh Kumar. Official repository for the paper melgan: Generative adversarial networks for condi[tional waveform synthesis. https://github.com/descriptinc/melgan-neurips,](https://github.com/descriptinc/melgan-neurips)
2019.

Max WY Lam, Jun Wang, Dan Su, and Dong Yu. Effective low-cost time-domain audio separation
using globally attentive locally recurrent networks. In 2021 IEEE Spoken Language Technology
_Workshop (SLT), pp. 801â€“808. IEEE, 2021._

Lars MaalÃ¸e, Marco Fraccaro, Valentin LiÂ´evin, and Ole Winther. Biva: A very deep hierarchy of
latent variables for generative modeling. Advances in neural information processing systems, pp.
6548â€“6558, 2019.

Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.
In International Conference on Machine Learning, pp. 8162â€“8171. PMLR, 2021.

Aaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu,
George Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al. Parallel wavenet: Fast
high-fidelity speech synthesis. In International conference on machine learning, pp. 3918â€“3926.
PMLR, 2018.

George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. JMLR, pp. 22(57):1â€“
64, 2021.

Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A flow-based generative network
for speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech
_and Signal Processing (ICASSP), pp. 3617â€“3621. IEEE, 2019._

Flavio Protasio Ribeiro, Dinei Florencio, Cha Zhang, and Mike Seltzer. CROWDMOS: An approach
for crowdsourcing mean opinion score studies. In ICASSP. IEEE, 2011. Edition: ICASSP.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. ICML, pp. 1278â€“1286, 2014.

Antony W Rix, John G Beerends, Michael P Hollier, and Andries P Hekstra. Perceptual evaluation
of speech quality (pesq)-a new method for speech quality assessment of telephone networks and
codecs. In 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing.
_Proceedings (Cat. No. 01CH37221), volume 2, pp. 749â€“752. IEEE, 2001._

Robin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion models. arXiv preprint arXiv:2104.02600, 2021.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.

Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In International Conference on Machine Learn_ing, pp. 2256â€“2265, 2015._

Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. Interna_tional conference on learning representations, 2021._


-----

Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
_Advances in neural information processing systems, 32, 2019._

Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
_Advances in neural information processing systems, 33:12438â€“12448, 2020._

Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach
to density and score estimation. UAI, pp. 574â€“584, 2020a.

Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. In Interna_tional conference on learning representations, 2020b._

Cees H Taal, Richard C Hendriks, Richard Heusdens, and Jesper Jensen. A short-time objective
intelligibility measure for time-frequency weighted noisy speech. In 2010 IEEE international
_conference on acoustics, speech and signal processing, pp. 4214â€“4217. IEEE, 2010._

Rafael Valle. Waveglow: a flow-based generative network for speech synthesis. [https://](https://github.com/NVIDIA/waveglow)
[github.com/NVIDIA/waveglow, 2020.](https://github.com/NVIDIA/waveglow)

Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
raw audio. Proc. 9th ISCA Speech Synthesis Workshop, pp. 125â€“125, 2016.

Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to efficiently sample from diffusion probabilistic models. arXiv preprint arXiv:2106.03802, 2021.

Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald. CSTR VCTK Corpus: English
multi-speaker corpus for CSTR voice cloning toolkit (version 0.92), 2019.

[Ryuichi Yamamoto. Wavenet vocoder. https://github.com/r9y9/wavenet_vocoder,](https://github.com/r9y9/wavenet_vocoder)
2020.

Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. Parallel. Wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. ICASSP,
2020.

Geng Yang, Shan Yang, Kai Liu, Peng Fang, Wei Chen, and Lei Xie. Multi-band melgan: Faster
waveform generation for high-quality text-to-speech. In 2021 IEEE Spoken Language Technology
_Workshop (SLT), pp. 492â€“498. IEEE, 2021._


-----

A THEORETICAL DERIVATIONS FOR BDDMS

In this section, we provide the theoretical supports for the following:

-  The derivation for upper bounding _Î²[Ë†]n (see Appendix A.1)._

-  The score network Î¸ trained with L[(]ddpm[t][)] [(][Î¸][)][ for the reverse process][ p][Î¸][(][x][t][âˆ’][1][|][x][t][)][ can be]
re-used for the reverse process pÎ¸(Ë†xnâˆ’1|xË†n) (see Appendix A.2).

-  The schedule network Ï† can be trained with Lstep[(][n][)][(][Ï†][;][ Î¸][âˆ—][)][ after the score network][ Î¸][ is opti-]
mized. (see Appendix A.3).

A.1 DERIVING AN UPPER BOUND FOR NOISE SCALE

Since monotonic noise schedules have been successfully applied to in many prior arts including
DPMs (Ho et al., 2020; Kingma et al., 2021) and score-based methods (Song et al., 2020a; Song
& Ermon, 2020), we also follow the monotonic assumption and derive an upper bound for _Î²[Ë†]n as_
below:
**Remark 1. Suppose the noise schedule for sampling is monotonic, i.e., 0 <** _Î²[Ë†]1 < . . . <_ _Î²[Ë†]N < 1,_
_then, for 1 â‰¤_ _n < N_ _,_ _Î²[Ë†]n satisfies the following inequality:_

0 < _Î²[Ë†]n < min_ 1 âˆ’ 1 _Î±Ë†n[2]Î²+1n+1_ _,_ _Î²[Ë†]n+1_ _._ (15)
 _âˆ’_ [Ë†] 

_Proof. By the general definition of noise schedule, we know that 0 <_ _Î²[Ë†]1, . . .,_ _Î²[Ë†]N < 1 (Note: no_

inequality sign in between). Given that Ë†Î±n = _i=1_ 1 _Î²i, we also have 0 < Ë†Î±1, . . ., Ë†Î±t < 1._

_âˆ’_ [Ë†]

_Î±Ë†[2]n+1_ q
First, we show that _Î²[Ë†]n < 1 âˆ’_ 1âˆ’Î²[Ë†]n+1 [:] [Q][n]


_Î±Ë†n_ 1 = _Î±Ë†n_ _< 1_ _Î²n < 1_ _Î±Ë†n[2]_ [= 1][ âˆ’] _Î±Ë†n[2]_ +1 _._ (16)
_âˆ’_ 1 _Î²n_ _â‡â‡’_ [Ë†] _âˆ’_ 1 âˆ’ _Î²[Ë†]n+1_

_âˆ’_ [Ë†]

q


Next, we show that _Î²[Ë†]n < 1 âˆ’_ _Î±Ë†n+1:_

_Î±Ë†n_ _Î±Ë†n_ 1 _Î²n_ _Î±Ë†n+1_

= _âˆ’_ [Ë†] = _< 1_ _Î²n < 1_ _Î±Ë†n+1._ (17)
1 _Î²n_ 1q âˆ’ _Î²[Ë†]n_ 1 âˆ’ _Î²[Ë†]n_ _â‡â‡’_ [Ë†] _âˆ’_
_âˆ’_ [Ë†]

q


Now, we have _Î²[Ë†]n < min_ 1 âˆ’ 1âˆ’Î±Ë†[2]nÎ²[Ë†]+1n+1 _[,][ 1][ âˆ’]_ _Î±[Ë†]n+1_ . When 1 âˆ’ _Î±Ë†n+1 < 1 âˆ’_ 1âˆ’Î±Ë†[2]nÎ²[Ë†]+1n+1 [, we can show]

that _Î²[Ë†]n+1 < 1 âˆ’_ _Î±Ë†n+1:_ n o

1 âˆ’ _Î±Ë†n+1 < 1 âˆ’_ 1 _Î±Ë†n[2]Î²+1n+1_ = 1 âˆ’ _Î±Ë†n[2]_ _[â‡]â‡’_ _Î±Ë†n+1 > Ë†Î±n[2]_ _[â‡]â‡’_ _Î±[Ë†]Î±Ë†n[2]_ +1n[2] _> Ë†Î±n+1_ (18)

_âˆ’_ [Ë†]

1 _Î±n[2]_ +1 _< 1_ _Î±Ë†n+1_ _Î²n+1 < 1_ _Î±Ë†n+1._ (19)
_â‡â‡’_ _âˆ’_ [Ë†]Î±Ë†n[2] _âˆ’_ _â‡â‡’_ [Ë†] _âˆ’_

By the assumption of monotonic sequence, we also have _Î²[Ë†]n <_ _Î²[Ë†]n+1. Knowing that_ _Î²[Ë†]n+1 <_

1âˆ’Î±Ë†n+1 is always true, we obtain a tighter bound for _Î²[Ë†]n: 0 <_ _Î²[Ë†]n < min_ 1 âˆ’ 1âˆ’Î±Ë†[2]nÎ²[Ë†]+1n+1 _[,][ Ë†]Î²n+1_ .
n o

A.2 DERIVING THE TRAINING OBJECTIVE FOR SCORE NETWORK

First, followed from the data distribution modeling of BDDMs as proposed in Eq. (8):

_pÎ¸(Ë†x0) := ExË†nâˆ’1âˆ¼q Ë†Î²[(Ë†]xnâˆ’1;xt,Ïµn)_ ExË†1:nâˆ’2âˆ¼pÎ¸(Ë†x1:nâˆ’2|xË†nâˆ’1) [[][p]Î¸[(Ë†]x0|xË†1:nâˆ’1)] _,_ (20)

we can derive a new lower bound to the log marginal likelihood as follows: 


-----

**Proposition 1. Given xt âˆ¼** _qÎ²(xt|x0), the following lower bound holds for n âˆˆ{2, . . ., N_ _}:_

log pÎ¸(Ë†x0) â‰¥Fscore[(][n][)] [(][Î¸][) :=][ âˆ’L][(]score[n][)] [(][Î¸][)][ âˆ’R][Î¸][(Ë†]x0, xt), (21)

_where_

_score[(][Î¸][) :=][ D][KL]_ _pÎ¸(Ë†xn_ 1 **_xË†n = xt)_** _q Ë†Î²[(Ë†]xn_ 1; xt, Ïµn) _,_ (22)
_L[(][n][)]_ _âˆ’_ _|_ _||_ _âˆ’_

_Î¸(Ë†x0, xt) :=_ EpÎ¸(Ë†x1 **_xË†n=xt)_** [[log][ p]Î¸[(Ë†]x0 **_xË†1)] ._**  (23)
_R_ _âˆ’_ _|_ _|_

_Proof._

log pÎ¸(Ë†x0) = log _pÎ¸(Ë†x0:n_ 2 **_xË†n_** 1)q Ë†Î²[(Ë†]xn 1; xt, Ïµn)dxË†1:n 1 (24)
_âˆ’_ _|_ _âˆ’_ _âˆ’_ _âˆ’_
Z

**_x1:n_** 1 **_xË†n = xt)_**
= log _pÎ¸(Ë†x0:n_ 2 **_xË†n_** 1)q Ë†Î²[(Ë†]xn 1; xt, Ïµn) _[p][Î¸][(Ë†]_ _âˆ’_ _|_ **_x1:n_** 1
_âˆ’_ _|_ _âˆ’_ _âˆ’_ _pÎ¸(Ë†x1:n_ 1 **_xË†n = xt)_** _[d]_ [Ë†] _âˆ’_
Z _âˆ’_ _|_

(25)


_pÎ¸(Ë†x0_ **_xË†1)q Ë†Î²[(Ë†]xn_** 1; xt, Ïµn)
_|_ _âˆ’_

_pÎ¸(Ë†xn_ 1 **_xË†n = xt)_**
_âˆ’_ _|_


= log EpÎ¸(Ë†x1,n 1 **_xË†n=xt)_**
_âˆ’_ _|_


(26)

(27)


_pÎ¸(Ë†x0_ **_xË†1)q Ë†Î²[(Ë†]xn_** 1; xt, Ïµn)
log _|_ _âˆ’_

_pÎ¸(Ë†xn_ 1 **_xË†n = xt)_**
_âˆ’_ _|_



[Jensenâ€™s Inequality] EpÎ¸(Ë†x1,xË†n 1 **_xË†n=xt)_**
_â‰¥_ _âˆ’_ _|_


=EpÎ¸(Ë†x1 **_xË†n=xt)_** [[log][ p]Î¸[(Ë†]x0 **_xË†1)]_** _DKL_ _pÎ¸(Ë†xn_ 1 **_xË†n = xt)_** _q Ë†Î²[(Ë†]xn_ 1; xt, Ïµn)
_|_ _|_ _âˆ’_ _âˆ’_ _|_ _||_ _âˆ’_
 (28)

= âˆ’L[(]score[n][)] [(][Î¸][)][ âˆ’R][Î¸][(Ë†]x0, xt) (29)

Next, we show that the score network Î¸ trained with L[(]ddpm[t][)] [(][Î¸][)][ can be re-used in BDDMs. We first]
provide the derivation for Eq. (9- 10). We have


1 _Î±Ë†n[2]_ **_[Ïµ][n]_**
_âˆ’_

_Î±Ë†n_


**_xË†n_** 1 **_xË†n = xt, Ë†x0 =_** **_[x][t][ âˆ’]_**
_âˆ’_ _|_


(30)

(31)

(32)


_q Ë†Î²[(Ë†]xn_ 1; xt, Ïµn) := q Ë†Î²
_âˆ’_


1 âˆ’ _Î²[Ë†]n(1 âˆ’_ _Î±Ë†n[2]_ _âˆ’1[)]_ **_xt,_** [1][ âˆ’] _Î±[Ë†]n[2]_ _âˆ’1_ _Î²Ë†nI_

1 _Î±Ë†n[2]_ 1 _Î±Ë†n[2]_
_âˆ’_ _âˆ’_


_Î±n_ 1Î²[Ë†]n **_xt_** 1 _Î±Ë†n[2]_ **_[Ïµ][n]_** 1 _Î²_
_âˆ’_ _âˆ’_ _âˆ’_ + _âˆ’_ [Ë†]

ï£« 1 _Î±Ë†n[2]_ _Î±Ë†n_ q 1

_âˆ’_ p

ï£­ [Ë†]

_Î±Ë†n_ 1Î²[Ë†]n 1 _Î²n(1_ _Î±Ë†n[2]_ 1[)]
_âˆ’_ _âˆ’_ [Ë†] _âˆ’_ _âˆ’_

ï£«ï£« _Î±Ë†n(1_ _Î±Ë†n[2]_ [) +] q 1 _Î±Ë†n[2]_

_âˆ’_ _âˆ’_

ï£­ï£­


= N

= N

= N


= _Î±Ë†nâˆ’1Î²[Ë†]n_ 1 âˆ’ _Î²[Ë†]n(1 âˆ’_ _Î±Ë†n[2]_ _âˆ’1[)]_ **_xt_** _Î±Ë†nâˆ’1Î²[Ë†]n_ **_Ïµn,_** [1][ âˆ’] _Î±[Ë†]n[2]_ _âˆ’1_ _Î²Ë†nI_ (32)
_N_ ï£«ï£« _Î±Ë†n(1_ _Î±Ë†n[2]_ [) +] q 1 _Î±Ë†n[2]_ ï£¶ _âˆ’_ _Î±Ë†n_ 1 _Î±Ë†n[2]_ 1 _Î±Ë†n[2]_ ï£¶

_âˆ’_ _âˆ’_ _âˆ’_ _âˆ’_

ï£­ï£­ ï£¸ p ï£¸

= 1 **_xt_** _Î²Ë†n_ **_Ïµn,_** [1][ âˆ’] _Î±[Ë†]n[2]_ _âˆ’1_ _Î²Ë†nI_ _._ (33)
_N_ ï£« _âˆ’_ 1 _Î±Ë†n[2]_ ï£¶

1 âˆ’ _Î²[Ë†]n_ (1 âˆ’ _Î²[Ë†]n)(1 âˆ’_ _Î±Ë†n[2]_ [)] _âˆ’_

ï£­ q q ï£¸

**Proposition 2. Suppose xt âˆ¼** _qÎ²(xt|x0), then any solution satisfying Î¸[âˆ—]_ = argminÎ¸L[(]ddpm[t][)] [(][Î¸][)][,][ âˆ€][t][ âˆˆ]

1, ..., T _, also satisfies Î¸[âˆ—]_ = argminÎ¸ _score[(][Î¸][)][,][ âˆ€][n][ âˆˆ{][2][, ..., N]_ _[}][.]_
_{_ _}_ _L[(][n][)]_


_Proof. By the definition in Eq. (4), we have_


_,_ [1][ âˆ’] _Î±[Ë†]n[2]_ _âˆ’1_ _Î²Ë†nI_

1 _Î±Ë†n[2]_
_âˆ’_


_Î²Ë†n_
**_xt_** **_ÏµÎ¸ (xt, Ë†Î±n)_**
_âˆ’_ 1 _Î±Ë†n[2]_

_âˆ’_

p


_pÎ¸(Ë†xn_ 1 **_xË†n = xt) =_** 1 **_xt_** _Î²n_ **_ÏµÎ¸ (xt, Ë†Î±n)_** _,_ [1][ âˆ’] _Î±[Ë†]nâˆ’1_ _Î²Ë†nI_ _._ (34)
_âˆ’_ _|_ _N_ ï£« 1 _Î²n_ _âˆ’_ 1 âˆ’ _Î±Ë†n[2]_ ! 1 âˆ’ _Î±Ë†n[2]_ ï£¶

_âˆ’_ [Ë†]

ï£­ q p ï£¸

Here, from the training objective in Eq. (5), since xt = Î±tx0+ 1 âˆ’ _Î±t[2][Ïµ][n][, the noise scale argument]_

for the score network is known to be Î±t. Therefore, we can use ÏµÎ¸ (xt, Î±t) instead of ÏµÎ¸ (xt, Ë†Î±n) for

p


_pÎ¸(Ë†xn_ 1 **_xË†n = xt) =_**
_âˆ’_ _|_ _N_


1 _Î²n_
_âˆ’_ [Ë†]


-----

expanding score[(][Î¸][)][. Since][ p]Î¸[(Ë†]xn 1 **_xË†n = xt) and q Ë†Î²[(Ë†]xn_** 1; xt, Ïµn) are two isotropic Gaussians
_L[(][n][)]_ _âˆ’_ _|_ _âˆ’_
with the same variance, the KL divergence is a scaled â„“2-norm of their meansâ€™ difference:

score[(][Î¸][) :=][D][KL] _pÎ¸(Ë†xn_ 1 **_xË†n = xt)_** _q Ë†Î²[(Ë†]xn_ 1; xt, Ïµn) (35)
_L[(][n][)]_ _âˆ’_ _|_ _||_ _âˆ’_
 


= 1 âˆ’ _Î±Ë†n[2]_

2(1 _Î±Ë†n[2]_ 1[)Ë†]Î²n
_âˆ’_ _âˆ’_

= [(1][ âˆ’] _Î²[Ë†]n)(1 âˆ’_ _Î±Ë†n[2]_ [)]

2(1 âˆ’ _Î²[Ë†]n âˆ’_ _Î±Ë†n[2]_ [)Ë†]Î²n


_Î²Ë†n_
**_xt_** **_ÏµÎ¸ (xt, Î±t)_**
_âˆ’_ 1 _Î±Ë†n[2]_

_âˆ’_

p


(36)

(37)

(38)


1 _Î²n_
_âˆ’_ [Ë†]


1 _Î²Ë†n_

**_xt_** **_Ïµn_**
_âˆ’_
1 âˆ’ _Î²[Ë†]n_ (1 âˆ’ _Î²[Ë†]n)(1 âˆ’_ _Î±Ë†n[2]_ [)]

q q 2

_Î²Ë†n_

(Ïµn âˆ’ **_ÏµÎ¸ (xt, Î±t))_**
(1 _Î²n)(1_ _Î±Ë†n[2]_ [)]
_âˆ’_ [Ë†] _âˆ’_ 2


= [(1][ âˆ’] _Î²[Ë†]n)(1 âˆ’_ _Î±Ë†n[2]_ [)] _Î²Ë†n[2]_ **_Ïµn_** **_ÏµÎ¸ (xt, Î±t)_** 2 (39)

2(1 âˆ’ _Î²Î²[Ë†]Ë†nn âˆ’_ _Î±Ë†n[2]_ [)Ë†]Î²n (1 âˆ’ _Î±Ë†n[2]_ [)(1][ âˆ’] _Î²[Ë†]n)_ _âˆ¥_ _âˆ’_ 2âˆ¥[2]

= _Î±tx0 +_ 1 _Î±t[2][Ïµ][n][, Î±][t]_ _,_ (40)

2(1 âˆ’ _Î²[Ë†]n âˆ’_ _Î±Ë†n[2]_ [)]  q _âˆ’_ 2 2

which is proportional to ddpm [:=] **_Ïµn[Ïµ][n][ âˆ’]ÏµÎ¸[Ïµ][Î¸]_** _Î±tx0 +_ 1 _Î±t[2][Ïµ][n][, Î±][t]_
_L[(][t][)]_ _âˆ’_ _âˆ’_ 2 [as defined in Eq. (5).]

Thus,  p 

argminÎ¸Lddpm[(][t][)] [(][Î¸][)][ â‰¡] [argmin]Î¸[L][(]score[n][)] [(][Î¸][)][.] (41)

Next, we can simplify _Î¸(Ë†x0, xt) to a reconstruction loss for Ë†x0:_
_R_
_Î¸(Ë†x0, xt) :=_ EpÎ¸(Ë†x1 **_xË†n=xt)_** [[log][ p]Î¸[(Ë†]x0 **_xË†1)]_** (42)
_R_ _âˆ’_ _|_ _|_

=EpÎ¸(Ë†x1 **_xË†n=xt)_** log 1 **_xË†1_** _Î²Ë†1_ **_ÏµÎ¸(Ë†x1, Ë†Î±1),_** _Î²[Ë†]1I_ (43)
_|_ ï£® _N_ ï£« 1 _Î²1_ _âˆ’_ 1 âˆ’ _Î±Ë†1[2]_ ![ï£¶]ï£¹

_âˆ’_ [Ë†]

ï£° ï£­ q p ï£¸ï£» 2

=EpÎ¸(Ë†x1|xË†n=xt) ï£® 2 [log 2][Ï€][ Ë†]Î²1 + 2Î²1[Ë†]1 **_xË†0 âˆ’_** 1 1 _Î²1_ ï£«xË†1 âˆ’ _Î²Ë†Î²1Ë†1_ **_ÏµÎ¸(Ë†x1, Ë†Î±1)ï£¶_** ï£¹

ï£¯ _âˆ’_ [Ë†] 2ï£º
ï£° _[D]_ q ï£­ q ï£¸ (44)ï£»

2

1 1

= _[D]2 [log 2][Ï€][ Ë†]Î²1 +_ 2Î²[Ë†]1 EpÎ¸(Ë†x1|xË†n=xt) ï£® **_xË†0 âˆ’_** 1 _Î²1_ xË†1 âˆ’ qÎ²Ë†1ÏµÎ¸(Ë†x1, Ë†Î±1) ï£¹ _,_

ï£¯ _âˆ’_ [Ë†] 2ï£º
ï£° q ï£»(45)

where pÎ¸(Ë†x1|xË†n = xt) can be efficiently sampled using the reverse process in (Song et al., 2021).
Yet, in practice, similar to the training in (Song et al., 2021; Chen et al., 2020; Kong et al., 2021),
we dropped _Î¸(Ë†x0, xt) when training Î¸. In theory, we know that_ _Î¸(Ë†x0, xt) achieves its optimal_
_R_ _R_
value at Î¸[âˆ—] = argminÎ¸âˆ¥ÏµÎ¸(Ë†x1, Ë†Î±1)âˆ’Ïµ1âˆ¥2[2][, which shares a similar objective as][ L][(]ddpm[t][)] [. By minimizing]

_Lddpm[(][t][)]_ [, we train a score network][ Î¸][âˆ—] [that best minimizes][ âˆ†][Ïµ][t][ :=][ âˆ¥][Ïµ][t][ âˆ’] **_[Ïµ][Î¸][âˆ—]_** [(][Î±][t][x][0][ +] 1 âˆ’ _Î±t[2][Ïµ][t][, Î±][t][)][âˆ¥]2[2]_

for all 1 _t_ _T_ . Since the first diffusion step has the smallest effect on corrupting Ë†x0 (i.e.,
_â‰¤_ _â‰¤_ p
_Î²1_ 0), it suffices to consider a Ë†Î±1 = 1 _Î²1 = Î±1, in which case we can jointly minimize_
_â‰ˆ_ _[âˆš]_ _âˆ’_
_RÎ¸(Ë†x0, xt) by minimizing Lddpm[(1)]_ [.]

In this sense, during training, given xt _qÎ²(xt_ **_x0), we can train the score network with the same_**
training objective as in DDPMs and DDIMs. Practically, it is beneficial for BDDMs as we can re-use âˆ¼ _|_
the score network Î¸ of any well-trained DDPM or DDIM.


-----

A.3 DERIVING THE TRAINING OBJECTIVE FOR SCHEDULE NETWORK

Given that Î¸ can be trained to maximize the log evidence with the pre-specified noise schedule Î² for
training, the consequent question of interest in BDDMs is how to find a fast and good enough noise
schedule **_Î²[Ë†] âˆˆ_** R[N] for sampling given an optimized Î¸[âˆ—]. In BDDMs, this problem is reduced to how
to effectively learn the network parameters Ï† .

**Proposition 3. Suppose Î¸ has been optimized and hypothetically converged to the optimal Î¸[âˆ—],**
_where by optimal it means that with Î¸[âˆ—]_ _we have pÎ¸âˆ—_ (Ë†xn 1 **_xË†n = xt) = q Ë†Î²[(Ë†]xn_** 1; xt, Ïµn) given
_âˆ’_ _|_ _âˆ’_

**_xt âˆ¼_** _qÎ²(xt|x0). When_ **_Î²[Ë†] is unknown but we have x0 = Ë†x0 and Ë†Î±n = Î±t, we can minimize the_**
_gap between the optimal lower bound_ _score[(][Î¸][âˆ—][)][ and][ log][ p]Î¸[âˆ—]_ [(Ë†]x0), i.e, log pÎ¸âˆ— (Ë†x0) _score[(][Î¸][âˆ—][)][, by]_
_F_ [(][n][)] _âˆ’F_ [(][n][)]
_minimizing the following objective with respect to_ _Î²[Ë†]n:_

_L[(]step[n][)][(Ë†]Î²n; Î¸[âˆ—]) :=DKL_ _pÎ¸âˆ—_ (Ë†xnâˆ’1|xË†n = xt)||q Ë†Î²n [(Ë†]xnâˆ’1|x0; Î±t) (46)
 


_Î´t_ _Î²Ë†n_

**_ÏµÎ¸âˆ—_** _Î±tx0 +_

2(Î´t _Î²n)_ _Î´t_
_âˆ’_ [Ë†] 

**_[Ïµ][n][ âˆ’]_**


+ C, (47)

(48)


_Î´tÏµn, Î±t_


_where_


_Î´t = 1_ _Î±t[2][,]_ _C = [1]_ + _[D]_
_âˆ’_ 4 [log][ Î´]Î²Ë†n[t] 2


_Î²Ë†n_

1
_Î´t_ _âˆ’_


_Proof. Note that Ë†x0 = x0, Ë†Î±n = Î±t, xt = Î±tx0 +_ 1 âˆ’ _Î±t[2][Ïµ][n]_ [and][ p][Î¸][âˆ—] [(Ë†]xnâˆ’1|xË†n = xt) =

_q Ë†Î²[(Ë†]xn_ 1; xt, Ïµn). When x0 is given to pÎ¸âˆ—, we can express the probability as follows:
_âˆ’_ p

_pÎ¸âˆ—_ (Ë†xnâˆ’1|xË†n = xt(x0), Ë†x0 = x0) (49)


1 âˆ’ _Î±t[2][z][)][d][z]_ (50)

1 _Î±t[2][z][,][ Ïµ][n]_ [=][ z][)][d][z] (51)
_âˆ’_


_N_ (z; 0, I)pÎ¸âˆ— (Ë†xnâˆ’1|xË†n = Î±tx0 +

(z; 0, I)q Ë†Î²[(Ë†]xn 1; xt = Î±tx0 +
_N_ _âˆ’_


ï£«xË†nâˆ’1; _[Î±][t][x][0][ +]1p âˆ’1Î²[Ë†] âˆ’n_ _Î±t[2][z]_ _âˆ’_ (1 âˆ’ _Î²[Ë†]Î²nË†n)(1 âˆ’_ _Î±Ë†n[2]_ [)] **_z,_** [1][ âˆ’]1 âˆ’Î±[Ë†]Î±Ë†n[2] _âˆ’n[2]_ 1 _Î²Ë†nI_

ï£­ q q


_dz_

ï£¶

ï£¸ (52)


_N_ (z; 0, I)N



[See Eq. (2) in (Frey, 1999)]


+ [1][ âˆ’] _[Î±]t[2][/][(1][ âˆ’]_ _Î²[Ë†]n)_ _Î²Ë†n_

1 _Î±t[2]_
_âˆ’_


1 _Î±t[2]_ _Î²Ë†n_
_âˆ’_

_âˆ’_
1 âˆ’ _Î²[Ë†]n_ (1 âˆ’ _Î²[Ë†]n)(1 âˆ’_ _Î±t[2][)]_
q


_Î±tx0_
**_xn_** 1;

ï£« _âˆ’_

1 _Î²n_

ï£¬ _âˆ’_ [Ë†]
ï£­ [Ë†] q


=N

=N


**_I_**
ï£· ï£·
ï£¸ (53)ï£¸


ï£«xË†nâˆ’1; _Î±1tx0Î²n_ _,_ [1][ âˆ’]1 âˆ’[Î±]t[2] _Î²[Ë†][âˆ’]nÎ²[Ë†]n_

_âˆ’_ [Ë†]

ï£­ q


ï£¶ =: q Ë†Î²n [(Ë†]xnâˆ’1; x0, Î±t), (54)

ï£¸


where, different from pÎ¸âˆ— (Ë†xnâˆ’1|xË†n = xt), from Eq. (49) to Eq. (50), instead of conditioning on a
specific xt, when x0 is given xt can be generated using any z âˆ¼N (0, I).


-----

From this, we can express the gap between log pÎ¸âˆ— (Ë†x0) and score[(][Î¸][âˆ—][)][ in the following form:]
_F_ [(][n][)]

log pÎ¸âˆ— (Ë†x0 = x0) âˆ’Fscore[(][n][)] [(][Î¸][âˆ—][)] (55)

_pÎ¸(Ë†x0_ **_xË†1)q Ë†Î²[(Ë†]xn_** 1; xt, Ïµn)

= log pÎ¸[âˆ—] (Ë†x0 = x0) âˆ’ EpÎ¸âˆ— (Ë†x1,nâˆ’1|xË†n=xt) "log _pÎ¸|(Ë†xnâˆ’1|xË†n =âˆ’_ **_xt)_** # (56)

**_x0:n_** 1 **_xË†n = xt)_**

= log pÎ¸âˆ— (Ë†x0 = x0) âˆ’ EpÎ¸âˆ— (Ë†x1:nâˆ’1|xË†n=xt) log _[p]p[Î¸]Î¸[âˆ—]âˆ—_ [(Ë†](Ë†x1:nâˆ’âˆ’1||xË†n = xt)  (57)

=EpÎ¸âˆ— (Ë†x1:nâˆ’1|xË†n=xt) log _pÎ¸[âˆ—]_ (Ë†xp1:Î¸âˆ—n(Ë†x11:xË†nâˆ’n =1|xË† xn =t, Ë†x x0 =t) **_x0)_** (58)

 _âˆ’_ _|_ 

**_xn_** 1 **_xË†n = xt)_**

=EpÎ¸âˆ— (Ë†xnâˆ’1|xË†n=xt) "log _[p]q[Î¸] Ë†Î²[âˆ—]n[(Ë†][(Ë†]xnâˆ’âˆ’|1; x0, Î±t)_ # (59)

=DKL _pÎ¸âˆ—_ (Ë†xnâˆ’1|xË†n = xt)||q Ë†Î²n [(Ë†]xnâˆ’1; x0, Î±t) (60)
 

Next, we evaluate the above KL divergence term. By definition, we have


_,_ [1][ âˆ’] _Î±[Ë†]n[2]_ _âˆ’1_ _Î²Ë†nI_

1 _Î±Ë†n[2]_
_âˆ’_


_Î²Ë†n_
**_xt_** **_ÏµÎ¸âˆ—_** (xt, Ë†Î±n)
_âˆ’_ 1 _Î±Ë†n[2]_

_âˆ’_

p


(61)


_pÎ¸âˆ—_ (Ë†xnâˆ’1|xË†n = xt) = N ï£­

Together with Eq. (54), we have


1 _Î²n_
_âˆ’_ [Ë†]


_Lstep[(][n][)][(Ë†]Î²n; Î¸[âˆ—]) := DKL_ _pÎ¸âˆ—_ (Ë†xnâˆ’1|xË†n = xt)||q Ë†Î²n [(Ë†]xnâˆ’1; x0, Î±t) (62)
 


1 _Î²n_
_âˆ’_ [Ë†]

2(1 âˆ’ _Î²[Ë†]n âˆ’_ _Î±t[2][)]_

1 _Î²n_
_âˆ’_ [Ë†]

2(1 âˆ’ _Î²[Ë†]n âˆ’_ _Î±t[2][)]_


_Î±t_

**_x0_**
_âˆ’_
1 _Î²n_
_âˆ’_ [Ë†]

_Î±t_

**_x0_**
_âˆ’_
1 _Î²n_
_âˆ’_ [Ë†]


_Î²Ë†n_
**_xt_** **_ÏµÎ¸[âˆ—]_** (xt, Î±t)
_âˆ’_ 1 _Î±t[2]_

_âˆ’_

p


+ C (63)


1 _Î²n_
_âˆ’_ [Ë†]

1

1 _Î²n_
_âˆ’_ [Ë†]


_Î²Ë†n_
1 _Î±t[2][Ïµ][n]_ **_ÏµÎ¸âˆ—_** (xt, Î±t)
_âˆ’_ _[âˆ’]_ 1 _Î±t[2]_

_âˆ’_

p


_Î±tx0 +_


+ C

!

2

(64)


1 _Î²n_
_âˆ’_ [Ë†]

2(1 âˆ’ _Î²[Ë†]n âˆ’_ _Î±t[2][)]_

1 _Î±t[2]_
_âˆ’_

2(1 âˆ’ _Î²[Ë†]n âˆ’_ _Î±t[2][)]_


1 âˆ’ _Î±t[2]_ **_Ïµn_** _Î²Ë†n_ **_ÏµÎ¸âˆ—_** (xt, Î±t)

1 âˆ’ _Î²n_ _âˆ’_ (1 âˆ’ _Î²n)(1 âˆ’_ _Î±t[2][)]_
p


+ C (65)


_Î²Ë†n_

**_ÏµÎ¸âˆ—_** (xt, Î±t) + C (66)

_Î±t[2][)]_ 1 âˆ’ _Î±t[2]_ 2

2

_Î²Ë†n_

**_[Ïµ][n][ âˆ’]ÏµÎ¸âˆ—_** _Î±tx0 +_ _Î´tÏµn, Î±t_ + C, (67)

_Î´t_

2

 p 

**_[Ïµ][n][ âˆ’]_**


_Î´t_
=

2(Î´t _Î²n)_
_âˆ’_ [Ë†]

where


_Î´t = 1_ _Î±t[2][,]_ _C = [1]_ + _[D]_
_âˆ’_ 4 [log][ Î´]Î²Ë†n[t] 2


_Î²Ë†n_

1
_Î´t_ _âˆ’_


(68)


As we use a schedule network Ï† to estimate _Î²[Ë†]n from (Ë†Î±n+1,_ _Î²[Ë†]n+1) as defined in Eq. (13), we obtain_
the final step loss for learning Ï†:


_Î´t_
_Lstep[(][n][)][(][Ï†][;][ Î¸][âˆ—][) =]_ 2(Î´t _Î²n(Ï†))_

_âˆ’_ [Ë†]


_Î²Ë†n(Ï†)_

**_ÏµÎ¸âˆ—_** (xt, Î±t)
_Î´t_

**_[Ïµ][n][ âˆ’]_**


_Î´t_

+ [1] + _[D]_

4 [log] _Î²Ë†n(Ï†)_ 2


_Î²Ë†n(Ï†)_

1
_Î´t_ _âˆ’_


(69)


-----

This proposed objective for training the schedule network can be interpreted as to better model
the data distribution (i.e., maximizing log pÎ¸(Ë†x0)) by correcting the gradient scale _Î²[Ë†]n for the next_
reverse step (from Ë†xn to Ë†xn 1) given the gradient vector ÏµÎ¸âˆ— estimated by the score network Î¸[âˆ—].
_âˆ’_

B EXPERIMENTAL DETAILS

B.1 CONVENTIONAL GRID SEARCH ALGORITHM FOR DDPMS

We reproduced the grid search algorithm in (Chen et al., 2020), in which a 6-step noise schedule was
searched. In our paper, we generalized the grid search algorithm by similarly sweeping the N -step
noise schedule over the following possibilities with a bin width M = 9:

_{1, 2, 3, 4, 5, 6, 7, 8, 9} âŠ—{10[âˆ’][6][Â·][N/N]_ _, 10[âˆ’][6][Â·][(][N]_ _[âˆ’][1)][/N]_ _, ..., 10[âˆ’][6][Â·][1][/N]_ _},_ (70)

where âŠ— denotes the cartesian product applied on two sets. LS-MSE was used as a metric to select
the solution during the search. When N = 6, we resemble the GS algorithm in (Chen et al., 2020).
Note that above searching method normally does not scale up to N > 6 steps for its exponential
computational cost O(9[N] ).

B.2 HYPERPARAMETER SETTING IN BDDMS

Algorithm 2 took a skip factor Ï„ to control the stride for training the schedule network. The value
of Ï„ would affect the coverage of step sizes when training the schedule network, hence affecting
the predicted number of steps N for inference â€“ the higher Ï„ is, the shorter the predicted inference
schedule tends to be. We set Ï„ = 66 for training the BDDM vocoders in this paper.

For initializing Algorithm 3 for noise scheduling, we could take as few as 1 training sample for
validation, perform a grid search on the hyperparameters {(Ë†Î±N = 0.1Î±T i, _Î²[Ë†]N = 0.1j)} for i, j =_
1, ..., 9, i.e., 81 possibilities in total, and use the PESQ measure as the selection metric. Then, the
predicted noise schedule corresponding to the maximum PESQ was stored and applied to the online
inference afterward, as shown in Algorithm 4. Note that this searching has a complexity of only
_O(M_ [2]) (e.g., M = 9 in this case), which is much more efficient than O(M _[N]_ ) in the conventional
grid search algorithm in (Chen et al., 2020), as discussed in Section B.1.

B.3 IMPLEMENTATION DETAILS

Our proposed BDDMs and the baseline methods were all implemented with the Pytorch library. The
score networks for the LJ and VCTK speech datasets were trained from scratch on a single NVIDIA
Tesla P40 GPU with batch size 32 for about 1M steps, which took about 3 days.

For the model architecture, we used the same architecture as in DiffWave (Kong et al., 2021) for
the score network with 128 residual channels; we adopted a lightweight GALR network (Lam et al.,
2021) for the schedule network. GALR was originally proposed for speech enhancement, so we considered it well suited for predicting the noise scales. For the configuration of the GALR network, we
used a window length of 8 samples for encoding, a segment size of 64 for segmentation and only two
GALR blocks of 128 hidden dimensions, and other settings were inherited from (Lam et al., 2021).
To make the schedule network output with a proper range and dimension, we applied a sigmoid function to the last blockâ€™s output of the GALR network. Then the result was averaged over the segments
and the feature dimensions to obtain the predicted ratio: ÏƒÏ†(x) = AvgPool2D(Ïƒ(GALR(x))),
where GALR(Â·) denotes the GALR network, AvgPool2D(Â·) denotes the average pooling operation
applied to the segments and the feature dimensions, and Ïƒ(x) := 1/(1 + e[âˆ’][x]). The same network architecture was used for the NE approach for estimating Î±t[2] [and was shown better than the]
ConvTASNet used in the original paper (San-Roman et al., 2021). It is also notable that the computational cost of a schedule network is indeed fractional compared to the cost of a score network,
as predicting a noise scalar variable is intrinsically a relatively much easier task. Our GALR-based
schedule network, while being able to produce stable and reliable results, was about 3.6 times faster
than the score network. The training of schedule networks for BDDMs took only 10k steps to
converge, which consumed no more than an hour on a single GPU.


-----

Table 3: Ratings that have been used in evaluation of speech naturalness of synthetic samples.

Rating Naturalness Definition

1 Unsatisfactory Very annoying, distortion is objectionable.
2 Poor Annoying distortion, but not objectionable.
3 Fair Perceptible distortion, slightly annoying.
4 Good Slight perceptible level of distortion, but not annoying.
5 Excellent Imperceptible level of distortion.

Table 4: Performances of different noise schedules on the multi-speaker VCTK speech dataset, each
of which used the same score network (Chen et al., 2020) ÏµÎ¸( ) that was trained on VCTK for about

_Â·_
1M iterations.

**Noise schedule** **LS-MSE (â†“)** **MCD (â†“)** **STOI (â†‘)** **PESQ (â†‘)** **MOS (â†‘)**

**DDPM (Ho et al., 2020; Chen et al., 2020)**
8 steps (Grid Search) 101 **2.09** **0.787** **3.31** **4.22 Â± 0.04**
1,000 steps (Linear) 85.0 2.02 0.798 3.39 4.40 Â± 0.05

**DDIM (Song et al., 2021)**
8 steps (Linear) 553 3.20 0.701 2.81 3.83 Â± 0.04
16 steps (Linear) 412 2.90 0.724 3.04 3.88 Â± 0.05
21 steps (Linear) 355 2.79 0.739 3.12 4.12 Â± 0.05
100 steps (Linear) 259 2.58 0.759 3.30 4.27 Â± 0.04

**NE (San-Roman et al., 2021)**
8 steps (Linear) 208 2.54 0.740 3.10 4.18 Â± 0.04
16 steps (Linear) 183 2.53 0.742 3.20 4.26 Â± 0.04
21 steps (Linear) 852 3.57 0.699 2.66 3.70 Â± 0.03

**BDDM (Ë†Î±N** _,_ _Î²[Ë†]N_ )
8 steps (0.2, 0.9) **98.4** 2.11 0.774 3.18 4.20 Â± 0.04
16 steps (0.5, 0.5) **73.6** **1.93** **0.813** **3.39** **4.35 Â± 0.05**
21 steps (0.5, 0.1) **76.5** **1.83** **0.827** **3.43** **4.48 Â± 0.06**


Regarding the image generation task, to demonstrate the generalizability of our method, we directly
[adopted a score network pre-trained on the CIFAR-10 dataset implemented by a third-party open-](https://heibox.uni-heidelberg.de/d/01207c3f6b8441779abf/)
[source repository. Regarding the schedule network, to demonstrate that it does not have to use](https://github.com/pesser/pytorch_diffusion)
specialized architecture, we replaced GALR by the VGG11 (Simonyan & Zisserman, 2014), which
was also used by as a noise estimator in (San-Roman et al., 2021). The output dimension (number
of classes) of VGG11 was set to 1. Similar to the setting for GALR in speech synthesis, we added
a sigmoid activation to the last layer to ensure a [0, 1] output. Similar to the training in speech
domain, we trained the VGG11-based schedule networks while freezing the score networks for 10k
steps, which normally can be finished in about two hours.

Our code for the speech vocoding and the image generation experiments will be uploaded to Github
after the final decision of ICLR is released.

B.4 CROWD-SOURCED SUBJECTIVE EVALUATION

All our Mean Opinion Score (MOS) tests were crowd-sourced. We refer to the MOS scores in (Protasio Ribeiro et al., 2011), and the scoring criteria have been included in Table 3 for completeness.
The samples were presented and rated one at a time by the testers.

C ADDITIONAL EXPERIMENTS

A demonstration page at [https://bilateral-denoising-diffusion-model.](https://bilateral-denoising-diffusion-model.github.io)
[github.io shows some samples generated by BDDMs trained on LJ speech and VCTK datasets.](https://bilateral-denoising-diffusion-model.github.io)


-----

C.1 MULTI-SPEAKER SPEECH SYNTHESIS

In addition to the single-speaker speech synthesis, we evaluated BDDMs on the multi-speaker
speech synthesis benchmark VCTK (Yamagishi et al., 2019). VCTK consists of utterances sampled at 48 KHz by 108 native English speakers with various accents. We split the VCTK dataset for
training and testing: 100 speakers were used for training the multi-speaker model and 8 speakers
for testing. We trained on a 44257-utterance subset (40 hours) and evaluated on a held-out 100utterance subset. For the score network, we used the Wavegrad architecture (Chen et al., 2020) so
as to examine whether the superiority of BDDMs remains in a different dataset and with a different
score network architecture.

Results are presented in Table 4. For this multi-speaker VCTK dataset, we obtained consistent
observations with that for the single-speaker LJ dataset presented in the main paper. Again, the
proposed BDDM with only 16 or 21 steps outperformed the DDPM with 1,000 steps. To the best
of our knowledge, ours was the first work that reported this degree of superior. When reducing
to 8 steps, BDDM obtained performance on par with (except for a worse PESQ) the costly gridsearched 8 steps (which were unscalable to more steps) in DDPM. For NE, we could again observe
a degradation from its 16 steps to 21 steps, indicating the instability of NE for the VCTK dataset
likewise. In contrast, BDDM gave continuously improved performance while increasing the step
number.

C.2 COMPARING DIFFERENT REVERSE PROCESSES FOR BDDMS

This section demonstrates that BDDMs do not restrict the sampling procedure to a specialized reverse process in Algorithm 4. In particular, we evaluated different reverse processes, including that
of DDPMs as shown in Eq. (4) and DDIMs (Song et al., 2021), for BDDMs and compared the objective scores on the generated samples. DDIMs (Song et al., 2021) formulate a non-Markovian generative process that accelerates the inference while keeping the same training procedure as DDPMs.
The original generative process in Eq. (4) in DDPMs is modified into


_p[(]Î¸[Ï„]_ [)][(][x][0:][T][ ) :=][ Ï€][(][x][T][ )] _p[(]Î¸[Î³][i][)](xÎ³iâˆ’1_ _|xÎ³i_ ) Ã— _p[(]Î¸[t][)][(][x][0][|][x][t][)][,]_ (71)

_iY=1_ _tYâˆˆÎ³Â¯_

where Î³ is a sub-sequence of length N of [1, ..., T ] with Î³N = T, and Â¯Î³ := {1, ..., T _} \ Î³ is defined_
as its complement; Therefore, only part of the models are used in the sampling process.

To achieve the above, DDIMs defined a prediction function fÎ¸[(][t][)][(][x][t][)][ that depends on][ Ïµ][Î¸][ to predict]
the observation x0 given xt directly:


_p[(]Î¸[Ï„]_ [)][(][x][0:][T][ ) :=][ Ï€][(][x][T][ )]


_p[(]Î¸[Î³][i][)](xÎ³iâˆ’1_ _|xÎ³i_ ) Ã—
_i=1_

Y


_fÎ¸[(][t][)][(][x][t][) := 1]_

_Î±t_


1 âˆ’ _Î±t[2][Ïµ][Î¸][(][x][t][, Î±][t][)]_ _._ (72)



**_xt_**
_âˆ’_


By leveraging this prediction function, the conditionals in Eq. (71) are formulated as

_p[(]Î¸[Î³][i][)](xÎ³iâˆ’1_ _|xÎ³i_ ) = N _Î±Î±Î³iÎ³âˆ’i_ 1 (xÎ³i âˆ’ _Ï‚ÏµÎ¸(xÎ³i_ _, Î±Î³i_ )), ÏƒÎ³[2]i **_[I]_** ifi âˆˆ [N ], i > 1 (73)
 

_p[(]Î¸[t][)][(][x][0][|][x][t][) =][ N]_ [(][f][ (]Î¸[t][)][(][x][t][)][, Ïƒ]t[2][I][)] otherwise, (74)

where the detailed derivation of Ïƒt and Ï‚ can be referred to (Song et al., 2021). In the original
DDIMs, the accelerated reverse process produces samples over the subsequence of Î² indexed by Î³:
**_Î²Ë† =_** _Î²n_ _n_ **_Î³_** . In BDDMs, to apply the DDIM reverse process, we use the **_Î²[Ë†] predicted by the_**
_{_ _|_ _âˆˆ_ _}_
schedule network in place of a subsequence of the training schedule Î².

Finally. the objective scores are given in Table 5. Note that the subjective evaluation (MOS) is
omitted here since the other assessments above have shown that the MOS scores are highly correlated with the objective measures, including STOI and PESQ. They indicate that applying BDDMs
to either DDPM or DDIM reverse process leads to comparable and competitive results. Meanwhile,
the results show some subtle differences: BDDMs over a DDPM reverse process gave slightly better
samples in terms of signal error and consistency metrics (i.e., LS-MSE and MCD), while BDDM
over a DDIM reverse process tended to generate better samples in terms of intelligibility and perceptual metrics (i.e., STOI and PESQ).


-----

Table 5: Performances of different reverse processes for BDDMs on the VCTK speech dataset, each
of which used the same score network (Chen et al., 2020) ÏµÎ¸( ) and the same noise schedule.

_Â·_

**Noise schedule** **LS-MSE (â†“)** **MCD (â†“)** **STOI (â†‘)** **PESQ (â†‘)**

**BDDM (DDPM reverse process)**
8 steps (0.3, 0.9, 1e[âˆ’][5]) **91.3** **2.19** 0.936 3.22
16 steps (0.7, 0.1, 1e[âˆ’][6]) **73.3** **1.88** 0.949 3.32
21 steps (0.5, 0.1, 1e[âˆ’][6]) **72.2** **1.91** 0.950 3.33

**BDDM (DDIM reverse process)**
8 steps (0.3, 0.9, 1e[âˆ’][5]) 91.8 **2.19** **0.938** **3.26**
16 steps (0.7, 0.1, 1e[âˆ’][6]) 77.7 1.96 **0.953** **3.37**
21 steps (0.5, 0.1, 1e[âˆ’][6]) 77.6 1.96 **0.954** **3.39**

Table 6: Comparing sampling methods for DDPM with different number of sampling steps in terms
of FIDs in CIFAR10.

|Sampling method|Sampling steps|FID|
|---|---|---|


|DDPM (baseline) (Ho et al., 2020)|1000|3.17|
|---|---|---|


|DDPM (sub-VP) (Song et al., 2020b)|100 âˆ¼|3.69|
|---|---|---|


|DDPM (DP + reweighting) (Watson et al., 2021)|128 64|5.24 6.74|
|---|---|---|


|DDIM (quadratic) (Song et al., 2021)|100 50|4.16 4.67|
|---|---|---|


|FastDPM (approx. STEP) (Kong & Ping, 2021)|100 50|2.86 3.20|
|---|---|---|


|2Improved DDPM (hybrid) (Nichol & Dhariwal, 2021)|100 50|4.63 5.09|
|---|---|---|


|VDM (augmented) (Kingma et al., 2021)|1000|7.413|
|---|---|---|


|Ours BDDM|100 50|2.38 2.93|
|---|---|---|



C.3 UNCONDITIONAL IMAGE GENERATION

For the unconditional image generation task, we evaluated the proposed BDDMs on the benchmark
CIFAR-10 (32 Ã— 32) dataset. The score functions, including those initially proposed in DDPMs
(Ho et al., 2020) or DDIMs (Song et al., 2021) and those pre-trained in the above third-party implementations, are all conditioned on a discrete step-index. We estimated the noise schedule **_Î²[Ë†] in_**
continuous space using the VGG11 schedule network and then mapped it to discrete time schedule
using the approximation method in (Kong & Ping, 2021).

Table 6 shows the performances of different sampling methods for DDPMs in CIFAR-10. By setting the maximum number of sampling steps (N ) for noise scheduling, we can fairly compare the
improvements achieved by BDDMs against related methods in the literature in terms of FID. Remarkably, BDDMs with 100 sampling steps not only surpassed the 1000-step DDPM baseline, but
also produced the SOTA FID performance amongst all generative models using less than or equal to
100 sampling steps.

[2Our implementation was based on https://github.com/openai/improved-diffusion](https://github.com/openai/improved-diffusion)
3The authors of VDM claimed that they tuned the hyperparameters only for minimizing the likelihood and
did not pursue further tuning of the model to improve FID.


-----

## ð‘›= 3

 ð‘›= 2

 ð‘›= 1

 ð‘›= 0

Figure 4: Spectrum plots of the speech samples produced by BDDM within 3 sampling steps. The
first row shows the spectrum of a random signal for starting the reverse process. Then, from the top
to the bottom, we show the spectrum of the resultant signal after each step of the reverse process
performed by the BDDM. We also provide the corresponding WAV files on our demo page.


-----

