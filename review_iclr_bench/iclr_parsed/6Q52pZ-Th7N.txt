# PSEUDO-LABELED AUTO-CURRICULUM LEARNING
### FOR SEMI-SUPERVISED KEYPOINT LOCALIZATION


**Can Wang[1]** **Sheng Jin[2][,][1]** **Yingda Guan[1]** **Wentao Liu[1][âˆ—]** **Chen Qian[1]**

**Ping Luo[2]** **Wanli Ouyang[3]**

1SenseTime Research and Tetras.AI 2The University of Hong Kong 3The University of Sydney
_{wangcan, jinsheng}@tetras.ai_ _{guanyingda, liuwentao, qianchen}@sensetime.com_
pluo@cs.hku.hk wanli.ouyang@sydney.edu.au

ABSTRACT

Localizing keypoints of an object is a basic visual problem. However, supervised
learning of a keypoint localization network often requires a large amount of data,
which is expensive and time-consuming to obtain. To remedy this, there is an
ever-growing interest in semi-supervised learning (SSL), which leverages a small
set of labeled data along with a large set of unlabeled data. Among these SSL
approaches, pseudo-labeling (PL) is one of the most popular. PL approaches apply
pseudo-labels to unlabeled data, and then train the model with a combination of
the labeled and pseudo-labeled data iteratively. The key to the success of PL is the
selection of high-quality pseudo-labeled samples. Previous works mostly select
training samples by manually setting a single confidence threshold. We propose
to automatically select reliable pseudo-labeled samples with a series of dynamic
thresholds, which constitutes a learning curriculum. Extensive experiments on six
keypoint localization benchmark datasets demonstrate that the proposed approach
significantly outperforms the previous state-of-the-art SSL approaches.

1 INTRODUCTION

Keypoints (also termed as landmarks) are a popular representation of objects that precisely represent
locations of object parts and contain concise information about shapes and poses. Example keypoints
are â€right shoulderâ€ on a human body or the â€tail tipâ€ of a cat. Keypoint localization is the basis of
many visual tasks, including action recognition (Yan et al., 2018), fine-grained classification (Gavves
et al., 2013; 2015), pose tracking (Jin et al., 2017; 2019) and re-identification (Zhao et al., 2017).

Keypoint localization has achieved great success with the advent of deep learning in recent
years (Newell et al., 2016; Xiao et al., 2018; Duan et al., 2019; Sun et al., 2019; Jin et al., 2020a; Xu
et al., 2021; Geng et al., 2021; Li et al., 2021b). However, the success of deep networks relies on vast
amounts of labeled data, which is often expensive and time-consuming to collect. Semi-supervised
learning (SSL) is one of the most important approaches for solving this problem. It leverages extensive amounts of unlabeled data in addition to sparsely labeled data to obtain gains in performance.
Pseudo-labeling (PL) has become one of the most popular SSL approaches due to its simplicity.
PL-based methods iteratively add unlabeled samples into the training data by pseudo-labeling them
with a model trained on a combination of labeled and pseudo-labeled samples.

PL-based methods commonly require a predefined handpicked threshold (Lee et al., 2013; Oliver
et al., 2018), to filter out low-confidence noisy predictions. However, a single fixed threshold does
not take into account the dynamic capacity of the current model for handling noisy pseudo-labels,
leading to sub-optimal performance. In this work, we borrow ideas from Curriculum Learning
(CL) (Bengio et al., 2009) and design our curriculum as a series of thresholds for PL, which is
tuned according to the feedback from the model. CL is a widely used strategy to control the model
training pace by selecting from easier to harder samples. With a carefully designed curriculum,
noticeable improvement is obtained. However, traditional CL methods suffer from hand-designed
curricula, which heavily rely on expertise and detailed analysis for specific domains. Manual cur
_âˆ—Corresponding author._


-----

riculum design based on handcrafted criteria is always tedious and sub-optimal. Moreover, curriculum design (or threshold setting) is complicated. High-confidence pseudo-labels typically correspond to easier samples with clean labels, while low-confidence pseudo-labels correspond to harder
samples with noisy labels. How to design a curriculum to balance the correctness, representativeness, and difficulty of pseudo-labeled data is an open problem. This paper is devoted to tackling
the aforementioned problem, i.e. how to automatically learn an optimal learning curriculum for
pseudo-labeling in a data-driven way. To this end, we propose a novel method, called PseudoLabeled Auto-Curriculum Learning (PLACL). PLACL formulates the curriculum design problem
as a decision-making problem and leverages the reinforcement learning (RL) framework to solve it.

Additionally, PL-based methods suffer from confirmation bias (Tarvainen & Valpola, 2017), also
known as noise accumulation (Zhang et al., 2016), and concept drift (Cascante-Bonilla et al., 2021).
This long-standing issue stems from the use of noisy or incorrect pseudo-labels in subsequent training stages. As a consequence, the noise accumulates and the performance degrades as the learning
process evolves over time. To mitigate this problem, we propose the cross-training strategy which
alternatively performs pseudo-label prediction and model training on separate sub-datasets.

We benchmark PLACL on six keypoint localization datasets, including LSPET (Johnson & Everingham, 2011), MPII (Andriluka et al., 2014), CUB-200-2011 (Welinder et al., 2010), ATRW (Li
et al., 2019c), MS-COCO (Lin et al., 2014), and AnimalPose (Cao et al., 2019). We empirically
show that PLACL is general and can be applied to various keypoint localization tasks (human and
animal pose estimation) and different keypoint localization networks. With a simple yet effective
search paradigm, our method significantly boosts the keypoint estimation performance and achieves
superior performance to other SSL methods. We hope our method will inspire the community to
rethink the potential of PL-based methods for semi-supervised keypoint localization.

Our main contributions can be summarized as follows:

-  We propose Pseudo-Labeled Auto-Curriculum Learning (PLACL). It is an an automatic
pseudo-labeled data selection method, which learns a series of dynamic thresholds (or curriculum) via reinforcement learning. To the best of our knowledge, this is the first work
that explores automatic curriculum learning for semi-supervised keypoint localization.

-  We propose the cross-training strategy for pseudo-labeling to mitigate the long-standing
problem of confirmation bias.

-  Extensive experiments on a wide range of popular datasets demonstrate the superiority of
PLACL over the previous state-of-the-art SSL approaches. In addition, PLACL is modelagnostic and can be easily applied to different keypoint localization networks.

2 RELATED WORKS

2.1 SEMI-SUPERVISED KEYPOINT LOCALIZATION

Keypoint localization focuses on predicting the keypoints of detected objects, e.g. human body
parts (Li et al., 2019b; Jin et al., 2020b), facial landmarks (Bulat & Tzimiropoulos, 2017), hand
keypoints (Zimmermann & Brox, 2017) and animal poses (Cao et al., 2019). However, training
a keypoint localization model often requires a large amount of data, which is expensive and timeconsuming to collect. Semi-supervised keypoint localization is one of the most promising ways to
solve this problem. Semi-supervised keypoint localization can be categorized into consistency regularization based methods and pseudo-labeling based methods. Consistency regularization methods (Honari et al., 2018; Moskvyak et al., 2020) assume that the output of the model should not be
invariant to realistic perturbations. These approaches typically rely on modality-specific augmentation techniques for regularization. Pseudo-labeling methods (Ukita & Uematsu, 2018; Dong &
Yang, 2019; Cao et al., 2019; Li & Lee, 2021) use labeled data to predict the labels of the unlabeled data, and then train the model in a supervised way with a combination of labeled and selected
pseudo-labeled data. Our approach also builds upon pseudo-labeling methods. In contrast to previous works, we propose to learn pseudo-labeled data selection via reinforcement learning.


-----

2.2 CURRICULUM LEARNING

Curriculum learning is firstly introduced by Bengio et al. (2009). It is a training strategy that trains
machine learning models from easy to complex samples, imitating human education. The curriculum is often pre-determined by heuristics (Khan et al., 2011; Bengio et al., 2009; Spitkovsky et al.,
2009). However, it requires expert domain knowledge and exhaustive trials to find a good curriculum suitable for a specific task and its dataset. Recently, automatic curriculum learning methods are
introduced to break through these limits. Popular ones include self-paced learning methods (Kumar
et al., 2010; Jiang et al., 2014; Zhao et al., 2015) and reinforcement learning (RL) based methods (Graves et al., 2017; Matiisen et al., 2019; Fan et al., 2018). Our approach can be categorized
as RL-based methods. Unlike previous works that focus on supervised learning, our approach is designed for the SSL paradigm. Our work is mostly related to Curriculum Labeling (Cascante-Bonilla
et al., 2021). It adopts a hand-crafted curriculum based on Extreme Value Theory (EVT) to facilitate model training. In contrast, we propose an automatic curriculum learning approach by searching
for dynamic thresholds for pseudo-labeling. In addition, the curriculum of (Cascante-Bonilla et al.,
2021) is coarse-grained on the round level, while our curriculum is fine-grained on the epoch level.

2.3 REINFORCEMENT LEARNING FOR AUTOML

Reinforcement learning (RL) has shown impressive results in a range of applications. Well-known
examples include game playing (Mnih et al., 2015; Silver et al., 2016; 2017) and robotics control(Schulman et al., 2015; Lillicrap et al., 2016). Recent works have employed RL to the AutoML,
automating the design of a machine learning system, e.g. searching for neural architectures (Zoph
& Le, 2017; Zoph et al., 2018; Baker et al., 2017; Pham et al., 2018), augmentation policies (Cubuk
et al., 2019), activation functions (Ramachandran et al., 2017), loss functions (Li et al., 2019a;
2021a), and training hyperparameters (Dong et al., 2020). In contrast to these works, we apply RL
to the automatic selection of pseudo-labeled data in the context of pseudo-labeling.

3 PSEUDO-LABELED AUTO-CURRICULUM LEARNING (PLACL)

3.1 OVERVIEW

Our PLACL algorithm is illustrated in Fig. 1. The training process consists of R self-training rounds
and each round consists of N training epochs. (0) In the initial round (r = 0), we pre-train a
keypoint localization network Î˜[0]Ï‰ [on the labeled data, where][ Ï‰][ denotes the weights of the network.]
And for the r-th round, (1) The trained network Î˜[r]Ï‰ [is used to predict pseudo-labels for unlabeled]
data. (2) We adopt reinforcement learning (RL) to automatically generate the learning curriculum.
Specifically, our curriculum (Î“[r]) consists of a series of thresholds for pseudo-labeled data selection.
Î“pseudo-labeled data by the searched curriculum. (4) We retrain a new model ([r] = [Î³1[r][, . . ., Î³]N[r] []][, where][ Î³]i[r] _[âˆˆ]_ [[0][,][ 1]][ is the threshold for each epoch][ i][. (3) We then select reliable]Î˜[r]Ï‰[+1]) using both the
labeled samples and selected pseudo-labeled samples. (5) This process is repeated for R rounds.

3.2 PSEUDO-LABEL SELECTION FOR SEMI-SUPERVISED KEYPOINT LOCALIZATION

We denote the labeled dataset with Nl samples as Dl = **_Ii[l][,][ Y][ l]i_** _i=1_,where Ii and Yi denote

the i-th training image and its keypoint annotations (the x-y coordinates ofn   o _K keypoints). The Nu_

_[N][l]_

unlabeled images are denoted as Du = (Ii[u][)][|]i[N]=1[u], which are not associated with any ground-truth

keypoint labels. Generally, we have _Nln_ _Nu_ .o
_|_ _| â‰ª|_ _|_

Pseudo-labeling based method builds upon the general idea of self-training (McLachlan, 1975),
where the keypoint localization network Î˜Ï‰ goes through multiple rounds of training. In the initialization round, the model is first trained with the small labeled training set Dtrain = Dl in a usual
supervised manner. In subsequent rounds, the trained model is used to estimate labels for the unla
beled data D[Ëœ] _u =_ **_Ii[u][,][ Ëœ]Yi[u]_** _Nu_ . Here, we omit the superscript r for simplicity. Specifically,

_i=1_

given an unlabeled image **_Ii[u][, the trained keypoint localization network]_** [ Î˜][Ï‰][ predicts][ K][ heatmaps.]
Each heatmap is a 2D Gaussian centered on the joint location, which represents the confidence of


-----

(5) Repeat

(0) Train (4) Retrain

Î˜"! Î˜"#$!! Î˜"#$!" ... Î˜"#$!#

labeled Samples

(1) Predict 3

9

## ...

(3) Select Samples

Unlabeled Samples Pseudo Labeled Samples

Î³#" Î³"$ (2) Curriculum Generation Î³"!


Figure 1: Pseudo-Labeled Auto-Curriculum Learning (PLACL). (0) In the initial round, the model
Î˜[0]Ï‰ [is pre-trained on the labeled data. And for the][ r][-th round, (1) the trained network][ Î˜]Ï‰[r] [is used]
to predict pseudo-labels for unlabeled data. (2) A learning curriculum consisting of a series of
thresholds (Î³i[r][) is generated. (3) Reliable pseudo-labeled data is selected by the searched curriculum.]
(4) A new model Î˜[r]Ï‰[+1] is retrained using both the labeled samples and selected pseudo-labeled
samples. (5) This process is repeated by re-labeling unlabeled data using the new model.

the k-th keypoint. The output pseudo-labeled keypoint location (Y[Ëœ]i[u][) is the highest response in the]
heatmap space. And the confidence score C (Î˜Ï‰(Ii[u][))][ is the response value at the keypoint location.]

Then, pseudo-label selection process is adopted. Let g = [g1, ..., gNu ] 0, 1 be a binary
_âŠ†{_ _}[N][u]_
vector representing the selection of pseudo-labels, where gi denotes whether the keypoint prediction
on Ii[u] [is selected.]

1, if (Î˜Ï‰(Ii[u][))][ > Î³]
_gi =_ _C_ (1)
0, otherwise


where Î³ âˆˆ (0, 1) is the confidence threshold. Pseudo-labeled samples with higher confidence are
added to the training set.

Dtrain = **_Ii[l][,][ Y][ l]i_** _i=1_ _âˆª_ **_Ii[u][,][ Ëœ]Yi[u]_** _Ni=1u_ [where][ g][i][ = 1] _._ (2)
n   o   

Then the keypoint localization network is retrained with a combination of labeled and pseudo-[N][l]
labeled training data Dtrain.

3.3 CROSS-TRAINING STRATEGY

In this section, we introduce the cross-training strategy in our curriculum learning framework. In
a typical self-training round, the model predicts noisy pseudo-labels which are used in subsequent
training stages. Since the pseudo-label prediction is performed on a dataset of known data (on
which training is performed), the noise accumulates in a positive feedback loop. This causes the
long-standing issue of confirmation bias (Tarvainen & Valpola, 2017). To mitigate this problem, we
propose the cross-training strategy. Specifically, we randomly partition the unlabeled data Du into
two complementary subsets Du[(1)] and Du[(2)][. The partition is fixed across all the training rounds. The]
self-training conducts alternatively for Du[(1)] and Du[(2)][. For][ r][ =][ {][1][, . . ., R][}][, when][ r][ is odd, we use a]
combination of D[Ëœ] _u[(1)]_ and Dl to train the model, and the trained model predicts pseudo-labels for the
next round on Du[(2)][. When][ r][ is even, we use a combination of][ Ëœ]Du[(2)] and Dl to train the model, and
perform pseudo-label prediction on Du[(1)][. Before each round, the model parameters are re-initialized]
with random weights following Cascante-Bonilla et al. (2021) to avoid noise accumulation.

3.4 CURRICULUM RESIDUAL LEARNING

Directly learning for R rounds of curriculum parameters separately can be inefficient. Considering
that the model is reinitialized and retrained in each self-training round, we assume that the optimal


-----

curricula for different rounds should have some similar patterns. We propose a greedy multi-step
searching algorithm. For round r, we use the searched curriculum in the previous round r âˆ’ 1 as
the base curriculum to guide the searching of current curricula Î“[r]. Inspired by ResNet (He et al.,
2016), we propose the curriculum residual learning strategy. Formally, we learn a bias term âˆ†Î“[r]

around the base curriculum, i.e. Î“[r] = (Î“[r][âˆ’][1])[âˆ—] + âˆ†Î“[r], where (Î“[r][âˆ’][1])[âˆ—] means the searched optimal
curriculum for round r âˆ’ 1, and (Î“[0])[âˆ—] is initialized with all zeros. We empirically find that this
strategy accelerates the model convergence speed and achieves marginally better performance. To
further reduce the search space, we choose every G epochs as an epoch group, which shares the
same threshold parameters for pseudo-labeled data selection. In total, we have NG epoch groups,
and the size of each epoch group is G. Therefore, the search space is reduced by a factor of G,
where G = 10 in our implementation.

3.5 CURRICULUM SEARCH VIA REINFORCEMENT LEARNING

Our PLACL can be formulated as an optimization problem shown in Eq. 3. In the inner-loop, we
optimize the weights Ï‰ of the keypoint localization network Î˜Ï‰ to minimize the training loss L
(see Alg. 1). In the outer-loop, we apply proximal policy optimization (PPO2) algorithm (Schulman
et al., 2017) to search for the curriculum Î“ that maximize the evaluation metric Î¾ (e.g. PCK) on the
validation set Dval (see Alg. 2).

maxÎ“ _Î¾(Î“) = Î¾(Î˜Ï‰âˆ—(Î“); Dval),_ s.t. _Ï‰[âˆ—](Î“) = arg minÏ‰_ _L(Î˜Ï‰; Dtrain, Î“)._ (3)

The training consists of R rounds. In each round r, the PPO2 search process consists of T sampling
steps. In each step, M sets of parameters are sampled independently from a truncated normal distribution (Nakano et al., 2012; Fujita & Maeda, 2018), âˆ†Î“[r] _âˆ¼Ntrunc[0,1]_ _Âµ[r]t_ _[, Ïƒ][2][I]_, where Âµ[r]t [and]
_Ïƒ[2]I are the mean and covariance (Ïƒ is fixed to 0.2 in practice). These sampled parameters are used_
  
to construct M different training curricula for training M keypoint localization networks separately.
Then the mean of the distribution is updated by PPO2 algorithm according to the evaluation score
of the M networks.

The objective function of PPO2 is formulated in Eq. 4.

_Ï€Âµr_ âˆ†Î“[r]j [;][ Âµ][r][, Ïƒ][2][I] _Ï€Âµr_ âˆ†Î“[r]j [;][ Âµ][r][, Ïƒ][2][I]

_J (Âµ[r]) = EÏ€_ min _Î¾[Ëœ]_ Î“[r]j _, CLIP_ _, 1_ _Ïµ, 1 + Ïµ_ _Î¾Ëœ_ Î“[r]j

" _Ï€Âµ[r]t_ [ ]âˆ†Î“[r]j [;][ Âµ]t[r][, Ïƒ][2][I]    _Ï€Âµ[r]t_ [ ]âˆ†Î“[r]j [;][ Âµ]t[r][, Ïƒ][2][I] _âˆ’_ ! (4)  !#

     

where the function CLIP(x, 1 âˆ’ _Ïµ, 1 + Ïµ) clips x to be no more than 1 + Ïµ and no less than 1 âˆ’_ _Ïµ._
Following the common practice (Li et al., 2021a), the mean reward is subtracted for better convergence. _Î¾[Ëœ]_ Î“[r]j = Î¾ Î“[r]j _âˆ’_ _M1_ _Mj=1_ _[Î¾]_ Î“[r]j and the policy Ï€Âµr is defined as the probability density

function (PDF) of the truncated normal distribution. PPO2 enforces the probability ratio between

      P   

old and new policiescontrol the size of each policy update. We then compute the gradients and update the parameters by Ï€Âµ[r] âˆ†Î“[r]j [;][ Âµ][r][, Ïƒ][2][I] _/Ï€Âµ[r]t_ âˆ†Î“[r]j [;][ Âµ]t[r][, Ïƒ][2][I] to stay within a small interval to
_Âµ[r]t+1_ _t_ [+][ Î±][âˆ‡][Âµ][r] _[J][ (][Âµ][r][ ][)][ with a learning rate of]_   _[ Î± >][ 0][. After]_ _[ T][ sampling steps, we choose][ Âµ]t[r]_
with the highest average evaluation score as[â†] _[Âµ][r]_ (Î“[r])[âˆ—]. And (Î“[r])[âˆ—] is used as the base curriculum for the
next round. And after R rounds, our final optimal curriculum is obtained, Î“[âˆ—] = [(Î“[1])[âˆ—], . . ., (Î“[R])[âˆ—]].


**Training details** In the training phase, the keypoint localization network and the curriculum search
policy are simultaneously optimized. For the outer-loop, the PPO2 (Schulman et al., 2017) search
procedure is conducted for T = 16 sampling steps, and in each step M = 8 sets of parameters (curriculum) are sampled. The clipping threshold is Ïµ = 0.2, and Âµ[r]t+1 [is updated with the learning rate]
of Î± = 0.2. We empirically use R = 6 self-training rounds, and group size G = 10 for curriculum
search. For the inner-loop, we follow the common practice (Sun et al., 2019; Contributors, 2020) to
train the keypoint localization network with Mean-Squared Error (MSE) loss for N = 210 epochs
per round. Adam (Kingma & Ba, 2015) with a learning rate of 0.001 is adopted. We reduce the
learning rate by a factor of 10 at the 170-th and 200-th epochs. Although the RL search process increases the training complexity, the total training cost is not too high (only 1.5 days with 32 NVIDIA
Tesla V100 GPUs). More detailed training settings for each task are provided in Â§4.2 and Â§4.3.


-----

Table 1: Keypoint localization with different percentage of labeled images. We report mean and
standard deviation from three runs for different randomly sampled labeled subsets. Pseudo-labeling
(PL) based methods are not evaluated for 100% of labeled data because there is no unlabeled data
to generate pseudo-labels for. The results marked with â€˜*â€™ are from (Moskvyak et al., 2020).

Percentage of labeled images
Method 5% 10% 20% 50% 100%

**Dataset 1: LSPET**
HRNet[âˆ—] (Sun et al., 2019) 40.19Â±1.46 45.17Â±1.15 55.22Â±1.41 62.61Â±1.25 72.12Â±0.30
ELT[âˆ—] (Honari et al., 2018) 41.77Â±1.56 47.22Â±0.91 57.34Â±0.94 66.81Â±0.62 72.22Â±0.13
Gen[âˆ—] (Jakab et al., 2018) 61.01Â±1.41 67.75Â±1.00 68.80Â±0.91 69.70Â±0.77 72.25Â±0.55
SSKL[âˆ—] (Moskvyak et al., 2020) 66.98Â±0.94 69.56Â±0.66 71.85Â±0.33 72.59Â±0.56 74.29Â±0.21
PL[âˆ—] (Radosavovic et al., 2018) 37.36Â±1.89 42.05Â±1.68 48.86Â±1.23 64.45Â±0.96 - 
CL (Cascante-Bonilla et al., 2021) 61.27Â±1.54 65.43Â±1.19 69.14Â±0.93 70.29Â±1.18 - 
PLACL (Ours) **70.76Â±1.47** **71.91Â±1.15** **72.30Â±0.88** **72.73Â±1.23** - 

**Dataset 2: MPII**
HRNet[âˆ—] (Sun et al., 2019) 66.22Â±1.60 69.18Â±1.03 71.83Â±0.87 75.73Â±0.35 81.11Â±0.15
ELT[âˆ—] (Honari et al., 2018) 68.27Â±0.64 71.03Â±0.46 72.37Â±0.58 77.75Â±0.31 81.01Â±0.15
Gen[âˆ—] (Jakab et al., 2018) 71.59Â±1.12 72.63Â±0.62 74.95Â±0.32 79.86Â±0.19 80.92Â±0.32
SSKL[âˆ—] (Moskvyak et al., 2020) 74.15Â±0.83 76.56Â±0.48 78.46Â±0.36 80.75Â±0.32 82.12Â±0.14
PL[âˆ—] (Radosavovic et al., 2018) 62.44Â±1.75 64.78Â±1.44 69.35Â±1.11 77.43Â±0.48 - 
CL (Cascante-Bonilla et al., 2021) 72.03Â±1.56 73.15Â±0.95 75.80Â±0.92 77.49Â±0.35 - 
PLACL (Ours) **77.83Â±1.41** **78.36Â±0.92** **79.68Â±0.72** **80.81Â±0.24** - 

**Dataset 3: CUB-200-2011**
HRNet[âˆ—] (Sun et al., 2019) 85.77Â±0.38 88.62Â±0.14 90.18Â±0.22 92.60Â±0.28 93.62Â±0.13
ELT[âˆ—] (Honari et al., 2018) 86.54Â±0.34 89.48Â±0.25 90.86Â±0.13 92.26Â±0.06 93.77Â±0.18
Gen[âˆ—] (Jakab et al., 2018) 88.37Â±0.40 90.38Â±0.22 91.31Â±0.21 92.79Â±0.14 93.62Â±0.25
SSKL[âˆ—] (Moskvyak et al., 2020) 91.11Â±0.33 91.47Â±0.36 92.36Â±0.30 92.80Â±0.24 93.81 Â±0.13
PL[âˆ—] (Radosavovic et al., 2018) 86.31Â±0.45 89.51Â±0.32 90.88Â±0.28 92.78Â±0.27 - 
CL (Cascante-Bonilla et al., 2021) 91.46Â±0.41 92.35Â±0.34 92.74Â±0.27 92.97Â±0.21 - 
PLACL (Ours) **93.01Â±0.33** **93.28Â±0.29** **93.45Â±0.25** **93.84Â±0.18** - 

**Dataset 4: ATRW**
HRNet[âˆ—] (Sun et al., 2019) 69.22Â±0.87 77.55Â±0.84 86.41Â±0.45 92.17Â±0.18 94.44Â±0.10
ELT[âˆ—] (Honari et al., 2018) 74.53Â±1.24 80.35Â±0.96 87.98Â±0.47 92.80Â±0.21 94.75Â±0.14
Gen[âˆ—] (Jakab et al., 2018) 89.54Â±0.57 90.48Â±0.49 91.16Â±0.13 92.27Â±0.24 94.80Â±0.13
SSKL[âˆ—] (Moskvyak et al., 2020) 92.57Â±0.64 94.29Â±0.66 94.49Â±0.36 94.63Â±0.18 95.31Â±0.12
PL[âˆ—] (Radosavovic et al., 2018) 67.97Â±1.07 75.26Â±0.74 84.69Â±0.57 92.15Â±0.24 - 
CL (Cascante-Bonilla et al., 2021) 87.01Â±1.08 89.13Â±0.94 92.34Â±0.51 93.57Â±0.26 - 
PLACL (Ours) **94.37Â±0.86** **94.59Â±0.80** **94.85Â±0.48** **95.01Â±0.17** - 

**Dataset 5: MS-COCOâ€™2017**
HRNet (Sun et al., 2019) 62.44Â±1.26 66.02Â±1.07 69.62Â±0.84 72.81Â±0.73 74.61Â±0.58
CL (Cascante-Bonilla et al., 2021) 64.47Â±1.18 67.82Â±0.95 70.36Â±0.89 72.92Â±0.84
PLACL (Ours) **69.39Â±1.03** **70.11Â±0.89** **71.84Â±0.66** **73.42Â±0.57** - 


4 EXPERIMENTS

4.1 DATASETS AND EVALUATION METRICS

**Datasets:** To show the versatility of PLACL, we conduct experiments on 5 diverse datasets.
**LSPET (Leeds Sports Pose Extended Dataset) (Johnson & Everingham, 2010; 2011) consists of im-**
ages of people doing sports activities. We use 10,000 images from (Johnson & Everingham, 2011)
for training and 2,000 images from (Johnson & Everingham, 2010) for validation and testing. MPII
Human Pose dataset (Andriluka et al., 2014) is a well-known benchmark for human pose estimation.
The images are collected from YouTube videos, showing people doing daily human activities. We
follow (Moskvyak et al., 2020) to use 10,000 random images from MPII train for training, 3,311
images from MPII train for validation and MPII val for evaluation. CUB-200-2011 (CaltechUCSD Birds-200-2011) (Welinder et al., 2010) dataset is a well-known dataset for SSL. It consists
of 200 fine-grained bird species with 15 keypoint annotations. We follow (Moskvyak et al., 2020) to
split dataset into training (100 categories with 5,864 images), validation (50 categories with 2,958
images) and testing (50 categories with 2,966 images). ATRW (Li et al., 2019c) dataset contains


-----

images of 92 Amur tigers captured from multiple wild zoos in challenging and unconstrained conditions. For each tiger, 15 body keypoints are annotated. The dataset consists of 3,610 images for
training, 516 for validation, and 1,033 for testing. MS-COCOâ€™2017 (Lin et al., 2014) is a popular
large-scale benchmark for human pose estimation, which contains over 150,000 annotated people.
We randomly select 500 images from COCO train for validation, the remaining training set (115k
images) for training, and COCO val (5k images) for evaluation. We use this dataset to validate the
applicability of our approach on large-scale data. AnimalPose (Cao et al., 2019) dataset contains
5,517 instances of five animal categories: dog, cat, horse, sheep, and cow. It consists of 2,798 images for training, and the 810 images for validation and 1,000 images for testing. We use it to test
the generalization and domain-transfer capacity of our proposed method.

**Evaluation Metrics:** **PCK (Probability of Correct Keypoint): A detected keypoint is considered**
correct if the distance between the predicted and true keypoint is within a certain threshold (Î±l),
where Î± is a constant and l is the longest side of the bounding box. We adopt PCK@0.1 (Î± = 0.1)
for LSPET (Johnson & Everingham, 2011), CUB-200-2011 (Welinder et al., 2010), and ATRW (Li
et al., 2019c) datasets. PCKh is adapted from PCK, where l is the head size that corresponds
to 60% of the diagonal length of the ground-truth head box. We adopt PCKh@0.5 (Î± = 0.5) for
MPII (Andriluka et al., 2014) dataset. Standard AP (Average Precision) is another commonly used
evaluation metric. It is based on object keypoint similarity (OKS), which measures the distance
between predicted keypoints and ground-truth keypoints normalized by the scale of the object. We
use mAP for AnimalPose (Cao et al., 2019) datasets.

4.2 COMPARISONS WITH THE STATE-OF-THE-ART SSL APPROACHES

In Table 1, we compare with the supervised baseline (HRNet (Sun et al., 2019)) and other stateof-the-art SSL approaches. We experiment with different percentages of labeled images (5%, 10%,
20%, 50%, and 100%). For fair comparisons, all results are obtained using HRNet-w32 backbone
with the input size of 256 Ã— 256. We follow (Moskvyak et al., 2020) to prepare datasets and exclude
half body transforms, and testing tricks (post-processing, and flip testing).

**Comparisons with consistency regularization methods. ELT (Honari et al., 2018) (equivariant**
landmark transformation) loss encourages the model to output keypoints that are equivariant to input transformations. Gen (Jakab et al., 2018) learns to extract geometry-related features through
conditional image generation. SSKL (Moskvyak et al., 2020) learns the pose invariant keypoint
representations with semantic keypoint consistency constraints. These consistency regularization
methods have shown superior results over the supervised baseline, however, they are inferior to our
PLACL method on all datasets and different percentages of labeled samples. Especially we show that
PLACL is mostly effective for low data regimes. For example, in CUB-200-2011 dataset, PLACL
with only 5% labeled data achieves better performance (93.01 vs 92.80) than SSKL with 50% labeled data. And in LSPET dataset, we show that PLACL improves the performance of baseline by
a large margin from 40.19 to 70.76 with 5% labeled images.

**Comparisons with pseudo-labeling method. We also compare with a pseudo-labeling (PL) base-**
line (Radosavovic et al., 2018). Overall PLACL significantly outperforms the PL baseline on all
datasets. As pointed out by Moskvyak et al. (2020), the vanilla PL approach does not perform well
for the keypoint localization task with a low data regime, due to the lack of an effective pseudolabel selection scheme. Instead, PLACL is able to automatically select high-quality pseudo-labeled
samples, which is the key to the success of pseudo-labeling based methods.

**Comparisons with curriculum-learning method. Curriculum Labeling (CL) (Cascante-Bonilla**
et al., 2021) is a recently proposed approach that applies a hand-crafted curriculum to facilitate
training of SSL. We observe that our proposed PLACL significantly outperforms CL, which validates the effectiveness of our proposed automatic curriculum learning.

**Experiments on large-scale datasets. Inspired by Zhou et al. (2020), in order to decrease the RL**
curriculum search cost for the large-scale MS-COCO (Lin et al., 2014) and full MPII (Andriluka
et al., 2014) datasets, we use a light proxy task with reduced number of training samples (5k) for RL
curriculum search. After the search procedure, we re-train the keypoint networks with the searched
curriculum on the full training set and evaluate them on the test set. Please refer to A.4 for more
analysis about proxy tasks and A.5 for experiments on the full MPII (Andriluka et al., 2014) dataset.


-----

Group Size (G)

10 15 20


95

85

75

65

55

45

35


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|70 1|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col21|Col22|Col23|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||PCK@0. Accuracy: 65 Test 60|||||||||||||
||||||||||||||||||||||||
||||||||ATRW|(5%)||||||||||F P P P P|ixed Th LACL ( LACL ( LACL ( LACL (|reshold G=20) G=15) G=10) G=5)|||
||||||||LSP (5 CUB (|%) 5%)|||||||||||||||
||||||||MPII (|5%)|||||||||||||||
||||||||||||||||||||||||


0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

Threshold

(b)


Number of Self-Training Rounds

(a)


Figure 2: (a) Accuracy on different datasets (5% labeled data) with various rounds (R). (b) Comparisons with different fixed thresholds (blue dots) and different group sizes G. Experiments are
conducted on LSPET dataset (5% labeled data).

4.3 EVALUATION OF GENERALIZATION CAPACITY


**Generalization to different domains. We investigate the generalization ability of our proposed**
model on domain transfer. To this end, we conduct experiments on AnimalPose (Cao et al., 2019)
dataset in Table 2. Specifically, we follow (Cao et al., 2019) to choose one animal class (e.g. cat) as
the target domain and the remaining four classes for the source domain. The images of the source domain are fully annotated, while the images of the target domain are unlabeled. For fair comparisons,
we adopt the AlphaPose model (Fang et al., 2017), which uses ResNet-101 (Xiao et al., 2018) as
the backbone. All models are trained with the pose-labeled human dataset involved. The AlphaPose
baseline model is pre-trained on the human dataset and fine-tuned on the labeled source animal data.
For pseudo-labeling based approaches (Inoue et al., 2018; Cao et al., 2019), the unlabeled target animal data is used for pseudo-labeling. For domain adaptation approaches (Tzeng et al., 2015; Long
et al., 2016), the unlabeled target data is used for domain transfer. Please refer to Cao et al. (2019)
for details about the compared approaches. We observe that our proposed approach consistently
outperforms the previous state-of-the-art methods on cross-domain semi-supervised learning.

Table 2: Dataset 6: AnimalPose. Evaluation of generalization capacity to the target unseen animal
class. All results are obtained using the AlphaPose model (Fang et al., 2017) with ResNet-101 (He
et al., 2016) as the backbone. Results marked with â€˜*â€™ are from Cao et al. (2019).

mAP for each class

Method cat dog sheep cow horse

AlphaPose Baseline[âˆ—] (Fang et al., 2017) 37.6 37.3 49.4 50.3 47.9
Dom Confusion[âˆ—] (Tzeng et al., 2015) 38.0 37.7 49.5 50.6 48.5
Residual Transfer[âˆ—] (Long et al., 2016) 37.8 38.2 49.1 50.8 48.6
CycleGAN+PL[âˆ—] (Inoue et al., 2018) 35.9 36.7 48.0 50.1 48.1
WS-CDA+PPLO[âˆ—] (Cao et al., 2019) 42.3 41.0 54.7 57.3 53.1
PLACL (Ours) **47.1 42.9 59.5 58.4 66.0**


4.4 ANALYSIS

**Number of self-training rounds. Along with the increasing of self-training rounds (R), the quality**
of the pseudo-labels gradually improves (see Fig. 3) and the test accuracy increases (see Fig. 2a)
until saturation. The experiments are conducted on multiple datasets with 5% labeled data. Interestingly, different datasets require a different number of rounds to achieve optimal, because four-legged
animals (ATRW (Li et al., 2019c)) have more pose variations than birds (CUB-200-2011 (Welinder
et al., 2010)). We use R = 6, because further increasing R does not bring significant gains.


-----

**Comparisons with pseudo-labeling with different fixed thresholds. As shown in Fig. 2b, we**
compare our PLACL with 10 static thresholds from 0.0 to 0.9. We observe that PLACL clearly
outperforms all these fixed threshold alternatives. Moreover, we find that the accuracy will be significantly affected by different thresholds (61.01% PCK for Î³ = 0.0 vs 67.35% PCK for Î³ = 0.7).

**Choice of epoch group size. In Fig. 2b, we also compare the performance of different epoch group**
sizes (G). We empirically find that smaller G will produce better performance, but at the cost of
increased search space. We choose G = 10 to trade-off between accuracy and efficiency.

4.5 ABLATION STUDIES

In Table 3, we present ablation studies to measure the contribution of each component. All ablative
experiments are conducted on LSPET (Johnson & Everingham, 2011) dataset with 5% labeled data.
PCK@0.1 is adopted as the evaluation metric.

**Effect of cross-training strategy. We find that without cross-training strategy the accuracy signif-**
icantly drops from 70.76 to 67.13, due to noise accumulation over time. Especially, we find that
there is little to no improvement after the first self-training round.

**Effect of curriculum learning. We compare PLACL with the alternative that only searches for a**
fixed threshold via RL. We find that using dynamic thresholds improves upon the fixed-threshold
alternative by a large margin, which validates the effectiveness of curriculum learning.

**Effect of parameter search. We compare PPO2 (Schulman et al., 2017) search with Random**
_Search (w/o PPO2 search). We randomly sampled T Ã— M curricula and pick out the best one for_
comparisons. PPO2 search obtains much better performance (70.76 vs 65.42). This indicates that
the searching problem is non-trivial and that our searching algorithm is very effective.

We also compare with manually designed curricula whose thresholds are gradually decreasing on
the epoch level. We tried five curricula with different decrease slopes and reported the best one
as Manually Design. We observe that PLACL significantly outperforms the manually-designed
curriculum baseline which validates the effectiveness of automatic curriculum search.

Table 3: Ablation studies on LSPET dataset with 5% labeled data.

Method PCK@0.1

PLACL, w/o cross-training strategy 67.13
PLACL, w/o curriculum learning 68.51
PLACL, w/o PPO2 search (Random Search) 65.42
PLACL, w/o PPO2 search (Manually Design) 65.71

PLACL, full method 70.76

5 CONCLUSIONS

We propose a novel Pseudo-labeled Auto-Curriculum Learning (PLACL) for the task of semisupervised keypoint localization. We propose to learn a curriculum to automatically select reliable
pseudo-labels and propose cross-training strategy to mitigate the confirmation bias problem. Extensive experiments on 6 diverse datasets validate the effectiveness and versatility of the proposed
method. We believe that our proposed approach is generic and we plan to investigate the applicability of PLACL on other visual tasks, such as object detection and semantic segmentation.

**Acknowledgement. We thank Lumin Xu and Wang Zeng for their valuable feedback to the paper.**
Ping Luo is supported by the General Research Fund of HK No.27208720 and 17212120. Wanli
Ouyang was supported by the Australian Research Council Grant DP200103223, FT210100228,
and Australian Medical Research Future Fund MRFAI000085.


-----

REFERENCES

Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2d human pose estimation: New
benchmark and state of the art analysis. In IEEE Conf. Comput. Vis. Pattern Recog., 2014. 2, 6, 7, 16

Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using
reinforcement learning. Int. Conf. Learn. Represent., 2017. 3

Yoshua Bengio, JÂ´erË†ome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Int. Conf.

Machine Learning., pp. 41â€“48, 2009. 1, 3

Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment problem?
In Int. Conf. Comput. Vis., pp. 1021â€“1030, 2017. 2

Jinkun Cao, Hongyang Tang, Hao-Shu Fang, Xiaoyong Shen, Cewu Lu, and Yu-Wing Tai. Cross-domain
adaptation for animal pose estimation. In Int. Conf. Comput. Vis., pp. 9498â€“9507, 2019. 2, 7, 8

Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, and Vicente Ordonez. Curriculum labeling: Revisiting pseudolabeling for semi-supervised learning. AAAI, 2021. 2, 3, 4, 6, 7, 16

[MMPose Contributors. Openmmlab pose estimation toolbox and benchmark. https://github.com/](https://github.com/open-mmlab/mmpose)
[open-mmlab/mmpose, 2020. 5](https://github.com/open-mmlab/mmpose)

Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning
augmentation strategies from data. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 113â€“123, 2019. 3

Xuanyi Dong and Yi Yang. Teacher supervises students how to learn from partially labeled images for facial
landmark detection. In Int. Conf. Comput. Vis., pp. 783â€“792, 2019. 2

Xuanyi Dong, Mingxing Tan, Adams Wei Yu, Daiyi Peng, Bogdan Gabrys, and Quoc V Le. Autohas: Efficient
hyperparameter and architecture search. arXiv preprint arXiv:2006.03656, 2020. 3

Haodong Duan, Kwan-Yee Lin, Sheng Jin, Wentao Liu, Chen Qian, and Wanli Ouyang. Trb: a novel triplet
representation for understanding 2d human body. In Int. Conf. Comput. Vis., 2019. 1

Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. Learning to teach. arXiv preprint

arXiv:1805.03643, 2018. 3

Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. Rmpe: Regional multi-person pose estimation. In Int.

Conf. Comput. Vis., pp. 2334â€“2343, 2017. 8

Yasuhiro Fujita and Shin-ichi Maeda. Clipped action policy gradient. In Int. Conf. Machine Learning., pp.
1597â€“1606, 2018. 5

Efstratios Gavves, Basura Fernando, Cees GM Snoek, Arnold WM Smeulders, and Tinne Tuytelaars. Finegrained categorization by alignments. In Int. Conf. Comput. Vis., pp. 1713â€“1720, 2013. 1

Efstratios Gavves, Basura Fernando, Cees GM Snoek, Arnold WM Smeulders, and Tinne Tuytelaars. Local
alignments for fine-grained categorization. Int. J. Comput. Vis., 111(2):191â€“212, 2015. 1

Zigang Geng, Ke Sun, Bin Xiao, Zhaoxiang Zhang, and Jingdong Wang. Bottom-up human pose estimation
via disentangled keypoint regression. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. 1

Alex Graves, Marc G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu. Automated curriculum
learning for neural networks. In Int. Conf. Machine Learning., pp. 1311â€“1320. PMLR, 2017. 3

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In

IEEE Conf. Comput. Vis. Pattern Recog., 2016. 5, 8

Sina Honari, Pavlo Molchanov, Stephen Tyree, Pascal Vincent, Christopher Pal, and Jan Kautz. Improving
landmark localization with semi-supervised learning. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 1546â€“
1555, 2018. 2, 6, 7

Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and Kiyoharu Aizawa. Cross-domain weakly-supervised
object detection through progressive domain adaptation. In IEEE Conf. Comput. Vis. Pattern Recog., pp.
5001â€“5009, 2018. 8

Tomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea Vedaldi. Unsupervised learning of object landmarks
through conditional image generation. In Adv. Neural Inform. Process. Syst., pp. 4020â€“4031, 2018. 6, 7


-----

Lu Jiang, Deyu Meng, Teruko Mitamura, and Alexander G Hauptmann. Easy samples first: Self-paced reranking for zero-example multimedia search. In ACM Int. Conf. Multimedia, pp. 547â€“556, 2014. 3

Sheng Jin, Xujie Ma, Zhipeng Han, Yue Wu, Wei Yang, Wentao Liu, Chen Qian, and Wanli Ouyang. Towards multi-person pose tracking: Bottom-up and top-down methods. In Int. Conf. Comput. Vis. Worksh.,
volume 2, pp. 7, 2017. 1

Sheng Jin, Wentao Liu, Wanli Ouyang, and Chen Qian. Multi-person articulated tracking with spatial and
temporal embeddings. In IEEE Conf. Comput. Vis. Pattern Recog., 2019. 1

Sheng Jin, Wentao Liu, Enze Xie, Wenhai Wang, Chen Qian, Wanli Ouyang, and Ping Luo. Differentiable
hierarchical graph grouping for multi-person pose estimation. In Eur. Conf. Comput. Vis., 2020a. 1

Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen Qian, Wanli Ouyang, and Ping Luo. Whole-body
human pose estimation in the wild. In Eur. Conf. Comput. Vis., 2020b. 2

Sam Johnson and Mark Everingham. Clustered pose and nonlinear appearance models for human pose estimation. In Brit. Mach. Vis. Conf., 2010. 6

Sam Johnson and Mark Everingham. Learning effective human pose estimation from inaccurate annotation. In

IEEE Conf. Comput. Vis. Pattern Recog., 2011. 2, 6, 7, 9

Faisal Khan, Bilge Mutlu, and Jerry Zhu. How do humans teach: On curriculum learning and teaching dimension. In Adv. Neural Inform. Process. Syst., pp. 1449â€“1457, 2011. 3

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Int. Conf. Learn.

Represent., 2015. 5

M Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. Adv. Neural

Inform. Process. Syst., 23:1189â€“1197, 2010. 3

Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural
networks. In Int. Conf. Machine Learning Worksh., volume 3, pp. 896, 2013. 1

Chen Li and Gim Hee Lee. From synthetic to real: Unsupervised domain adaptation for animal pose estimation.
In IEEE Conf. Comput. Vis. Pattern Recog., pp. 1482â€“1491, 2021. 2

Chuming Li, Xin Yuan, Chen Lin, Minghao Guo, Wei Wu, Junjie Yan, and Wanli Ouyang. Am-lfs: Automl for
loss function search. In Int. Conf. Comput. Vis., pp. 8410â€“8419, 2019a. 3

Hao Li, Chenxin Tao, Xizhou Zhu, Xiaogang Wang, Gao Huang, and Jifeng Dai. Auto seg-loss: Searching
metric surrogates for semantic segmentation. Int. Conf. Learn. Represent., 2021a. 3, 5

Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, and Cewu Lu. Crowdpose: Efficient crowded
scenes pose estimation and a new benchmark. In IEEE Conf. Comput. Vis. Pattern Recog., 2019b. 2

Jiefeng Li, Siyuan Bian, Ailing Zeng, Can Wang, Bo Pang, Wentao Liu, and Cewu Lu. Human pose regression
with residual log-likelihood estimation. In Int. Conf. Comput. Vis., 2021b. 1

Shuyuan Li, Jianguo Li, Weiyao Lin, and Hanlin Tang. Amur tiger re-identification in the wild. In Int. Conf.

Comput. Vis. Worksh., 2019c. 2, 6, 7, 8

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver,
and Daan Wierstra. Continuous control with deep reinforcement learning. Int. Conf. Learn. Represent.,
2016. 3

Tsung-Yi Lin, M. Maire, Serge J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick.
Microsoft coco: Common objects in context. In Eur. Conf. Comput. Vis., 2014. 2, 7, 16

Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with
residual transfer networks. Adv. Neural Inform. Process. Syst., 2016. 8

Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacherâ€“student curriculum learning. IEEE

Transactions on Neural Networks and Learning Systems, 31(9):3732â€“3740, 2019. 3

Geoffrey J McLachlan. Iterative reclassification procedure for constructing an asymptotically optimal rule of
allocation in discriminant analysis. Journal of the American Statistical Association, 70(350):365â€“369, 1975.


-----

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep
reinforcement learning. Nature, 518(7540):529â€“533, 2015. 3

Olga Moskvyak, Frederic Maire, Feras Dayoub, and Mahsa Baktashmotlagh. Semi-supervised keypoint localization. In Int. Conf. Learn. Represent., 2020. 2, 6, 7, 16

Daichi Nakano, Shin-ichi Maeda, and Shin Ishii. Control of a free-falling cat by policy-based reinforcement
learning. In International Conference on Artificial Neural Networks, pp. 116â€“123. Springer, 2012. 5

Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose estimation. In Eur.

Conf. Comput. Vis., 2016. 1

Avital Oliver, Augustus Odena, Colin Raffel, Ekin D Cubuk, and Ian J Goodfellow. Realistic evaluation of
deep semi-supervised learning algorithms. Adv. Neural Inform. Process. Syst., 2018. 1

Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via
parameters sharing. In Int. Conf. Machine Learning., pp. 4095â€“4104. PMLR, 2018. 3

Ilija Radosavovic, Piotr DollÂ´ar, Ross Girshick, Georgia Gkioxari, and Kaiming He. Data distillation: Towards
omni-supervised learning. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 4119â€“4128, 2018. 6, 7

Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint

arXiv:1710.05941, 2017. 3

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Int. Conf. Machine Learning., pp. 1889â€“1897. PMLR, 2015. 3

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017. 5, 9

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go
with deep neural networks and tree search. Nature, 529(7587):484â€“489, 2016. 3

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas
Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354â€“359, 2017. 3

Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. Baby steps: How â€œless is moreâ€ in unsupervised
dependency parsing. 2009. 3

Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human
pose estimation. In IEEE Conf. Comput. Vis. Pattern Recog., 2019. 1, 5, 6, 7, 15, 16

Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets
improve semi-supervised deep learning results. Int. Conf. Learn. Represent., 2017. 2, 4

Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and
tasks. In Int. Conf. Comput. Vis., pp. 4068â€“4076, 2015. 8

Norimichi Ukita and Yusuke Uematsu. Semi-and weakly-supervised human pose estimation. Computer Vision

and Image Understanding, 170:67â€“78, 2018. 2

P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200.
Technical Report CNS-TR-2010-001, California Institute of Technology, 2010. 2, 6, 7, 8

Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines for human pose estimation and tracking. In Eur.

Conf. Comput. Vis., 2018. 1, 8, 15

Lumin Xu, Yingda Guan, Sheng Jin, Wentao Liu, Chen Qian, Ping Luo, Wanli Ouyang, and Xiaogang Wang.
Vipnas: Efficient video pose estimation via neural architecture search. In IEEE Conf. Comput. Vis. Pattern
Recog., 2021. 1

Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for skeleton-based
action recognition. In AAAI, 2018. 1

Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, and Ce Zhu. Distribution-aware coordinate representation for
human pose estimation. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 7093â€“7102, 2020. 15


-----

Zixing Zhang, Fabien Ringeval, Bin Dong, Eduardo Coutinho, Erik Marchi, and BjÂ¨orn SchÂ¨uller. Enhanced
semi-supervised learning for multimodal emotion recognition. In ICASSP, pp. 5185â€“5189. IEEE, 2016. 2

Haiyu Zhao, Maoqing Tian, Shuyang Sun, Jing Shao, Junjie Yan, Shuai Yi, Xiaogang Wang, and Xiaoou Tang.
Spindle net: Person re-identification with human body region guided feature decomposition and fusion. In
IEEE Conf. Comput. Vis. Pattern Recog., pp. 1077â€“1085, 2017. 1

Qian Zhao, Deyu Meng, Lu Jiang, Qi Xie, Zongben Xu, and Alexander G Hauptmann. Self-paced learning for
matrix factorization. In AAAI, 2015. 3

Dongzhan Zhou, Xinchi Zhou, Wenwei Zhang, Chen Change Loy, Shuai Yi, Xuesen Zhang, and Wanli Ouyang.
Econas: Finding proxies for economical neural architecture search. In IEEE Conf. Comput. Vis. Pattern
Recog., pp. 11396â€“11404, 2020. 7, 16

Christian Zimmermann and Thomas Brox. Learning to estimate 3d hand pose from single rgb images. In Int.

Conf. Comput. Vis., pp. 4903â€“4911, 2017. 2

Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. Int. Conf. Learn.

Represent., 2017. 3

Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 8697â€“8710, 2018. 3


-----

A APPENDIX

A.1 PSEUDO-CODE FOR PLACL ALGORITHM

Here we present the pseudo-code for the proposed Pseudo-Labeled Auto-Curriculum Learning
(PLACL) algorithm. In PLACL, the keypoint localization network and the curriculum policy are
jointly optimized. In the inner-loop, we optimize the keypoint localization network as shown in
Algorithm 1. In the outer-loop, the curriculum policy is updated according to the performance of
the keypoint localization network, as shown in Algorithm 2.

**Algorithm 1: Inner-loop network training**
**Input:**

1. Labeled data Dl, pseudo-labeled data D[Ëœ] _u._
2. Number of training groups per round NG;
3. Curriculum Î“;

**Output: Obtained optimal network weights Ï‰[âˆ—](Î“).**


Random initialization of network weights ;
**for g = 1 to NG do**

Update current threshold Î³g = Î“[g] ;
Compute data selection vector g (Eq. 1) ;
Construct the training set Dtrain (Eq. 2) ;
Update network weights Ï‰ via back-propagation;

**return Ï‰[âˆ—](Î“) ;**


**Algorithm 2: Pseudo-Labeled Auto-Curriculum Learning (PLACL)**
**Input:**

1. Labeled data Dl, unlabeled data Du.
2. Initialized distribution Âµ and Ïƒ[2].
3. Number of rounds R.
3. Searching steps T ; Sampling number M .
4. Evaluation metric (e.g. PCK@0.1) Î¾.

**Output: Obtained the optimal curriculum Î“[âˆ—]** and the final network Î˜Ï‰âˆ— .

Initialize (Î“[0])[âˆ—] with all zeros.
Pre-train keypoint localization network Î˜[0]Ï‰ [using labeled data][ D][l][.]
**for r = 1 to R do**

**if r%2 == 1 then**

Predict keypoint pseudo-labels D[Ëœ] _u[(1)]_ with Î˜Ï‰[r][âˆ’][âˆ—] [1][;]

DËœ _u = D[Ëœ]_ _u[(1)]_


**else**

Predict keypoint pseudo-labels D[Ëœ] _u[(2)]_ with Î˜Ï‰[r][âˆ’][âˆ—] [1][;]
DËœ _u = D[Ëœ]_ _u[(2)]_

**for t = 1 to T do**

**for j = 1 to M do**

Sample parameter âˆ†Î“[r]j,t _Âµ[r]t_ _[, Ïƒ][2][I]_ ;

_[âˆ¼N][trunc][ [0][,][1]]_
Î“[r]j,t [= âˆ†Î“]j,t[r] [+ (Î“][r][âˆ’][1][)][âˆ—] [;]
  
Get Î˜[r]Ï‰[âˆ—],j,t [via inner-loop network training using][ Î“]j,t[r] [(Alg.][ 1][) ;]
Compute the evaluation metric Î¾ Î“[r]j,t = Î¾(Î˜[r]Ï‰[âˆ—],j,t[;][ D][val][)][ ;]

Compute the objective function J (Âµ) (Eq. 4);

  

(Î“[r])Update[âˆ—] = arg max Âµ[r]t+1 _[â†]Âµ[r]t_ _[Âµ]t[r]Mj[+]=1[ Î±][Î¾][âˆ‡]Î“[Âµ][r][r]j,t[J][ (][Âµ], âˆ€[r]t[)] = 1[ ;]_ _, . . ., T ;_
Get Î˜[r]Ï‰[âˆ—] [via network training using][ (Î“][r][)][âˆ—] [(Alg.][ 1][) ;]
P   

**return Î“[âˆ—]** = [(Î“[1])[âˆ—], . . ., (Î“[R])[âˆ—]] and Î˜[R]Ï‰[âˆ—] [;]


-----

A.2 VISUALIZATION OF PSEUDO-LABELED SAMPLES

In order to provide a better illustration of how pseudo-labels evolve in self-training rounds, we
visualize some pseudo-labeled samples for different datasets. We observe that the quality of pseudolabels gradually improves with the increase of the self-training rounds.

Round 0 Round 1 Round 2 Round 3 Round 4

Figure 3: Visualization of how pseudo-labels evolve in self-training rounds.

A.3 GENERALIZATION TO DIFFERENT KEYPOINT LOCALIZATION MODELS.

Table 4 shows the improvement when PLACL is applied to the recent state-of-the-art keypoint localization models which vary in model architectures and training/testing techniques. The experiments
are conducted on LSPET dataset with 5% labeled images and 95% unlabeled images. We show that
PLACL consistently improves the performance of the state-of-the-art approaches by a large margin.
PLACL does not require any knowledge of the keypoint localization models, making it easy to use
in practice.

Table 4: Performance improvement of different keypoint localization methods by PLACL. Experiments are conducted on CUB-200-2011 dataset (5% labeled data) with PCK@0.1 as the metric.

Method Backbone w/o PLACL w/ PLACL

SimpleBaseline (Xiao et al., 2018) ResNet-50 79.16 93.51
SimpleBaseline (Xiao et al., 2018) ResNet-101 81.34 93.66
SimpleBaseline (Xiao et al., 2018) ResNet-152 86.15 94.27
HRNet (Sun et al., 2019) HRNet-w32 85.86 93.01
HRNet (Sun et al., 2019) HRNet-w48 85.89 94.26
DARK (Zhang et al., 2020) HRNet-w32 86.67 94.18


-----

A.4 ANALYSIS OF PROXY TASKS

There are a lot of methods that target at improving the searching efficiency in literature, e.g. using
reduced-training proxy tasks (input size, model size, training samples, and training epochs can be
reduced (Zhou et al., 2020)). With these techniques, we are able to get orders of magnitude less
computation cost, but can still match the performance. Therefore, we believe that the scalability is
not a problem. For example, in order to further reduce the complexity, we use a light proxy task
(with reduced training samples) for the RL search process. Specifically, we randomly select a small
proportion of the training data (e.g. 5k images) for efficient curriculum search. After the search
procedure, we re-train the keypoint networks with the searched curriculum on the full training set
and evaluate them on the test set.

As shown in Table 5, we randomly select different number of training images (5k and 10k) for
RL curriculum search. We find that reducing the number of training images by half (from 10k to
5k) does not decrease the final performance much, which validates the effect of using proxy tasks.
Such a strategy enables us to apply the proposed PLACL approach to large-scale datasets, such as
MS-COCO (Lin et al., 2014) and the full MPII (Andriluka et al., 2014) datasets.

Table 5: We randomly select different number of training images (#Images) for RL curriculum
search, and re-train the keypoint networks with the searched curriculum on the full training set.

#Images 5% 10% 20% 50% 100%

**LSPET**
5k 70.72Â±1.49 **71.93Â±1.17** 72.24Â±0.82 72.71Â±1.19 - 
10k **70.76Â±1.47** 71.91Â±1.15 **72.30Â±0.88** **72.73Â±1.23** - 

**MS-COCOâ€™2017**
5k **69.39Â±1.03** 70.11Â±0.89 **71.84Â±0.66** 73.42Â±0.57 - 
10k 69.24Â±1.02 **70.12Â±0.87** 71.61Â±0.63 **73.43Â±0.61** - 

A.5 EXPERIMENTS ON THE FULL MPII DATASET

As shown in Table 6, we provide the results on the full MPII (Andriluka et al., 2014) dataset.
Since the codes of SSKL (Moskvyak et al., 2020) are not publicly available, we only compare with
CL (Cascante-Bonilla et al., 2021) in the experiments. We see that our proposed PLACL consistently outperforms CL, especially for low labeled data regime (5% and 10%). Note that our results
on the full MPII are obtained using reduced-training proxy tasks, i.e. we use 5K images for RL
curriculum search, and re-train the model with the obtained curriculum on the full training set.

Table 6: Comparisons with CL on the full MPII dataset.

Method 5% 10% 20% 50% 100%

**Full MPII**
HRNet (Sun et al., 2019) 78.00Â±1.35 81.89Â±0.94 82.94Â±0.67 88.34Â±0.45 89.76Â±0.17
CL (Cascante-Bonilla et al., 2021) 80.38Â±1.31 83.06Â±0.89 84.57Â±0.68 88.72Â±0.34
PLACL (Ours) **82.21Â±1.22** **85.42Â±0.85** **86.24Â±0.56** **89.16Â±0.21** - 


-----

