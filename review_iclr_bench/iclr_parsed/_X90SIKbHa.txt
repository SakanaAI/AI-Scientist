# A CLASS OF SHORT-TERM RECURRENCE ANDERSON MIXING METHODS AND THEIR APPLICATIONS

**Fuchao Wei** [1], Chenglong Bao [3][,][4][‚àó], Yang Liu [1][,][2]

1Department of Computer Science and Technology, Tsinghua University
2Institute for AI Industry Research, Tsinghua University
3Yau Mathematical Sciences Center, Tsinghua University
4Yanqi Lake Beijing Institute of Mathematical Sciences and Applications
wfc16@mails.tsinghua.edu.cn, {clbao,liuyang2011}@tsinghua.edu.cn

ABSTRACT

Anderson mixing (AM) is a powerful acceleration method for fixed-point iterations, but its computation requires storing many historical iterations. The extra
memory footprint can be prohibitive when solving high-dimensional problems in
a resource-limited machine. To reduce the memory overhead, we propose a novel
class of short-term recurrence AM methods (ST-AM). The ST-AM methods only
store two previous iterations with cheap corrections. We prove that the basic version of ST-AM is equivalent to the full-memory AM in strongly convex quadratic
optimization, and with minor changes it has local linear convergence for solving
general nonlinear fixed-point problems. We further analyze the convergence properties of the regularized ST-AM for nonconvex (stochastic) optimization. Finally,
we apply ST-AM to several applications including solving root-finding problems
and training neural networks. Experimental results show that ST-AM is competitive with the long-memory AM and outperforms many existing optimizers.

1 INTRODUCTION

Anderson mixing (AM) (Anderson, 1965) is a powerful sequence acceleration method (Brezinski
et al., 2018) for fixed-point iterations and has been widely used in scientific computing (Lin et al.,
2019; Fu et al., 2020; An et al., 2017), e.g., the self-consistent field iterations in electronic structure
computations (Garza & Scuseria, 2012; Arora et al., 2017). Specifically, we consider a fixed-point
iteration xk+1 = g(xk), k = 0, 1, . . ., where g : R[d] _7‚Üí_ R[d] is the fixed-point map. By using
_m historical iterations, AM(m) aims to extrapolate a new iterate that satisfies certain optimality_
property. When the function evaluation is costly, the reduction of the number of iterations brought
by AM can save a large amount of computation (Fang & Saad, 2009).

AM can be used as a method for solving nonlinear equations (Kelley, 2018) as the fixed-point
problem x = g(x) is equivalent to h(x) := x ‚àí _g(x) = 0. In practice, since computing the_
Jacobian of h(x) is commonly difficult or even unavailable (Nocedal & Wright, 2006), AM can be
seen as a practical alternate for Newton‚Äôs method (An et al., 2017). Also, compared with the classical
iterative methods such as the nonlinear conjugate gradient (CG) method (Hager & Zhang, 2006), no
line-search or trust-region technique is used in AM, which is preferable for large-scale unconstrained
optimization. Empirically, it is observable that AM can largely accelerate convergence, though its
theoretical analysis is still under-explored. It turns out that in the linear case (Walker & Ni, 2011;
Potra & Engler, 2013), the full-memory AM (m = k) is essentially equivalent to GMRES (Saad
& Schultz, 1986), a powerful Krylov subspace method that can exhibit superlinear convergence
behaviour in solving linear systems (Van der Vorst & Vuik, 1993). For general nonlinear problems,
AM is recognized as a multisecant quasi-Newton method (Fang & Saad, 2009; Brezinski et al.,
2018). As far as we know, only local linear convergence has been obtained for the limited-memory
AM (m < k) in general (Toth & Kelley, 2015; Evans et al., 2020; De Sterck & He, 2021).

For the application of AM, one of the major concerns is the historical length m, a critical factor
related to the efficiency of AM (Walker & Ni, 2011). A larger m can incorporate more historical

_‚àóCorresponding author._


-----

information into one extrapolation, but it incurs heavier memory overhead since 2m vectors of
dimension d need to be stored in AM(m). The additional memory footprint can be prohibitive for
solving high-dimensional problems in a resource-limited machine (Deng, 2019). Using small m
can alleviate the memory overhead but may deteriorate the efficacy of AM since much historical
information is omitted in the extrapolation (Walker & Ni, 2011; Evans et al., 2020).

To address the memory issue of AM, we deeply investigate the properties of the historical iterations
produced by AM and leverage them to develop the short-term recurrence variant, namely ST-AM.
The basic version of ST-AM imposes some orthogonality property on the historical sequence, which
is inspired by the CG method (Hestenes & Stiefel, 1952) that enjoys a three-term recurrence. Furthermore, to better suit the more difficult nonconvex optimization, a regularized short-term form is
introduced. We highlight the main contributions of our work as follows.

1. We develop a novel class of short-term recurrence AM methods (ST-AM), including the
basic ST-AM, the modified ST-AM (MST-AM), and the regularized ST-AM (RST-AM).
The basic ST-AM is applicable for linear systems; MST-AM can solve general fixed-point
problems; RST-AM aims for solving stochastic optimization. An important feature of STAM is that all methods only need to store two previous iterations with cheap corrections,
which significantly reduces the memory requirement compared with the classical AM.

2. A complete theoretical analysis of the ST-AM methods is given. When solving strongly
convex quadratic optimization, we prove that the basic ST-AM is equivalent to the fullmemory AM and the convergence rate is similar to that of the CG method. We also prove
that MST-AM has improved local linear convergence for solving fixed-point problems.
Besides, we establish the global convergence property and complexity analysis for RSTAM when solving stochastic optimization problems.

3. The numerical results on solving (non)linear equations and cubic-regularized quadratic optimization are consistent with the theoretical results for the basic ST-AM and MST-AM.
Furthermore, extensive experiments on training neural networks for image classification
and language modeling show that RST-AM is competitive with the long-memory AM and
outperforms many existing optimizers such as SGD and Adam.

2 RELATED WORK

AM is also known as an extrapolation algorithm in scientific computing (Anderson, 2019). A parallel method is Shanks transformation (Shanks, 1955) which transforms an existing sequence to a new
sequence for faster convergence. Related classical algorithms include Minimal Polynomial Extrapolation (Cabay & Jackson, 1976) and Reduced Rank Extrapolation (Eddy, 1979), and a framework
of these extrapolation algorithms including AM is given in (Brezinski et al., 2018). Note that an
elegant recursive algorithm named œµ-algorithm had been discovered for Shanks transformation for
scalar sequence (Wynn, 1956), and was later generalized as the vector œµ-algorithm (Wynn, 1962) to
handle vector sequences, but this short-term recurrence form is not equivalent to the original Shanks
transformation in general (Brezinski & Redivo-Zaglia, 2017). Since AM is closely related to quasiNewton methods (Fang & Saad, 2009), there are also some works trying to derive equivalent forms
of the full-memory quasi-Newton methods using limited memory (Kolda et al., 1998; Berahas et al.,
2021), while no short-term recurrence is available. To the best of our knowledge, ST-AM is the first
attempt to short-term recurrence quasi-Newton methods.

Recently, there have been growing demands for solving large-scale and high-dimensional fixedpoint problems in scientific computing (Lin et al., 2019) and machine learning (Bottou et al., 2018).
For these applications, Newton-like methods (Byrd et al., 2016; Wang et al., 2017; Mokhtari et al.,
2018) are less appealing due to the heavy memory and computational cost, especially in nonconvex
stochastic optimization, where only sublinear convergence can be expected if only stochastic gradients can be accessed (Nemirovski & Yudin, 1983). On the other side, first-order methods (Necoara
et al., 2019) stand out for their low per-iteration cost, though the convergence can be slow in practice. When training neural networks, SGD with momentum (SGDM) (Qian, 1999), and adaptive
learning rate methods, e.g. AdaGrad (Duchi et al., 2011), RMSprop (Tieleman & Hinton, 2012),
Adam (Kingma & Ba, 2014), are very popular optimizers. Our methods have the nature of quasiNewton methods while the memory footprint is largely reduced to be close to first-order methods.
Thus, ST-AM can be a competitive optimizer from both theoretical and practical perspectives.


-----

3 METHODOLOGY

In this section, we give the details of the proposed ST-AM. We always assume the objective function
as f : R[d] _‚Üí_ R, the fixed-point map g : R[d] _7‚Üí_ R[d]. Moreover, we do not distinguish rk = ‚àí‚àáf (xk)
and rk = g(xk) ‚àí _xk in our discussion as ‚àáf_ (x) = 0 is equivalent to g(x) = x ‚àí‚àáf (x) = x.

3.1 ANDERSON MIXING

The AM finds the fixed point of g via maintaining two sequences of length m (m ‚â§ _k):_

_Xk = [‚àÜxk_ _m, ‚àÜxk_ _m+1,_ _, ‚àÜxk_ 1], Rk = [‚àÜrk _m, ‚àÜrk_ _m+1,_ _, ‚àÜrk_ 1] R[d][√ó][m], (1)
_‚àí_ _‚àí_ _¬∑ ¬∑ ¬∑_ _‚àí_ _‚àí_ _‚àí_ _¬∑ ¬∑ ¬∑_ _‚àí_ _‚àà_

can be decoupled into two steps, namely thewhere the operator ‚àÜ denotes the forward difference, e.g. projection step ‚àÜ and thexk = x mixing stepk+1 ‚àí _xk. Each update of AM:_

_x¬Øk = xk ‚àí_ _XkŒìk,_ (Projection step), _xk+1 = ¬Øxk + Œ≤kr¬Øk,_ (Mixing step), (2)

where ¬Ørk := rk ‚àí _RkŒìk and Œ≤k > 0 is the mixing parameter. The Œìk is determined by_

Œìk = arg min (3)
Œì R[m][ ‚à•][r][k][ ‚àí] _[R][k][Œì][‚à•][2][.]_
_‚àà_

Thus, the full form of AM (Fang & Saad, 2009; Walker & Ni, 2011) is

_xk+1 = xk + Œ≤krk ‚àí_ (Xk + Œ≤kRk) Œìk. (4)

**Remark 1. To see the rationality of AM, assume g is continuously differentiable, then we have**
_h(xj)‚àíh(xj‚àí1) ‚âà_ _h[‚Ä≤](xk)(xj_ _‚àíxj‚àí1) around xk, where h[‚Ä≤](xk) is the Jacobian of h(x) := x‚àíg(x)._
_Thus, we can recognize (3) as solvingSo, it is reasonable to assume Rk ‚âà‚àíh h[‚Ä≤]([‚Ä≤]x(kx)kX)dkk, and we see = h(xk) in a least-squares sense, where ‚à•rk ‚àí_ _RkŒì‚à•2 ‚âà‚à•rk + h[‚Ä≤](xk)X dkŒìk =‚à•2._
_XkŒìk. The mixing step incorporates rk into the new update xk+1 if Œ≤k > 0. Otherwise, if Œ≤k = 0,_
_then xk+1 = ¬Øxk is an interpolation of the previous iterates, leading to a stagnation._

3.2 THE BASIC SHORT-TERM RECURRENCE ANDERSON MIXING

The basic ST-AM is to solve the strongly convex quadratic optimization:

min (5)
_x_ R[d][ f] [(][x][) := 1]2 _[x][T][Ax][ ‚àí]_ _[b][T][x,]_
_‚àà_


where A 0. Let p 1 = q 1 = p0 = q0 = 0 R[d]. At the k-th iteration, given the two matrices
_‚âª_ _‚àí_ _‚àí_ _‚àà_
_Pk_ 1 = (pk 2, pk 1) R[d][√ó][2], Qk 1 = (qk 2, qk 1) R[d][√ó][2] and defining p = xk _xk_ 1 and
_q =‚àí rk ‚àí_ _rk‚àí‚àí1, the basic ST-AM constructs‚àí_ _‚àà_ _‚àí_ _‚àí_ _‚àí_ _‚àà_ _‚àí_ _‚àí_

_pÀú = p_ _Pk_ 1(Q[T]k 1[q][)][,] _qÀú = q_ _Qk_ 1(Q[T]k 1[q][)][,] (6a)
_‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_

_pk = Àúp/‚à•qÀú‚à•2,_ _qk = Àúq/‚à•qÀú‚à•2._ (6b)

Then, we update Pk = (pk‚àí1, pk), Qk = (qk‚àí1, qk) ‚àà R[d][√ó][2]. Such construction ensures Q[T]k _[Q][k][ =]_
_I2 for k ‚â•_ 2 and the storage of Pk and Qk is equal to AM(2). With the corrected Pk and Qk, the
ST-AM method modifies the projection step and the mixing step accordingly, that is,

_x¬Øk = xk ‚àí_ _PkŒìk,_ (Projection step), _xk+1 = ¬Øxk + Œ≤kr¬Øk,_ (Mixing step), (7)

whereand Rk Œì in (1) byk = arg min Pk and ‚à•r Qk ‚àík respectively and imposes the orthogonality condition onQkŒì‚à•2 = Q[T]k _[r][k][ and][ ¬Ø]rk = rk ‚àí_ _QkŒìk. Thus, the ST-AM replaces Qk. The details Xk_
of basic ST-AM are given in Algorithm 2 in Appendix C.1. Define _P[¬Ø]k = (p1, p2, . . ., pk),_ _Q[¬Ø]k =_
(q1, q2, . . ., qk), the Krylov subspace _m(A, v)_ span _v, Av, A[2]v, . . ., A[m][‚àí][1]v_, the range of X
_K_ _‚â°_ _{_ _}_
as range(X). We give the properties of the basic ST-AM in Theorem 1.
**Theorem 1. Let {xk} be the sequence generated by the basic ST-AM. The following relations hold:**
_(i) ‚à•qÀú‚à•2 > 0, range( P[¬Ø]k) = range(Xk) = Kk(A, r0), range( Q[¬Ø]k) = range(Rk) = AKk(A, r0);_
_(ii)_ _Q[¬Ø]k = ‚àíAP[¬Ø]k,_ _Q[¬Ø][T]k_ _Q[¬Ø]k = Ik;_
_(iii) ¬Ørk ‚ä•_ range( Q[¬Ø]k) and ¬Øxk = x0 + zk, where zk = arg minz‚ààKk(A,r0) ‚à•r0 ‚àí _Az‚à•2._
_If ‚à•r¬Øk‚à•2 = 0, then xk+1 is the exact solution._


-----

The proof is in Appendix C.1. Note that the property (iii) in Theorem 1 exactly describes the relation
_x¬Øk = x[G]k_ [, where][ x][G]k [is the output of the][ k][-th iteration of GMRES (Saad & Schultz, 1986). Moreover,]
let ¬Øx[AM]k be the k-th intermediate iterate in the full-memory AM. It holds that ¬Øx[AM]k = x[G]k [(See]
Proposition 1 in Appendix C.1.), which induces that ¬Øxk = ¬Øx[AM]k = x[G]k [. This equivalence indicates]
that ST-AM is more efficient than AM and GMRES since only two historical iterations need to
be stored. Moreover, by directly applying the convergence analysis of GMRES (Corollary 6.33 in
(Saad, 2003)), we obtain the convergence rate of the basic ST-AM for solving (5):
**Corollary 1. Suppose the eigenvalues of A lie in [¬µ, L] with ¬µ > 0, and let {xk} be the se-**
_quence generated by the basic ST-AM, then thek_ _k-th intermediate residual ¬Ørk satisfies ‚à•r¬Øk‚à•2 ‚â§_

_‚àöL/¬µ_ 1
2 _‚àí_ _r0_ 2. Moreover, the algorithm finds the exact solution in at most (d + 1) iterations.

_‚àöL/¬µ+1_ _‚à•_ _‚à•_

 

**Remark 2. The GMRES can be simplified to an elegant three-term recurrence algorithm called the**
conjugate residual (CR) method (Algorithm 6.20 in (Saad, 2003)) when solving (5). Thus, a similar
_simplification for AM is expected to exist. Like CG and Chebyshev acceleration (Algorithm 12.1 in_
_(Saad, 2003)), the convergence rate of ST-AM has the optimal dependence on the condition number,_
_while ST-AM does not form the Hessian-vector products explicitly._

3.3 THE MODIFIED SHORT-TERM RECURRENCE ANDERSON MIXING

For general nonlinear fixed-point problems, global convergence may be unavailable for the basic STAM, as a counter-example exists for AM (Mai & Johansson, 2020). Thus, we propose a modified
version of the basic ST-AM (MST-AM) and prove the local linear convergence rate under similar
conditions used in (Toth & Kelley, 2015; Evans et al., 2020). Concretely, the MST-AM makes three
main changes to the basic ST-AM.

**Change 1: Instead of applying the normalization (6b), the MST-AM constructs pk and qk via**

_Œ∂k = (Q[T]k_ 1[Q][k][‚àí][1][)][‚Ä†][Q]k[T] 1[q,] _pk = p_ _Pk_ 1Œ∂k, _qk = q_ _Qk_ 1Œ∂k, (8)
_‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_

(whereQ[T]k _[Q] ‚Äú[k][)] ‚Ä†[‚Ä†][Q] ‚Äù is the Moore-Penrose inverse. Accordingly, we choosek[T][r][k][. This change relaxes the orthonormality for][ Q][k][ (][k][ ‚â•] Œì[2]k[), but keeps the orthogonality] = arg min ‚à•rk ‚àí_ _QkŒì‚à•2 =_
condition: Q[T]k 1[q][k][ = 0][. In fact,][ ¬Ø]Q[T]k 1[q][k][ = 0][ in the case of solving (5).]
_‚àí_ _‚àí_

**Change 2: MST-AM imposes the boundedness constraints on Pk** 1Œ∂k and Qk 1Œ∂k: If _Pk_ 1Œ∂k 2 >
_‚àí_ _‚àí_ _‚à•_ _‚àí_ _‚à•_
_cp_ _p_ 2 or _Qk_ 1Œ∂k 2 > cq _q_ 2, then Pk = Pk 1, Qk = Qk 1, where cp > 0, cq (0, 1) are
predefined constants. It is worth mentioning that adding some boundedness condition is common in‚à• _‚à•_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚à•_ _‚àí_ _‚àí_ _‚àà_
the analysis of AM (Toth & Kelley, 2015; Evans et al., 2020).

**Change 3: MST-AM restarts, i.e. setting Pk = Qk = 0 ‚àà** R[d][√ó][2] every m iterations. This restart
operation is to limit the number of higher-order terms appeared in the residual expansion in our
analysis and we can set m to be a large number in practice.

The detailed description of MST-AM is given in Appendix C.2. In the next theorem, we establish
the convergence rate analysis for the MST-AM.
**Theorem 2. Let {xk} be the sequence generated by MST-AM, x[‚àó]** _‚àà_ R[d] _be a fixed point of g and m_
_be the restarting period for MST-AM. Suppose that in the ball B(œÅ) := {x ‚àà_ R[d]|‚à•x _‚àí_ _x[‚àó]‚à•2 < œÅ} for_
_some œÅ > 0, g is Lipschitz continuously differentiable and there are constants Œ∫ ‚àà_ (0, 1) and ÀÜŒ∫ > 0
_Œ∫for everywith (i)0 ‚àà_ (0 ‚à•, 1) x, yg(. Ify) ‚ààB ‚àí x0g is sufficiently close to(x(œÅ))‚à•, where2 ‚â§ _Œ∫‚à• gy ‚àí[‚Ä≤]_ _is the Jacobian ofx‚à•2 for every x[‚àó], then for x, y g ‚ààB r. Assumek :=(œÅ g), and (ii)(x |k1) ‚àí ‚àí_ _xŒ≤ ‚à•kkg, the following bound holds:|[‚Ä≤] +(y Œ∫Œ≤) ‚àíkg ‚â§[‚Ä≤](xŒ∫)‚à•02 for a constant ‚â§_ _Œ∫ÀÜ‚à•y ‚àí_ _x‚à•2_

_mk_


_O_ _‚à•rk‚àíj‚à•2[2]_ _,_ (9)
_j=0_

X   


_‚à•rk+1‚à•2 ‚â§_ _Œ∏k(|1 ‚àí_ _Œ≤k| + Œ∫Œ≤k)‚à•rk‚à•2 + ÀÜŒ∫_


_and the errorswhere Œ∏k = ‚à•r¬Øk‚à•2x/k‚à•rkx‚à•2[‚àó] ‚â§2_ 1 converge and mk = R k-linearly. mod m. Thus, the residuals {rk} converge Q-linearly,
_{‚à•_ _‚àí_ _‚à•_ _}_
**Remark 3. In a local region around x[‚àó], the convergence rate is determined by the first-order term**
_Œ∏k(|1 ‚àí_ _Œ≤k| + Œ∫Œ≤k)‚à•rk‚à•2. We can choose Œ≤k = 1 such that |1 ‚àí_ _Œ≤k| + Œ∫Œ≤k = Œ∫ < 1. Since_
_r¬Øk is the orthogonal projection of rk onto the subspace range(Qk)[‚ä•], Œ∏k has the interpretation of_


-----

**Algorithm 1 RST-AM for stochastic programming**

**Input: x0** R[d], Œ≤k > 0, Œ±k [0, 1], Œ¥k[(1)] _> 0, Œ¥k[(2)]_ _> 0._
_‚àà_ _‚àà_
**Output: x ‚àà** R[d]

1: P0, Q0 = 0 ‚àà R[d][√ó][2], p0, q0 = 0 ‚àà R[d]

2: for k = 0, 1, . . ., until convergence, do
3: _rk = ‚àí‚àáfSk_ (xk)

4: **if k > 0 then**

5: _p = xk ‚àí_ _xk‚àí1, q = rk ‚àí_ _rk‚àí1_

6: _Œ∂k = (Q[T]k_ 1[Q][k][‚àí][1][ +][ Œ¥]k[(1)][P]k[ T] 1[P][k][‚àí][1][)][‚Ä†][Q][T]k 1[q]
_‚àí_ _‚àí_ _‚àí_

7: _qk = q_ _Qk_ 1Œ∂k, pk = p _Pk_ 1Œ∂k
_‚àí_ _‚àí_ _‚àí_ _‚àí_

8: _Pk = [pk_ 1, pk], Qk = [qk 1, qk]
_‚àí_ _‚àí_

9: **end if**

10: Check Condition (13) and use smaller Œ±k if (13) is violated

11: Œìk = (Q[T]k _[Q][k][ +][ Œ¥]k[(2)][P][ T]k_ _[P][k][)][‚Ä†][Q]k[T][r][k]_

13:12: _xx¬Økk =+1 = ¬Ø xk ‚àíxk +Œ±k Œ≤Pkkr¬ØŒìkk, ¬Ørk = rk ‚àí_ _Œ±kQkŒìk_

14: Apply learning rate schedule of Œ±k, Œ≤k

15: end for
16: return xk


_the direction-sine between rk and the subspace range(Qk). When Œ∏k is small, e.g., rk nearly lies in_
range(Qk), the acceleration by MST-AM is significant. Compared to AM(m), MST-AM incorporates
_historical information with orthogonalization. In the SPD linear case and without restart, the global_
_orthogonality property holds, i.e. ¬Ørk_ _range( Q[¬Ø]k), which means there is no loss of historical_
_information, while AM(m) (Evans et al., 2020) does not have such property in this ideal case. ‚ä•_

3.4 THE REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING

Inspired by the recent work on stochastic Anderson mixing (SAM) method (Wei et al., 2021), we
develop a regularized ST-AM (RST-AM) for solving nonconvex stochastic optimization problems.

_T_

Consider the nonconvex optimization problem minx‚ààRd f (x) := _T[1]_ _i=1_ _[f][Œæ]i_ [(][x][)][, where][ f][Œæ]i [:][ R][d][ ‚Üí]

R is the loss function corresponding to i-th data sample and T is the number of data samples. In
mini-batch training, the gradient is evaluated for fSk (xk) := _n1k_ _i‚ààPSk_ _[f][Œæ]i_ [(][x][k][)][, where][ S][k][ ‚äÜ] [[][T] [] :=]

1, 2, . . ., T is the sampled mini-batch, and nk := _Sk_ is the batch size. In this case, we set
_{_ _}_ _|_ _|_ P
_rk = ‚àí‚àáfSk_ (xk) (Line 3 in Algorithm 1), which is an unbiased estimate of the negative gradient.

Recalling from (8), Œ∂k = (Q[T]k‚àí1[Q][k][‚àí][1][)][‚Ä†][Q][T]k‚àí1[‚àÜ][r][k][‚àí][1][ = arg min][ ‚à•][‚àÜ][r][k][‚àí][1][ ‚àí] _[Q][k][‚àí][1][Œ∂][‚à•][2][ as][ q][ = ‚àÜ][r][k][‚àí][1]_
by definition. Since ST-AM is based on a local quadratic approximation (5) in a small region around
_xk, a large magnitude of_ _Pk_ 1Œ∂k 2 tends to make the change from ‚àÜxk 1 to pk = ‚àÜxk 1
_Pk_ 1Œ∂k too aggressively, which may lead to instability. Consequently, we add a penalty term in the ‚à• _‚àí_ _‚à•_ _‚àí_ _‚àí_ _‚àí_
_‚àí_
above least squares problem, i.e.

_Œ∂k = arg min_ ‚àÜrk 1 _Qk_ 1Œ∂ 2 [+][ Œ¥]k[(1)] 2[,] (10)
_‚à•_ _‚àí_ _‚àí_ _‚àí_ _‚à•[2]_ _[‚à•][P][k][‚àí][1][Œ∂][‚à•][2]_

where Œ¥k[(1)] _> 0. The same as SAM (Wei et al., 2021), we also add a regularization term for comput-_
ing Œìk via
Œìk = arg min ‚à•rk ‚àí _QkŒì‚à•2[2]_ [+][ Œ¥]k[(2)][‚à•][P][k][Œì][‚à•]2[2][,] (11)

where Œ¥k[(2)] _> 0, and a damping term Œ±k is used as shown in Line 12 in Algorithm 1. In practice, we_
choose the two regularization parameters as

2

_Œ¥k[(1)]_ = _‚à•‚àÜxck1‚àí‚à•r1k‚à•‚à•2[2]_ 2[2][+][ œµ][0] _, Œ¥k[(2)]_ = max  c‚à•pk2‚à•‚à•2[2]rk[+]‚à•[ œµ]2 [0] _, CŒ≤k[‚àí][2]_ _,_ (12)

where c1, c2, C > 0 are constants, and œµ0 > 0 is a small constant to bound the denominators
away from zero. Assuming _pk_ 1 2 _pk_ 2 = ( ‚àÜxk 1 2), the choices of (12) make
_‚à•_ _‚àí_ _‚à•_ _‚âà‚à•_ _‚à•_ _O_ _‚à•_ _‚àí_ _‚à•_
_Œ¥k[(1)][P]k[ T]_ 1[P][k][‚àí][1][‚à•][2][ ‚âàO][(][‚à•][r][k][‚à•]2[2][)][ and][ ‚à•][Œ¥]k[(2)][P]k[ T][P][k][‚à•][2][ ‚âàO][(][‚à•][r][k][‚à•]2[2][)][ aware of the change of the local]
_‚à•_ _‚àí_
curvature: large (small) ‚à•rk‚à•2 tends to lead to a large (small) regularization.


-----

**Remark 4.** _One update of xk given by Line 11-13 in Algorithm 1 can be formulated as xk+1 =_
_xk + Hkrk, where Hk = Œ≤kI ‚àí_ _Œ±kYkZk[‚Ä†][Q]k[T][, Y][k][ =][ P][k][ +][ Œ≤][k][Q][k][, Z][k][ =][ Q]k[T][Q][k][ +][ Œ¥]k[(2)][P]k[ T][P][k][. To]_
_guarantee the positive definiteness of Hk, we follow the same procedure in SAM. Let Œªk be the_
_largest eigenvalue of YkZk[‚Ä†][Q]k[T]_ [+][ Q][k][Z]k[‚Ä†][Y][ T]k _[. If][ Œ±][k][ satisfies]_
_Œ±kŒªk_ 2Œ≤k(1 _¬µ),_ (13)
_‚â§_ _‚àí_

_thenobtained by computing the largest eigenvalue of a matrix of s[T]k_ _[H][k][s][k][ ‚â•]_ _[Œ≤][k][¬µ][‚à•][s][k][‚à•]2[2][,][ ‚àÄ][s][k]_ _[‚àà]_ [R][d][, where][ ¬µ][ ‚àà] [(0][,][ 1)][ is a constant. Note that] R[4][√ó][4] _(see Appendix C.3.1).[ Œª][k]_ _[can be cheaply]_

We summarize the RST-AM in Algorithm 1 and establish its convergence properties here. First, we
impose the same assumptions on the objective function f as those in (Wei et al., 2021).
**Assumption 1. f : R[d]** _‚Üí_ R is continuously differentiable. f (x) ‚â• _f_ _[low]_ _> ‚àí‚àû_ _for any x ‚àà_ R[d].
_‚àáf is globally L-Lipschitz continuous; namely ‚à•‚àáf_ (x)‚àí‚àáf (y)‚à•2 ‚â§ _L‚à•x‚àíy‚à•2 for any x, y ‚àà_ R[d].
**Assumption 2. For any iteration k, the stochastic gradient ‚àáfŒæk** (xk) satisfies EŒæk [‚àáfŒæk (xk)] =
_‚àáf_ (xk), EŒæk [‚à•‚àáfŒæk (xk) ‚àí‚àáf (xk)‚à•2[2][]][ ‚â§] _[œÉ][2][,][ where][ œÉ >][ 0][, and][ Œæ][k][, k][ = 0][,][ 1][, . . .][ are independent]_
_samples that are independent of {xj}j[k]=0[.]_

The diminishing condition about Œ≤k is


+‚àû

_Œ≤k = +‚àû,_
_k=0_

X


+‚àû

_Œ≤k[2]_ _[<][ +][‚àû][.]_ (14)
_k=0_

X


We give the convergence properties of RST-AM in nonconvex (stochastic) optimization and proofs
are deferred to Appendix C.3.2.
**Theorem 3. Suppose Assumption 1 hold and** _xk_ _is the sequence generated by full-batch RST-AM,_
_¬µ_ _{_ _}_
_i.e. nk = T_ _. Let Œ≤k = Œ≤ ‚àà_ (0, 2L(1+C[‚àí][1]) []][ be a constant,][ Œ±][k][ ‚àà] [[0][,][ 1]][ and satisfies (13), then]

1 _N_ _‚àí1_

_N_ _k=0_ _‚à•‚àáf_ (xk)‚à•2[2] _[‚â§]_ [2(][f] [(][x]N¬µŒ≤[0][)][ ‚àí] _[f][ low][)]_ _,_ (15)

X

_in the N iterations. To ensure_ _N[1]_ _kN=0‚àí1_ 2 _[< œµ][, the number of iterations is][ O][(1][/œµ][)][ .]_

_[‚à•‚àá][f]_ [(][x][k][)][‚à•][2]

**Theorem 4. Suppose Assumptions 1 and 2 hold and** _xk_ _is the sequence generated by RST-AM_

_with batch size nk = n ‚â§_ _T_ _. If Œ≤Pk ‚àà_ (0, 4L(1+¬µC[‚àí][1]) []][ and satisfies (14),] { _}_ _[ Œ±][k][ ‚àà]_ [[0][,][ min][{][1][, Œ≤]k12 _[}][]][ and]_

_satisfies (13), then_
lim inf _and_ _Mf > 0_ E[f (xk)] _Mf_ _,_ _k._ (16)
_k_ _‚àÉ_ _‚Üí_ _‚â§_ _‚àÄ_
_‚Üí‚àû_ _[‚à•‚àá][f]_ [(][x][k][)][‚à•][2][ = 0][ with probability][ 1]

_If EŒæk_ [ _fŒæk_ (xk) 2[]][ ‚â§] _[M][g][,][ ‚àÄ][k,][ where][ M][g]_ _[>][ 0][ is a constant, we have]_
_‚à•‚àá_ _‚à•[2]_
lim (17)
_k‚Üí‚àû_ _[‚à•‚àá][f]_ [(][x][k][)][‚à•][2][ = 0][ with probability][ 1][.]

_RST-AM with fixed batch sizeTheorem 5. Suppose Assumptions 1 and 2 hold and nk = n. Let Œ≤k = min {xk4}Lk[N](1+=0[‚àí]¬µ[1]C[is the first][‚àí][1])_ _[,]_ _œÉ‚àöDÀúN[ N][ iterations generated by]D is a problem-_

1 _{_

_independent constant; Œ±k_ [0, min 1, Œ≤k2 _[}][, where][ Àú]_

_ing PR(k) := Prob_ _R = ‚àà k_ = 1/N{, then[}][]][ and satisfies (13). Let][ R][ be a random variable follow-]
_{_ _}_

_œÉ_ 4Df _D_

E[ _f_ (xR) 2[]][ ‚â§] [16][D][f] _[L][(1 +][ C]_ _[‚àí][1][)]_ + + [4(][L][ +][ ¬µ][‚àí][1][)(1 +][ C] _[‚àí][1][) Àú]_ _,_ (18)
_‚à•‚àá_ _‚à•[2]_ _N¬µ[2]_ _¬µ‚àöN_ _DÀú_ _n_ !

_where Df := f_ (x0) ‚àí _f_ _[low]_ _and the expectation is taken with respect to R and {Sj}j[N]=0[‚àí][1][. To ensure]_
E[‚à•‚àáf (xR)‚à•2[2][]][ ‚â§] _[œµ][, the number of iterations is][ O][(1][/œµ][2][)][.]_

**Remark 5. The proofs of Theorem 4 and 5 are based on the analysis of SAM (Wei et al., 2021).**
_The theorems show that the convergence of RST-AM is no worse than SGD (Robbins & Monro,_
_1951). There are two key differences between RST-AM and SAM: RST-AM is based on short-term_
_recurrences while SAM usually maintains longer historical sequences to ensure effectiveness; RST-_
_AM uses additional correction and regularization terms (Line 5-8 in Algorithm 1) to incorporate_
_historical information while SAM simply discards the oldest iteration to make space for ‚àÜxk‚àí1_
_and ‚àÜrk‚àí1. The reduced memory requirement in RST-AM makes it applicable for solving more_
_challenging problems in machine learning._


-----

4 EXPERIMENTS

We validated the effectiveness of our proposed ST-AM methods in various applications in fixed-point
iterations and nonconvex optimization, including linear and nonlinear problems, deterministic and
stochastic optimization. Specifically, we first tested ST-AM in linear problems, cubic-regularized
quadratic minimization (Carmon & Duchi, 2020) and a multiscale deep equilibrium (MDEQ) model
(Bai et al., 2020). Then we applied RST-AM to train neural networks and compared them with
several first-order and second-order optimizers. Experimental details are in Appendix D.


4.1 EXPERIMENTS ABOUT THE BASIC ST-AM AND MST-AM

We verified the properties of ST-AM declared in Theorem 1 and 2 by solving four problems (details
are in Appendix D.1): (I) strongly convex quadratic optimization (corresponding to Theorem 1);
(II) solving a nonsymmetric linear system Ax = b (corresponding to ÀÜŒ∫ = 0 in Theorem 2); (III)

_Œ∫ >cubic-regularized quadratic minimizationÀÜ_ 0 in Theorem 2); (IV) root-finding problems in MDEQ on CIFAR-10 (Krizhevsky et al., 2009). minx‚ààRd f (x) := ‚à•Ax _‚àí_ _b‚à•2[2]_ [+][ M]3 _[‚à•][x][‚à•]2[3]_ [(corresponding to]

The compared methods were gradient descent (GD), fixed-point iteration (FP), conjugate residual
method (CR) (Saad, 2003), BFGS (Nocedal & Wright, 2006), Broyden‚Äôs method (Broyden, 1965),
and the full-memory AM (AM). We used the basic ST-AM to solve Problem I and II, and MST-AM
(cp = cq = 1) to solve Problem III and IV.


10 2

10 5

10 8

10 11 GD: ||rk||2/||r0||2

CR: ||rk||2/||r0||2

10 14 AM: ||ST-AM: ||rk||2r/||k||r20/||||2r0||2

0 10 20 30 40 50

iteration


10 1

||/||||||rr202k 101010 1047 FP

CR

10 13 AM

ST-AM

0 10 20 30 40 50

iteration


10 2

||/||||||rr202k 101010 1158 GD

BFGS

10 14 AMMST-AM

0 10 20 30 40 50

iteration


(a) Problem I

0.8 Broyden

2 AM
||z MST-AM
/||)||z(f2 0.60.4

0.2

Forward: ||

0.0

2 4 6 8 10 12 14 16

step


(d) Forward process


(b) Problem II

1.4 Broyden

||z/||)||z(f22 1.21.00.8 AMMST-AM

0.6

0.4

0.2

Backward: ||

0.0

2 4 6 8 10 12 14 16 18

step


(e) Backward process


(c) Problem III (M = 0.1)

85

86

8075 8584 85.3184.9084.52 1.2

70 8325 30 35 40 45 50 1.0

est Accuracy %T 6560 BroydenAMMST-AM 0.8est LossT

0.6

55

0 10 20 30 40 50

epoch


(f) Test accuracy and loss


Figure 1: (a) ‚à•rk‚à•2/‚à•r0‚à•2 of GD and CR, and ‚à•r¬Øk‚à•2/‚à•r0‚à•2 of AM and ST-AM for solving Problem I; (b) ‚à•rk‚à•2/‚à•r0‚à•2 for solving Problem II; (c) ‚à•rk‚à•2/‚à•r0‚à•2 for solving Problem III (M = 0.1);
(d)(e) relative residuals of the forward and backward root-finding processes in MDEQ, and shaded
areas correspond to the standard deviations; (f) test accuracy and loss in MDEQ/CIFAR-10.

The numerical results shown in Figure 1 demonstrate the power of ST-AM as a variant of Krylov
subspace methods. It significantly accelerates the slow convergence of the GD or FP method, and
can outperform AM. Figure 1(a) clearly verifies the correctness of Theorem 1: within the machine
precision, the intermediate residual ¬Ørk of ST-AM coincides with the residual rk of CR. Note that
AM fails to coincide with CR and ST-AM due to the intrinsic numerical weakness to solve (3), as
also pointed out in (Walker & Ni, 2011). Figure 1(b) shows that ST-AM can outperform CR, though
both methods enjoy short-term recurrences and are equivalent for solving SPD linear systems. Figure 1(c) also shows MST-AM surpasses BFGS in solving cubic-regularized problems. The tests in
MDEQ/CIFAR-10 indicate that MST-AM is comparable to the full-memory methods in the forward
root-finding process and converges faster in the backward process. The accuracy is also comparable.


-----

10[0]

10 2

10 4

10 6

10 8


10[2]


10[0]

10 2

10 4

10 6


10

10

10

10 10

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
|SGD Adam|||||
|SAM(2) SAM(5) SAM(10)|||||

|Col1|Col2|
|---|---|
|||
|||
|||
|)||

|Col1|Col2|Col3|
|---|---|---|
||||
|grad prop grad_SAM(2)|||
|prop_SAM(2) grad_RST-AM prop_RST-AM|||


0 50 100 150 200

SGD
Adam
SAM(2)
SAM(5)
SAM(10)
RST-AM

epoch

(a) Train loss (w/o preconditioning)


0 50 100 150 200

SGD
Adam
SAM(2)
SAM(5)
SAM(10)
RST-AM

epoch

(b) SNG (w/o preconditioning)


0 50 100 150 200

Adagrad
RMSprop
Adagrad_SAM(2)
RMSprop_SAM(2)
Adagrad_RST-AM
RMSprop_RST-AM
RST-AM

epoch

(c) Train loss (w/ preconditioning)


Figure 2: Experiments on MNIST. (a)(b) Training loss and the squared norm of gradient (SNG) (w/o
preconditioning for SAM, RST-AM); (c) Training loss (w/ preconditioning for SAM, RST-AM).

4.2 EXPERIMENTS ABOUT RST-AM


We applied RST-AM to train neural networks, with full-batch training on MNIST (LeCun et al.,
1998), and mini-batch training on CIFAR-10/CIFAR-100 and Penn Treebank (Marcus et al., 1993).

**Experiments on MNIST. We trained a convolutional neural network (CNN) on MNIST to see the**
convergence behaviour of RST-AM in nonconvex optimization (cf. Theorem 3), for which we were
only concerned about the training loss. Figure 2(a)(b) show that the short-memory SAMs (m = 2, 5)
hardly show any improvement over the first-order optimizers SGD and Adam, while RST-AM can
close the gap of the long-memory (m = 10) and the short-memory methods. We also considered
the effect of preconditioning on RST-AM (see Appendix A.3). The notation ‚ÄúA B‚Äù means B method
preconditioned by A method. Figure 2(c) indicates that preconditioning also works much better for
RST-AM than SAM(2), and RMSprop RST-AM can outperform the non-preconditioned RST-AM.

Table 1: Experiments on CIFAR10/CIFAR100. ‚Äú-‚Äù means failing to complete the test in our device
due to memory limit. ‚Äú*‚Äù indicates numbers published in (Wei et al., 2021).


(a) Final TOP1 test accuracy (mean ¬± standard deviation) (%) for training 160 epochs.

Test accuracy on CIFAR10 Test accuracy on CIFAR100
Method

ResNet18 ResNet20 ResNet32 ResNet44 ResNet56 WRN16-4 ResNet18 ResNeXt DenseNet

AdamAdaBoundAdaBeliefLookaheadAdaHessianSAM(2)SAM(10)SGDMRST-AM[‚àó][‚àó] _[‚àó][‚àó][‚àó][‚àó]_ 93.0394.6594.9294.3695.1794.8295.2794.2595.07¬±¬±¬±¬±¬±¬±¬±¬±¬±.07.31.13.33.09.04.10.15.04 91.1790.7791.1592.0791.9292.1492.0392.4392.39¬±¬±¬±¬±¬±¬±¬±¬±¬±.16.13.08.21.04.32.33.19.11 92.0391.7392.1592.8692.1893.0492.8693.2293.24¬±¬±¬±¬±¬±¬±¬±¬±¬±.28.06.17.15.18.23.32.15.36 93.1092.2892.0092.7993.2692.7493.4693.5793.52¬±¬±¬±¬±¬±¬±¬±¬±¬±.62.18.24.24.11.09.14.23.02 92.3992.4493.3093.3692.4093.6693.7793.4793.69¬±¬±¬±¬±¬±¬±¬±¬±¬±.23.04.07.13.06.06.12.28.18 92.4593.5094.4694.9094.0495.0795.2394.9095.21¬±¬±¬±¬±¬±¬±¬±¬±¬±.11.12.13.15.12.16.07.09.09 72.4175.0776.2577.6376.5977.5178.1377.2777.91¬±¬±¬±¬±¬±¬±¬±¬±¬±.09.17.14.06.35.42.24.14.22 79.3178.4173.5775.7478.2778.9379.0279.53-¬±¬±¬±¬±¬±¬±¬±¬±.54.17.20.16.12.21.27.34 78.4970.8076.0678.8379.3780.0080.0980.36-¬±¬±¬±¬±¬±¬±¬±¬±.12.23.13.15.16.23.52.25


(b) The memory and computation cost compared with SGDM. The notations ‚Äúm‚Äù,‚Äút/e‚Äù and ‚Äút‚Äù are abbreviations of memory, per-epoch time and total running time, respectively.

Cost CIFAR10/ResNet18 CIFAR10/WRN16-4 CIFAR100/ResNeXt50 CIFAR100/DenseNet121
(√ó SGDM) m t/e t m t/e t m t/e t m t/e t

SGDM[‚àó] 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
SAM(10)[‚àó] 1.73 1.78 1.00 1.26 1.28 0.80 1.30 1.16 0.58 1.16 1.19 0.60
RST-AM 1.05 1.46 0.82 1.03 1.14 0.71 1.04 1.07 0.54 1.01 1.11 0.55


**Experiments on CIFAR. We trained ResNet18/20/32/44/56 (He et al., 2016), WideResNet16-4**
(Zagoruyko & Komodakis, 2016) (abbr. WRN16-4) on CIFAR-10, and ResNet18, ResNeXt50 (Xie
et al., 2017), DenseNet121 (Huang et al., 2017) on CIFAR-100. The baseline optimizers were
SGDM, Adam, AdaBound (Luo et al., 2018), AdaBelief (Zhuang et al., 2020), Lookahead (Zhang
et al., 2019), AdaHessian (Yao et al., 2021) and SAM. Here, some results of the baselines in (Wei
et al., 2021) were used for reference since the experimental settings were the same. Table 1(a) shows
RST-AM improves SAM(2) and has comparable test accuracy to SAM(10). RST-AM also outperforms other baseline optimizers. Table 1(b) reports the memory and computation cost, where we


-----

used SGDM as the baseline and other optimizers were terminated when achieving a comparable or
better test accuracy than SGDM. It indicates that RST-AM introduces ‚â§ 5% extra memory overhead compared with SGDM, and significantly reduces the memory footprint of AM. Since RST-AM
needs fewer training epochs, the total running time is less than SGDM.


120

115

110

105

100

95

90

85

80


100

95

90

85

80

75

70

65

60


100

95

90

85

80

75

70

65


|Col1|Col2|Col3|Col4|SGDM|
|---|---|---|---|---|
|||||Adam|
|||||AdaBelief SAM(2)|
|||||SAM(10) RST-AM|
||||||
||||||
||||||
||||||


100 200 300 400 500

epoch

(a) 1-Layer LSTM


|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
|||||SGDM Adam|
|||||AdaBelief SAM(2)|
|||||SAM(10)|
|||||RST-AM|
||||||
||||||
||||||
||||||
||||||


100 200 300 400 500

epoch

(b) 2-Layer LSTM


|Col1|Col2|Col3|Col4|SGDM|
|---|---|---|---|---|
|||||Adam|
|||||AdaBelief SAM(2)|
|||||SAM(10) RST-AM|
||||||
||||||
||||||
||||||


100 200 300 400 500

epoch

(c) 3-Layer LSTM


Figure 3: Validation perplexity of training 1,2,3-layer LSTM on Penn Treebank.


**Experiments on Penn Treebank. We trained**
LSTMs with 1-3 layer(s) on Penn Treebank and
report the validation perplexity in Figure 3 and
test perplexity in Table 2 (lower is better). The
results suggest that RST-AM is comparable to
or even better than SAM(10). The improvement
of RST-AM over other optimizers is also significant. We report the computation and memory cost in Appendix D.2.4. RST-AM can still
surpass Adam while using much fewer epochs,
thus reducing the total running time.


Table 2: Test perplexity of training 1,2,3-layer
LSTM on Penn Treebank. Lower is better.

Method 1-Layer 2-Layer 3-Layer


SGDM 83.48¬±.03 65.89¬±.18 61.88¬±.23
Adam 80.33¬±.15 64.32¬±.06 59.72¬±.13
AdaBelief 81.29¬±.35 64.68¬±.10 60.46¬±.07
SAM(2) 80.79¬±.19 65.52¬±.29 61.13¬±.12
SAM(10) 78.78¬±.14 62.46¬±.11 58.93¬±.09
RST-AM **78.41¬±.18** **62.46¬±.08** **58.31¬±.23**

Table 4: FID score for SN-GAN.

Method Adam AdaBelief RST-AM

Best FIDFinal FID 13.3413.07¬±¬±.18.14 12.8013.59¬±¬±.09.21 **12.0512.50¬±¬±.15.29**


Table 3: Test accuracy (%) for adversarial training.

CIFAR10/ResNet18 CIFAR100/DenseNet121
Optimizer
Clean FGSM PGD-20 C&W‚àû Clean FGSM PGD-20 C&W‚àû

SGD 82.16 63.23 51.91 50.22 59.45 39.76 30.92 29.00
RST-AM **82.53** **63.78** **52.43** **50.52** **60.48** **40.41** **31.20** **29.52**


**Adversarial training. We applied RST-AM to adversarial training (Madry et al., 2018) as the outer-**
optimizer and compared it with SGD by the clean test accuracy and robust test accuracy. The results
on CIFAR10/ResNet18 and CIFAR100/DenseNet121 are reported in Table 3. It can be seen that
RST-AM can achieve both higher clean test accuracy and higher robust test accuracy. More results
can be found in Appendix D.2.5.

**Generative adversarial network (GAN). We tested RST-AM by training a GAN equipped with**
spectral normalization (SN-GAN) (Miyato et al., 2018), where the generator and discriminator networks were ResNets and the dataset was CIFAR-10. Table 4 shows that RST-AM can achieve lower
FID score (better accuracy) than Adam and AdaBelief.

5 CONCLUSION


In this paper, to address the memory issue of Anderson mixing (AM), we develop a novel class
of short-term recurrence AM methods (ST-AM) and test it in various applications, including solving linear and nonlinear problems, deterministic and stochastic optimization. We give a complete
theoretical analysis of the proposed methods. We prove that the basic ST-AM is equivalent to the
full-memory AM in strongly convex quadratic optimization. With some minor changes, it has local
linear convergence for solving general fixed-point problems under some common assumptions. We
also introduce the regularized form of ST-AM and analyze its convergence properties. The numerical
results show that the ST-AM methods are comparable to or even better than the long-memory AM
while consuming less memory. The regularized ST-AM also outperforms many existing optimizers
in training neural networks in various tasks.


-----

ACKNOWLEDGMENTS

This work was supported by the National Key R&D Program of China (No. 2021YFA1001300),
National Natural Science Foundation of China (No.61925601), Tsinghua University Initiative Scientific Research Program, National Natural Science Foundation of China (No.11901338), and Huawei
Noah‚Äôs Ark Lab. We thank all anonymous reviewers for their valuable comments and suggestions
on this work.

REFERENCES

Hengbin An, Xiaowei Jia, and Homer F Walker. Anderson acceleration and application to the threetemperature energy equations. Journal of Computational Physics, 347:1‚Äì19, 2017.

Donald G Anderson. Iterative procedures for nonlinear integral equations. Journal of the ACM
_(JACM), 12(4):547‚Äì560, 1965._

Donald G Anderson. Comments on ‚ÄúAnderson acceleration, mixing and extrapolation‚Äù. Numerical
_Algorithms, 80(1):135‚Äì234, 2019._

Marcin Andrychowicz, Misha Denil, Sergio G¬¥omez Colmenarejo, Matthew W Hoffman, David
Pfau, Tom Schaul, Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient
descent by gradient descent. In Proceedings of the 30th International Conference on Neural
_Information Processing Systems, pp. 3988‚Äì3996, 2016._

Akash Arora, David C Morse, Frank S Bates, and Kevin D Dorfman. Accelerating self-consistent
field theory of block polymers in a variable unit cell. The Journal of Chemical Physics, 146(24):
244902, 2017.

Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. Advances in Neural
_Information Processing Systems, 32:690‚Äì701, 2019._

Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Multiscale deep equilibrium models. In Advances
_in Neural Information Processing Systems (NeurIPS), 2020._

Albert S Berahas, Frank E Curtis, and Baoyu Zhou. Limited-memory BFGS with displacement
aggregation. Mathematical Programming, pp. 1‚Äì37, 2021.

Lon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223‚Äì311, 2018.

Claude Brezinski and Michela Redivo-Zaglia. Shanks function transformations in a vector space.
_Applied Numerical Mathematics, 116:57‚Äì63, 2017._

Claude Brezinski, Michela Redivo-Zaglia, and Yousef Saad. Shanks sequence transformations and
Anderson acceleration. SIAM Review, 60(3):646‚Äì669, 2018.

Charles G Broyden. A class of methods for solving nonlinear simultaneous equations. Mathematics
_of Computation, 19(92):577‚Äì593, 1965._

Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-Newton
method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008‚Äì1031, 2016.

Stan Cabay and LW Jackson. A polynomial extrapolation method for finding limits and antilimits
of vector sequences. SIAM Journal on Numerical Analysis, 13(5):734‚Äì752, 1976.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
_IEEE Symposium on Security and Privacy (SP), pp. 39‚Äì57. IEEE, 2017._

Yair Carmon and John C Duchi. First-order methods for nonconvex quadratic minimization. SIAM
_Review, 62(2):395‚Äì436, 2020._

Mauro Cettolo, Jan Niehues, Sebastian St¬®uker, Luisa Bentivogli, and Marcello Federico. Report
on the 11th IWSLT evaluation campaign, IWSLT 2014. In Proceedings of the International
_Workshop on Spoken Language Translation, Hanoi, Vietnam, volume 57, 2014._


-----

Hans De Sterck and Yunhui He. On the asymptotic linear convergence speed of Anderson acceleration, Nesterov acceleration, and nonlinear GMRES. SIAM Journal on Scientific Computing, (0):
S21‚ÄìS46, 2021.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248‚Äì255. IEEE, 2009.

Yunbin Deng. Deep learning on mobile devices: A review. In Mobile Multimedia/Image Processing,
_Security, and Applications 2019, volume 10993, pp. 109930A. International Society for Optics_
and Photonics, 2019.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(7), 2011.

Iain S Duff, Albert Maurice Erisman, and John Ker Reid. Direct methods for sparse matrices.
Oxford University Press, 2017.

Rick Durrett. Probability: Theory and examples, volume 49. Cambridge University Press, 2019.

R P Eddy. Extrapolating to the limit of a vector sequence. In Peter C.C. Wang, Arthur L. Schoenstadt,
Bert I. Russak, and Craig Comstock (eds.), Information Linkage Between Applied Mathematics
_and Industry, pp. 387‚Äì396. Academic Press, 1979._

Claire Evans, Sara Pollock, Leo G Rebholz, and Mengying Xiao. A proof that Anderson acceleration improves the convergence rate in linearly converging fixed-point methods (but not in those
converging quadratically). SIAM Journal on Numerical Analysis, 58(1):788‚Äì810, 2020.

Haw-ren Fang and Yousef Saad. Two classes of multisecant methods for nonlinear acceleration.
_Numerical Linear Algebra with Applications, 16(3):197‚Äì221, 2009._

Anqi Fu, Junzi Zhang, and Stephen Boyd. Anderson accelerated Douglas‚ÄìRachford splitting. SIAM
_Journal on Scientific Computing, 42(6):A3560‚ÄìA3583, 2020._

Alejandro J Garza and Gustavo E Scuseria. Comparison of self-consistent field convergence acceleration techniques. The Journal of Chemical Physics, 137(5):054110, 2012.

Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341‚Äì2368, 2013.

Gene H Golub and Charles F Van Loan. Matrix computations, 4th. Johns Hopkins, 2013.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.

William W Hager and Hongchao Zhang. A survey of nonlinear conjugate gradient methods. Pacific
_Journal of Optimization, 2(1):35‚Äì58, 2006._

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770‚Äì778, 2016.

Magnus R. Hestenes and Eduard Stiefel. Methods of conjugate gradients for solving linear systems.
_Journal of Research of the National Bureau of Standards, 49:409‚Äì435, 1952._

Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
_Neural Information Processing Systems, 30, 2017._

Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
_Recognition, pp. 4700‚Äì4708, 2017._

Carl T Kelley. Numerical methods for nonlinear equations. Acta Numerica, 27:207‚Äì287, 2018.


-----

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
_arXiv:1412.6980, 2014._

Tamara G Kolda, Dianne P O‚Äôleary, and Larry Nazareth. BFGS with update skipping and varying
memory. SIAM Journal on Optimization, 8(4):1060‚Äì1083, 1998.

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.

Yann LeCun, L¬¥eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278‚Äì2324, 1998.

Lin Lin, Jianfeng Lu, and Lexing Ying. Numerical methods for Kohn‚ÄìSham density functional
theory. Acta Numerica, 28:405‚Äì539, 2019.

Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization.
_Mathematical Programming, 45(1):503‚Äì528, 1989._

Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. In International Conference on
_Learning Representations, 2019._

Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. In International Conference on Learning Representations, 2018.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
_Learning Representations, 2018._

Vien Mai and Mikael Johansson. Anderson acceleration of proximal gradient methods. In Interna_tional Conference on Machine Learning, pp. 6620‚Äì6629. PMLR, 2020._

Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of English: The Penn Treebank. 1993.

Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018.

Aryan Mokhtari, Mark Eisen, and Alejandro Ribeiro. IQN: An incremental quasi-Newton method
with local superlinear convergence rate. SIAM Journal on Optimization, 28(2):1670‚Äì1698, 2018.

Ion Necoara, Yu Nesterov, and Francois Glineur. Linear convergence of first order methods for
non-strongly convex optimization. Mathematical Programming, 175(1):69‚Äì107, 2019.

Arkadij SemenoviÀác Nemirovski and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. 1983.

Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,
2006.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association
_for Computational Linguistics, pp. 311‚Äì318, 2002._

Florian A Potra and Hans Engler. A characterization of the behavior of the Anderson acceleration
on linear problems. Linear Algebra and Its Applications, 438(3):1002‚Äì1011, 2013.

Ning Qian. On the momentum term in gradient descent learning algorithms. Neural Networks, 12
(1):145‚Äì151, 1999.

Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In
_International Conference on Machine Learning, pp. 8093‚Äì8104. PMLR, 2020._


-----

Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat_ical Statistics, pp. 400‚Äì407, 1951._

Youcef Saad and Martin H Schultz. GMRES: A generalized minimal residual algorithm for solving
nonsymmetric linear systems. SIAM Journal on Scientific and Statistical Computing, 7(3):856‚Äì
869, 1986.

Yousef Saad. Iterative methods for sparse linear systems. SIAM, 2003.

Damien Scieur, Alexandre dAspremont, and Francis Bach. Regularized nonlinear acceleration.
_Mathematical Programming, 179(1):47‚Äì83, 2020._

Daniel Shanks. Non-linear transformations of divergent and slowly convergent sequences. Journal
_of Mathematics and Physics, 34(1-4):1‚Äì42, 1955._

Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International Conference on Machine Learning, pp.
1139‚Äì1147, 2013.

Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26‚Äì
31, 2012.

Alex Toth and CT Kelley. Convergence analysis for Anderson acceleration. _SIAM Journal on_
_Numerical Analysis, 53(2):805‚Äì819, 2015._

Henk A Van der Vorst and C Vuik. The superlinear convergence behaviour of GMRES. Journal of
_Computational and Applied Mathematics, 48(3):327‚Äì341, 1993._

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor_mation Processing Systems, pp. 5998‚Äì6008, 2017._

Homer F Walker and Peng Ni. Anderson acceleration for fixed-point iterations. SIAM Journal on
_Numerical Analysis, 49(4):1715‚Äì1735, 2011._

Xiao Wang, Shiqian Ma, Donald Goldfarb, and Wei Liu. Stochastic quasi-Newton methods for
nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927‚Äì956, 2017.

Fuchao Wei, Chenglong Bao, and Yang Liu. Stochastic Anderson mixing for nonconvex stochastic
optimization. Advances in Neural Information Processing Systems, 34, 2021.

Peter Wynn. On a device for computing the em(Sn) transformation. Mathematical Tables and Other
_Aids to Computation, pp. 91‚Äì96, 1956._

Peter Wynn. Acceleration techniques for iterated vector and matrix problems. Mathematics of
_Computation, 16(79):301‚Äì322, 1962._

Saining Xie, Ross Girshick, Piotr Doll¬¥ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision
_and Pattern Recognition, pp. 1492‚Äì1500, 2017._

Yunan Yang. Anderson acceleration for seismic inversion. Geophysics, 86(1):R99‚ÄìR108, 2021.

Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael Mahoney.
AdaHessian: An adaptive second order optimizer for machine learning. In Proceedings of the
_AAAI Conference on Artificial Intelligence, volume 35, pp. 10665‚Äì10673, 2021._

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision
_Conference 2016. British Machine Vision Association, 2016._

Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey E Hinton. Lookahead optimizer: k steps
forward, 1 step back. In Advances in Neural Information Processing Systems, pp. 9597‚Äì9608,
2019.

Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan. AdaBelief optimizer: Adapting stepsizes by the belief in observed
gradients. Advances in Neural Information Processing Systems, 33, 2020.


-----

A ADDITIONAL PRELIMINARIES

We provide some additional preliminaries in this section for readers that are not familiar with Anderson mixing, fixed-point iterations and some techniques mentioned in the main paper.

A.1 FIXED-POINT ITERATION

The fixed-point problem and the optimization problem are the main application scenarios of our
methods. It is worth pointing out that there are some minor differences between these two problems that make algorithm designs different. The key difference is that for a fixed-point problem,
the Jacobian (if exists) is generally not symmetric while for optimization, the Hessian is naturally
symmetric. In principle, a fixed-point solver can also be applicable for an optimization problem
since the first-order necessary condition of minx‚ààRd f (x), where f : R[d] _‚Üí_ R, is ‚àáf (x) = 0.

_yConsider a contraction mapping‚à•2, ‚àÄx, y ‚àà_ R[d]. According to the contraction mapping theorem, a unique fixed point g : R[d] _7‚Üí_ R[d], i.e. for some Œ∫ < 1, ‚à•g(x) ‚àí _g(y)‚à•2 ‚â§ x[‚àó]Œ∫exists‚à•x ‚àí_
for g and the iterates generated by the iteration xk+1 = g(xk) converge to x[‚àó], starting from any
_xthe update is0 ‚àà_ R[d]. In practice, the damped fixed-point iteration is also commonly used: given the xk+1 = (1 ‚àí _Œ≤k)xk + Œ≤kg(xk) = xk + Œ≤krk, where rk := g(xk) ‚àí_ _xk is called the Œ≤k ‚àà_ (0, 2),
_residual._

The fixed-point iteration, also known as Picard iteration in some areas, can converge very slowly in
practice. Anderson mixing is a method to improve the convergence.

A.2 ANOTHER FORM OF ANDERSON MIXING

The derivation of Anderson mixing (AM) in Section 3.1 explicitly interprets AM as a two-step
procedure. In the literature, there is another equivalent form of AM.

Let the projection coefficients Œìk = (Œ≥k[(1)][, . . ., Œ≥]k[(][m][)])[T] _‚àà_ R[m]. Define the auxiliary coefficients
_{Œ∏k[(][j][)][}]j[m]=0_ [as][ Œ∏]k[(0)] = Œ≥k[(1)][, Œ∏]k[(][j][)] = ‚àÜŒ≥k[(][j][)][(][j][ = 1][, . . ., m][ ‚àí] [1)][, Œ∏]k[(][m][)] = 1 ‚àí _Œ≥k[(][m][)], then_ _j=0_ _[Œ∏]k[(][j][)]_ =

1 and ¬Ørk = _j=0_ _[Œ∏]k[(][j][)][r][k][‚àí][m][+][j][. Hence the least squares problem (3) can be reformulated as a]_

[P][m]

constrained problemas a linear combination of the historical residuals[P][m] min{Œ∏k(j)[}]j[m]=0 _[‚à•]_ [P]j[m]=0 _[Œ∏]k[(][j][)][r][k][‚àí]r[m]j_ [+]j[j]=[‚à•]k[2][ s.t.]m[,][ P][ ¬Ø]rk is minimal in terms of thej[m]=0 _[Œ∏]k[(][j][)]_ = 1, which indicates that L2-norm.

_{_ _}[m]_ _‚àí_

Also, the projection step and the mixing step (2) can be reformulated as ¬Øxk = _j=0_ _[Œ∏]k[(][j][)][x][k][‚àí][m][+][j]_
and xk+1 = (1 _Œ≤k)_ _j=0_ _[Œ∏]k[(][j][)][x][k][‚àí][m][+][j][ +][Œ≤][k]_ _mj=0_ _[Œ∏]k[(][j][)][g][(][x][k][‚àí][m][+][j][)][, respectively. Such formulation]_
_‚àí_
is also adopted in the literature (Toth & Kelley, 2015; Mai & Johansson, 2020; Scieur et al., 2020).[P][m]

Let Hk be the solution to the constrained optimization problem (Fang & Saad, 2009)[P][m] P

min (19)
_Hk_

_[‚à•][H][k][ ‚àí]_ _[Œ≤][k][I][‚à•][F][ subject to][ H][k][R][k][ =][ ‚àí][X][k][,]_

then the update (4) is xk+1 = xk +Hkrk. It suggests that AM is a multisecant quasi-Newton method
(Fang & Saad, 2009).

A.3 PRECONDITIONED ANDERSON MIXING

Like preconditioning for Krylov subspace methods (Saad, 2003), preconditioning can also be incorporated into Anderson mixing to mitigate the ill-conditioning of the original problem (Wei et al.,
2021). The idea is to replace the mixing step in (2) via a preconditioned mixing.

Suppose that there is a basic solver preconditioner(xk, sk), which works as a black-box procedure
that updates xk given the residual sk, i.e. xk+1 = preconditioner(xk, sk), then the preconditioned
mixing of RST-AM is
_xk+1 = preconditioner(¬Øxk, ¬Ørk)._
which substitutes for the Line 13 in Algorithm 1. The simple mixing (Line 13 in Algorithm 1
can be seen as a special case by defining preconditioner(xk, sk) = xk + Œ≤ksk, i.e. preconditioned by a damped fixed-point iteration. Moreover, if we write the preconditioning operation as


-----

_preconditioner(xk, sk) := xk + Gksk, where Gk is the matrix to approximate the inverse Jaco-_
bian, then the preconditioned AM is

_xk+1 = xk + Gkrk_ (Xk + GkRk)(Rk[T][R][k][)][‚Ä†][R]k[T][r][k] (20)
_‚àí_

(cf. the definitions offorms a low-rank updated approximation to the inverse Jacobian: it solves Xk, Rk in Section 3.1). The matrix Hk = Gk ‚àí (Xk + GkRk)(Rk[T][R][k][)][‚Ä†][R]k[T]

min
_Hk_

_[‚à•][H][k][ ‚àí]_ _[G][k][‚à•][F][ subject to][ H][k][R][k][ =][ ‚àí][X][k][,]_

which is a direct extension of (19).

For the stochastic Anderson mixing, using damped projection can be helpful, i.e. ¬Øxk = (1‚àíŒ±k)xk +
AM with damped projection isŒ±k(xk ‚àíXkŒìk) = xk ‚àíŒ±kXkŒìk and ¬Ørk = rk ‚àíŒ±kRkŒìk correspondingly. Then the preconditioned

_xk+1 = xk + Gkrk ‚àí_ _Œ±k(Xk + GkRk)Œìk._ (21)

B ADDITIONAL DISCUSSION

B.1 THE MEMORY AND COMPUTATIONAL EFFICIENCY

Since the ST-AM methods only need to store two previous iterations, the memory and computational
cost can be reduced to be close to first-order methods.

**Memory cost. Assume that the iteration number is k and the model parameter size is d. The full-**
memory AM stores all previous iterations, thus the additional memory is 2kd. To reduce the memory
overhead, the limited-memory AM(m) only maintains the most recent m iterations while discarding
the older historical information (cf. (1)). Hence the additional memory of AM(m) is 2md. Choosing
a suitable m can be problem-dependent (Walker & Ni, 2011), and it is often suggested that m ‚â• 5
(Mai & Johansson, 2020; Fu et al., 2020) to avoid too much historical information being discarded.
There is no equivalence between AM(m) and the full-memory AM. On the other side, our ST-AM
methods have the same memory footprint as that of AM(2), i.e. only introducing 4d additional
memory. For a large model that contains millions or even billions of parameters, such reduction in
memory requirement can be significant.

**Computational cost. For ST-AM, besides the gradient evaluations, the main additional computa-**
tional cost comes from vector corrections, the projection and mixing step. These operations are
very cheap: matrix multiplications of R[2][√ó][d] _√ó R[d][√ó][2], R[2][√ó][d]_ _√ó R[d][√ó][1], and R[d][√ó][2]_ _√ó R[2][√ó][1]; the pseudo-_
inverse can be exactly solved as the size of the matrix is R[2][√ó][2]. Also, the Q[T]k‚àí1[Q][k][‚àí][1][ and][ P][ T]k‚àí1[P][k][‚àí][1]
in Line 6 in Algorithm 1 can reuse the results in the previous iteration (cf. Line 11). So the total
additional computational cost of RST-AM is O(d).

B.2 APPLICABILITY

In the main paper, we develop a class of short-term recurrence Anderson mixing. Among them, the
basic ST-AM can serve as a new linear solver for linear systems; MST-AM can be used as a new
fixed-point solver for nonlinear equations; RST-AM is a new method for optimization. These methods are based on Anderson mixing and have close relationship between each other. The theoretical
analysis and numerical results show the great potential of ST-AM methods for various applications.
Here, we give some comparisons between the ST-AM methods and some other classical methods.

**ST-AM versus current solvers for linear systems.** ST-AM is an iterative solver. It is equivalent
to the full-memory AM and GMRES in strongly convex quadratic optimization (or SPD linear systems), and can also have linear convergence rate for general nonsymmetric linear systems. Since it is
based on short-term recurrences, like the CG method, the memory and per-iteration cost of ST-AM
is economical. On the other side, the LU factorization based methods (Golub & Van Loan, 2013)
are direct solvers that can incur overwhelming memory and computation cost for large sparse linear
systems. Although sparse direct solvers (Duff et al., 2017) can alleviate overhead, they are often
more complicated to implement and difficult for parallel computing. Moreover, iterative solvers can
benefit from the preconditioning technique that improves convergence (Saad, 2003). Also, ST-AM


-----

has advantages over other iterative methods since it does not need to directly access the matrix and
only the residual is required. In the case that explicitly accessing the matrix is difficult, this property
is appealing. Hence, unlike nonlinear CG that relies on line search, it is direct to extend ST-AM
to unconstrained optimization where the gradient is commonly available while the Hessian can be
too costly to obtain. Moreover, ST-AM is very flexible and any iterative solver can be viewed as a
black-box iterative process to be accelerated by ST-AM.

**MST-AM versus other quasi-Newton methods for nonlinear equations.** MST-AM has the nature of quasi-Newton methods since it is built upon AM that is recognized as a multisecant quasiNewton method (Fang & Saad, 2009). In scientific computing, AM has been successfully applied to
solve many difficult nonlinear problems arising from many areas (An et al., 2017; Lin et al., 2019;
Fu et al., 2020; Yang, 2021). Since AM only manipulates residuals and does not use line-search
or trust-region technique, it is efficient to apply AM to accelerate a slowly convergent black-box
iterative process. A comprehensive discussion about the applicability of AM for nonlinear problems
and the relation between AM and Broyden‚Äôs methods can be found in (Fang & Saad, 2009).

One of the biggest issues of AM and other quasi-Newton methods is the additional memory overhead, because they need to store historical iterations to form the secant equations. To make a compromise, limited-memory quasi-Newton methods such as L-BFGS (Liu & Nocedal, 1989) are proposed in which only limited number of historical iterations are stored. In each update, the oldest
iteration is discarded to make space for the up-to-date secant pair. As a result, the limited-memory
quasi-Newton methods can lose the local superlinear convergence properties achieved by the fullmemory schemes (Berahas et al., 2021). Also, as stated by Berahas et al. (2021), the choice of the
number of historical iterations (i.e. historical length) is problem-dependent, and one does not know
_a priori the best choice when solving a particular problem._

Unlike the limited-memory quasi-Newton methods whose performance can be sensitive to the historical length, our MST-AM method only needs to store two corrected historical iterations. MSTAM carefully incorporates historical information through orthogonalization. In the ideal case, i.e.
strongly convex quadratic optimization (or SPD linear systems), it is equivalent to the full-memory
AM which means there is no loss of historical information. Our experiments verify the theoretical
properties of MST-AM and indicate MST-AM can be a competitive method for nonlinear equations.

**RST-AM versus first-order methods for stochastic optimization.** For stochastic optimization
such as training deep neural networks in machine learning, the first-order methods have come to
dominate the field due to the low memory and per-iteration cost. The RST-AM is an extension of
ST-AM and MST-AM to tackle this challenging problem. RST-AM has theoretical guarantees in
deterministic/stochastic optimization and also inherits several advantages of AM and ST-AM:

_‚Ä¢ Fast convergence in quadratic optimization. It is important to possess such property for an_
optimizer since a smooth function can be approximated by a quadratic function in the local
region around the optima and many techniques such as trust region (Nocedal & Wright,
2006) rely on this local approximation. Adaptive learning rate methods such as AdaGrad
(Duchi et al., 2011), Adam (Kingma & Ba, 2014) use a diagonal approximation of the
_Fisher information matrix that is an approximation of the Hessian. However, in the sim-_
ple quadratic case, these methods can only roughly match the performance of the Jacobi
method for solving linear systems (Saad, 2003), which is better than gradient descent but
far inferior to the powerful Krylov subspace methods. The momentum method mimics
CG method by incorporating a historical iteration into the search direction. However, the
choice of momentum and stepsize can be an art (Sutskever et al., 2013). For RST-AM,
there is no need to determine the stepsize and the fast convergence rate of ST-AM can be
recovered by simply setting Œ±k = 1 and Œ¥k[(1)] = Œ¥k[(2)] = 0. For more difficult functions, the
damping and regularization in RST-AM can be enabled to improve stability.

_‚Ä¢ Theoretical guarantee in stochastic optimization. If only first-order information can be ac-_
cessed, the SGD (Ghadimi & Lan, 2013) achieves an optimal convergence rate O(1/œµ[2]) to
obtain an œµ-accurate solution (Nemirovski & Yudin, 1983). In such case, it seems to be a big
mismatch for current second-order methods, because there is no theoretical improvement
albeit with more memory and computation resource. Such mismatch may also account for
the popularity of first-order methods. Since RST-AM has very limited additional memory


-----

overhead, and also achieves the O(1/œµ[2]) complexity, it can be applied to many applications
that are dominated by first-order methods.

_‚Ä¢ Flexibility in use. The application of RST-AM can be very flexible. In principle, RST-AM_
can be applied to improve any slowly convergent black-box iterative process by viewing
the latter as a fixed-point iteration. For example, consider accelerating a solver of a commercial software, where we have no access to the underlying codes to provide our custom
implementation, or some case rewriting the codes is too cumbersome. Moreover, RSTAM can efficiently incorporate the preconditioning technique. Hence any optimizer, even
an optimizer built upon neural networks (Andrychowicz et al., 2016), can be used as a
preconditioner for RST-AM. Preconditioning largely enhances the applicability of RSTAM for various applications. For example, RST-AM can be preconditioned by Adam for
the language task and SGDM for image classification. So any fine-tuned first-order optimizer can be combined with RST-AM to achieve an overall improvement. As RST-AM is
light-weight, this additional cost is marginal and can be largely counteracted by the actual
improvement. So in some sense, the purpose of RST-AM is not to totally replace current
off-the-shelf optimizers but to achieve collaborative effectiveness: RST-AM is aware of the
second-order information while the first-order method can mitigate the ill-conditioning of
the problem.

Overall, the ST-AM methods have wide applicability and can be competent methods from both
theoretical and practical perspectives.

C PROOFS

We give more details about ST-AM and the proofs of the theorems in the main paper.

C.1 THE BASIC ST-AM FOR STRONGLY CONVEX QUADRATIC OPTIMIZATION

Recall that the strongly convex quadratic optimization is formulated as

min (22)
_x_ R[d][ f] [(][x][) := 1]2 _[x][T][Ax][ ‚àí]_ _[b][T][x,]_
_‚àà_

where A ‚àà R[d][√ó][d] is SPD, b ‚àà R[d]. Solving (22) is equivalent to solving the SPD linear system

_Ax = b._ (23)

The detail of the basic ST-AM is given in Algorithm 2.

We first state the relationship of AM with GMRES in the following proposition. Similar results can
also be found in (Walker & Ni, 2011; Wei et al., 2021).

Let x[G]k _[, r]k[G]_ [:=][ b][ ‚àí] _[Ax]k[G]_ [denote the][ k][-th GMRES iterate and residual, respectively, and][ K][k][(][A, v][) :=]
span{v, Av, . . ., A[k][‚àí][1]v} denotes the k-th Krylov subspace generated by A and v. Define e[j] :=
(1, 1, . . ., 1)[T] _‚àà_ R[j] for j ‚â• 1. Let range(X) denote the linear space spanned by the columns
of X. The main results of the full-memory AM are stated in Proposition 1 and Proposition 2.
The Proposition 1 is the same as the Proposition 2 in (Wei et al., 2021), and we restate it here for
completeness. The Proposition 2 is new as far as we know.

**Proposition 1 (General linear system). For solving a general linear system Ax = b with the full-**
_memory AM (m = k), suppose that Œ≤k > 0 and the fixed-point map is g(x) = (I ‚àí_ _A)x + b. If_
_the initial point of AM is x0 = x[G]0_ _[and][ rank(][R][k][) =][ m][, then the intermediate iterate][ ¬Ø]xk satisfies_
_x¬Øk = x[G]k_ _[.]_

_Proof. The definition of the fixed-point map suggests that the residual rk = g(xk)_ _‚àí_ _xk = b_ _‚àí_ _Axk._

Since Rk = ‚àíAXk and A is nonsingular, we have rank(Xk) = m. We first show

range(Xk) = Kk(A, r0[G][)] (24)

by induction. We abbreviate Kk(A, r0[G][)][ as][ K][k][ in this proof.]


-----

**Algorithm 2 ST-AM for strongly convex quadratic optimization**

**InputOutput: x: x0 ‚àà ‚ààRR[d][d], Œ≤k > 0, 0 < max iter ‚â§** _d._

1: P0, Q0 = 0 ‚àà R[d][√ó][2], p0, q0 = 0 ‚àà R[d]

2: for k = 0, 1, . . ., max iter do
3: _rk = ‚àí‚àáf_ (xk)

4: **if k > 0 then**

6:5: _qpÀú = = q xk ‚àíQxkk‚àí11(, qQ[T]k =1 r[q]k[)] ‚àí[,][ Àú]p =rk p‚àí1_ _Pk_ 1(Q[T]k 1[q][)] (Compute(qÀú _Qk_ ‚àÜ1) _xk‚àí1, ‚àÜrk‚àí1)_
_‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚ä•_ _‚àí_

7: _pk = Àúp/‚à•qÀú‚à•2, qk = Àúq/‚à•qÀú‚à•2_ (‚à•qk‚à•2 = 1)

8: _Pk = [pk‚àí1, pk], Qk = [qk‚àí1, qk]_ (Q[T]k _[Q][k][ =][ I][2][)]_

9: **end if**

10: Œìk = Q[T]k _[r][k]_

11:12: _xx¬Økk =+1 = ¬Ø xk ‚àíxk +Pk Œ≤Œìkkr¬Ø,k ¬Ørk = rk ‚àí_ _QkŒìk_ (Mixing step)(Projection step: ¬Ørk ‚ä• _Qk)_

13: **if ‚à•r¬Øk‚à•2 = 0 then**

14: break

15: **end if**

16: end for
17: return xk


First, ‚àÜx0 = Œ≤0r0 = Œ≤0r0[G] [since][ x][1][ =][ x][0][ +][ Œ≤][0][r][0][. If][ k][ = 1][, then the proof is complete. Then,]
suppose that k > 1 and, as an inductive hypothesis, that range(Xk‚àí1) = Kk‚àí1. With (4) we have

‚àÜxk 1 = xk _xk_ 1
_‚àí_ _‚àí_ _‚àí_
= Œ≤k 1rk 1 (Xk 1 + Œ≤k 1Rk 1)Œìk 1
_‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_
= Œ≤k‚àí1(b ‚àí _Axk‚àí1) ‚àí_ (Xk‚àí1 ‚àí _Œ≤k‚àí1AXk‚àí1)Œìk‚àí1_
= Œ≤k 1b _Œ≤k_ 1A(x0 + ‚àÜx0 + + ‚àÜxk 2) (Xk 1 _Œ≤k_ 1AXk 1)Œìk 1
_‚àí_ _‚àí_ _‚àí_ _¬∑ ¬∑ ¬∑_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_
= Œ≤k‚àí1r0 ‚àí _Œ≤k‚àí1AXk‚àí1e[k][‚àí][1]_ _‚àí_ (Xk‚àí1 ‚àí _Œ≤k‚àí1AXk‚àí1)Œìk‚àí1._ (25)

Since r0 _k_ 1, and by the inductive hypothesis range(Xk 1) _k_ 1 which also implies
rank(range(XAXk ‚ààK) =k‚àí m1) ‚äÜK =‚àí _k which impliesk, we know ‚àÜx dim(range(k‚àí1 ‚ààKk, which impliesXk)) = dim(_ range(k‚àí), we have ‚äÜKXk) ‚äÜK range(‚àí _k. Since we assumeXk) =_ _k, thus_
_K_ _K_
completing the induction. As a result, we also have

range(Rk) = range(AXk) = AKk(A, r0[G][)][.] (26)


Recalling that to determine Œìk, we solve the least squares problem (3) and Rk = ‚àíAXk. We have

Œìk = arg min (27)
Œì R[m][ ‚à•][r][k][ +][ AX][k][Œì][‚à•][2][.]
_‚àà_

Since rank(AXk) = rank(Xk) = m, (27) has a unique solution. Also, since rk = b ‚àí _Axk =_
_b ‚àí_ _A(x0 + Xke[k]) = r0 ‚àí_ _AXke[k], we have rk + AXkŒì = r0 ‚àí_ _AXke[k]_ + AXkŒì = r0 ‚àí _AXkŒì[Àú],_
where Œì =[Àú] _e[k]_ _‚àí_ Œì. So Œìk solves (27) if and only if Œì[Àú]k = e[k] _‚àí_ Œìk solves

min Œì 2, (28)
ŒìÀú R[m][ ‚à•][r][0][ ‚àí] _[AX][k]_ [Àú]‚à•
_‚àà_

According to (24), (28) is equal to minz _m(A,r0G[)][ ‚à•][r][0][ ‚àí]_ _[Az][‚à•][2][ which is the GMRES minimization]_
_‚ààK_
problem. Since the solution of (28) is also unique, we have

_x¬Øk = xk ‚àí_ _XkŒìk = xk ‚àí_ _Xk(e[k]_ _‚àí_ Œì[Àú]k) = x0 + XkŒì[Àú]k = x[G]k _[.]_

In Proposition 1, the assumption that Rk has full column rank is critical to ensure no stagnation
occurs in AM for solving a general linear system. In fact, for SPD linear systems (23) or strongly
convex quadratic optimization (22), when AM breaks down, i.e. Rk is rank deficient, AM obtains
the exact solution, as shown in the next proposition.


-----

**Proposition 2 (SPD). For applying the full-memory AM to minimize a strongly convex quadratic**
_problem (22), or equivalently, solve a SPD linear system (23), suppose that Œ≤k > 0 and the fixed-_
_point map is g(x) = (I_ _A)x + b. If rank(Rk) = k holds for 1_ _k < s while failing to hold for_
_‚àí_ _‚â§_
_k = s, where s_ 1, then the residual of AM satisfies rs = ¬Ørs 1 = 0.
_‚â•_ _‚àí_

_Proof. The definition of g suggests that the residual rk = g(xk)‚àíxk = b‚àíAxk. The relation Rk =_
_‚àíAXk holds during the iterations and the nonsingularity of A implies rank(Xk) = rank(Rk)._

For s = 1, since the first step of AM is x1 = x0 + Œ≤0r0, the assumption rank(R1) = 0 implies that
rank(r0) = rank(X1) = 0, i.e. r1 = ¬Ør0 := 0.

For s > 1, because ‚àÜxs 1 = xs _xs_ 1 = _Xs_ 1Œìs 1 + Œ≤s 1r¬Øs 1, the rank deficiency of
_Xs implies ‚àÜxs_ 1 range(‚àí _Xs_ 1) ‚àí, which further implies‚àí _‚àí_ _‚àí_ _‚àí ¬Ørs_ 1 _‚àírange(‚àí_ _Xs_ 1). So there exists
_Œ∂_ R[s][‚àí][1], such that‚àí _‚àà ¬Ørs_ 1 = Xs _‚àí1Œ∂. Note that according to (3),‚àí_ ¬Ø ‚ààrs 1 _Rs_ 1‚àí = _AXs_ 1, so we
have ‚àà _‚àí_ _‚àí_ _‚àí_ _‚ä•_ _‚àí_ _‚àí_ _‚àí_

0 = ¬Ørs[T] 1[AX][s][‚àí][1] [= (][X][s][‚àí][1][Œ∂][)][T][AX][s][‚àí][1] [=][ Œ∂] [T][X]s[T] 1[AX][s][‚àí][1][.] (29)
_‚àí_ _‚àí_

Because rank(Xs 1) = s 1 and A is SPD, we know Xs[T] 1[AX][s][‚àí][1] [is also SPD. So][ Œ∂][ = 0][, which]
_‚àí_ _‚àí_ _‚àí_
implies ¬Ørs 1 = 0. Hence xs = ¬Øxs 1 and rs = ¬Ørs 1 = 0.
_‚àí_ _‚àí_ _‚àí_

Now we give the proof of Theorem 1.

**_Proof of Theorem 1. Besides relations (i)-(iii), we add an auxiliary relation here:_**
(iv)We prove the relations (i)-(iv) by induction. rk = r0 + Q[¬Ø]kŒì[¬Ø]k ‚ààKk+1(A, r0), where Œì[¬Ø]k ‚àà R[k].

For k = 1, since ¬Ør0 = 0, according to Proposition 2, rank(‚àÜx0) = rank(X1) = 1, rank(‚àÜr0) =
rank(R1) = 1, so Àúq Ã∏= 0, which implies Line 7 in Algorithm 2 is well-defined. The relation (i) holds.
_Ã∏_
Since Àúq = q = ‚àÜx0, Àúp = p = ‚àÜr0, and ‚àÜr0 = ‚àíA‚àÜx0, the equality _Q[¬Ø]1 = ‚àíAP[¬Ø]1 also holds. Due_
the last equality is due toto the normalization in Line 7,rit is clear that¬Ø1 ‚ä• range(Q r11) = range( = r0 _‚àíQ[¬Ø]1 spanQŒì[¬Ø][¬Ø]11 ‚ààK). Also,Q[¬Ø]{[T]1ArQ2[¬Ø](01A, r} = 1 ¬Ør = range(1 =0). Since, namely relation (iv). Due to the projection step Line 11, r1 ‚àí_ _Q rQ111Œì =) = range( 1 = r0 r ‚àí0 ‚àíŒ≤0Œ≤Ar0Q[¬Ø]Ar01 and)0. For ‚àí range( Q r1Œì1[G]1 =[=]Q[¬Ø][ r] r1) = range(0[0] ‚àí[ ‚àí]Q[¬Ø][Az]1Œ∑[1]1[, where], whereAr0),_
_z1 = arg minz‚ààK1(A,r0) ‚à•r0 ‚àí_ _Az‚à•2, it holds r1[G]_ _[‚ä•]_ _[A][K][1][(][A, r][0][) = range( ¬Ø]Q1). As a result, both ¬Ør1_
and r1[G] [are the orthogonal projections of][ r][0][ onto the subspace][ range( ¬Ø]Q1)[‚ä•], which implies ¬Ør1 = r1[G][.]
So ¬Øx1 = x[G]1 [=][ x][0][ +][ z][1][ because their residuals are equal and][ A][ is nonsingular. Hence relation (iii)]
holds.

Suppose that k > 1, and as an inductive hypothesis, the relations (i)-(iv) hold for j = 1, . . ., k ‚àí 1.
Consider the k-th iteration. From Line 6 in Algorithm 2, Àúq ‚àà range(‚àÜrk‚àí1, Qk‚àí1), and Àúp ‚àà
range(‚àÜxk‚àí1, Pk‚àí1). We first prove that Àúq Ã∏= 0 by contradiction.

‚àÜIf Àúxq = 0k 1, then from Line 6 in Algorithm 2,range(Pk 1) range( P[¬Ø]k 1) as ‚àÜ ‚àÜrkr‚àík 1 ‚àà1 =range(A‚àÜQxkk‚àí11) and ‚äÜ range( Q[¬Ø]k 1 =Q[¬Ø]k‚àí1A), which impliesP[¬Ø]k 1 and A is
nonsingular. From Line 11 and Line 12, we have‚àí _‚àà_ _‚àí_ _‚äÜ_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_

‚àÜxk 1 = xk _xk_ 1 = _Pk_ 1Œìk 1 + Œ≤k 1r¬Øk 1. (30)
_‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_

_Œ∂So ¬ØrkR‚àí[k]1[‚àí] ‚àà[1], such thatrange(P ¬Økr‚àík_ 1)1 ‚äÜ = P[¬Ø]range( k 1Œ∂. From the inductive hypothesis, we knowP[¬Ø]k‚àí1) since ‚àÜxk‚àí1 ‚àà range(Pk‚àí1). Hence there exists ¬Ørk 1 _Qk_ 1 =
_‚àí ‚ààAP[¬Ø]k‚àí1, so we have_ _‚àí_ _‚àí_ _‚àí_ _‚ä•_ [¬Ø] _‚àí_

0 = ¬Ørk[T] 1[A][ ¬Ø]Pk 1 = ( P[¬Ø]k 1Œ∂)[T]AP[¬Ø]k 1 = Œ∂ [T][ ¬Ø]Pk[T] 1[A][ ¬Ø]Pk 1.
_‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_

Since _Q[¬Ø][T]k‚àí1Q[¬Ø]k‚àí1 = Ik‚àí1, we know rank( Q[¬Ø]k‚àí1) = k ‚àí_ 1, which implies rank( P[¬Ø]k‚àí1) = k ‚àí 1 due
to _Q[¬Ø]k_ 1 = _AP[¬Ø]k_ 1. Hence _P[¬Ø]k[T]_ 1[A][ ¬Ø]Pk 1 is also SPD. Then Œ∂ = 0 which implies ¬Ørk 1 = 0. It is
_‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_
impossible otherwise Algorithm 2 has terminated in the (k ‚àí 1)-th iteration. So Àúq Ã∏= 0 and Line 7 is
well-defined.

Since ¬Ørk 1 = rk 1 _Qk_ 1Œìk 1, and rk 1 _k(A, r0), range(Qk_ 1) range( Q[¬Ø]k 1) =
_AKk‚àí1(A, r‚àí_ 0) as the inductive hypothesis, we have‚àí _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚ààK ¬Ørk‚àí1 ‚ààKk(A, r0), which together with (30)‚àí_ _‚äÜ_ _‚àí_


-----

and range(Pk 1) range( P[¬Ø]k 1) = _k_ 1(A, r0) infers ‚àÜxk 1 _k(A, r0). Hence ‚àÜrk_ 1 =
_A‚àÜxk_ 1 _‚àíA_ _‚äÜk(A, r0). As a result,‚àí_ _K_ _q‚àík = Àúq/_ _qÀú_ 2 range(‚àÜ‚àí _‚ààKrk_ 1, Qk 1) _A_ _k(A, r‚àí_ 0).
So‚àí range( ‚àí _Q ‚àà[¬Ø]k) = range( K_ _Q[¬Ø]k_ 1, qk) _A_ _k(A, r0‚à•). Moreover,‚à•_ _‚àà_ _qk /‚àí_ range( ‚àí _Q[¬Ø] ‚äÜk_ 1), otherwiseK
_‚àí_ _‚äÜ_ _K_ _‚àà_ _‚àí_
_qÀú_ range( Q[¬Ø]k 1) that implies ‚àÜrk 1 = q range( Q[¬Ø]k 1), which is impossible following the
_‚àà_ _‚àí_ _‚àí_ _‚àà_ _‚àí_
former proof of Àúq = 0. So we have range( Q[¬Ø]k) = A _k(A, r0)._
_Ã∏_ _K_

Because ‚àÜrk 1 = _A‚àÜxk_ 1 and Qk 1 = _APk_ 1 due to _Q[¬Ø]k_ 1 = _AP[¬Ø]k_ 1, Line 6 in Algo_‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_
rithm 2 infers Àúq = ‚àíApÀú, which implies qk = ‚àíApk. So _Q[¬Ø]k = ‚àíAP[¬Ø]k. Since A is nonsingular and_
range( Q[¬Ø]k) = A _k(A, r0), we have range( P[¬Ø]k) =_ _k(A, r0)._
_K_ _K_

As range(Xk) = _k(A, r0) and range(Rk) = A_ _k(A, r0) has been proved in Proposition 1, the_
_K_ _K_
relation (i) holds for the k-th iteration.

To prove _Q[¬Ø][T]k_ _Q[¬Ø]k = Ik, it suffices to show qk ‚ä•_ _Q[¬Ø]k‚àí1, as the equalities_ _Q[¬Ø][T]k‚àí1Q[¬Ø]k‚àí1 = Ik‚àí1 and_
_qk_ 2 = 1 has already held. It is equivalent to prove Àúq _Qk_ 1. From the construction of Àúq in
_‚à•_ _‚à•_ _‚ä•_ [¬Ø] _‚àí_
Line 6 in Algorithm 2, we know Q[T]k 1q[Àú] = 0, so Àúq span(qk 2, qk 1) (for k = 2, Àúq _q0 = 0_
_‚àí_ _‚ä•_ _‚àí_ _‚àí_ _‚ä•_
clearly holds). To further prove Àúq ‚ä• range( Q[¬Ø]k‚àí3)(k ‚â• 4), note that

‚àÜrk 1 = _A‚àÜxk_ 1 = APk 1Œìk 1 _Œ≤k_ 1Ar¬Øk 1 = _Qk_ 1Œìk 1 _Œ≤k_ 1Ar¬Øk 1,
_‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_

where the second equality is a direct substitution with (30). Therefore,

_Q¬Ø[T]k‚àí3[‚àÜ][r][k][‚àí][1]_ [=][ ‚àí]Q[¬Ø][T]k‚àí3[Q][k][‚àí][1][Œì][k][‚àí][1] _[‚àí]_ _[Œ≤][k][‚àí][1]Q[¬Ø][T]k‚àí3[A]r[¬Ø]k‚àí1 = 0 ‚àí_ _Œ≤k‚àí1(AQ[¬Ø]k‚àí3)[T]r¬Øk‚àí1 = 0,_ (31)

where the second equality is due to range(Qk‚àí1) = span(qk‚àí2, qk‚àí1) ‚ä• range( Q[¬Ø]k‚àí3) and A is
_ASPD, the third equality is due to[2]Kk‚àí3(A, r0) ‚äÜ_ _AKk‚àí1(A, r0) ¬Ø. As a result, noting thatrk‚àí1 ‚ä•_ range( Q[¬Ø]k‚àí1) = q A = ‚àÜKkr‚àík1‚àí(1A, r, we obtain0) and range(AQ[¬Ø]k‚àí3) =

_Q¬Ø[T]k_ 3q[Àú] = Q[¬Ø][T]k 3[q][ ‚àí] _Q[¬Ø][T]k_ 3[Q][k][‚àí][1][(][Q]k[T] 1[q][) = 0][,]
_‚àí_ _‚àí_ _‚àí_ _‚àí_

which is due to (31) and range(Qk‚àí1) = span{qk‚àí2, qk‚àí1} ‚ä• range( Q[¬Ø]k‚àí3). Therefore, we show
that _Q[¬Ø][T]k_ _Q[¬Ø] = Ik, which along with_ _Q[¬Ø]k = ‚àíAP[¬Ø]k proves relation (ii) in the k-th iteration._

Next, we prove the relation (iv). We have

_rk = ¬Ørk_ 1 _Œ≤k_ 1Ar¬Øk 1
_‚àí_ _‚àí_ _‚àí_ _‚àí_
= rk‚àí1 ‚àí _Qk‚àí1Œìk‚àí1 ‚àí_ _Œ≤k‚àí1A(rk‚àí1 ‚àí_ _Qk‚àí1Œìk‚àí1)_
= rk 1 _Qk_ 1Œìk 1 _Œ≤k_ 1Ark 1 + Œ≤k 1AQk 1Œìk 1
_‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_
= r0 + Q[¬Ø]k 1Œì[¬Ø]k 1 _Qk_ 1Œìk 1 _Œ≤k_ 1(Ar0 + AQ[¬Ø]k 1Œì[¬Ø]k 1) + Œ≤k 1AQk 1Œìk 1,
_‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_

where the last equality is due to rk 1 = r0 + Q[¬Ø]k 1Œì[¬Ø]k 1 by the inductive hypothesis. Since
_‚àí_ _‚àí_ _‚àí_
range( Q[¬Ø]k‚àí1) = AKk‚àí1(A, r0) ‚äÜ _AKk(A, r0), range(Qk‚àí1) ‚äÜ_ range( Q[¬Ø]k), span{Ar0} ‚äÜ
_AKk(A, r0), range(AQk‚àí1)_ _‚äÜ_ range(AQ[¬Ø]k‚àí1) _‚äÜ_ _A[2]Kk‚àí1(A, r0)_ _‚äÜ_ _AKk(A, r0), and_
The relation (iv) is proved.range( Q[¬Ø]k) = AKk(A, r0), it is clear that rk = r0 + Q[¬Ø]kŒì[¬Ø]k ‚ààKk+1(A, r0) for some Œì[¬Ø]k ‚àà R[k].

range(Finally, we prove the relation (iii). For provingQk) already holds due to the projection step (Line 10 and Line 11 in Algorithm 2). It suffices ¬Ørk ‚ä• range( Q[¬Ø]k), note that ¬Ørk ‚ä• span{qk‚àí1, qk} =
to prove ¬Ørk range( Q[¬Ø]k 2). In fact, since we have ¬Ørk = rk _QkŒìk, we can prove that rk_
range( Q[¬Ø]k‚àí ‚ä•2) and QkŒìk ‚ä•‚àí range( Q[¬Ø]k‚àí2): _‚àí_ _‚ä•_

Since range(Qk) = span{qk‚àí1, qk} ‚ä• range( Q[¬Ø]k‚àí2) as induced from _Q[¬Ø][T]k_ _Q[¬Ø]k = Ik, it is clear_
that QkŒìk range( Q[¬Ø]k 2). For rk, according to Line 12 in Algorithm 2, we have rk = ¬Ørk 1
_QŒ≤¬Øk[T]k‚àí12A[A]r¬Ør[¬Ø]kk‚àí ‚ä•11. We have = (AQ[¬Ø]k_ ¬Ør2‚àík)[T]‚àír¬Ø1k ‚ä•1 = 0range( due toQ[¬Ø]k‚àí range(1) ‚äá range( AQ[¬Ø]k _Q2[¬Ø]) =k‚àí2 A) by the inductive hypothesis. Also,[2]_ _k_ 2(A, r0) _A_ _k_ 1(A, r‚àí0) = ‚àí
_‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _K_ _‚àí_ _‚äÜ_ _K_ _‚àí_
range( Q[¬Ø]k‚àí1).

range( Therefore, we obtainQ[¬Ø]k). ¬Ørk ‚ä• range( Q[¬Ø]k‚àí2), which along with ¬Ørk ‚ä• span{qk‚àí1, qk} implies ¬Ørk ‚ä•

To prove ¬Øxk = x[G]k [:=][ x][0][ +][ z][k][, where][ z][k][ = arg min]z _k(A,r0)_
_‚ààK_ _[‚à•][r][0][ ‚àí]_ _[Az][‚à•][2][, first we have][ r][k][ =]_
_rŒ∑On the other side, for GMRES,0k + ‚ààQ[¬Ø]Rk[k]Œì[¬Ø]. Sincek, where ¬ØrkŒì[¬Ø] ‚ä•k ‚ààQ[¬Ø]Rk[k], ¬Ø. Hencerk is the orthogonal projection of rk[G]_ ¬Ø[=]rk[ r] =[0] r[ ‚àí]k ‚àí[Az]Q[k][ ‚ä•]kŒìk[A] =[K] r[k]0[(] + [A, r]Q r[¬Ø][0]k[) = range( ¬Ø]0Œì[¬Ø] onto the subspacek ‚àí _QkŒìkQ =k) r, so0 ‚àí r range( Qk[G][¬Ø]k[is also the]Œ∑k, whereQ[¬Ø]k)[‚ä•]._


-----

orthogonal projection of r0 onto the subspace range( Q[¬Ø]k)[‚ä•]. So ¬Ørk = rk[G][, which further indicates]
_x¬Øk = x[G]k_ [. Hence, the relation (iii) holds.]

With relations (i)-(iv) being proved in the k-th iteration, we complete the induction.

C.2 MODIFIED ST-AM FOR GENERAL FIXED-POINT ITERATIONS

Algorithm 2 is suitable for analysis and implementation in the linear case. For general nonlinear
fixed-point iterations, we adopt an alternative form as described in Algorithm 3 which discards the
normalization of Àúq in each iteration (Line 7 in Algorithm 2). In Line 7 in Algorithm 3, the orthogonal
projection of ‚àÜrk 1 is checked to ensure ‚àÜrk 1 is ‚Äúless linearly dependent‚Äù on range(Qk), which
_‚àí_ _‚àí_
ensures _qk_ 2 is bounded away from zero; the check of _Pk_ 1Œ∂k 2 ensures that _pk_ ‚àÜxk 1 2
_cp_ ‚àÜxk ‚à• 1 _‚à•2, which is also important since a large deviation from ‚à•_ _‚àí_ _‚à•_ ‚àÜxk 1 can make ‚à• _‚àí_ _pk_ _‚àí2 > œÅ‚à•_ _‚â§_
_‚à•_ _‚àí_ _‚à•_ _‚àí_ _‚à•_ _‚à•_
(œÅ is the radius introduced in Theorem 2). When this condition cannot be satisfied, the algorithm
simply reuses the old Pk‚àí1, Qk‚àí1. The main procedure of MST-AM restarts every m iterations, i.e.
_Pk, Qk = 0. Such restart mechanism is to restrict the higher-order terms in the residual expansion,_
as shown in (9). Also, restart can flush out the outdated historical information that may weaken the
quality of Pk and Qk that are used to pursue a local first-order approximation of g in MST-AM.

**Algorithm 3 MST-AM for nonlinear fixed-point problems**

**Input: x0 ‚àà** R[d], Œ≤k ‚àà (0, 1], cp > 0, cq ‚àà (0, 1), m > 0.
**Output: x ‚àà** R[d]

1: P0, Q0 = 0 ‚àà R[d][√ó][2], p0, q0 = 0 ‚àà R[d]

2: for k = 0, 1, . . ., until convergence do
3: _rk = g(xk) ‚àí_ _xk_

4: **if k mod m Ã∏= 0 then**

5:6: _pŒ∂k = = ( xkQ ‚àí[T]k_ _x1k[Q]‚àí[k]1[‚àí], q[1] =[)][‚Ä†][Q] r[T]kk ‚àí1[q]rk‚àí1_

_‚àí_ _‚àí_

7: **if** _Pk_ 1Œ∂k 2 _cp_ _p_ 2 and _Qk_ 1Œ∂k 2 _cq_ _q_ 2 then

8: _‚à•pk =‚àí p_ _‚à•_ _P ‚â§k_ 1Œ∂‚à•k, q‚à•k = q ‚à• _Q‚àík_ 1Œ∂‚à•k _‚â§_ _‚à•_ _‚à•_
_‚àí_ _‚àí_ _‚àí_ _‚àí_

9: _Pk = [pk_ 1, pk], Qk = [qk 1, qk] (qk _Qk_ 1)

10: **else** _‚àí_ _‚àí_ _‚ä•_ _‚àí_

11: _Pk = Pk_ 1, Qk = Qk 1
_‚àí_ _‚àí_

12: **end if**

13: **else**

14: _Pk, Qk = 0 ‚àà_ R[d][√ó][2], pk, qk = 0 ‚àà R[d]

15: **end if**

16: Œìk = (Q[T]k _[Q][k][)][‚Ä†][Q]k[T][r][k]_

17:18: _xx¬Økk =+1 = ¬Ø xk ‚àíxk +Pk Œ≤Œìkkr¬Ø,k ¬Ørk = rk ‚àí_ _QkŒìk_ (Mixing step)(Projection step: ¬Ørk ‚ä• _Qk)_

19: end for
20: return xk
(The notation ‚Äú‚Ä†‚Äù is the Moore-Penrose pseudoinverse.)


In the linear case, Algorithm 2 and Algorithm 3 (with m = ‚àû) are equivalent. Similar to Algorithm 2, we have the following properties held for MST-AM:

**Claim 1. In the k-th iteration (k > 0) of Algorithm 1 applied to minimize a strongly convex**
_quadratic problem (5), assuming cp = ‚àû, cq = 1, m = ‚àû, the following relations hold:_
_(i) ‚à•qk‚à•2 > 0, range( P[¬Ø]k) = range(Xk) = Kk(A, r0), range( Q[¬Ø]k) = range(Rk) = AKk(A, r0);_
_(ii)_ _Q[¬Ø]k = ‚àíAP[¬Ø]k, qi ‚ä•_ _qj(1 ‚â§_ _i Ã∏= j ‚â§_ _k);_
_(iii) ¬Ørk ‚ä•_ range( Q[¬Ø]k) and ¬Øxk = x0 + zk, where zk = arg minz‚ààKk(A,r0) ‚à•r0 ‚àí _Az‚à•2._
_If ‚à•r¬Øk‚à•2 = 0, then xk+1 is the exact solution._

The proof of Claim 1 is essentially the same as the proof of Theorem 1, with a special care that
_Q¬Ø[T]k_ _Q[¬Ø]k = Ik is replaced by the relation that columns of_ _Q[¬Ø]k are orthogonal to each other, in other_
words, _Q[¬Ø][T]k_ _Q[¬Ø]k = diag{‚à•q1‚à•2[2][, . . .,][ ‚à•][q][k][‚à•][2]2[}][.]_


-----

For minimizing nonlinear functions or accelerating nonlinear fixed-point iterations, the long-term
relation that _Q[¬Ø][T]k_ _Q[¬Ø]k = diag{‚à•q1‚à•2[2][, . . .,][ ‚à•][q][k][‚à•][2]2[}][ generally cannot hold, while the orthogonalization]_
procedure in Line 6 still leads to a short-term orthogonality relation: qi ‚ä• _qj for |i ‚àí_ _j| ‚â§_ 2.
Hence, Q[T]k _[Q][k][ = diag][{‚à•][q][k][‚àí][1][‚à•]2[2][,][ ‚à•][q][k][‚à•][2]2[}][ for][ k][ ‚â•]_ [1][.] Thus the pseudoinverse (Q[T]k _[Q][k][)][‚Ä†][ =]_
diag ( _qk_ 1 2 = 0)1/ _qk_ 1 2[,][ I][(][‚à•][q][k][‚à•][2] 2[}][, where][ I][(][¬∑][)][ is the indicator function]
that _{I_ _‚à•_ _‚àí_ _‚à•_ _Ã∏_ _‚à•_ _‚àí_ _‚à•[2]_ _[Ã∏][= 0)1][/][‚à•][q][k][‚à•][2]_

1 _x is true,_
(x) =
_I_ 0 _x is false._


Now, we give the proof of Theorem 2.

**_Proof of Theorem 2. For convenience, we restate the main assumptions of g here:_**

(i) _g(y)_ _g(x)_ 2 _Œ∫_ _y_ _x_ 2, Œ∫ (0, 1), for _x, y_ (œÅ), (32a)
_‚à•_ _‚àí_ _‚à•_ _‚â§_ _‚à•_ _‚àí_ _‚à•_ _‚àà_ _‚àÄ_ _‚ààB_

(ii) _g[‚Ä≤](y)_ _g[‚Ä≤](x)_ 2 _Œ∫ÀÜ_ _y_ _x_ 2, ÀÜŒ∫ > 0, for _x, y_ (œÅ). (32b)
_‚à•_ _‚àí_ _‚à•_ _‚â§_ _‚à•_ _‚àí_ _‚à•_ _‚àÄ_ _‚ààB_

Also, since |1 _‚àí_ _Œ≤k|_ + _Œ∫Œ≤k < 1, we know Œ≤k > 0 is bounded, i.e. Œ≤k ‚â§_ _Œ≤ where Œ≤ > 0 is a constant._

The proof is based on the two lemmas given in Lemma 1 and Lemma 2. Besides (9), we also prove
that ‚à•rk‚à•2 ‚â§‚à•r0‚à•2 by induction.

_xFor[‚àó]‚à• k2 + = 0 Œ≤(1 +, ¬Øx0 Œ∫ =)‚à• xx00 ‚àí ‚ààBx[‚àó](‚à•œÅ2) ‚â§, and due to (48b),œÅ provided ‚à•x0 ‚àí ‚à•xx[‚àó]1 ‚àí‚à•2 ‚â§x[‚àó]œÅ/‚à•2 ‚â§‚à•(1 + Œ≤x0(1 + ‚àí_ _x Œ∫[‚àó]))‚à•2. Since + Œ≤0‚à•r0‚à•2 ‚â§‚à•x0 ‚àí_

_r1 = g(x1) ‚àí_ _x1 = g(x1) ‚àí_ (x0 + Œ≤0r0) = g(x1) ‚àí (x0 + r0) + (1 ‚àí _Œ≤0)r0_
= g(x1) _g(x0) + (1_ _Œ≤0)r0,_
_‚àí_ _‚àí_

it follows that

_‚à•r1‚à•2 ‚â§‚à•g(x1) ‚àí_ _g(x0)‚à•2 + |1 ‚àí_ _Œ≤0|‚à•r0‚à•2_
_‚â§_ _Œ∫Œ≤0‚à•r0‚à•2 + |1 ‚àí_ _Œ≤0|‚à•r0‚à•2 = (Œ∫Œ≤0 + |1 ‚àí_ _Œ≤0|)‚à•r0‚à•2._

Also note that Œ∏0 = ‚à•r¬Ø0‚à•2/‚à•r0‚à•2 = 1. Thus (9) holds. Because Œ∫Œ≤0 + |1 ‚àí _Œ≤0| ‚â§_ _Œ∫0 < 1, we have_
_‚à•r1‚à•2 < ‚à•r0‚à•2._

Now, suppose that (9) and ‚à•rk‚à•2 ‚â§‚à•r0‚à•2 hold for k ‚â• 0. We establish the results for k + 1.

Let Œìk = (Œ≥k[(1)][, Œ≥]k[(2)][)][T][ ‚àà] [R][2][. Since][ Œì][k][ = (][Q]k[T][Q][k][)][‚Ä†][Q]k[T][r][k][,][ Q]k[T][Q][k][ = diag][{‚à•][q][k][‚àí][1][‚à•]2[2][,][ ‚à•][q][k][‚à•][2]2[}][, it]
follows that

_k_ _[q][k][‚àí][1]_ _k_ _[q][k]_
_Œ≥k[(1)]_ = I(qk‚àí1 Ã∏= 0) _q[r]k[T]‚àí[T]1[q][k][‚àí][1]_ _, Œ≥k[(2)]_ = I(qk Ã∏= 0) _q[r]k[T][T][q][k]_ _._ (33)

Therefore,


_|Œ≥k[(1)][| ‚â§I][(][q][k][‚àí][1][ Ã∏][= 0)][ ‚à•]‚à•qk[r]‚àí[k][‚à•]1[2]‚à•2_ _, |1 ‚àí_ _Œ≥k[(1)][| ‚â§]_ [max] I(qk‚àí1 = 0), I(qk‚àí1 Ã∏= 0) _[‚à•][r][k]‚à•[ ‚àí]qk‚àí[q]1[k]‚à•[‚àí]2[1][‚à•][2]_

_|Œ≥k[(2)][| ‚â§I][(][q][k][ Ã∏][= 0)]_ _[‚à•]‚à•[r]qk[k]‚à•[‚à•]2[2]_ _, |1 ‚àí_ _Œ≥k[(2)][| ‚â§]_ [max] I(qk = 0), I(qk Ã∏= 0) _[‚à•][r][k]‚à•[ ‚àí]qk‚à•[q]2[k][‚à•][2]_  _._

Define c = (1‚àíŒ∫1+)(1cp‚àícq) [. We have]


(34)


_‚à•PkŒìk‚à•2 = ‚à•pkŒ≥k[(2)]_ + pk‚àí1Œ≥k[(1)]‚àí1[‚à•][2][ ‚â§‚à•][p][k][Œ≥]k[(2)][‚à•][2][ +][ ‚à•][p][k][‚àí][1][Œ≥]k[(1)]‚àí1[‚à•][2]

_rk_ 2 _rk_ 2
_‚â§I(qk Ã∏= 0)‚à•pk‚à•2_ _‚à•qk‚à•2_ + I(qk‚àí1 Ã∏= 0)‚à•pk‚àí1‚à•2 _q‚à•k_ _‚à•1_ 2 _‚â§_ 2c‚à•rk‚à•2, (35)
_‚à•_ _‚à•_ _‚à•_ _‚àí_ _‚à•_

where the second inequality is due to (34) and the third inequality is due to (49). Then

_‚à•x¬Øk ‚àí_ _x[‚àó]‚à•2 = ‚à•xk ‚àí_ _PkŒìk ‚àí_ _x[‚àó]‚à•2 ‚â§‚à•xk ‚àí_ _x[‚àó]‚à•2 + ‚à•PkŒìk‚à•2_


1 _Œ∫_ _[‚à•][r][k][‚à•][2][ + 2][c][‚à•][r][k][‚à•][2][ = (]_
_‚àí_


1 _Œ∫_ [+ 2][c][)][‚à•][r][k][‚à•][2][,]
_‚àí_


-----

and due to _r¬Øk_ 2 _rk_ 2, it holds that
_‚à•_ _‚à•_ _‚â§‚à•_ _‚à•_

_‚à•xk+1 ‚àí_ _x[‚àó]‚à•2 = ‚à•x¬Øk + Œ≤kr¬Øk ‚àí_ _x[‚àó]‚à•2 ‚â§‚à•x¬Øk ‚àí_ _x[‚àó]‚à•2 + Œ≤k‚à•r¬Øk‚à•2_

1
_x¬Øk_ _x[‚àó]_ 2 + Œ≤ _rk_ 2 = (
_‚â§‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚à•_ 1 _Œ∫_ [+ 2][c][ +][ Œ≤][)][‚à•][r][k][‚à•][2][.]

_‚àí_

By the inductive hypothesis that _rk_ 2 _r0_ 2, and (48b), it has
_‚à•_ _‚à•_ _‚â§‚à•_ _‚à•_


1 1
_x¬Øk_ _x[‚àó]_ 2 ( (1 + Œ∫) _x0_ _x[‚àó]_ 2, (36)
_‚à•_ _‚àí_ _‚à•_ _‚â§_ 1 _Œ∫_ [+ 2][c][)][‚à•][r][0][‚à•][2][ ‚â§] 1 _Œ∫_ [+ 2][c] _‚à•_ _‚àí_ _‚à•_

_‚àí_  _‚àí_ 

and

1 1
_xk+1_ _x[‚àó]_ 2 _r0_ 2 (1 + Œ∫) _x0_ _x[‚àó]_ 2. (37)
_‚à•_ _‚àí_ _‚à•_ _‚â§_ 1 _Œ∫_ [+ 2][c][ +][ Œ≤] _‚à•_ _‚à•_ _‚â§_ 1 _Œ∫_ [+ 2][c][ +][ Œ≤] _‚à•_ _‚àí_ _‚à•_
 _‚àí_   _‚àí_ 

As a result, we can choosewhich ensure the g(¬Øxk) and ‚à• gx(x0 ‚àík+1x)[‚àó] are well defined.‚à•2 sufficiently small to ensure ¬Øxk ‚ààB(œÅ) and xk+1 ‚ààB(œÅ),

At the end of the k-th iteration of Algorithm 3, we have

_rk+1 = g(xk+1) ‚àí_ _xk+1_
= g(xk+1) ‚àí _g(¬Øxk) + g(¬Øxk) ‚àí_ (¬Øxk + Œ≤kr¬Øk)
= (g(xk+1) _g(¬Øxk)) + (g(¬Øxk)_ _x¬Øk_ _r¬Øk) + (1_ _Œ≤k)¬Ørk._ (38)
_‚àí_ _‚àí_ _‚àí_ _‚àí_

Let Lk := g(xk+1) ‚àí _g(¬Øxk) + (1 ‚àí_ _Œ≤k)¬Ørk, Hk := g(¬Øxk) ‚àí_ _x¬Øk ‚àí_ _r¬Øk, then_

_‚à•Lk‚à•2 ‚â§_ _Œ∫‚à•xk+1 ‚àí_ _x¬Øk‚à•2 + |1 ‚àí_ _Œ≤k|‚à•r¬Øk‚à•2_
= Œ∫Œ≤k‚à•r¬Øk‚à•2 + |1 ‚àí _Œ≤k|‚à•r¬Øk‚à•2_
= Œ∏k(Œ∫Œ≤k + |1 ‚àí _Œ≤k|)‚à•rk‚à•2,_ (39)

which bounds the linear part of the residual rk+1.

For the higher-order terms _k, we have_
_H_

_Hk = g(¬Øxk) ‚àí_ (xk ‚àí _PkŒìk + rk ‚àí_ _QkŒìk)_
= g(¬Øxk) ‚àí _g(xk) + (Pk + Qk)Œìk._

= g(¬Øxk) ‚àí _g(xk) + (pk + qk)Œ≥k[(2)]_ + (pk‚àí1 + qk‚àí1)Œ≥k[(1)][.] (40)


1
According to the formula 0 _[g][‚Ä≤][(][x][ +][ t][(][y][ ‚àí]_ _[x][))(][y][ ‚àí]_ _[x][)][dt][ =][ g][(][y][)][ ‚àí]_ _[g][(][x][)][, we have]_
R

_g(¬Øxk) ‚àí_ _g(xk) = g(¬Ø 1xk) ‚àí_ _g(xk ‚àí_ _pkŒ≥k[(2)][) +][ g][(][x][k][ ‚àí]_ _[p][k][Œ≥]k[(2)][)][ ‚àí]_ _[g][(][x][k][)]_

= 0 _‚àíg[‚Ä≤](xk ‚àí_ _pkŒ≥k[(2)]_ _‚àí_ _tpk‚àí1Œ≥k[(1)][)][p][k][‚àí][1][Œ≥]k[(1)][dt]_
Z

1
+ 0 _‚àíg[‚Ä≤](xk ‚àí_ _tpkŒ≥k[(2)][)][p][k][Œ≥]k[(2)][dt.]_ (41)
Z

Also, by Lemma 2, we have

1 _k‚àí1_
_pk + qk =_ _g[‚Ä≤](xk_ _tpk)pkdt + ÀÜŒ∫_ ‚àÜrœÄ(k) 1 2 ( ‚àÜrj 2),
Z0 _‚àí_ _‚à•_ _‚àí_ _‚à•_ _j=Xk‚àímk_ _O_ _‚à•_ _‚à•_


1
_pk_ 1 + qk 1 =
_‚àí_ _‚àí_ 0
Z


_k‚àí2_

( ‚àÜrj 2),
_O_ _‚à•_ _‚à•_
_j=Xk‚àímk_


_g[‚Ä≤](xk‚àí1 ‚àí_ _tpk‚àí1)pk‚àí1dt + ÀÜŒ∫‚à•‚àÜrœÖ(k)‚àí1‚à•2_


where œÄ(k) denotes that the latest update of qk by Line 8 occurred in the œÄ(k)-th iteration and
_œÖ(k) = œÄ(œÄ(k)_ 1) marks that qk 1 records qœÖ(k) that is the penultimate update by Line 8 occurring
_‚àí_ _‚àí_
in the œÖ(k)-th iteration.


-----

By substituting these relations to (40), it follows that


1 _k‚àí1_

_Hk =_ Ô£´Z0 _g[‚Ä≤](xk ‚àí_ _tpk)pkdt + ÀÜŒ∫‚à•‚àÜrœÄ(k)‚àí1‚à•2_ _j=Xk‚àímk_ _O(‚à•‚àÜrj‚à•2)Ô£∂_ _Œ≥k[(2)]_

Ô£≠ 1 _k‚àí2_ Ô£∏

+ Ô£´Z0 _g[‚Ä≤](xk‚àí1 ‚àí_ _tpk‚àí1)pk‚àí1dt + ÀÜŒ∫‚à•‚àÜrœÖ(k)‚àí1‚à•2_ _j=Xk‚àímk_ _O(‚à•‚àÜrj‚à•2)Ô£∂_ _Œ≥k[(1)]_

Ô£≠ 1 1 Ô£∏

_‚àí_ 0 _g[‚Ä≤](xk ‚àí_ _pkŒ≥k[(2)]_ _‚àí_ _tpk‚àí1Œ≥k[(1)][)][p][k][‚àí][1][Œ≥]k[(1)][dt][ ‚àí]_ 0 _g[‚Ä≤](xk ‚àí_ _tpkŒ≥k[(2)][)][p][k][Œ≥]k[(2)][dt]_
Z Z

1
= 0 (g[‚Ä≤](xk ‚àí _tpk) ‚àí_ _g[‚Ä≤](xk ‚àí_ _tpkŒ≥k[(2)][))][p][k][Œ≥]k[(2)][dt]_
Z

1
+ 0 (g[‚Ä≤](xk‚àí1 ‚àí _tpk‚àí1) ‚àí_ _g[‚Ä≤](xk ‚àí_ _pkŒ≥k[(2)]_ _‚àí_ _tpk‚àí1Œ≥k[(1)][))][p][k][‚àí][1][Œ≥]k[(1)][dt]_
Z

_k‚àí1_

+ ÀÜŒ∫‚à•‚àÜrœÄ(k)‚àí1‚à•2 _O(‚à•‚àÜrj‚à•2)Œ≥k[(2)]_

_j=Xk‚àímk_

_k‚àí2_

+ ÀÜŒ∫‚à•‚àÜrœÖ(k)‚àí1‚à•2 _O(‚à•‚àÜrj‚à•2)Œ≥k[(1)]_

_j=Xk‚àímk_

= Ak + Bk + Ck + Dk. (42)
Then we can bound each terms of _k as follows (Here we assume qk_ = 0 and qk 1 = 0 as qk = 0
_H_ _Ã∏_ _‚àí_ _Ã∏_
leads to Œ≥k[(2)] = 0 and qk‚àí1 = 0 leads to Œ≥k[(1)] = 0 where the result is trivial.):

1

_‚à•Ak‚à•2 =_ 0 (g[‚Ä≤](xk ‚àí _tpk) ‚àí_ _g[‚Ä≤](xk ‚àí_ _tpkŒ≥k[(2)][))][p][k][Œ≥]k[(2)][dt]_ 2

Z

1
_‚â§_ 0 _‚à•g[‚Ä≤](xk ‚àí_ _tpk) ‚àí_ _g[‚Ä≤](xk ‚àí_ _tpkŒ≥k[(2)][)][‚à•][2][‚à•][p][k][Œ≥]k[(2)][‚à•][2][dt]_
Z

1
_Œ∫ÀÜ_ _tpk_ _tpkŒ≥k[(2)]_ _k_
_‚â§_ 0 _‚à•_ _‚àí_ _[‚à•][2][‚à•][p][k][‚à•][2][|][Œ≥][(2)][|][dt]_
Z

= Œ∫[ÀÜ]2 2[|][1][ ‚àí] _[Œ≥]k[(2)]_ _k_

_[‚à•][p][k][‚à•][2]_ _[||][Œ≥][(2)][|]_ _k_

_Œ∫_ ( _rk_ 2 _rk_ _qk_ 2)
= [ÀÜ] 2 _O_ _‚à•_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ = ÀÜŒ∫ ( _rj_ 2[)][,] (43)
2 _[‚à•][p][k][‚à•][2]_ _‚à•qk‚à•2[2]_ _j=Xk‚àímk_ _O_ _‚à•_ _‚à•[2]_

where the third equality is due to (34), and the last equality is due to (49) and (50b);

1

_‚à•Bk‚à•2 =_ 0 (g[‚Ä≤](xk‚àí1 ‚àí _tpk‚àí1) ‚àí_ _g[‚Ä≤](xk ‚àí_ _pkŒ≥k[(2)]_ _‚àí_ _tpk‚àí1Œ≥k[(1)][))][p][k][‚àí][1][Œ≥]k[(1)][dt]_ 2

Z

1
_‚â§_ 0 _‚à•g[‚Ä≤](xk‚àí1 ‚àí_ _tpk‚àí1) ‚àí_ _g[‚Ä≤](xk ‚àí_ _pkŒ≥k[(2)]_ _‚àí_ _tpk‚àí1Œ≥k[(1)][)][‚à•][2][‚à•][p][k][‚àí][1][Œ≥]k[(1)][‚à•][2][dt]_
Z

1
_‚â§_ _Œ∫ÀÜ_ 0 _‚à•‚àÜxk‚àí1 ‚àí_ _pkŒ≥k[(2)]_ _‚àí_ _tpk‚àí1Œ≥k[(1)]_ + tpk‚àí1‚à•2‚à•pk‚àí1‚à•2|Œ≥k[(1)][|][dt]
Z

_‚â§_ _Œ∫ÀÜ_ ‚à•‚àÜxk‚àí1‚à•2 + ‚à•pk‚à•2|Œ≥k[(2)][|][ + 1]2 _[‚à•][p][k][‚àí][1][‚à•][2][|][1][ ‚àí]_ _[Œ≥]k[(1)][|]_ _‚à•pk‚àí1‚à•2|Œ≥k[(1)][|]_


_Œ∫ÀÜ_ ‚àÜxk 1 2 + _pk_ 2 _‚à•rk‚à•2_ + [1] _‚à•rk ‚àí_ _qk‚àí1‚à•2_ _pk_ 1 2 _‚à•rk‚à•2_
_‚â§_ ‚à• _‚àí_ _‚à•_ _‚à•_ _‚à•_ _‚à•qk‚à•2_ 2 _[‚à•][p][k][‚àí][1][‚à•][2]_ _‚à•qk‚àí1‚à•2_  _‚à•_ _‚àí_ _‚à•_ _‚à•qk‚àí1‚à•2_

= ÀÜŒ∫ _rk_ 2 ( ( ‚àÜrk 1 2) + ( _rk_ 2) + ( _rk_ _qk_ 1 2)
_‚à•_ _‚à•_ _O_ _‚à•_ _‚àí_ _‚à•_ _O_ _‚à•_ _‚à•_ _O_ _‚à•_ _‚àí_ _‚àí_ _‚à•_


_O(‚à•rj‚à•2[2][)][,]_ (44)
_j=Xk‚àímk_


= ÀÜŒ∫


-----

where the last inequality is due to (34), and the second equality is due to (48a), (49), (50b);


_k‚àí1_

_‚à•Ck‚à•2 = ÀÜŒ∫‚à•‚àÜrœÄ(k)‚àí1‚à•2_ _O(‚à•‚àÜrj‚à•2)Œ≥k[(2)]_

_j=Xk‚àímk_

_rk_ 2 _k‚àí1_
_‚â§_ _Œ∫ÀÜ‚à•‚àÜrœÄ(k)‚àí1‚à•2_ _‚à•qk‚à•2_ _O(‚à•‚àÜrj‚à•2)_
_‚à•_ _‚à•_ _j=Xk‚àímk_

_rk_ 2 _k‚àí1_
= ÀÜŒ∫‚à•‚àÜrœÄ(k)‚àí1‚à•2 _q‚à•œÄ(k‚à•)_ 2 _O(‚à•‚àÜrj‚à•2)_

_‚à•_ _‚à•_ _j=Xk‚àímk_


= ÀÜŒ∫ _O(‚à•rj‚à•2[2][)][,]_ (45)

_j=Xk‚àímk_

where the first inequality is from (34), and the second equality is due to (50b);


_k‚àí2_

( ‚àÜrj 2)Œ≥k[(1)]
_O_ _‚à•_ _‚à•_
_j=Xk‚àímk_


_Dk_ 2 = ÀÜŒ∫ ‚àÜrœÖ(k) 1 2
_‚à•_ _‚à•_ _‚à•_ _‚àí_ _‚à•_


_k‚àí2_

( ‚àÜrj 2)
_O_ _‚à•_ _‚à•_
_j=Xk‚àímk_

_k‚àí2_

( ‚àÜrj 2)
_O_ _‚à•_ _‚à•_
_j=Xk‚àímk_


_rk_ 2
_‚â§_ _Œ∫ÀÜ‚à•‚àÜrœÖ(k)‚àí1‚à•2_ _‚à•q‚à•k‚àí‚à•1‚à•2_

_rk_ 2
= ÀÜŒ∫‚à•‚àÜrœÖ(k)‚àí1‚à•2 _q‚à•œÖ(k‚à•)_ 2

_‚à•_ _‚à•_


= ÀÜŒ∫ _O(‚à•rj‚à•2[2][)][,]_ (46)

_j=Xk‚àímk_

where the first inequality is from (34), and the second equality is due to (50b). Then with the bounds
(43), (44), (45) and (46), we obtain


_O(‚à•rj‚à•2[2][)][.]_ (47)
_j=Xk‚àímk_


_‚à•Hk‚à•2 = ÀÜŒ∫_

Combining (39) and (47) to (38), we obtain


_‚à•rk+1‚à•‚â§‚à•Lk‚à•2 + ‚à•Hk‚à•2 ‚â§_ _Œ∏k(Œ∫Œ≤k + |1 ‚àí_ _Œ≤k|)‚à•rk‚à•2 + ÀÜŒ∫_ _j=Xk‚àímk_ _O(‚à•rj‚à•2[2][)][,]_

as desired. Since mk _m, the higher-order terms are limited. Note that_
_‚â§_

_Œ∫Œ≤k + |1 ‚àí_ _Œ≤k| ‚â§_ _Œ∫0 < 1_

by assumption. Then, for ‚à•x0‚àíx[‚àó]‚à•2 sufficiently small, the residuals {rk} are Q-linearly convergent,
which infers _rk_ 2 _r0_ 2. Therefore, we complete the induction.
_‚à•_ _‚à•_ _‚â§‚à•_ _‚à•_

**Remark 6. There may be some concern about whether ÀÜŒ∫ can be counteracted by the constant hidden**
_in the Big-O notation. In fact, since ÀÜŒ∫ is the Lipschitz constant of g[‚Ä≤]_ _and the constants in O(¬∑) are_
_composed of Œ∫, cp, cq, it follows that ÀÜŒ∫ is unrelated to_ ( ). Hence a small ÀÜŒ∫ can lead to a small
_O_ _¬∑_
_uniform boundedness of the higher-order terms. In the extreme case where ÀÜŒ∫ = 0, e.g. g is a linear_
_map, the residual only consists of the first-order term_ _k._
_L_
**Lemma 1. Under the same assumptions of Theorem 2, for k ‚â•** 1, we have the following bounds:

(1 ‚àí _Œ∫)‚à•‚àÜxk‚àí1‚à•2 ‚â§‚à•‚àÜrk‚àí1‚à•2 ‚â§_ (1 + Œ∫)‚à•‚àÜxk‚àí1‚à•2, (48a)
(1 _Œ∫)_ _xk_ _x[‚àó]_ 2 _rk_ 2 (1 + Œ∫) _xk_ _x[‚àó]_ 2, (48b)
_‚àí_ _‚à•_ _‚àí_ _‚à•_ _‚â§‚à•_ _‚à•_ _‚â§_ _‚à•_ _‚àí_ _‚à•_

_If qk_ = 0, then
_Ã∏_ _‚à•pk‚à•2_ 1 + cp (49)
_qk_ 2 _‚â§_ (1 _Œ∫)(1_ _cq)_
_‚à•_ _‚à•_ _‚àí_ _‚àí_


-----

_If the condition in Line 7 is true, then_

_‚à•pk‚à•2 ‚â§_ (1 + cp)‚à•‚àÜxk‚àí1‚à•2, (50a)
_qk Ã∏= 0, (1 ‚àí_ _cq)‚à•‚àÜrk‚àí1‚à•‚â§‚à•qk‚à•2 ‚â§‚à•‚àÜrk‚àí1‚à•2_ (50b)

_Proof. From the assumption (32a) of g, we have_

(1 ‚àí _Œ∫)‚à•‚àÜxk‚àí1‚à•2 ‚â§‚à•xk ‚àí_ _xk‚àí1‚à•2 ‚àí‚à•g(xk) ‚àí_ _g(xk‚àí1)‚à•2_
_‚â§‚à•g(xk) ‚àí_ _g(xk‚àí1) ‚àí_ (xk ‚àí _xk‚àí1)‚à•2_
= _rk_ _rk_ 1 2 = ‚àÜrk 1 2
_‚à•_ _‚àí_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_
_g(xk)_ _g(xk_ 1) 2 + _xk_ _xk_ 1
_‚â§‚à•_ _‚àí_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚àí_ _‚à•_
_‚â§_ (1 + Œ∫)‚à•xk ‚àí _xk‚àí1‚à•2,_

(1 _Œ∫)_ _xk_ _x[‚àó]_ 2 _xk_ _x[‚àó]_ 2 _g(xk)_ _g(x[‚àó])_ 2
_‚àí_ _‚à•_ _‚àí_ _‚à•_ _‚â§‚à•_ _‚àí_ _‚à•_ _‚àí‚à•_ _‚àí_ _‚à•_
_‚â§‚à•g(xk) ‚àí_ _g(x[‚àó]) ‚àí_ (xk ‚àí _x[‚àó])‚à•2 = ‚à•rk‚à•2_
_‚â§‚à•g(xk) ‚àí_ _g(x[‚àó])‚à•2 + ‚à•xk ‚àí_ _x[‚àó]‚à•2_
(1 + Œ∫) _xk_ _x[‚àó]_ 2.
_‚â§_ _‚à•_ _‚àí_ _‚à•_

haveIf the condition in Line 7 is true, i.e., ‚à•Pk‚àí1Œ∂k‚à•2 ‚â§ _cp‚à•‚àÜxk‚àí1‚à•2, ‚à•Qk‚àí1Œ∂k‚à•2 ‚â§_ _cq‚à•‚àÜrk‚àí1‚à•2, we_

_pk_ 2 = ‚àÜxk 1 _Pk_ 1Œ∂k 2 ‚àÜxk 1 2 + _Pk_ 1Œ∂k 2 (1 + cp) ‚àÜxk 1 2,
_‚à•_ _‚à•_ _‚à•_ _‚àí_ _‚àí_ _‚àí_ _‚à•_ _‚â§‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _‚â§_ _‚à•_ _‚àí_ _‚à•_

_qk_ 2 = ‚àÜrk 1 _Qk_ 1Œ∂k 2 ‚àÜrk 1 2 _Qk_ 1Œ∂k 2 (1 _cq)_ ‚àÜrk 1 2.
_‚à•_ _‚à•_ _‚à•_ _‚àí_ _‚àí_ _‚àí_ _‚à•_ _‚â•‚à•_ _‚àí_ _‚à•_ _‚àí‚à•_ _‚àí_ _‚à•_ _‚â•_ _‚àí_ _‚à•_ _‚àí_ _‚à•_

The inequality _qk_ 2 ‚àÜrk 1 is due to the fact that qk is the orthogonal projection of ‚àÜrk 1
onto range(Qk ‚à•1)[‚ä•]‚à• . Also, ‚â§‚à• _qk_ _‚àí= 0‚à•_ must hold otherwise q = Qk 1Œ∂k which violates the condition‚àí
_Qk_ 1Œ∂k 2 _c‚àíq_ ‚àÜrk 1 2 as Ã∏ cq (0, 1). _‚àí_
_‚à•_ _‚àí_ _‚à•_ _‚â§_ _‚à•_ _‚àí_ _‚à•_ _‚àà_

Ifupdate by Line 8 occurred in the qk Ã∏= 0, then qk must be updated by Line 8 in some previous iteration. We assume the latest j-th iteration, i.e., qk = qj, pk = pj, hence

_pk_ 2 1 + cp
_‚à•_ _‚à•_ =
_‚à•qk‚à•2_ _[‚à•]‚à•[p]qj[j]‚à•[‚à•]2[2]_ _‚â§_ [(1 +](1 ‚àí[ c]c[p]q[)])[‚à•]‚à•[‚àÜ]‚àÜ[x]rj[j]‚àí[‚àí]1[1]‚à•[‚à•]2[2] _‚â§_ (1 ‚àí _cq)(1 ‚àí_ _Œ∫)_ _[.]_

where the first inequality is due to (50a) and (50b) and the second inequality is due to (48a).

**Lemma 2. Under the same assumptions of Theorem 2, in the k-th iteration (k ‚â•** 0) of the restarted
_MST-AM (Algorithm 3), we have_


1
_pk + qk =_

0

Z


_k‚àí1_

( ‚àÜrj 2), (51)
_O_ _‚à•_ _‚à•_
_j=Xk‚àímk_


_g[‚Ä≤](xk ‚àí_ _tpk)pkdt + ÀÜŒ∫‚à•‚àÜrœÄ(k)‚àí1‚à•2_


_where mk = k mod m, ‚àÜrk_ _mk_ 1 := rk _mk_ _, œÄ(k) denotes that the latest update of qk by Line 8_
_‚àí_ _‚àí_ _‚àí_
_occurred in the œÄ(k)-th iteration and œÄ(k) = k_ _‚àímk if Line 8 is never executed up to the k-iteration._

_Proof. We prove (51) by induction. Denote Œ∂k = (Œ∂k[(1)][, Œ∂]k[(2)][)][T][.]_

For k = 0, the relation trivially holds.

For k = 1, p1 = ‚àÜx0, q1 = ‚àÜr0. It follows that


1
_p1 + q1 = g(x1) ‚àí_ _g(x0) =_ 0
Z

So (51) holds for k = 1.


_g[‚Ä≤](x1_ _tp1)p1dt._
_‚àí_


Suppose that (51) holds for 0 ‚â§ _j ‚â§_ _k ‚àí_ 1, where k ‚â• 2. For k ‚â• 2, if mk := k mod m = 0 or 1,
(51) holds because it is the same as the case of k = 0 or k = 1. Now consider the nontrivial cases.


-----

For mk = 0 and mk = 1, if the condition in Line 7 in Algorithm 3 is true, then œÄ(k) = k. With the
convention that Ã∏ ‚àÜrk Ã∏ _mk_ 1 = rk _mk_, we have
_‚àí_ _‚àí_ _‚àí_

_pk + qk = ‚àÜxk_ 1 _Pk_ 1Œ∂k + ‚àÜrk 1 _Qk_ 1Œ∂k
_‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_

= g(xk) ‚àí _g(xk‚àí1) ‚àí_ (pk‚àí1 + qk‚àí1)Œ∂k[(2)] _‚àí_ (pk‚àí2 + qk‚àí2)Œ∂k[(1)]

1
= 0 _g[‚Ä≤](xk ‚àí_ _t‚àÜxk‚àí1)‚àÜxk‚àí1dt_
Z

1 _k‚àí2_
_‚àí_ Z0 _g[‚Ä≤](xk‚àí1 ‚àí_ _tpk‚àí1)pk‚àí1Œ∂k[(2)][dt][ + ÀÜ]Œ∫‚à•‚àÜrœÄ(k‚àí1)‚àí1‚à•2_ _j=k‚àíX1‚àímk‚àí1_ _O(‚à•‚àÜrj‚à•2)Œ∂k[(2)]_

1 _k‚àí3_
_‚àí_ Z0 _g[‚Ä≤](xk‚àí2 ‚àí_ _tpk‚àí2)pk‚àí2Œ∂k[(1)][dt][ + ÀÜ]Œ∫‚à•‚àÜrœÖ(k‚àí1)‚àí1‚à•2_ _j=k‚àíX2‚àímk‚àí2_ _O(‚à•‚àÜrj‚à•2)Œ∂k[(1)][,]_

(52)

where œÖ(k 1) = œÄ(œÄ(k 1) 1) denotes that qk 2 records qœÖ(k 1) that is the penultimate update
_‚àí_ _‚àí_ _‚àí_ _‚àí_ _‚àí_
by Line 8 occurring in the œÖ(k ‚àí 1)-th iteration. Considering the terms in pk + qk, we know

_g[‚Ä≤](xk ‚àí_ _t‚àÜxk‚àí1)‚àÜxk‚àí1 ‚àí_ _g[‚Ä≤](xk‚àí1 ‚àí_ _tpk‚àí1)pk‚àí1Œ∂k[(2)]_ _‚àí_ _g[‚Ä≤](xk‚àí2 ‚àí_ _tpk‚àí2)pk‚àí2Œ∂k[(1)]_
= g[‚Ä≤](xk ‚àí _t‚àÜxk‚àí1)(‚àÜxk‚àí1 ‚àí_ _pk‚àí1Œ∂k[(2)]_ _‚àí_ _pk‚àí2Œ∂k[(1)][)]_

+ (g[‚Ä≤](xk ‚àí _t‚àÜxk‚àí1) ‚àí_ _g[‚Ä≤](xk‚àí1 ‚àí_ _tpk‚àí1))pk‚àí1Œ∂k[(2)]_
+ (g[‚Ä≤](xk ‚àí _t‚àÜxk‚àí1) ‚àí_ _g[‚Ä≤](xk‚àí2 ‚àí_ _tpk‚àí2))pk‚àí2Œ∂k[(1)][.]_ (53)

For Œ∂k = (Q[T]k 1[Q][k][‚àí][1][)][‚Ä†][Q][T]k 1[‚àÜ][r][k][‚àí][1][, because][ Q][T]k 1[Q][k][‚àí][1][ = diag][{‚à•][q][k][‚àí][2][‚à•]2[2][,][ ‚à•][q][k][‚àí][1][‚à•][2]2[}][, it follows]
_‚àí_ _‚àí_ _‚àí_
that

_Œ∂k[(1)]_ = I(qk‚àí2 Ã∏= 0) [‚àÜ]q[r]k[T]k‚àí[T]‚àí21[q][q][k][k][‚àí][‚àí][2][2] _, Œ∂k[(2)]_ = I(qk‚àí1 Ã∏= 0) [‚àÜ]q[r]k[T]k‚àí[T]‚àí11[q][q][k][k][‚àí][‚àí][1][1] _._

Therefore,

_|Œ∂k[(1)][| ‚â§I][(][q][k][‚àí][2][ Ã∏][= 0)]_ _[‚à•]‚à•[‚àÜ]q[r]k[k]‚àí[‚àí]2[1]‚à•[‚à•]2[2]_ _, |Œ∂k[(2)][| ‚â§I][(][q][k][‚àí][1][ Ã∏][= 0)]_ _[‚à•]‚à•[‚àÜ]q[r]k[k]‚àí[‚àí]1[1]‚à•[‚à•]2[2]_ _._ (54)


Now, to bound the terms in (53), we have

_‚à•(g[‚Ä≤](xk ‚àí_ _t‚àÜxk‚àí1) ‚àí_ _g[‚Ä≤](xk‚àí1 ‚àí_ _tpk‚àí1))pk‚àí1Œ∂k[(2)][‚à•][2]_

_‚â§_ _Œ∫ÀÜ‚à•xk ‚àí_ _xk‚àí1 ‚àí_ _t(‚àÜxk‚àí1 ‚àí_ _pk‚àí1)‚à•2‚à•pk‚àí1Œ∂k[(2)][‚à•][2]_

_‚â§_ _Œ∫ÀÜ((1 ‚àí_ _t)‚à•‚àÜxk‚àí1‚à•2 + t‚à•pk‚àí1‚à•2)‚à•pk‚àí1‚à•2|Œ∂k[(2)][|]_

_Œ∫ÀÜ((1_ _t)_ ‚àÜxk 1 2 + t _pk_ 1 2) _pk_ 1 2 (qk 1 = 0) _[‚à•][‚àÜ][r][k][‚àí][1][‚à•][2]_
_‚â§_ _‚àí_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _I_ _‚àí_ _Ã∏_ _qk_ 1 2

_‚à•_ _‚àí_ _‚à•_

_k‚àí1_ _k‚àí1_

_‚â§_ _Œ∫ÀÜ‚à•‚àÜrk‚àí1‚à•2_ _O(‚à•‚àÜrj‚à•2) = ÀÜŒ∫‚à•‚àÜrk‚àí1‚à•2_ _O(‚à•‚àÜrj‚à•2),_ (55)

_j=k‚àíX1‚àímk‚àí1_ _j=Xk‚àímk_

where the third inequality is due to (54), and the last inequality is due to (48a), (50a) and (49), where
the constants are absorbed into the big-O notation. Similarly, it follows that

_‚à•(g[‚Ä≤](xk ‚àí_ _t‚àÜxk‚àí1) ‚àí_ _g[‚Ä≤](xk‚àí2 ‚àí_ _tpk‚àí2))pk‚àí2Œ∂k[(1)][‚à•][2]_

_‚â§_ _Œ∫ÀÜ‚à•‚àÜxk‚àí1 + ‚àÜxk‚àí2 ‚àí_ _t‚àÜxk‚àí1 + tpk‚àí2‚à•2‚à•pk‚àí2Œ∂k[(1)][‚à•][2]_

_‚â§_ _Œ∫ÀÜ((1 ‚àí_ _t)‚à•‚àÜxk‚àí1‚à•2 + ‚à•‚àÜxk‚àí2‚à•2 + t‚à•pk‚àí2‚à•2)‚à•pk‚àí2‚à•2|Œ∂k[(1)][|]_

_Œ∫ÀÜ((1_ _t)_ ‚àÜxk 1 2 + ‚àÜxk 2 2 + t _pk_ 2 2) _pk_ 2 2 (qk 2 = 0) _[‚à•][‚àÜ][r][k][‚àí][1][‚à•][2]_
_‚â§_ _‚àí_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _I_ _‚àí_ _Ã∏_ _qk_ 2 2

_‚à•_ _‚àí_ _‚à•_

_k‚àí1_ _k‚àí1_

_‚â§_ _Œ∫ÀÜ‚à•‚àÜrk‚àí1‚à•2_ _O(‚à•‚àÜrj‚à•2) = ÀÜŒ∫‚à•‚àÜrk‚àí1‚à•2_ _O(‚à•‚àÜrj‚à•2)._ (56)

_j=k‚àíX2‚àímk‚àí2_ _j=Xk‚àímk_


-----

For the remaining terms in (52), we have


_k‚àí2_

_O(‚à•‚àÜrj‚à•2)Œ∂k[(2)]_ + ÀÜŒ∫‚à•‚àÜrœÖ(k‚àí1)‚àí1‚à•2
_j=k‚àíX1‚àímk‚àí1_


_k‚àí3_

( ‚àÜrj 2)Œ∂k[(1)]
_O_ _‚à•_ _‚à•_
_k=k‚àíX2‚àímk‚àí2_


_Œ∫ÀÜ‚à•‚àÜrœÄ(k‚àí1)‚àí1‚à•2_


_k‚àí2_

_‚â§_ _Œ∫ÀÜ‚à•‚àÜrœÄ(k‚àí1)‚àí1‚à•2I(qk‚àí1 Ã∏= 0)_ _[‚à•]‚à•[‚àÜ]q[r]k[k]‚àí[‚àí]1[1]‚à•[‚à•]2[2]_ _j=k‚àíX1‚àímk‚àí1_ _O(‚à•‚àÜrj‚à•2)_

_k‚àí3_

+ ÀÜŒ∫‚à•‚àÜrœÖ(k‚àí1)‚àí1‚à•2I(qk‚àí2 Ã∏= 0) _[‚à•]‚à•[‚àÜ]q[r]k[k]‚àí[‚àí]2[1]‚à•[‚à•]2[2]_ _k=k‚àíX2‚àímk‚àí2_ _O(‚à•‚àÜrj‚à•2)_

_k‚àí2_

_‚â§_ _Œ∫ÀÜ‚à•‚àÜrk‚àí1‚à•2_ _O(‚à•‚àÜrj‚à•2),_ (57)

_k=Xk‚àímk_

where the last inequality is due to (50b) and that qk 1 = qœÄ(k 1), qk 2 = qœÖ(k 1).
_‚àí_ _‚àí_ _‚àí_ _‚àí_

With relations (52), (53), (55), (56) and (57), and noting thatand _pk = ‚àÜxk‚àí1 ‚àí_ _pk‚àí1Œ∂k[(2)]_ _‚àí_ _pk‚àí2Œ∂k[(1)][,]_

1

0 (g[‚Ä≤](xk ‚àí _t‚àÜxk‚àí1)‚àÜxk‚àí1 ‚àí_ _g[‚Ä≤](xk‚àí1 ‚àí_ _tpk‚àí1)pk‚àí1Œ∂k[(2)]_ _‚àí_ _g[‚Ä≤](xk‚àí2 ‚àí_ _tpk‚àí2)pk‚àí2Œ∂k[(1)][)][dt]_ 2

Z

1
_‚â§_ 0 _‚à•g[‚Ä≤](xk ‚àí_ _t‚àÜxk‚àí1)‚àÜxk‚àí1 ‚àí_ _g[‚Ä≤](xk‚àí1 ‚àí_ _tpk‚àí1)pk‚àí1Œ∂k[(2)]_ _‚àí_ _g[‚Ä≤](xk‚àí2 ‚àí_ _tpk‚àí2)pk‚àí2Œ∂k[(1)][‚à•][2][dt,]_
Z

we can estimate pk + qk as

1 _k‚àí1_
_pk + qk =_ _g[‚Ä≤](xk_ _t‚àÜxk_ 1)pkdt + ÀÜŒ∫ ‚àÜrk 1 2 ( ‚àÜrj 2). (58)
Z0 _‚àí_ _‚àí_ _‚à•_ _‚àí_ _‚à•_ _j=Xk‚àímk_ _O_ _‚à•_ _‚à•_

To further obtain (51), notice that the difference

1 1

0 _g[‚Ä≤](xk ‚àí_ _tpk)pkdt ‚àí_ 0 _g[‚Ä≤](xk ‚àí_ _t‚àÜxk‚àí1)pkdt_ 2

Z Z

1

= 0 (g[‚Ä≤](xk ‚àí _tpk) ‚àí_ _g[‚Ä≤](xk ‚àí_ _t‚àÜxk‚àí1))pkdt_ 2

Z

1
_‚â§_ 0 _‚à•g[‚Ä≤](xk ‚àí_ _tpk) ‚àí_ _g[‚Ä≤](xk ‚àí_ _t‚àÜxk‚àí1)‚à•2‚à•pk‚à•2dt_
Z

1
_‚â§_ _Œ∫ÀÜ_ 0 _t‚à•pk‚àí1Œ∂k[(2)]_ + pk‚àí2Œ∂k[(1)][‚à•][2][‚à•][p][k][‚à•][2][dt]
Z

_Œ∫_
_‚â§_ [ÀÜ]2 _[c][p][‚à•][‚àÜ][x][k][‚àí][1][‚à•][2][‚à•][p][k][‚à•][2][ = ÀÜ]Œ∫‚à•‚àÜrk‚àí1‚à•2O(‚à•‚àÜrk‚àí1‚à•2),_

where the last inequality is due to the condition _Pk_ 1Œ∂k 2 _cp_ _p_ 2 and inequalities (48a) and
(50a). Hence the difference can be absorbed in the Big- ‚à• _‚àí_ _O‚à• notation in (58). Thus (51) holds when ‚â§_ _‚à•_ _‚à•_
the condition in Line 7 is true for the k-th iteration.

We consider the case where the condition in Line 7 is false for the k-th iteration. Then Pk =
_Pk_ 1, Qk = Qk 1. In other words, the memory recording pk, qk keeps unchanged from the (œÄ(k)+
_‚àí_ _‚àí_
1)-th to k-th iteration. Since œÄ(k) < k and œÄ(œÄ(k)) = œÄ(k), with the inductive hypothesis, we have

_pk + qk = pœÄ(k) + qœÄ(k)_


1

0

Z

1

0

Z


_œÄ(k)‚àí1_

( ‚àÜrj 2)
_O_ _‚à•_ _‚à•_
_j=œÄ(k)_ _mœÄ(k)_

X‚àí


_g[‚Ä≤](xœÄ(k) ‚àí_ _tpœÄ(k))pœÄ(k)dt + ÀÜŒ∫‚à•‚àÜrœÄ(k)‚àí1‚à•2_


_k‚àí1_

( ‚àÜrj 2). (59)
_O_ _‚à•_ _‚à•_
_j=Xk‚àímk_


_g[‚Ä≤](xœÄ(k) ‚àí_ _tpk)pkdt + ÀÜŒ∫‚à•‚àÜrœÄ(k)‚àí1‚à•_


-----

To further obtain (51), note that the difference

1 1

_g[‚Ä≤](xk_ _tpk)pkdt_ _g[‚Ä≤](xœÄ(k)_ _tpk)pkdt_
0 _‚àí_ _‚àí_ 0 _‚àí_ 2

Z Z

1

_g[‚Ä≤](xk_ _tpk)_ _g[‚Ä≤](xœÄ(k)_ _tpk)_ 2 _pk_ 2dt

_‚â§_ 0 _‚à•_ _‚àí_ _‚àí_ _‚àí_ _‚à•_ _‚à•_ _‚à•_
Z

_Œ∫ÀÜ_ _xk_ _xœÄ(k)_ 2 _pk_ 2
_‚â§_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚à•_

_k‚àí1_ _k‚àí1_

= ÀÜŒ∫ ‚àÜxj 2 _pk_ 2 = ÀÜŒ∫ ‚àÜrœÄ(k) 1 2 ( ‚àÜrj 2)

_‚à•_ _‚à•_ _‚à•_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _O_ _‚à•_ _‚à•_
_j=œÄ(k)_ _j=œÄ(k)_

X X


can be absorbed into the Big-O notation in (59). Thus (51) also holds when the condition in Line 7
is false for the k-th iteration.

As a result, we complete the induction in the k-th iteration.

C.3 REGULARIZED SHORT-TERM RECURRENCE ANDERSON MIXING

C.3.1 CHECK OF POSITIVE DEFINITENESS

We describe the check of positive definiteness in RST-AM, which follows the same procedure as
that of SAM (Wei et al., 2021). From Line 11-13 in Algorithm 1, one update of xk RST-AM is
_xk+1 = xk + Hkrk, where Hk = Œ≤kI ‚àí_ _Œ±kYkZk[‚Ä†][Q]k[T][,][ Y][k][ =][ P][k][ +][ Œ≤][k][Q][k][,][ Z][k][ =][ Q]k[T][Q][k][ +][ Œ¥][k][P]k[ T][P][k][.]_
_Hk is generally not symmetric. For the convergence analysis of RST-AM, a critical condition is the_
_positive definiteness of Hk, i.e._

_s[T]k_ _[H][k][s][k]_ 2[,] _sk_ R[d], (60)

_[‚â•]_ _[Œ≤][k][¬µ][‚à•][s][k][‚à•][2]_ _‚àÄ_ _‚àà_

where ¬µ ‚àà (0, 1) is a constant. Next, we show how to guarantee it.

Let Œªmin( ) denote the smallest eigenvalue, and Œªmax( ) denote the largest eigenvalue. Since

_¬∑_ _¬∑_ 1
_s[T]k_ _[H][k][s][k][ =][ 1]2_ _[s]k[T][(][H][k][ +][ H]k[T][)][s][k][, Condition (60) is equivalent to][ Œª][min]_ 2 _Hk + Hk[T]_ _Œ≤k¬µ. By_

1 _‚â•_
some simple algebraic operations, we obtain Œªmin 2 _Hk + Hk[T]_ =  Œ≤k  2 _[Œ±][k][Œª][max][(][Y][k][Z]k[‚Ä†][Q]k[T]_ [+]

_‚àí_ [1]

_QkZk[‚Ä†][Y][ T]k_ [)][. Let][ Œª][k][ :=][ Œª][max][(][Y][k][Z]k[‚Ä†][Q]k[T] [+][ Q][k][Z]k[‚Ä†][Y][ T]k  [)][, then Condition (60) is equivalent to]  

_Œ±kŒªk_ 2Œ≤k(1 _¬µ),_ (61)
_‚â§_ _‚àí_

namely, (13) in Remark 4. To check Condition (61), note that

_Œªk = Œªmax_ (Yk _Qk)_ 0 _Zk[‚Ä†]_ _Yk[T]_ = Œªmax _Yk[T]_ (Yk _Qk)_ 0 _Zk[‚Ä†]_ _._ (62)

_Zk[‚Ä†]_ 0 _Q[T]k_ _Q[T]k_ _Zk[‚Ä†]_ 0
        

Since _Yk[T]_ (Yk _Qk),_ 0 _Zk[‚Ä†]_ R[4][√ó][4], Œªk can be computed cheaply. This cost is negligible
_Q[T]k_ _Zk[‚Ä†]_ 0 _‚àà_
   

compared with those to form Pk[T][P][k][, Q]k[T][Q][k][, which need][ O][(][d][)][ flops. Then, to guarantee the positive]
definiteness of Hk, we check whether Œ±k satisfies (61) and use a smaller Œ±k if necessary, e.g. Œ±k =
2Œ≤k(1 _¬µ)/Œªk._
_‚àí_

C.3.2 PROOFS OF THE THEOREMS

We first give the proof of the boundedness of _Pk_ 1Œ∂k 2 and _Qk_ 1Œ∂k 2.
_‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_
**Lemma 3. For P, Q ‚àà** R[d][√ó][m](d ‚â• _m), Œ¥ > 0, and Z = Q[T]Q + Œ¥P_ [T]P _, we have_

_PZ_ _[‚Ä†]Q[T]_ 2 _Œ¥[‚àí]_ [1]2, (63a)
_‚à•_ _‚à•_ _‚â§_

_QZ_ _[‚Ä†]Q[T]_ 2 1. (63b)
_‚à•_ _‚à•_ _‚â§_

_Proof. We first consider the case that Z is nonsingular, then PZ_ _[‚Ä†]Q[T]_ = PZ _[‚àí][1]Q[T], QZ_ _[‚Ä†]Q[T]_ =
_QZ_ _[‚àí][1]Q[T]. It can be seen that P_ [T]P and Q[T]Q are symmetric positive semidefinite, and Z is SPD as
it is assumed to be nonsingular. Also, we have Œ¥P [T]P ‚™Ø _Z and Q[T]Q ‚™Ø_ _Z, where the notation ‚Äú‚™Ø‚Äù_


-----

denotes the Loewner partial order, i.e., A ‚™Ø _B with A, B ‚àà_ R[m][√ó][m] means that B ‚àí _A is positive_
semidefinite. Hence, we have


_Z_ _[‚àí]_ 2[1] Œ¥P [T]PZ _[‚àí]_ 2[1] ‚™Ø _I, Z_ _[‚àí]_ 2[1] Q[T]QZ _[‚àí]_ 2[1] ‚™Ø _I,_


which implies


_Z_ _[‚àí]_ [1]2 _P_ [T]P _Z_ _[‚àí]_ 2[1] 2 _Œ¥[‚àí][1],_
_‚à•_ _‚à•_ _‚â§_

_Z_ _[‚àí]_ [1]2 _Q[T]Q_ _Z_ _[‚àí]_ 2[1] 2 1.
_‚à•_ [ ] _‚à•_ _‚â§_

Let Œªmax( ) denote the largest eigenvalue, we have

_¬∑_ [ ]

_‚à•QZ_ _[‚àí][1]Q[T]‚à•2 = Œªmax_ _QZ_ _[‚àí][1]Q[T][]_ = Œªmax _Q[T]QZ_ _[‚àí][1][]_ = Œªmax _Z_ _[‚àí]_ 2[1] Q[T]QZ _[‚àí]_ 2[1]
    

_PZ_ _[‚àí][1]Q[T]_ 2 [=][ Œª][max] _PZ_ _[‚àí][1]Q[T]QZ_ _[‚àí][1]P_ [T][]
_‚à•_ _‚à•[2]_

= Œªmax  P [T]PZ _[‚àí][1]Q[T]QZ_ _[‚àí][1][]_

= Œªmax   _Z_ _[‚àí]_ 2[1] _P_ [T]P _Z_ _[‚àí]_ 2[1] _Z_ _[‚àí]_ 2[1] _Q[T]Q_ _Z_ _[‚àí]_ 2[1]

_¬∑_

_Z_ _[‚àí]_ 2[1] _P_ [T]P _Z_ _[‚àí]_ [1]2 _Z_ _[‚àí]_ 2[1] _Q[T]Q_ _Z_ _[‚àí]_ 2[1] 2 
_‚â§‚à•_ [ ] _¬∑_ [ ] _‚à•_

_Z_ _[‚àí]_ 2[1] _P_ [T]P  _Z_ _[‚àí]_ [1]2 2 _Z_ _[‚àí]_ 2[1] _Q[T]Q_ _Z_ _[‚àí]_ 2[1] 2 _Œ¥[‚àí][1]._
_‚â§‚à•_ [ ] _‚à•_ _‚à•_ [ ] _‚à•_ _‚â§_

Therefore, _PZ_ _[‚àí][1]Q[T]_ _Œ¥[‚àí]_ 2[1] and _QZ_ _[‚àí][1]Q[T]_ 2 1. 

[ ] [ ]

_‚à•_ _‚à•‚â§_ _‚à•_ _‚à•_ _‚â§_

For the case that Z is singular, it can be proved that


_‚â§_ 1,


ker(Z) := {x ‚àà R[m]|Zx = 0} = {x ‚àà R[m]|Px = 0 and Qx = 0}. (64)

In fact, if Px = 0 and Qx = 0, it is obvious that Zx = 0. On the other side, if Zx = 0, then
_x[T]Zx = 0, i.e. x[T]Q[T]Qx + Œ¥x[T]P_ [T]Px = 0. Since 0 ‚™Ø _P_ [T]P, 0 ‚™Ø _Q[T]Q, Œ¥ > 0, it follows that_
_x[T]Q[T]Qx = 0 and Œ¥x[T]P_ [T]Px = 0, which further implies Qx = 0 and Px = 0. Hence (64) holds.

Let U1 satisfy U1[T][U][1] [=][ I][ and][ range(][U][1][) = ker(][Z][)][, i.e. the orthonormal basis of][ ker(][Z][)][, and][ U][2]
satisfy U2[T][U][2] [=][ I][ and][ U][ T]2 _[U][1]_ [= 0][, i.e. the orthonormal basis of][ ker(][Z][)][‚ä•][. With the equality (64),]
we know PU1 = 0, QU1 = 0. Define U = (U1, U2) ‚àà R[m][√ó][m], then U [T]U = Im and by direct
computation, we have

0 0
_U_ [T]ZU = _,_
0 _U2[T][ZU][2]_
 

where U2[T][ZU][2] [= (][QU][2][)][T][QU][2] [+][ Œ¥][(][PU][2][)][T][PU][2] [is nonsingular according to the definition of][ U][2][.]
So

0 0
_Z_ _[‚Ä†]_ = U _U_ [T].
0 (U2[T][ZU][2][)][‚àí][1]
 

As a result, we can further compute PZ _[‚Ä†]Q[T]_ and QZ _[‚Ä†]Q[T]_ as

_PZ_ _[‚Ä†]Q[T]_ = (PU2)(U2[T][ZU][2][)][‚àí][1][(][QU][2][)][T][, QZ] _[‚Ä†][Q][T][ = (][QU][2][)(][U][ T]2_ _[ZU][2][)][‚àí][1][(][QU][2][)][T][.]_


Then let _P[ÀÜ] = PU2,_ _Q[ÀÜ] = QU2, and_ _Z[ÀÜ] = Q[ÀÜ][T][ ÀÜ]Q + Œ¥P[ÀÜ][T][ ÀÜ]P_, and noting that _Z[ÀÜ] is nonsingular, we can_
obtain ‚à•PZ _[‚Ä†]Q[T]‚à•2 = ‚à•P[ÀÜ]Z[ÀÜ][‚àí][1][ ÀÜ]Q[T]‚à•2 ‚â§_ _Œ¥[‚àí]_ [1]2, ‚à•QZ _[‚Ä†]Q[T]‚à•2 = ‚à•Q[ÀÜ]Z[ÀÜ][‚àí][1][ ÀÜ]Q[T]‚à•2 ‚â§_ 1.

As a result of Lemma 3, _Pk_ 1Œ∂k 2, _Qk_ 1Œ∂k 2 in Algorithm 1 are bounded by ( ‚àÜrk 1 2), as
_‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _O_ _‚à•_ _‚àí_ _‚à•_
shown in the following corollary.

**Corollary 2.** _Pk_ 1Œ∂k 2, _Qk_ 1Œ∂ 2 in Algorithm 1 are bounded, i.e.
_‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_

_‚à•Pk‚àí1Œ∂k‚à•2 ‚â§_ (Œ¥k[(1)][)][‚àí] 2[1] ‚à•‚àÜrk‚àí1‚à•2, ‚à•Qk‚àí1Œ∂k‚à•2 ‚â§‚à•‚àÜrk‚àí1‚à•2, (65)

_where Œ∂k = (Q[T]k_ 1[Q][k][‚àí][1][ +][ Œ¥]k[(1)][P][k][‚àí][1][P][k][‚àí][1][)][‚Ä†][Q]k[T] 1[‚àÜ][r][k][‚àí][1][.]
_‚àí_ _‚àí_


-----

Now, we turn to the proofs of the theorems about RST-AM in Section 3.4. For brevity, we use Œ¥k to
denote Œ¥k[(2)][, i.e.][ Œ¥][k][ ‚â°] _[Œ¥]k[(2)][. The proofs follow those of SAM (Wei et al., 2021). Nonetheless, since]_
RST-AM uses different historical sequences compared with SAM, we give the detailed proofs for
RST-AM for completeness.

From Assumption 2, for the mini-batch gradient fSk (xk) = _n1k_ _i‚ààSk_ _[f][Œæ]i_ [(][x][k][)][, where][ n][k][ =][ |][S][k][|][,]

the following properties hold:

P

E[‚àáfSk (x)|xk] = ‚àáf (xk), (66a)

E[ _fSk_ (xk) _f_ (xk) 2[|][x][k][]][ ‚â§] _[œÉ][2]_ _._ (66b)
_‚à•‚àá_ _‚àí‚àá_ _‚à•[2]_ _nk_


Consider the update of RST-AM. From Line 11-13 in Algorithm 1, it can be written as xk+1 =
_xk + Hkrk, where rk = ‚àí‚àáfSk_ (xk), and for k ‚â• 0,

_Hk = Œ≤kI ‚àí_ _Œ±k (Pk + Œ≤kQk)_ _Q[T]k_ _[Q][k]_ [+][ Œ¥][k][P][ T]k _[P][k]_ _‚Ä† QTk_ _[.]_ (67)
  

To prove the theorems, the critical points are (i) the positive definiteness of the approximate Hessian
_Hk and (ii) an adequate suppression of the noise from the gradient estimates in the stochastic case._

We first give a lemma related to the projection step.

_we haveLemma 4. Suppose that {xk} is generated by RST-AM. If Œ±k ‚â•_ 0, Œ≤k > 0, then for any vk ‚àà R[d],

_Hkvk_ 2 _Œ≤k[2]_ 1 + 2Œ±k[2] + Œ±k[2][Œ¥]k[‚àí][1] _vk_ 2[.] (68)
_‚à•_ _‚à•[2]_ _[‚â§]_ [2] _[‚àí]_ [2][Œ±][k] _‚à•_ _‚à•[2]_
     

_Proof. The result holds when k = 0 as H0 = Œ≤0I. For k ‚â•_ 1,

_Hkvk = Œ≤kvk ‚àí_ (Œ±kPk + Œ±kŒ≤kQk)Œìk, (69)

where Œìk = (Q[T]k _[Q][k][ +][ Œ¥][k][P]k[ T][P][k][)][‚Ä†][Q]k[T][v][k][ solves]_

min 2 [+][ Œ¥][k][‚à•][P][k][Œì][‚à•]2[2][.] (70)
Œì

_[‚à•][v][k][ ‚àí]_ _[Q][k][Œì][‚à•][2]_

thusBy direct computation, ‚à•vk ‚àí _QkŒìk‚à•2[2]_ [+] _[Œ¥][k][‚à•][P][k][Œì][k][‚à•]2[2]_ [=][ ‚à•][v][k][‚à•]2[2] _[‚àí]_ _[v]k[T][Q][k][(][Q]k[T][Q][k][ +]_ _[Œ¥][k][P][ T]k_ _[P][k][)][‚Ä†][ ¬∑]_ _[Q]k[T][v][k][,]_
_‚à•vk ‚àí_ _QkŒìk‚à•2[2]_ [+][ Œ¥][k][‚à•][P][k][Œì][k][‚à•]2[2] _[‚â§‚à•][v][k][‚à•]2[2][.]_ (71)

Therefore,

_Hkvk_ 2
_‚à•_ _‚à•[2]_
= _Œ≤kvk_ (Œ±kPk + Œ±kŒ≤kQk)Œìk 2
_‚à•_ _‚àí_ _‚à•[2]_
= _Œ≤k (vk_ _Œ±kQkŒìk)_ _Œ±kPkŒìk_ 2
_‚à•_ _‚àí_ _‚àí_ _‚à•[2]_ 1

= ‚à•Œ≤k(1 ‚àí _Œ±k)vk + Œ≤kŒ±k(vk ‚àí_ _QkŒìk) ‚àí_ _Œ±kŒ¥k‚àí_ [1]2 _Œ¥k2_ _[P][k][Œì][k][‚à•]2[2]_

_‚â§_ _Œ≤k[2][(1][ ‚àí]_ _[Œ±][k][)][2][ +][ Œ≤]k[2][Œ±]k[2]_ [+][ Œ±]k[2][Œ¥]k[‚àí][1] _¬∑_ _‚à•vk‚à•2[2]_ [+][ ‚à•][v][k] _[‚àí]_ _[Q][k][Œì][k][‚à•]2[2]_ [+][ Œ¥][k][‚à•][P][k][Œì][k][‚à•]2[2]
 Œ≤k[2] 1 + 2Œ±k[2] + Œ±k[2][Œ¥]k[‚àí][1]   vk 2 [+][ ‚à•][v][k][‚à•]2[2] 

_‚â§_ _[‚àí]_ [2][Œ±][k] _‚à•_ _‚à•[2]_

= 2  _Œ≤k [2]_ 1 + 2Œ±k[2]  + Œ±k[2][Œ¥]k[‚àí]  [1] _vk_ 2[.]  (72)

_[‚àí]_ [2][Œ±][k] _‚à•_ _‚à•[2]_

In the above, the first inequality uses the inequality     


2
!


_n_

**xi** 2

! _i=1_ _‚à•_ _‚à•[2]_
X


_aixi_ 2
_i=1_ _‚à•[2]_ _[‚â§]_

X


_a[2]i_
_i=1_

X


(73)


_ai_ **xi** 2
_|_ _|‚à•_ _‚à•_
_i=1_

X


where ai ‚àà R, xi ‚àà R[d]. The second inequality is based on inequality (71).

With Lemma 4, we can prove the deterministic case of RST-AM.


-----

**_Proof of Theorem 3. Since 1+2Œ±k[2]_** _k_ _C_ _[‚àí][1]Œ≤[2], with Lemma 4,_
we have _[‚àí]_ [2][Œ±][k][ ‚â§] [1][ for][ 0][ ‚â§] _[Œ±][k][ ‚â§]_ [1][, and][ Œ¥][‚àí][1] _‚â§_
_‚à•Hkrk‚à•2[2]_ _[‚â§]_ [2][Œ≤][2][(1 +][ C] _[‚àí][1][)][‚à•][r][k][‚à•]2[2][.]_

Since Œ±k satisfies (61), we have
_rk[T][H][k][r][k]_ 2[.]

_[‚â•]_ _[Œ≤¬µ][‚à•][r][k][‚à•][2]_

Then, under Assumption 1, we have

_f_ (xk+1) ‚â§ _f_ (xk) + ‚àáf (xk)[T](xk+1 ‚àí _xk) +_ _[L]2_ _[‚à•][x][k][+1][ ‚àí]_ _[x][k][‚à•]2[2]_

= f (xk) _rk[T][H][k][r][k]_ [+][ L] 2
_‚àí_ 2 _[‚à•][H][k][r][k][‚à•][2]_

_‚â§_ _f_ (xk) ‚àí _Œ≤¬µ‚à•rk‚à•2[2]_ [+][ LŒ≤][2][(1 +][ C] _[‚àí][1][)][‚à•][r][k][‚à•]2[2]_
= f (xk) _Œ≤_ _¬µ_ _Œ≤L(1 + C_ _[‚àí][1])_ _rk_ 2
_‚àí_ _‚àí_ _‚à•_ _‚à•[2]_

_f_ (xk)   2[,]  (74)
_‚â§_ _‚àí_ 2[1] _[Œ≤¬µ][ ¬∑ ‚à•‚àá][f]_ [(][x][k][)][‚à•][2]


where the last inequality is due to 0 _<_ _Œ≤_ _‚â§_ 2L(1+¬µC[‚àí][1]) [.] Thus, f (xk+1) ‚àí _f_ (xk) _‚â§_

_‚àí_ 2[1] _[Œ≤¬µ][‚à•‚àá][f]_ [(][x][k][)][‚à•]2[2][.]

Summing both sides of this inequality for k ‚àà{0, . . ., N ‚àí 1} and recalling f (x) > f _[low]_ in
Assumption 1 gives


_N_ _‚àí1_

_‚à•‚àáf_ (xk)‚à•2[2][.]
_k=0_

X


_f_ _[low]_ _f_ (x0) _f_ (xN ) _f_ (x0)
_‚àí_ _‚â§_ _‚àí_ _‚â§‚àí_ 2[1] _[Œ≤¬µ]_

Rearranging and dividing further by N yields (15).

The next lemmas and proofs are about the stochastic case.


**Lemma 5. Suppose that Assumption 2 holds for {xk} generated by RST-AM. In addition, if Œ≤k > 0,**
_and Œ±k_ 0 and satisfies (13), then
_‚â•_

ESk [ _Hkrk_ 2[]][ ‚â§] [2] _Œ≤k[2]_ 1 + 2Œ±k[2] + _[Œ±]k[2]_ _f_ (xk) 2 [+][ œÉ][2] _,_ (75a)
_‚à•_ _‚à•[2]_ _[‚àí]_ [2][Œ±][k] _Œ¥k_ _¬∑_ _‚à•‚àá_ _‚à•[2]_ _nk_
   
  


_Œ±k[2][(][Œ¥]k‚àí_ [1]2 + Œ≤k)[2]

_[œÉ][2]_ _,_ (75b)
_Œ≤k¬µ_ _¬∑_ _nk_


_f_ (xk)[T]ESk [Hkrk] 2 [+ 1]
_‚àá_ _‚â§‚àí_ [1]2 _[Œ≤][k][¬µ][‚à•‚àá][f]_ [(][x][k][)][‚à•][2] 2

_where ¬µ > 0 is the constant introduced in Remark 4._

_Proof. (i) From Lemma 4, we have_


ESk [‚à•Hkrk‚à•2[2][]][ ‚â§] [2] _Œ≤k[2]_ 1 + 2Œ±k[2] _[‚àí]_ [2][Œ±][k] + _[Œ±]Œ¥kk[2]_

  

From Assumption 2, we have


ESk [‚à•rk‚à•2[2][]][.] (76)


ESk [‚à•rk‚à•2[2][] =][ E][S]k [[][‚à•][r][k] _[‚àí]_ [E][S]k [[][r][k][]][‚à•][2]2[] +][ ‚à•][E][S]k [[][r][k][]][‚à•][2]2 _[‚â§‚à•‚àá][f]_ [(][x][k][)][‚à•]2[2] [+][ œÉ][2][/n][k][.] (77)

With (76), (77), we obtain (75a).

(ii) The result holds for k = 0 since H0 = Œ≤0I. Consider k ‚â• 1. Define œµk = ‚àáfSk (xk) ‚àí
_‚àáf_ (xk 1) = ‚àírk ‚àí‚àáf (xk). Then Hkrk = Hk (‚àíœµk ‚àí‚àáf (xk)) . Since Œ±k satisfies (13), it has
_Œªmin_ 2 _Hk + Hk[T]_ _Œ≤k¬µ. Thus_

_‚â•_

    

_f_ (xk)[T]Hk _f_ (xk) = [1] _Hk + Hk[T]_ _f_ (xk) _Œ≤k¬µ_ _f_ (xk) 2[,]
_‚àá_ _‚àá_ 2 _[‚àá][f]_ [(][x][k][)][T][  ] _‚àá_ _‚â•_ _‚à•‚àá_ _‚à•[2]_



which implies
ESk [‚àáf (xk)[T]Hk‚àáf (xk)] ‚â• _Œ≤k¬µ‚à•‚àáf_ (xk)‚à•2[2][.] (78)


-----

Let Mk = Œ±k (Pk + Œ≤kQk) _Q[T]k_ _[Q][k][ +][ Œ¥][k][P]k[ T][P][k]_ _‚Ä† QTk_ [, then][ H][k][ =][ Œ≤][k][I][ ‚àí] _[M][k][.][ With the assumption]_
(66a), i.e. ESk [œµk] = 0, we have
  

ESk [‚àáf (xk)[T]Hkœµk] = ESk [‚àáf (xk)[T] (Œ≤kœµk ‚àí _Mkœµk)]_

= Œ≤k‚àáf (xk)[T]ESk [œµk] ‚àí ESk [‚àáf (xk)[T]Mkœµk] = ‚àíESk [‚àáf (xk)[T]Mkœµk].
Using the Cauchy-Schwarz inequality with expectations, we obtain


_|ESk_ [‚àáf (xk)[T]Hkœµk]| = |ESk [‚àáf (xk)[T]Mkœµk]| ‚â§


ESk [‚à•‚àáf (xk)‚à•2[2][]]


ESk [‚à•Mkœµk‚à•2[2][]]


_‚â§‚à•‚àáf_ (xk)‚à•2 ESk [‚à•Mkœµk‚à•2[2][]][.] (79)
q

We now bound ‚à•Mkœµk‚à•2[2][. For brevity, let][ Z][k] [=][ Q][T]k _[Q][k][ +][ Œ¥][k][P]k[ T][P][k][, and][ N][1][ =][ P][k][Z]k[‚Ä†][Q]k[T][, N][2][ =]_
_Œ≤kQkZk[‚Ä†][Q]k[T][, then]_

_Mk_ 2 = _Œ±k (N1 + N2)_ 2 _Œ±k (_ _N1_ 2 + _N2_ 2) _Œ±k(Œ¥k‚àí_ [1]2 + Œ≤k), (80)
_‚à•_ _‚à•_ _‚à•_ _‚à•_ _‚â§_ _‚à•_ _‚à•_ _‚à•_ _‚à•_ _‚â§_

where the last inequality is from Lemma 3.


With (80), we have _Mkœµk_ 2 _Œ±k(Œ¥k‚àí_ [1]2 + Œ≤k) _œµk_ 2, which implies
_‚à•_ _‚à•_ _‚â§_ _‚à•_ _‚à•_

ESk [ _Mkœµk_ 2[]][ ‚â§] _[Œ±]k[2][(][Œ¥]k‚àí_ [1]2 + Œ≤k)[2]ESk [ _œµk_ 2[]][ ‚â§] _[Œ±]k[2][(][Œ¥]k‚àí_ 2[1]
_‚à•_ _‚à•[2]_ _‚à•_ _‚à•[2]_


+ Œ≤k)[2][ œÉ][2] _,_ (81)

_nk_


where the last inequality is due to (66b). Now we bound |ESk [‚àáf (xk)[T]Hkœµk]| as follows (cf. (79)):

_|ESk_ [‚àáf (xk)[T]Hkœµk]|


ESk [‚à•Mkœµk‚à•2[2][]]


_f_ (xk) 2
_‚â§‚à•‚àá_ _‚à•_


_Œ±k(Œ¥k‚àí_ 2[1]
_‚â§_

_Œ±k(Œ¥k‚àí_ [1]2
_‚â§_


+ Œ≤k)‚à•‚àáf (xk)‚à•2 ESk [‚à•œµk‚à•2[2][]]
q

+ Œ≤k) _[œÉ]_ _f_ (xk) 2
_‚àönk_ _‚à•‚àá_ _‚à•_


_Œ≤k¬µ_ _f_ (xk) 2 _[Œ±][k][(][Œ¥]k‚àí_ 2[1] + Œ≤k) _œÉ_
_‚à•‚àá_ _‚à•_ _¬∑_ _‚àöŒ≤k¬µ_ _‚àönk_


_Œ±k[2][(][Œ¥]k‚àí_ 2[1] + Œ≤k)[2]

_[œÉ][2]_ _._ (82)
_Œ≤k¬µ_ _¬∑_ _nk_


2 [+ 1]

_‚â§_ 2[1] _[Œ≤][k][¬µ][‚à•‚àá][f]_ [(][x][k][)][‚à•][2] 2


With the inequality (78) and (82), we obtain
_‚àáf_ (xk)[T]ESk [Hkrk]

= ‚àí‚àáf (xk)[T]ESk [Hk (œµk + ‚àáf (xk))]

= ‚àíESk [‚àáf (xk)[T]Hk‚àáf (xk)] ‚àí ESk [‚àáf (xk)[T]Hkœµk]

_‚â§‚àíESk_ [‚àáf (xk)[T]Hk‚àáf (xk)] + |ESk [‚àáf (xk)[T]Hkœµk]|

_Œ±k[2][(][Œ¥]k‚àí_ 2[1] + Œ≤k)[2]

_Œ≤k¬µ_ _f_ (xk) 2 [+ 1] 2 [+ 1] _[œÉ][2]_
_‚â§‚àí_ _‚à•‚àá_ _‚à•[2]_ 2 _[Œ≤][k][¬µ][‚à•‚àá][f]_ [(][x][k][)][‚à•][2] 2 _Œ≤k¬µ_ _¬∑_ _nk_


= 2 [+ 1] _Œ±k[2][(][Œ¥]k‚àí_ 2[1] + Œ≤k)[2] _[œÉ][2]_ _._ (83)
_‚àí_ [1]2 _[Œ≤][k][¬µ][‚à•‚àá][f]_ [(][x][k][)][‚à•][2] 2 _Œ≤k¬µ_ _¬∑_ _nk_


By imposing one more restriction on Œ±k, we can obtain a convenient corollary:
**Corollary 3. Suppose that Assumption 2 holds for** _xk_ _generated by RST-AM. C > 0 is the_

1 _{_ _}_

_constant in (12). If Œ≤k > 0, 0_ _Œ±k_ min 1, Œ≤k2
_‚â§_ _‚â§_ _{_ _[}][ and satisfies (61), then]_

ESk [ _Hkrk_ 2[]][ ‚â§] [2][Œ≤]k[2] 1 + C _[‚àí][1][]_ _f_ (xk) 2 [+][ œÉ][2] _,_ (84a)
_‚à•_ _‚à•[2]_ _¬∑_ _‚à•‚àá_ _‚à•[2]_ _nk_
 
 

_f_ (xk)[T]ESk [Hkrk] 2 [+][ Œ≤]k[2] 1 + C _[‚àí][1][][ œÉ][2]_ _._ (84b)
_‚àá_ _‚â§‚àí_ [1]2 _[Œ≤][k][¬µ][‚à•‚àá][f]_ [(][x][k][)][‚à•][2] _[¬∑][ ¬µ][‚àí][1][  ]_ _nk_


-----

_Proof. The first result (84a) is clear by considering (75a) and noticing that 1+2Œ±k[2]_

1

haveŒ±k ‚àà [0, 1] and Œ¥k[‚àí][1] _‚â§_ _C_ _[‚àí][1]Œ≤k[2][.][ Since][ Œ±][k][ ‚â§]_ _[Œ≤]k2_ _[, Œ¥][k][ ‚â•]_ _[CŒ≤]k[‚àí][2]_ and (1 + C _[‚àí]_ 2[1] )[2] _‚â§[‚àí]2(1 +[2][Œ±][k][ ‚â§] C_ _[‚àí][1][ when][1]) we_


_Œ±k[2][(][Œ¥]k‚àí_ [1]2 + Œ≤k)[2]

_[œÉ][2]_
_Œ≤k¬µ_ _¬∑_ _nk_ _‚â§_ 2[1]


_Œ≤k(C_ _[‚àí]_ 2[1] Œ≤k + Œ≤k)[2] _[œÉ][2]_

_Œ≤k¬µ_ _¬∑_ _nk_


= [1]2 _[¬µ][‚àí][1][(][C]_ _[‚àí]_ 2[1] + 1)[2]Œ≤k[2] _[¬∑][ œÉ]nk[2]_

_Œ≤k[2][¬µ][‚àí][1][(1 +][ C]_ _[‚àí][1][)]_ _[œÉ][2]_ _._
_‚â§_ _nk_

Substituting it into (75b), we obtain (84b).

With Corollary 3, we establish the descent property of RST-AM:


**Lemma 6. Suppose that Assumptions 1 and 2 hold for** _xk_ _generated by RST-AM. C > 0 is the_

_constant in (12). If 0 < Œ≤k ‚â§_ 4L(1+¬µC[‚àí][1]) _[,][ 0][ ‚â§]_ _[Œ±][k][ ‚â§]_ [min] {[{][1][, Œ≤]}k12 _[}][ and satisfies (61), then]_

_œÉ[2]_

ESk [f (xk+1)] _f_ (xk) 2 [+][ Œ≤]k[2] (L + ¬µ[‚àí][1])(1 + C _[‚àí][1])_ _._ (85)
_‚â§_ _‚àí_ 4[1] _[Œ≤][k][¬µ][‚à•‚àá][f]_ [(][x][k][)][‚à•][2] _nk_

  

_Proof. According to Assumption 1, we have_

_f_ (xk+1) ‚â§ _f_ (xk) + ‚àáf (xk)[T](xk+1 ‚àí _xk) +_ _[L]2_ _[‚à•][x][k][+1][ ‚àí]_ _[x][k][‚à•]2[2]_

= f (xk) + _f_ (xk)[T]Hkrk + _[L]_ 2[.] (86)
_‚àá_ 2 _[‚à•][H][k][r][k][‚à•][2]_

Taking expectation with respect to the mini-batch Sk on both sides of (86) and using Corollary 3 we
obtain

ESk [f (xk+1)]


_‚â§_ _f_ (xk) + ‚àáf (xk)[T]ESk [Hkrk] + _[L]2_ [E][S][k] _[‚à•][H][k][r][k][‚à•]2[2]_

_f_ (xk) 2 [+][ Œ≤]k[2][¬µ][‚àí][1][(1 +][ C] _[‚àí][1][)]_ _[œÉ][2]_ + LŒ≤k[2][(1 +][ C] _[‚àí][1][)]_ _f_ (xk) 2 [+][ œÉ][2]
_‚â§_ _‚àí_ [1]2 _[Œ≤][k][¬µ][‚à•‚àá][f]_ [(][x][k][)][‚à•][2] _nk_ _‚à•‚àá_ _‚à•[2]_ _nk_

 

1

= f (xk) _Œ≤k_ _f_ (xk) 2 [+][ Œ≤]k[2][(][L][ +][ ¬µ][‚àí][1][)(1 +][ C] _[‚àí][1][)]_ _[œÉ][2]_ _._ (87)
_‚àí_ 2 _[¬µ][ ‚àí]_ _[Œ≤][k][L][(1 +][ C]_ _[‚àí][1][)]_ _‚à•‚àá_ _‚à•[2]_ _nk_

 

Then (87) combined with the assumption Œ≤k ‚â§ 4L(1+¬µC[‚àí][1]) [implies (85).]

Lemma 6 suggests that the term related to the noise from gradient estimates is bounded as a secondorder term (i.e. O(Œ≤k[2][)][). Thus, with the diminishing stepsize, the effect of noise also diminishes.]
To establish the global convergence, we introduce the definition of a supermartingale following the
proofs in (Wang et al., 2017; Wei et al., 2021).

**Definition 1. Let {Fk} be an increasing sequence of œÉ-algebras. If {Xk} is a stochastic process**
_is called a supermartingale.satisfying (i) E[|Xk|] < ‚àû, (ii) Xk ‚ààFk for all k, and (iii) E[Xk+1|Fk] ‚â§_ _Xk for all k, then {Xk}_

**Proposition 3 (Supermartingale convergence theorem, see, e.g., Theorem 4.2.12 in (Durrett, 2019)).**
_If_ _Xk_ _is a nonnegative supermartingale, then limk_ _Xk_ _X almost surely and E[X]_
E {[X0].} _‚Üí‚àû_ _‚Üí_ _‚â§_

Now, we prove the convergence theory of RST-AM in the nonconvex stochastic case.

**_Proof of Theorem 4. (i) Define œÜk :=_** _Œ≤k4¬µ_ _[‚à•‚àá][f]_ [(][x][k][)][‚à•]2[2] [and][ Àú]L := (L + ¬µ[‚àí][1])(1 + C _[‚àí][1]), Œ≥k :=_

_f_ (xk)+ L[Àú] _[œÉ]n[2]_ _‚àûi=k_ _[Œ≤]i[2][. Let][ F][k][ be the][ œÉ][-algebra measuring][ œÜ][k][, Œ≥][k][,][ and][ x][k][. From (85) we know that]_

P


-----

for any k,


E[Œ≥k+1 _k] = E[f_ (xk+1) _k] + L[Àú]_ _[œÉ][2]_
_|F_ _|F_ _n_


_Œ≤i[2]_
_i=k+1_

X


_‚àû_

_f_ (xk) 2 [+ Àú]L _[œÉ][2]_ _Œ≤i[2]_ [=][ Œ≥][k] (88)
_‚â§_ _‚àí_ 4[1] _[Œ≤][k][¬µ][‚à•‚àá][f]_ [(][x][k][)][‚à•][2] _n_ _i=k_ _[‚àí]_ _[œÜ][k][,]_

X

Proposition 3 indicates that there exists afsome constantwhich implies that[low]] ‚â§ _Œ≥0 ‚àí_ _f M[low]f > E<[ + 0Œ≥k. According to Definition 1,‚àû+1. As the diminishing condition (14) holds, we obtain ‚àí_ _f_ _[low]|Fk] ‚â§_ _Œ≥Œ≥k such that ‚àí_ _f_ _[low] {Œ≥‚àík lim ‚àíœÜk. Sincefk_ _[low]}Œ≥ is a supermartingale. Therefore,k œÜ =k ‚â• Œ≥ with probability 1, and0, we have E[f_ (xk 0)] ‚â§ ‚â§ EM[Œ≥f fork ‚àí
_‚Üí‚àû_
E[Œ≥] ‚â§ E[Œ≥0]. Note that from (88) we have E[œÜk] ‚â§ E[Œ≥k] ‚àí E[Œ≥k+1]. Thus,

_‚àû_ _‚àû_

E "k=0 _œÜk#_ _‚â§_ _k=0(E[Œ≥k] ‚àí_ E[Œ≥k+1]) < +‚àû,

X X


which further yields that
_‚àû_ _‚àû_

_œÜk =_ _[¬µ]_ _Œ≤k_ _f_ (xk) 2 _[<][ +][‚àû]_ [with probability][ 1][.] (89)

4 _‚à•‚àá_ _‚à•[2]_

_k=0_ _k=0_

X X

Since _k=0_ _[Œ≤][k][ = +][‚àû][, it follows that (16) holds.]_

(ii) If the noisy gradient is bounded, i.e.,

[P][‚àû]

EŒæk [‚à•‚àáfŒæk (xk)‚à•2[2][]][ ‚â§] _[M][g][,]_ (90)
where Mg > 0 is a constant, then a stronger result can be obtained.

_œµFor any give. Then if (17) does not hold, there must exist two infinite sequences of indices œµ > 0, according to (16), there exist infinitely many iterates xk such that ‚à•‚àási_ _,f_ (xtik) with‚à•2 ‚â§
_{_ _}_ _{_ _}_
_ti > si, such that for i = 0, 1, . . ., k = si + 1, . . ., ti ‚àí_ 1,
_‚à•‚àáf_ (xsi )‚à•2 ‚â• 2œµ, ‚à•‚àáf (xti )‚à•2 < œµ, ‚à•‚àáf (xk)‚à•2 ‚â• _œµ._ (91)
Then from (89) it follows that


+‚àû

_i=0_

X


_ti‚àí1_ +‚àû

_Œ≤k_ _f_ (xk) 2
_k=si_ _‚à•‚àá_ _‚à•[2]_ _[‚â•]_ _[œµ][2]_ _i=0_

X X


_ti‚àí1_

_Œ≤k with probability 1,_
_k=si_

X


_Œ≤k_ _f_ (xk) 2
_k=0_ _‚à•‚àá_ _‚à•[2]_ _[‚â•]_

X


+‚àû _>_


which implies that
_ti‚àí1_

_Œ≤k_ 0 with probability 1, as i + _._ (92)
_k=si_ _‚Üí_ _‚Üí_ _‚àû_

X

According to (77) and (72), we have
E[‚à•xk+1 ‚àí _xk‚à•2|xk]_
= E[‚à•Hkrk‚à•2|xk]

_‚â§_ 2 _Œ≤k[2]_ [(1 + 2][Œ±]k[2] _[‚àí]_ [2][Œ±][k][) +][ Œ±]k[2][Œ¥]k[‚àí][1] E[‚à•rk‚à•2|xk]
q

_‚â§_ _Œ≤k_  2(1 + C _[‚àí][1])E[‚à•rk‚à•2|xk]_ 1 

_‚â§_ _Œ≤kp2(1 + C_ _[‚àí][1])(E[1‚à•rk‚à•2[2][|][x][k][])]_ 2

2

_‚â§_ _Œ≤kp2(1 + C_ _[‚àí][1])Mg_ _[,]_ (93)

where the last inequalities are due to Cauchy-Schwarz inequality and (90). Then it follows from (93)

p

that


_ti‚àí1_

_Œ≤k,_
_k=si_

X


E[‚à•xti ‚àí _xsi_ _‚à•2] ‚â§_


2(1 + C _[‚àí][1])M_


which together with (92) implies that _xti_ _xsi_ 2 0 with probability 1, as i + . Hence,
from the Lipschitz continuity of _f_, it follows that ‚à• _‚àí_ _‚à•_ _‚Üí_ _f_ (xti ) _f_ (xsi ) 2 0 ‚Üí with probability‚àû
1 as i ‚Üí +‚àû. However, this contradicts (91). Therefore, the assumption that (17) does not hold is ‚àá _‚à•‚àá_ _‚àí‚àá_ _‚à•_ _‚Üí_
not true.


-----

**_Proof of Theorem 5. According to (87) in Lemma 6, we have_**


1

2 _[¬µ][ ‚àí]_ _[Œ≤][k][L][(1 +][ C]_ _[‚àí][1][)]_ E‚à•‚àáf (xk)‚à•2[2]

 


_N_ _‚àí1_

_Œ≤k_

_k=0_

X


_N_ _‚àí1_

_Œ≤k[2][(][L][ +][ ¬µ][‚àí][1][)(1 +][ C]_ _[‚àí][1][)]_ _[œÉ][2]_ _,_ (94)

_nk_

_k=0_

X


_f_ (x0) _f_ _[low]_ +
_‚â§_ _‚àí_


where the expectation is taken with respect to {Sj}j[N]=0[‚àí][1][. Define]

1
def _Œ≤k_ 2 _[¬µ][ ‚àí]_ _[Œ≤][k][L][(1 +][ C]_ _[‚àí][1][)]_
_PR(k)_ = Prob _R = k_ = _N_ 1 1 _, k = 0, . . ., N_ 1, (95)
_{_ _}_ _‚àí_ _‚àí_

_j=0_  [Œ≤][j] 2 _[¬µ][ ‚àí]_ _[Œ≤][j][L][(1 +][ C]_ _[‚àí]_ [1][)]

then P   

_kN=0‚àí1_ _[Œ≤][k]_ 12 _[¬µ][ ‚àí]_ _[Œ≤][k][L][(1 +][ C]_ _[‚àí][1][)]_ E _f_ (xk) 2
E _f_ (xR) 2 = _N_ 1 1 _‚à•‚àá_ _‚à•[2]_
_‚à•‚àá_ _‚à•[2]_ _‚àí_
P  j=0 _[Œ≤][j]_ 2 _[¬µ][ ‚àí]_ _[Œ≤][j][L][(1 +]_ _[ C][‚àí][1][)]_ 
 

P   _k=0_ _[Œ≤]k[2][/n][k]_

_N_ 1 1 _._ (96)
_‚â§_ _[D][f][ +][ œÉ][2][(][L]‚àí[ +][ ¬µ][‚àí][1][)(1 +][ C]_ _[‚àí][1][)][ P][N]_ _[‚àí][1]_
_j=0_ _[Œ≤][j]_ 2 _[¬µ][ ‚àí]_ _[Œ≤][j][L][(1 +][ C]_ _[‚àí][1][)]_

_¬µ_ _DÀú_
Let _D[Àú] be a problem-independent constant. If we chooseP_   _Œ≤k = Œ≤ := min_ 4L(1+C[‚àí][1]) _[,]_ _œÉ‚àöN_

_nk = n, then the definition of PR simplifies to PR(k) = 1/N_ . From (96) we have{ _[}][,][ and]_

E[ _f_ (xR) 2[]][ ‚â§] _[D][f][ +][ œÉ][2][(][L]N[ +]1[ ¬µ][‚àí][1][)(1 +][ C]_ _[‚àí][1][)][ NŒ≤]n_ [2]
_‚à•‚àá_ _‚à•[2]_ _‚àí_

_j=0_ _[Œ≤][(][ 1]2_ _[¬µ][ ‚àí]_ _[¬µ]4_ [)]

= _[D][f][ +][ œÉ][2]P[(][L][ +][ ¬µ][‚àí][1][)(1 +][ C]_ _[‚àí][1][)][ ¬∑][ NŒ≤]n_ [2]

_NŒ≤ ¬∑_ [1]4 _[¬µ]_

= NŒ≤¬µ[4][D][f] [+][ œÉ][2][(][L][ +][ ¬µ][‚àí][1]1[)(1 +][ C] _[‚àí][1][)][ ¬∑][ Œ≤]_

4 _[n¬µ]_


4L(1 + C _[‚àí][1])_


+ [4][œÉ][2][(][L][ +][ ¬µ][‚àí][1][)(1 +][ C] _[‚àí][1][)]_

_n¬µ_

)

+ [4][œÉ][(][L][ +][ ¬µ][‚àí][1][)(1 +][ C] _[‚àí][1][) Àú]D_

_n¬µ‚àöN_


_‚â§_ [4]N¬µ[D][f] [max]


_, [œÉ]_


4L(1 + C _[‚àí][1])_


_‚â§_ [4]N¬µ[D][f]


+ _[œÉ]_


= [16][D][f] _[L][(1 +][ C]_ _[‚àí][1][)]_

_N¬µ[2]_


+ [4(][L][ +][ ¬µ][‚àí][1][)(1 +][ C] _[‚àí][1][) Àú]D_


4Df


Therefore, to ensure E[‚à•‚àáf (xR)‚à•2[2][]][ ‚â§] _[œµ][, the number of iterations is][ O][(1][/œµ][2][)][.]_

D EXPERIMENTAL DETAILS

Our main codes were written based on the PyTorch framework [1] and one GeForce RTX 2080 Ti
GPU was used for the tests in training neural networks. Our methods are ST-AM (MST-AM) and
the regularized version RST-AM.

D.1 EXPERIMENTAL DETAILS ABOUT ST-AM/MST-AM

The experiments about ST-AM were conducted to verify the main theorems, i.e. Theorem 1, Corollary 1 and Theorem 2. Four types of problems were used for the experiments. The conjugate residual
(CR) method is a short-term recurrence version of the full-memory GMRES and needs matrix-vector
products to fulfill the algorithm. We give the pseudocode of the CR method (Algorithm 6.20 in
(Saad, 2003)) here for readers who are not familiar with this numerical algorithm.

[1 Information about this framework can be found in https://pytorch.org.](https://pytorch.org)


-----

**Algorithm 4 CR Algorithm for solving a SPD linear system Ax = b.**

**InputOutput: x: x0 ‚àà ‚ààRR[d][d].**

1: r0 = b ‚àí _Ax0, p0 = r0_
2: for k = 0, 1, . . ., until convergence, do

3: _Œ±k =_ (Aprk[T]k)[Ar][T]Ap[k] _k_

4: _xk+1 = xk + Œ±kpk_

6:5: _Œ≤rkk+1 = =rk[T] r+1rk[T]k ‚àí[Ar][Ar][k][k]Œ±[+1]kApk_

7: _pk+1 = rk+1 + Œ≤kpk_

8: _Apk+1 = Ark+1 + Œ≤kApk_

9: end for

10: return xk


According to Theorem 1, the CR method is essentially equivalent to ST-AM for solving strongly
convex quadratic optimization. However, it should be pointed out that the CR method needs to
directly assess the matrix A, and the residual update (Line 5) is based on the linear assumption,
which makes it inapplicable for nonlinear problems or the case that A is unavailable. Also, even
though finite difference technique can be used to construct the matrix-vector products, the number
of gradient evaluations is twice of that of ST-AM.

D.1.1 STRONGLY CONVEX QUADRATIC OPTIMIZATION

To construct a case of (5), we considered the least squares problem:

min 2[,] (97)
_x_ R[d][ f] [(][x][) := 1]2 _[‚à•][Ax][ ‚àí]_ _[b][‚à•][2]_
_‚àà_


where A ‚àà R[‚Ñì][√ó][d], b ‚àà R[‚Ñì], which can be reformulated as a form of (5).

We generated a dense random matrix A R[500][√ó][100] and a dense random vector b R[500] for the
_‚àà_ 1 _‚àà_
test. The gradient descent used a fixed stepsize Œ∑ ‚â§ _‚à•A‚à•2[2]_ [that can guarantee convergence, and the]

same Œ∑ was used as the Œ≤k for the full-memory AM and ST-AM.

We also conducted additional tests about solving the problem (97) with different condition numbers,
where the eigenvalues of A were set to be uniformly distributed. The results with different condition
numbers (cond(A[T]A) = 10[2], 10[4], 10[6]) are shown in Figure 4. It can be observed that the curves
of CR, AM and ST-AM nearly coincide, which verify the correctness of Theorem 1. Also, since
the eigenvalues are uniformly distributed, the superlinear convergence behaviour may not happen.
Nonetheless, CR, AM and ST-AM are still much faster then the GD method.


10 1

10 2

10 3

10 4 GD: ||rk||2/||r0||2

10 5 CR: ||AM: ||rrkk||||22/||/||rr00||||22

10 6 ST-AM: ||rk||2/||r0||2

0 10 20 30 40 50

iteration


GD: ||rk||2/||r0||2

10 1 CR: ||AM: ||rrkk||||22/||/||rr00||||22

ST-AM: ||rk||2/||r0||2

10 2

10 3

10 4

0 10 20 30 40 50

iteration


GD: ||rk||2/||r0||2

10 1 CR: ||AM: ||rrkk||||22/||/||rr00||||22

ST-AM: ||rk||2/||r0||2

10 2

10 3

10 4

0 10 20 30 40 50

iteration


(a) cond(A[T]A) = 1 √ó 10[2]


(b) cond(A[T]A) = 1 √ó 10[4]


(c) cond(A[T]A) = 1 √ó 10[6]


Figure 4: Solving (97) with different condition numbers: cond(A[T]A) = Œªmax(A[T]A)/Œªmin(A[T]A).


-----

D.1.2 SOLVING A NONSYMMETRIC LINEAR SYSTEM

For the solution of a nonsymmetric linear system


_Ax = b,_ (98)

where A ‚àà R[d][√ó][d], b ‚àà R[d], the fixed-point iteration (FP) is xk+1 = g(xk) := xk + Œ∑(b ‚àí _Axk)._
Theorem 2 requires g to be a contractive map, i.e. ‚à•I ‚àí _Œ∑A‚à•2 < 1. We used a test matrix ‚Äúfidap029‚Äù_
from the Matrix Market [2]. This matrix is a banded matrix and not symmetric. We used Jacobi
preconditioner for all the tested iterative methods. Since solving linear systems is not the main
focus of this work, we did not do a thorough test of applying ST-AM to solve various nonsymmetric
linear systems.


D.1.3 CUBIC-REGULARIZED QUADRATIC MINIMIZATION

The concerned cubic-regularized quadratic minimization is


min 2 [+][ M] 2[,] (99)
_x_ R[d][ f] [(][x][) := 1]2 _[‚à•][Ax][ ‚àí]_ _[b][‚à•][2]_ 3
_‚àà_ _[‚à•][x][‚à•][3]_

where A ‚àà R[‚Ñì][√ó][d], b ‚àà R[‚Ñì], and M ‚â• 0 is the regularization parameter. It can be computed that


_f_ (x) = A[T](Ax _b) + M_ _x_ 2x = (A[T]A + M _x_ 2I)x _A[T]b._
_‚àá_ _‚àí_ _‚à•_ _‚à•_ _‚à•_ _‚à•_ _‚àí_

Hence, for the gradient descent xk+1 = g(xk) := xk ‚àí _Œ∑‚àáf_ (xk), the Jacobian of g is I ‚àí _Œ∑A[T]A ‚àí_
_Œ∑M_ (‚à•x‚à•2[‚àí][1][xx][T][ +][ ‚à•][x][‚à•][2][I][)][, which has][ ÀÜ]Œ∫ > 0 in the local region B(œÅ).

We generated a dense random matrix A ‚àà R[500][√ó][100] and a dense vector b ‚àà R[500] for the test. The
official implementation of L-BFGS in PyTorch was used for comparison. The historical length m of
L-BFGS was 50, i.e., BFGS was actually used in the test.


10[0]


10[0]


10 1


10 1


|03 06 09 12 15|AM: M= AM: M= MST-AM MST-AM|1 0.01 : M=100 : M=1|
|---|---|---|
||MST-AM|: M=0.01|
||||

|03 06 09 12 15|AM: M AM: M ST-AM ST-AM|=1 =0.01 : M=100 : M=1|
|---|---|---|
||ST-AM|: M=0.01|
||||


10 20 30 40 50

AM: M=100
AM: M=1
AM: M=0.01
MST-AM: M=100
MST-AM: M=1
MST-AM: M=0.01

iteration

(a) MST-AM


10 20 30 40 50

AM: M=100
AM: M=1
AM: M=0.01
ST-AM: M=100
ST-AM: M=1
ST-AM: M=0.01

iteration

(b) ST-AM

|00 P : M = P : M = 1 P : M = Q : M Q : M Q : M 0 10 20 3 iteratio|Col2|
|---|---|
||100 1 0.01|
||= 100 = 1|
||= 0.01 0 40 50 n|

|00 P : M P : M 1 P Q : : M M Q : M Q : M 0 10 20 iterati|Col2|
|---|---|
||= 100 = 1 = 0.01|
||= 100 = 1|
||= 0.01 30 40 50 on|


(c) MST-AM


(d) ST-AM


Figure 5: Solving (99) with different M . (a) ‚à•rk‚à•2/‚à•r0‚à•2 of MST-AM; (b) ‚à•rk‚à•2/‚à•r0‚à•2 of ST-AM;
(c) _Pk_ 1Œ∂k 2/ ‚àÜxk 1 2 and _Qk_ 1Œ∂k 2/ ‚àÜrk 1 2 of MST-AM; (d) _Pk_ 1Œ∂k 2/ ‚àÜxk 1 2
_‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_
and _Qk_ 1Œ∂k 2/ ‚àÜrk 1 2 of ST-AM.
_‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_

We also conducted the tests related to different regularization parameters M, and the cases that
_M = 0.01, 1, 100 are shown in Figure 5. Figure 5 shows that with the large M = 100, both AM and_
MST-AM converge faster. An ablation study was also conducted about the boundedness restriction
of _Pk_ 1Œ∂k 2, _Qk_ 1Œ∂k 2 in Theorem 2. In Figure 5(b), we show the convergence behaviour of
_‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_
ST-AM without the boundedness check. In the case of the rather large regularization M = 100, STAM does not show a monotone decrease of the residual. To further investigate the cause, we plot the
magnitude of _Pk_ 1Œ∂k 2/ ‚àÜxk 1 2, _Qk_ 1Œ∂k 2/ ‚àÜrk 1 2 in Figure5(c) and Figure 5(d). It can
_‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_
be observed that the evolutions of _Pk_ 1Œ∂k 2/ ‚àÜxk 1 2, _Qk_ 1Œ∂k 2/ ‚àÜrk 1 2 are quite oscilla_‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•_
tory in ST-AM, while being roughly bounded below 1 in MST-AM. This phenomenon may accounts
for the more stable convergence behaviour of MST-AM and verifies the necessity of the changes of
MST-AM compared with ST-AM. In our experiments, we also found the restarting period can be set
quite large and has little effect on ST-AM.

[2 https://math.nist.gov/MatrixMarket/.](https://math.nist.gov/MatrixMarket/)


-----

D.1.4 ROOT-FINDING PROBLEMS IN THE MULTISCALE DEEP EQUILIBRIUM MODEL

The multiscale deep equilibrium (MDEQ) model is a recent extension of the deep equilibrium (DEQ)
model (Bai et al., 2019) for computer vision. One of the central engines for these DEQ models is
the root-finding problem:

_fŒ∏(z; x) = gŒ∏(z; x)_ _z_ find z[‚àó] s.t. fŒ∏(z[‚àó]; x) = 0, (100)
_‚àí_ _‚áí_

where Œ∏ and x are parameters and the input representation, respectively. In (Bai et al., 2020), the
Broyden‚Äôs method is employed to solve this problem. Since MST-AM is suitable for solving nonlinear systems of equations, we can apply MST-AM to solve (100).

We implemented the MST-AM method and integrated it into the MDEQ framework [3] . The task was
image classification on CIFAR-10 and we used the small model for test. We followed the suggested
experimental setting of the framework. The optimizer was Adam with learning rate of 0.001 and the
weight-decay was 2.5 _√ó_ 10[‚àí][6]. The batch size was 128 and the number of epochs was 50. The tested
fixed-point solver was used for the forward root-finding process and for the backward root-finding
process. The threshold of the steps for the forward process was 18 and the threshold of the steps for
the backward process was 20.

We used the built-in AM method and Broyden‚Äôs method as the baseline methods. The m for AM
was 20, i.e. using the full-memory AM.

D.2 EXPERIMENTAL DETAILS ABOUT RST-AM

Our experiments on RST-AM focused on training neural networks. Since ST-AM can be regarded
as a special case of RST-AM with Œ¥k[(1)] = Œ¥k[(2)] = 0, the basic ST-AM is also covered. In the
Line 10 in Algorithm 1, Œ±k should be adjusted to meet the positive definiteness check (13), which
can be simplified to (‚àÜxk)[T]rk _Œ≤k¬µ_ _rk_ 2 [in practice. The adjustment of][ Œ±][k] [can be (i)][ Œ±][k] [=]
min{Œ±k, 2Œ≤k(1 ‚àí _¬µ)/Œªk}, or (ii) ‚â• Œ±k = 0‚à•. We used the option (ii) since the violation of (13) seldom‚à•[2]_
happened in our tests and option (ii) is more simple to apply.

In the experiments on MNIST, Penn Treebank, we incorporated preconditioning (described in Appendix A.3) into the baseline method SAM and our method. Preconditioning is found to be effective
for difficult problems, e.g. mini-batch training with very small batch sizes and the scaling of the
model‚Äôs parameters being important.

D.2.1 HYPERPARAMETER SETTING OF RST-AM

The hyperparameters of RST-AM are easy to tune. The only hyperparameters that need to be carefully tuned are the regularization parameters c1, c2 in Œ¥k[(1)] and Œ¥k[(2)][. We found RST-AM is more]
sensitive to c1, possibly due to the fact that it influences the construction of secant equations. c2 can
be quite small in our tests. The hyperparameter C in Œ¥k[(2)] was set to be very small (C = 1 10[‚àí][16])

_c2_ _rk_ 2 _√ó_
so as to ensure Œ¥k[(2)] = _pk‚à•_ 2[+]‚à•[œµ][2][0][ almost all the time.][ œµ][0][ is introduced to prevent the denominators]

_‚à•_ _‚à•[2]_
from being zero. We found _pk_ 2 > 0 and ‚àÜxk 1 2 > 0 always held in the tests, so œµ0 can be
_‚à•_ _‚à•_ _‚à•_ _‚àí_ _‚à•_
omitted. In the experiments except for deterministic optimization on MNIST, we kept the setting
_c1 = 1, c2 = 1 √ó 10[‚àí][7]_ unchanged and found such setting is quite robust.

The other hyperparameters are Œ±k, Œ≤k, which can be always initially set as 1. So RST-AM has the
same number of hyperparameters to tune as SGDM, since SGDM needs to tune learning rate and
the momentum.

D.2.2 EXPERIMENTS ON MNIST

The experiments on MNIST [4] aimed to validate the effectiveness of RST-AM in deterministic optimization, so we were only concerned about the training loss by regarding it as a nonlinear function
to be optimized. To facilitate the full-batch training, we used a subset of the training dataset by
randomly selecting 10k images from the total 60k images.

[3 https://github.com/locuslab/mdeq.](https://github.com/locuslab/mdeq)
[4Based on the official PyTorch implementation https://github.com/pytorch/examples/blob/master/mnist.](https://github.com/pytorch/examples/blob/master/mnist)


-----

The baselines were SGDM, Adam, Adagrad, RMSprop and SAM. We tried our best to ensure that
the baselines had the best performance in the tests. We tuned the learning rates by log-scale gridsearches from 10[‚àí][3] to 100. For SGDM, Adam, Adagrad, and RMSprop, the learning rates were
0.2, 0.001, 0.01, 0.001, respectively. For SAM, we used the hyperparameter setting recommended
in (Wei et al., 2021).

For RST-AM, we setxk+1 = xk + 0.2rk was used as the new update. c1 = 0.05, c2 = 1 √ó 10[‚àí][7], Œ±k = Œ≤k = 1. When (‚àÜxk)[T]rk ‚â§ 0 occurs,

For the preconditioned RST-AM, we set c1 = 1 √ó 10[‚àí][2], c2 = 1 √ó 10[‚àí][7] for the Adagradpreconditioned RST-AM and c1 = 1 √ó 10[‚àí][2], c2 = 1 √ó 10[‚àí][8] for the RMSprop-preconditioned
RST-AM.

For all these tests, we trained the model for 200 epochs.


10[3]

10[0]

10 3

10 6

10 9


10[0]

10 2

10 4

10 6


50 100 150 200

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
|Adam RMSp|rop||||
|Adam RMSp|_SAM(2) rop_SAM(2)||||
|Adam RMSp|_RST-AM rop_RST-AM||||


Adam
RMSprop
Adam_SAM(2)
RMSprop_SAM(2)
Adam_RST-AM
RMSprop_RST-AM

epoch

(a) Train loss


0 50 100 150 200

|Col1|Col2|
|---|---|
|||
|||
|p||
|SAM(2) p_SAM(2)||
|RST-AM p_RST-AM||


Adam
RMSprop
Adam_SAM(2)
RMSprop_SAM(2)
Adam_RST-AM
RMSprop_RST-AM

epoch

(b) Squared norm of gradient


Figure 6: Training on MNIST with the Adam/RMSprop preconditioner.

In Figure 2 in the main paper, we report the RMSprop/Adagrad preconditioned SAM/RST-AM. We
give the result about using Adam as the preconditioner in Figure 6. It suggests that Adam does not
perform as well as the RMSprop method to serve as a preconditioner for SAM/RST-AM in this task.
Nevertheless, Adam RST-AM is still better than Adam and Adam SAM(2), which demonstrates the
effect of our proposed short-term recurrence scheme.


D.2.3 EXPERIMENTS ON CIFAR

This group of experiments were the same as those in (Wei et al., 2021) so that we can have a direct
comparison between RST-AM and SAM. We still give the details here for completeness.

We followed the same way of training ResNet (He et al., 2016). The batchsize was set to be 128.
For N iterations of training, the learning rate of each optimizer was decayed at the ( 2

(‚åä 4[3] _[N]_ _[‚åã][)][-th iterations. Here, for SAM and RST-AM, the][ Œ±][k][ and][ Œ≤][k][ serve as the learning rates, so]‚åä_ _[N]_ _[‚åã][)][-th and the]_

the learning rate decay denotes decaying Œ±k, Œ≤k simultaneously. The experiments were run with 3
random seeds.

The baseline optimizers were SGDM, Adam, AdaBound (Luo et al., 2018), AdaBelief (Zhuang
et al., 2020), Lookahead (Zhang et al., 2019) and AdaHessian (Yao et al., 2021). Adam and the
recently proposed optimizers AdaBound and AdaBelief are adaptive learning rate methods which
use different learning rates for different model parameters. Lookahead is a k-step method and has
an inner optimizer. In each cycle of Lookahead, the inner-optimizer optim is used to iterate for k
steps and then the first iterate and the last iterate are interpolated to obtain the starting point of the
next cycle. Compared to Adam, AdaHessian uses Hessian-vector products to construct a diagonal
approximation of the Hessian.

For fair comparison, the hyperparameters of all the optimizers (including RST-AM) were tuned
through experiments on CIFAR-10/ResNet20. For each optimizer, the hyperparameter setting that
attained the highest final test accuracy on CIFAR-10/ResNet20 was kept unchanged for training
the other networks on CIFAR. We found the results of hyperparameter tuning were consistent with
those reported in (Wei et al., 2021). For completeness, we list the settings of hyperparameters here
(learning rate is abbreviated as lr, and ‚Äú*‚Äù indicates the same setting as that in (Wei et al., 2021)).



_‚Ä¢ SGDM[‚àó]: lr = 0.1, momentum = 0.9, weight-decay = 5 √ó 10[‚àí][4], lr-decay = 0.1._


-----

90

80

70

60

50

40

30

|95 94 93 120 140 160 SGDM Adam AdaBound AdaBelief Lookahead Adahessian SAM(2) SAM(10) RST-AM|Col2|
|---|---|
|0 20 40|60 80 100 120 140 160 epochs|


95

94

93

120 140 160

SGDM
Adam
AdaBound
AdaBelief
Lookahead
Adahessian
SAM(2)
SAM(10)
RST-AM


90

80 95

94

70 93

92

120 140 160

60 SGDM

Adam

Test Accuracy % AdaBound

50 AdaBelief

Lookahead
Adahessian

40 SAM(2)

SAM(10)
RST-AM

0 20 40 60 80 100 120 140 160

epochs


(a) CIFAR-10/ResNet18

80

70

80

60

78

50 76

40 74

120 140 160

30 SGDM

Test Accuracy % Adam

AdaBound

20 AdaBelief

Lookahead

10 SAM(2)

SAM(10)
RST-AM

0

0 20 40 60 80 100 120 140 160

epochs


(c) CIFAR-100/ResNeXt50


(b) CIFAR-10/WRN16-4

80

70

60 80

78

50

76

40

120 140 160

SGDM

Test Accuracy %30 Adam

AdaBound
AdaBelief

20 Lookahead

SAM(2)

10 SAM(10)RST-AM

0 20 40 60 80 100 120 140 160

epochs


(d) CIFAR-100/DenseNet121


Figure 7: Test accuracy of training ResNet18 and WideResNet16-4 on CIFAR-10 and training
ResNeXt50 and DenseNet121 on CIFAR-100.

_‚Ä¢ Adam[‚àó]: lr = 0.001, (Œ≤1, Œ≤2) = (0.9, 0.999), weight-decay = 5 √ó 10[‚àí][4], lr-decay = 0.1._

_‚Ä¢ AdaBound: lr = 0.001, (Œ≤1, Œ≤2) = (0.9, 0.999), final lr = 0.1, gamma = 0.001, weight-_
decay = 5 √ó 10[‚àí][4], lr-decay = 0.1.

_‚Ä¢ AdaBelief[‚àó]: lr = 0.001, (Œ≤1, Œ≤2) = (0.9, 0.999), eps = 1_ _√ó_ 10[‚àí][8], weight-decay = 5 _√ó_ 10[‚àí][4],
lr-decay = 0.1.

_‚Ä¢ Lookahead[‚àó]: optim: SGDM (lr = 0.1, momentum = 0.9, weight-decay = 1 √ó 10[‚àí][3]),_
_Œ± = 0.8, steps = 10, lr-decay = 0.1._

_‚Ä¢ AdaHessian[‚àó]: lr = 0.15, (Œ≤1, Œ≤2) = (0.9, 0.999), eps=1_ _√ó_ 10[‚àí][4], hessian-power: 1, weightdecay = 5 √ó 10[‚àí][4]/0.15, lr-decay = 0.1.

_‚Ä¢ SAM(2): optim: SGDM (lr = 0.1, momentum = 0, weight-decay = 1.5 √ó 10[‚àí][3]), Œ±k =_
1.0, Œ≤k = 1.0, c1 = 0.01, p = 1, m = 2, weight-decay = 1.5 √ó 10[‚àí][3], lr-decay = 0.06.

_‚Ä¢ SAM(10)[‚àó]: optim: SGDM (lr = 0.1, momentum = 0, weight-decay = 1.5 √ó 10[‚àí][3]), Œ±k =_
1.0, Œ≤k = 1.0, c1 = 0.01, p = 1, m = 10, weight-decay = 1.5 √ó 10[‚àí][3], lr-decay = 0.06.

_‚Ä¢ RST-AM: c1 = 1, c2 = 1 √ó 10[‚àí][7], Œ±0 = Œ≤0 = 1, weight-decay = 1 √ó 10[‚àí][3], lr-decay = 0.1._


For the tests of SGDM, Adam, AdaBelief, Lookahead, AdaHessian and SAM(10), we also had
consistent numerical results with those reported in (Wei et al., 2021), so we reported their results of
these methods in the main paper for reference.

Figure 7 shows the curves of test accuracy for training ResNet18 and WRN16-4 on CIFAR-10 and
training ResNeXt50 and DenseNet121 on CIFAR-100. The numerical results of final test accuracy
can be found in Table 1(a) in the main paper. It can be found in Figure 7 that the convergence
behaviour of RST-AM is less erratic than SAM(10) and SAM(2). In the first 80 epochs, RST-AM


-----

converges faster than SAM, partly due to using a smaller weight decay. The learning rate schedule
has a large impact on the convergence of each optimizer. Similar to SAM(10), RST-AM can always
climb up and stabilize to a higher test accuracy in the final 40 epochs. It is found that RST-AM is
comparable to SAM(10), while improving the short-memory SAM(2).


100

90

80

70

SGDM

60 RST-AM: c2=0.1

RST-AM: c2=0.001

Train Accuracy % 50 RST-AM: c2=10 5

RST-AM: c2=10 7

40 RST-AM: c2=10 9

0 20 40 60 80 100 120 140 160

epochs


90

80

70

SGDM

60 RST-AM: c2=0.1

RST-AM: c2=0.001

Test Accuracy %50 RST-AM: c2=10 5

RST-AM: c2=10 7

40 RST-AM: c2=10 9

0 20 40 60 80 100 120 140 160

epochs


(a) Train accuracy


(b) Test accuracy


Figure 8: Train accuracy and test accuracy of RST-AM with different c2.


100

90

80

70

60 SGDM

Train Accuracy % 50 RST-AM: c1=10RST-AM: c1=1

40 RST-AM: c1=0.1

RST-AM: c1=0.01

0 20 40 60 80 100 120 140 160

epochs


90

80

70

60

SGDM

est Accuracy %T50 RST-AM: c1=10RST-AM: c1=1

RST-AM: c1=0.1

40 RST-AM: c1=0.01

0 20 40 60 80 100 120 140 160

epochs


(a) Train accuracy


(b) Test accuracy


Figure 9: Train accuracy and test accuracy of RST-AM with different c1.


100

90

80

70

60 SGDM

RST-AM: wd=0.0005

Train Accuracy % 50 RST-AM: wd=0.0008

RST-AM: wd=0.001

40 RST-AM: wd=0.0015

0 20 40 60 80 100 120 140 160

epochs


90

80

70

60 SGDM

RST-AM: wd=0.0005

Test Accuracy % 50 RST-AM: wd=0.0008

RST-AM: wd=0.001

40 RST-AM: wd=0.0015

0 20 40 60 80 100 120 140 160

epochs


(a) Train accuracy


(b) Test accuracy


Figure 10: Train accuracy and test accuracy of RST-AM with different weight-decays.

Since our hyperparameter tuning was conducted on CIFAR10/ResNet20, we give some results about
the hyperparameters on this model.

**Effect of c2 in RST-AM. Figure 8 shows the effect of c2, where we kept c1 = 1, weight-decay=5 √ó**
10[‚àí][4] fixed, and c2 = 10[‚àí][1], 10[‚àí][3], 10[‚àí][5], 10[‚àí][7], 10[‚àí][9]. It indicates that RST-AM is not sensitive to
_c2._

**Effect of c1 in RST-AM. Figure 9 shows the effect of c1, where we kept c2 = 10[‚àí][7], weight-**
decay=5√ó10[‚àí][4] fixed, and c1 = 0.01, 0.1, 1, 10. It suggests that with smaller c1, RST-AM converges
faster in terms of train accuracy, but the test accuracy is worse.


-----

**Effect of weight decay in RST-AM. Weight-decay is a common hyperparameter that can affect the**
generalization of each optimizer. In Figure 10, we show the convergence behaviour of RST-AM
with different weight-decays. It suggests that with a too small weight-decay, RST-AM tends to be
overfitting in the test dataset.


90

80

70


90

80


70

60

50


60

50


|95 94.78 94.95 95.21 94.90 94 93 92 60 80 100 120 140 160 SGDM: epoch=80 SGDM: epoch=100 SGDM: epoch=160 SAM: epoch=80 SAM: epoch=100 SAM: epoch=160 RST-AM: epoch=80 RST-AM: epoch=100 RST-AM: epoch=160|Col2|
|---|---|
|0 20 40 6|0 80 100 120 140 160 epochs|


(b) Test accuracy on CIFAR10/WideResNet16-4


40

80

70

60

50

40

30

20

10

|95.27 95 94.93 94.53 94.82 94 93 92 60 80 100 120 140 160 SGDM: epoch=80 SGDM: epoch=100 SGDM: epoch=160 SAM: epoch=80 SAM: epoch=100 SAM: epoch=160 RST-AM: epoch=80 RST-AM: epoch=100 RST-AM: epoch=160|Col2|
|---|---|
|0 20 40 6|0 80 100 120 140 160 epochs|


(a) Test accuracy on CIFAR10/ResNet18


80

70

50

|80.36 80 79.44 79 78.90 78.49 78 77 60 80 100 120 140 160 SGDM: epoch=80 SGDM: epoch=100 SGDM: epoch=160 SAM: epoch=80 SAM: epoch=100 SAM: epoch=160 RST-AM: epoch=80 RST-AM: epoch=100 RST-AM: epoch=160|Col2|
|---|---|
|0 20 40 60|80 100 120 140 160 epochs|



(d) Test accuracy on CIFAR100/DenseNet121

|80 79.53 79 78.96 78.39 78 78.41 77 76 60 80 100 120 140 160 SGDM: epoch=80 SGDM: epoch=100 SGDM: epoch=160 SAM: epoch=80 SAM: epoch=100 SAM: epoch=160 RST-AM: epoch=80 RST-AM: epoch=100 RST-AM: epoch=160|Col2|
|---|---|
|0 20 40 60|80 100 120 140 160 epochs|


(c) Test accuracy on CIFAR100/ResNeXt50


Figure 11: Training deep neural networks for 80,100,160 epochs. The results of final test accuracy
of RST-AM for training 80,100,160 epochs and the final test accuracy of SGDM for training 160
epochs are shown in the nested figures for comparison.

Table 5: The cost and final test accuracy compared with SGDM. The notations ‚Äúm‚Äù,‚Äút/e‚Äù, ‚Äúe‚Äù,
‚Äút‚Äù and ‚Äúa‚Äù are abbreviations of memory, per-epoch time, training epochs, total running time, and
accuracy, respectively. ‚Äú*‚Äù indicates numbers published in (Wei et al., 2021).


Cost (√ó SGDM) CIFAR10/ResNet18 CIFAR10/WRN16-4
& accuracy m t/e e t a(%) m t/e e t a(%)

SGDM[‚àó] 1.00 1.00 1.00 1.00 94.82 1.00 1.00 1.00 1.00 94.90
SAM(10)[‚àó] 1.73 1.78 0.56 1.00 94.81 1.26 1.28 0.63 0.80 94.94
RST-AM 1.05 1.46 0.56 0.82 94.84 1.03 1.14 0.63 0.71 94.95

Cost (√ó SGDM) CIFAR100/ResNeXt50 CIFAR100/DenseNet121
& accuracy m t/e e t a(%) m t/e e t a(%)

SGDM[‚àó] 1.00 1.00 1.00 1.00 78.41 1.00 1.00 1.00 1.00 78.49
SAM(10)[‚àó] 1.30 1.16 0.50 0.58 78.37 1.16 1.19 0.50 0.60 78.84
RST-AM 1.04 1.07 0.50 0.54 78.39 1.01 1.11 0.50 0.55 78.90

Table 1(a) in the main paper reports the test accuracy of each optimizer when training for the same
epochs. In fact, as shown in Figure 11, within 100 epochs, RST-AM can achieve a better test


-----

accuracy than SGD. So if the running time of the training process matters, it is expected that RSTAM can use less total running time due to the large number of reduction in training epochs. In
Table 1(b), we set SGD as the baseline, and compare the computation and memory cost with SGD.
In Table 5, we give more details about the saving in training epochs and the final test accuracy. It
can be seen that RST-AM can achieve a comparable or better test accuracy than SGDM with less
computation time, while reducing the memory footprint of SAM.

We also tried using Adam as a preconditioner for RST-AM but found the final test accuracy was
often worse. For example, the test accuracy of Adam RST-AM for CIFAR-10/ResNet20 is only
90.79%. We suppose it is the worse generalization ability of Adam (Luo et al., 2018) that makes
Adam not suitable as a preconditioner for RST-AM in the image classification task.

D.2.4 EXPERIMENTS ON PENN TREEBANK

This group of experiments were the same as those in (Wei et al., 2021) for direct comparison. Results
in Table 2 were measured with 3 random seeds. The parameter settings of the LSTM models were
the same as those in (Zhuang et al., 2020; Wei et al., 2021). The baseline optimizers were SGDM,
Adam, AdaBelief, and SAM. The validation dataset was used for tuning hyperparameters.

For SGDM, the learning rate (abbr. lr) was tuned via grid-search in {1, 10, 30, 100}. We set lr = 10,
momentum = 0.9 for the 2-layer/3-layer LSTM, and lr = 30, momentum = 0 for the 1-layer LSTM.

For Adam, the learning rate was tuned via grid-search in {1√ó10[‚àí][3], 2√ó10[‚àí][3], 5√ó10[‚àí][3], 8√ó10[‚àí][3], 1√ó
10[‚àí][2], 2 √ó 10[‚àí][2]}. We found the setting that lr = 5 √ó 10[‚àí][3] performs best.

For AdaBelief, we set lr = 5 √ó 10[‚àí][3] which is better than the recommended settings of the official
implementation[5].

For SAM, we used the recommended setting in (Wei et al., 2021) and used the baseline Adam as the
preconditioner. The cases m = 2 and m = 20 are denoted as SAM(2) and SAM(10), respectively.

For our method RST-AM, we kept c1 = 1, c2 = 1 √ó 10[‚àí][7] unchanged and used the same preconditioner (Adam) as SAM.

The batch size was 20. We trained the model for 500 epochs and the learning rate was decayed by
0.1 at the 250th epoch and the 375th epoch.

Table 6: The cost to achieve comparable results of Adam. The notations ‚Äúm‚Äù,‚Äút/e‚Äù and ‚Äút‚Äù are
abbreviations of memory, per-epoch time and total running time, respectively.

Cost 1-Layer 2-Layer 3-Layer
(√ó Adam) m t/e t m t/e t m t/e t

Adam 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
SAM(10) 1.20 1.84 0.74 1.36 1.88 0.75 1.90 1.67 0.67
RST-AM 1.06 1.73 0.69 1.15 1.78 0.71 1.11 1.53 0.61

Table 7: Test perplexity of training 1,2,3-layer LSTM on Penn Treebank for 200 epochs. Lower is
better. ‚Äú*‚Äù indicates numbers published in (Wei et al., 2021).

Method 1-Layer 2-Layer 3-Layer

Adam[‚àó] 80.88¬±.15 64.54¬±.18 60.34¬±.22
SAM(2) 81.82¬±.09 66.62¬±.26 61.55¬±.11
SAM(10) **79.30¬±.12** **63.21¬±.02** 59.47¬±.07
RST-AM 79.49¬±.11 63.61¬±.26 **59.34¬±.23**

In Table 6, we report the memory footprint and per-epoch running time of SAM(10) and RST-AM
compared with Adam. It indicates that RST-AM also largely reduces the memory overhead of the

[5https://github.com/juntang-zhuang/Adabelief-Optimizer/tree/update 0.2.0/PyTorch Experiments/LSTM.](https://github.com/juntang-zhuang/Adabelief-Optimizer/tree/update_0.2.0/PyTorch_Experiments/LSTM)


-----

long-memory SAM(10) in the language modeling task. Since the batch size is very small, the cost of
stochastic gradient evaluation is quite cheap, which makes the additional cost of matrix computation
in RST-AM and SAM(10) become considerable. However, if we consider achieving a comparable
validation/test perplexity to that of Adam, RST-AM does not need to train for the same number of
epochs as Adam. Table 7 shows the test perplexity of Adam, SAM(2), SAM(10), and RST-AM for
training 200 epochs. By comparing the results of Table 2 and Table 7, we see RST-AM is better than
Adam with much fewer training epochs, thus RST-AM can save a large amount of training time, as
shown in Table 6.

D.2.5 EXPERIMENTS ON ADVERSARIAL TRAINING

The adversarial training considers the empirical adversarial risk minimization (EARM) problem:

_T_

1
min max _Œæi_ [(][x][)][,] (101)
_x‚ààR[d]_ _T_ _i=1_ _‚à•Œæ[¬Ø]i‚àíŒæi‚à•2‚â§œµ_ _[f]_ [¬Ø]

X

where _Œæ[¬Ø]i is the i-th adversarial data within the œµ-ball centered at Œæi. We followed the standard PGD_
adversarial training in (Madry et al., 2018), using projection gradient descent (PGD) to solve the
inner maximization problem and the tested optimizers (SGD and RST-AM) to solve the outer minimization problem. The experiments were conducted on CIFAR10/ResNet18, CIFAR10/WRN16-4,
CIFAR100/ResNet18, and CIFAR100/DenseNet121. We trained the neural networks for 200 epochs
and decayed the learning rate at the 100th and 150th epoch.

Since adversarial training is much more time consuming than the ordinary training in Section D.2.3,
we tuned the hyperparameters in CIFAR10/ResNet20, and applied the same hyperparameters to
other models. We found the setting that c1 = 1, c2 = 1 √ó 10[‚àí][7] and weight-decay = 0.001 is still
suitable for this task.

The CIFAR-10 (CIFAR-100) dataset contains 50K images for training and 10K images for testing.
Since it is found that the phenomenon of overfitting is severer (Rice et al., 2020) in adversarial
training, we randomly selected 5K images from the total 50K training dataset as the validation
dataset (the other 45K images remained as the training dataset), and chose the best checkpoint
model in the validation set to evaluate on the test dataset. We consider two types of test accuracy:
(i) the clean test accuracy, where clean data was used for model evaluation;
(ii) the robust test accuracy, where corrupted data was used for model evaluation. The attacking
methods are FGSM (Goodfellow et al., 2014), PGD-20 (Madry et al., 2018), and C&W attack
_‚àû_
(Carlini & Wagner, 2017).

Table 8: Test accuracy (%) for adversarial training.

CIFAR10/ResNet18 CIFAR100/DenseNet121
Optimizer
Clean FGSM PGD-20 C&W Clean FGSM PGD-20 C&W
_‚àû_ _‚àû_

SGD 82.16 63.23 51.91 50.22 59.45 39.76 30.92 29.00
RST-AM 82.53 63.78 52.43 50.52 60.48 40.41 31.20 29.52


CIFAR10/WRN16-4 CIFAR100/ResNet18
Optimizer
Clean FGSM PGD-20 C&W Clean FGSM PGD-20 C&W
_‚àû_ _‚àû_

SGD 80.84 60.97 49.29 47.62 55.42 36.17 28.18 26.31
RST-AM 81.36 61.38 49.93 47.95 56.49 37.00 28.50 26.66

In Tabel 8, we report the average results of tests with three different random seeds. It shows that
RST-AM can achieve both higher clean test accuracy and higher robust test accuracy across various
models on CIFAR10/CIFAR100. To see the convergence behaviour of SGD and RST-AM, we plot
the curves of the clean validation accuracy and the PGD-10 attacked validation accuracy in Figure 12. We can observe the phenomenon of robust overfitting (Rice et al., 2020) from these curves,
which justifies our experimental setting with validation set for the checkpoint selection. Nonetheless,
the numerical results suggest that RST-AM can still be better than SGDM in adversarial training.

It is also found that due to the heavy cost of gradient evaluations in PGD adversarial training, the
additional computational cost incurred by RST-AM is negligible and the per-epoch running time of
SGD and RST-AM is roughly the same. So we do not report it here.


-----

SGDM

80 RST-AM

70

60

50

40

Validation Accuracy %

30

0 25 50 75 100 125 150 175 200

epochs


55

SGDM

50 RST-AM

45

40

35

30

25

Adv-Validation Accuracy %

20

0 25 50 75 100 125 150 175 200

epochs


(a) CIFAR10/ResNet18

SGDM

80

RST-AM

70

60

50

40

Validation Accuracy %

30

0 25 50 75 100 125 150 175 200

epochs


(c) CIFAR10/WRN16-4


(b) CIFAR10/ResNet18

50 SGDM

RST-AM

45

40

35

30

25

Adv-Validation Accuracy %

0 25 50 75 100 125 150 175 200

epochs


(d) CIFAR10/WRN16-4


SGDM

50 RST-AM

40

30

20

Validation Accuracy %

10

0 25 50 75 100 125 150 175 200

epochs


SGDM

25 RST-AM

20

15

10

Adv-Validation Accuracy %

5

0 25 50 75 100 125 150 175 200

epochs


(e) CIFAR100/ResNet18

60

SGDM
RST-AM

50

40

30

20

Validation Accuracy %

10

0 25 50 75 100 125 150 175 200

epochs


(g) CIFAR100/DenseNet121


(f) CIFAR100/ResNet18

30 SGDM

RST-AM

25

20

15

10

Adv-Validation Accuracy %

5

0 25 50 75 100 125 150 175 200

epochs


(h) CIFAR100/DenseNet121


Figure 12: Clean accuracy and PGD-10 attacked accuracy on the validation set in training different
neural networks.


-----

D.2.6 EXPERIMENTS ON TRAINING A GENERATIVE ADVERSARIAL NETWORK

We describe our setting of training a generative adversarial network (GAN) here. Like the adversarial training, the GAN training process is also a min-max problem. The stability of an optimizer
is critical for the training process. To demonstrate the applicability of RST-AM, we conducted
experiments on a GAN which was equipped with spectral normalization (Miyato et al., 2018) (SNGAN). The experimental setting was the same as that of AdaBelief (Zhuang et al., 2020): the dataset
was CIFAR-10; ResNets were used as the generator and the discriminator networks; the steps for
optimization in the discriminator and the generator per iteration were 5 and 1, respectively; the minbatch size was 64 and the total iteration number was 100000. The Frechet Inception Distance (FID)
(Heusel et al., 2017) was used as the evaluation metric: lower FID score means better accuracy.

The baseline optimizer were Adam and AdaBelief as they perform well in this task (Zhuang et al.,
2020). We also used the recommended hyperparameter settings for the two optimizers. For our
RST-AM method, due to the ill-conditioning of the min-max problem, we used the AdaBelief as the
preconditioner and set the damping parameter Œ±k = 0.6. The regularization parameters c1 = 1 and
_c2 = 1 √ó 10[‚àí][7]_ were still kept unchanged just as the previous experiments.

45 Adam

AdaBelief

40 RST-AM

35

30

FID score 25

20

15

0 20000 40000 60000 80000 100000

iteration


Figure 13: FID score for training SN-GAN on CIFAR-10.

In Figure 13, we show the curve of FID score for each optimizer, which is the average of three
independent runs. It indicates that the RST-AM method is stable for this min-max optimization
problem.

Table 9: The effect of Œ±k for RST-AM.

Method Adam AdaBelief _Œ± = 0.8_ _Œ± = 0.6_ _Œ± = 0.5_ _Œ± = 0.4_ _Œ± = 0.2_ _Œ± = 0.1_

FID score 13.07 12.80 12.48 **12.05** 12.75 13.13 13.07 12.59

Since we only tuned the damping parameter Œ±k for RST-AM, we report the FID scores of other
choices of Œ±k in Table 9 during our experiment. It shows that even with the suboptimal choices of
_Œ±k, e.g. Œ±k = 0.1, 0.5, 0.8, RST-AM can still outperform the baselines._

D.3 ADDITIONAL EXPERIMENTS

To further test the performance of our method in training neural networks on larger datasets or
different models, we conducted additional experiments of the image classification task in ImageNet
(Deng et al., 2009) and the Transformer (Vaswani et al., 2017) based neural machine translation task
in the IWSTL14 DE-EN (Cettolo et al., 2014) dataset.

D.3.1 EXPERIMENTS ON IMAGENET

We trained ResNet50 on ImageNet with SGDM and RST-AM. We used the built-in ResNet50 model
in PyTorch. We ran the tests of each optimizer with three random seeds and four GeForce RTX 2080


-----

Ti GPUs were used for each test. The hyperparameters of SGDM were set as the recommended
setting in PyTorch. For RST-AM, the hyperparameters were kept the same as those in the CIFAR
experiments. The weight-decay was 1 √ó 10[‚àí][4]. The number of the total training epochs is 90. The
learning rate decay of SGD was at the 30th and 60th epochs. For RST-AM, since the experiments
on CIFAR show it can often converge faster to an acceptable solution than SGDM, we adopted the
early learning rate decay strategy recommended in (Zhang et al., 2019): decay the Œ±k and Œ≤k for
RST-AM at the 30th, 50th and 70th epochs.

Table 10: TOP 1 test accuracy (%) w.r.t. epoch, the best TOP1 test accuracy (%), and the cost. The
memory, per-epoch time and total time of SGDM are set as the units. The total time is the time to
first achieve the accuracy ‚â• 75.90%.

Method epoch = 72 epoch = 88 epoch = 90 best memory per-epoch time total time

SGDM 75.75 75.90 75.81 75.93¬±.15 1.00 1.00 1.00
RST-AM 75.90 75.95 75.98 76.04¬±.06 1.06 1.01 0.83

In Table 10, we report the TOP1 accuracy in the validation dataset during training. Note that we also
report the epoch number for each optimizer to achieve the accuracy equal or exceeding 75.90%. It
shows RST-AM needs 72 epochs while SGDM needs 88 epochs. The curves of the training accuracy
and test accuracy are shown in Figure 14. The results suggest that RST-AM is still a competitive
optimizer in training a larger model in a larger dataset.

|Col1|75|
|---|---|
||75|
|||

|20 40 epochs|60 80|
|---|---|


80

SGDM

70 RST-AM

60

50

40

30

Train Accuracy %

20

10

0 20 40 60 80

epochs


SGDM

70 RST-AM

77

60 76 75.98

75.81

50 75

74

40 50 60 70 80 90

Test Accuracy % 30

20

10

0 20 40 60 80

epochs


(a) Train accuracy


(b) Test accuracy


Figure 14: Train and test accuracy for training ImageNet/ResNet50.

D.3.2 TRAINING TRANSFORMER

We conducted the neural machine translation task with Transformer. We implemented our RST-AM
method and integrated it into the fairseq framework [6]. The basic experimental setting was set as
the recommended setting in (Yao et al., 2021). We trained the model for 50 epochs and the BLEU
(Papineni et al., 2002) score was calculated using the average model of the last five checkpoints. We
added a baseline optimizer RAdam (Liu et al., 2019) which was inspired by the warmup procedure
in training Transformer.

Table 11: The BLEU score of training Transformer on IWSLT14.

Method SGD Adam AdaBelief RAdam RST-AM

BLEU score 28.14¬±.08 35.71¬±.03 35.15¬±.14 35.60¬±.06 **35.89¬±.02**

[6 https://github.com/pytorch/fairseq.](https://github.com/pytorch/fairseq)


-----

The numerical results reported in Table 11 show that Adam is well-suited for this neural machine
translation task, though it does not perform well in the image classification task. Also, the results
demonstrate that RST-AM can still outperform Adam in this task.

Table 12: BLEU score evaluated at the 40/45/50-th epoch, and the cost. The memory, per-epoch
time and total time of Adam are set as the units. The total time is the time of RST-AM to achieve
a BLEU score matching the final BLEU of Adam: | BLEU(RST-AM) ‚àí BLEU(Adam) | ‚â§ 0.03,
where 0.03 is the standard deviation of the results of Adam.

Method epoch = 40 epoch = 45 epoch = 50 memory per-epoch time total time

Adam 35.42¬±.10 35.54¬±.08 35.71¬±.03 1.00 1.00 1.00
RST-AM **35.59¬±.14** **35.69¬±.06** **35.89¬±.02** 1.16 1.00 0.90

In Table 12, we report the BLEU scores evaluated in the test dataset at the 40th, 45th and 50th epochs.
The results show the better performance of RST-AM over Adam and the per-epoch computational
cost is nearly the same. To achieve a comparable solution to Adam, RST-AM can save 10% training
time.


-----

