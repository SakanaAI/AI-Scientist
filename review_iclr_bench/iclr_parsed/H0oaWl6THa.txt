# HYBRID LOCAL SGD FOR FEDERATED LEARNING
## WITH HETEROGENEOUS COMMUNICATIONS


**Yuanxiong Guo**
The University of Texas at San Antonio
San Antonio, Texas 78249 USA
yuanxiong.guo@utsa.edu

**Rui Hu & Yanmin Gong**
The University of Texas at San Antonio
San Antonio, Texas 78249 USA
_{rui.hu@my.,yanmin.gong@}utsa.edu_

ABSTRACT


**Ying Sun**
Pennsylvania State University
State College, PA 16801 USA
ysun@psu.edu


Communication is a key bottleneck in federated learning where a large number
of edge devices collaboratively learn a model under the orchestration of a central
server without sharing their own training data. While local SGD has been proposed to reduce the number of FL rounds and become the algorithm of choice
for FL, its total communication cost is still prohibitive when each device needs to
communicate with the remote server repeatedly for many times over bandwidthlimited networks. In light of both device-to-device (D2D) and device-to-server
(D2S) cooperation opportunities in modern communication networks, this paper
proposes a new federated optimization algorithm dubbed hybrid local SGD (HLSGD) in FL settings where devices are grouped into a set of disjoint clusters with
high D2D communication bandwidth. HL-SGD subsumes previous proposed algorithms such as local SGD and gossip SGD and enables us to strike the best
balance between model accuracy and runtime. We analyze the convergence of
HL-SGD in the presence of heterogeneous data for general nonconvex settings.
We also perform extensive experiments and show that the use of hybrid model
aggregation via D2D and D2S communications in HL-SGD can largely speed up
the training time of federated learning.

1 INTRODUCTION

Federated learning (FL) is a distributed machine learning paradigm in which multiple edge devices
or clients cooperate to learn a machine learning model under the orchestration of a central server,
and enables a wide range of applications such as autonomous driving, extended reality, and smart
manufacturing (Kairouz et al., 2021). Communication is a critical bottleneck in FL as the clients are
typically connected to the central server over bandwidth-limited networks. Standard optimization
methods such as distributed SGD are often not suitable in FL and can cause high communication
costs due to the frequent exchange of large-size model parameters or gradients. To tackle this issue,
local SGD, in which clients update their models by running multiple SGD iterations on their local
datasets before communicating with the server, has emerged as the de facto optimization method in
FL and can largely reduce the number of communication rounds required to train a model (McMahan
et al., 2017; Stich, 2019).

However, the communication benefit of local SGD is highly sensitive to non-iid data distribution as
observed in prior work (Rothchild et al., 2020; Karimireddy et al., 2020). Intuitively, taking many
local iterations of SGD on local dataset that is not representative of the overall data distribution will
lead to local over-fitting, which will hinder convergence. In particular, it is shown in (Zhao et al.,
2018) that the convergence of local SGD on non-iid data could slow down as much as proportionally
to the number of local iteration steps taken. Therefore, local SGD with a large aggregation period


-----

can converge very slow on non-iid data distribution, and this may nullify its communication benefit
(Rothchild et al., 2020).

Local SGD assumes a star network topology where each device connects to the central server for
model aggregation. In modern communication networks, rather than only communicating with the
server over slow communication links, devices are increasingly connected to others over fast communication links. For instance, in 5G-and-beyond mobile networks, mobile devices can directly
communicate with their nearby devices via device-to-device links of high data rate (Asadi et al.,
2014; Yu et al., 2020). Also, edge devices within the same local-area network (LAN) domain can
communicate with each other rapidly without traversing through slow wide-area network (WAN)
(Yuan et al., 2020). This gives the potential to accelerate the FL convergence under non-iid data
distribution by leveraging fast D2D cooperation so that the total training time can be reduced in FL
over bandwidth-limited networks.

Motivated by the above observation, this paper proposes hybrid local SGD (HL-SGD), a new distributed learning algorithm for FL with heterogeneous communications, to speed up the learning
process and reduce the training time. HL-SGD extends local SGD with fast gossip-style D2D communication after local iterations to mitigate the local over-fitting issue under non-iid data distribution
and accelerate convergence. A hybrid model aggregation scheme is designed in HL-SGD to integrate both fast device-to-device (D2D) and slow device-to-server (D2S) cooperations. We analyze
the convergence of HL-SGD in the presence of heterogeneous data for general nonconvex settings,
and characterize the relationship between the optimality error bound and algorithm parameters. Our
algorithm and analysis are general enough and subsume previously proposed SGD variations such
as distributed SGD, local SGD and gossip SGD.

Specifically, we consider the FL setting in which all devices are partitioned into disjoint clusters,
each of which includes a group of connected devices capable of communicating with each other
using fast D2D links. The clustering can be a natural result of devices belonging to different LAN
domains so that those devices connected to the same LAN domain are considered as one cluster. In
another example, clustering is based on the geographic locations of mobile devices so that devices
in a cluster are connected to each other through D2D communication links.

In summary, the paper makes the following main contributions:

_â€¢ We propose a novel distributed learning algorithm for FL called HL-SGD to address the commu-_
nication challenge of FL over bandwidth-limited networks by leveraging the availability of fast D2D
links to accelerate convergence under non-iid data distribution and reduce training time.

_â€¢ We provide the convergence analysis of HL-SGD under general assumptions about the loss func-_
tion, data distribution, and network topology, generalizing previous results on distributed SGD, local
SGD, and gossip SGD.

_â€¢ We conduct extensive empirical experiments on two common benchmarks under realistic network_
settings to validate the established theoretical results of HL-SGD. Our experimental results show
that HL-SGD can largely accelerate the learning process and speed up the runtime.

2 BACKGROUND AND RELATED WORK

Large-scale machine learning based on distributed SGD has been well studied in the past decade, but
often suffers from large network delays and bandwidth limits (Bottou et al., 2018). Considering that
communication is a major bottleneck in federated settings, local SGD has been proposed recently to
reduce the communication frequency by running SGD independently in parallel on different devices
and averaging the sequences only once in a while (Stich, 2019; Lin et al., 2019; Haddadpour et al.,
2019; Yu et al., 2019; Wang et al., 2021). However, they all assume the client-server architecture and
do not leverage the fast D2D communication capability in modern communication networks. Some
studies (Liu et al., 2020; Abad et al., 2020; Castiglia et al., 2021) develop hierarchical FL algorithms
that first aggregate client models at local edge servers before aggregating them at the cloud server or
with neighboring edge servers, but they still rely on D2S communication links only and suffer from
the scalability and fault-tolerance issues of centralized setting. On the other hand, while existing
works on decentralized or gossip SGD consider D2D communications (Tsitsiklis, 1984; Boyd et al.,
2006), they assume a connected cluster with homogeneous communication links and will converge


-----

very slow on the large and sparse network topology that is typically found in FL settings. Unlike
previous works, HL-SGD leverages both D2S and D2D communications in the system.

Some recent studies aim to encapsulate variants of SGD under a unified framework. Specifically,
a cooperative SGD framework is introduced in (Wang & Joshi, 2021) that includes communication reduction through local SGD steps and decentralized mixing between clients under iid data
distribution. A general framework for topology-changing gossip SGD under both iid and non-iid
data distributions is proposed in (Koloskova et al., 2020). Note that all of the above works assume
undirected network topology for communications in every iteration. In comparison, our proposed
HL-SGD is different: the D2S communication is asymmetric due to the use of device sampling
and model broadcasting in each global aggregation round and cannot be modeled in an undirected
graph. Therefore, the convergence analysis of HL-SGD does not fit into the prior frameworks and
is much more challenging. Moreover, our major focus is on the runtime of the algorithm rather than
its convergence speed in iterations.

3 SYSTEM MODEL

In this section, we introduce the FL system model, problem formulation, and assumptions we made.

**Notation. All vectors in this paper are column vectors by default. For convenience, we use 1 to**
denote the all-ones vector of appropriate dimension, 0 to denote the all-zeros vector of appropriate
dimension, and [n] to denote the set of integers {1, 2, . . ., n} with any positive integer n. Let âˆ¥Â·âˆ¥
denote the â„“2 vector norm and Frobenius matrix norm and âˆ¥Â·âˆ¥2 denote the spectral norm of a matrix.

We consider a FL system consisting of a central server and K disjoint clusters of edge devices.
Devices in each cluster k âˆˆ [K] can communicate with others across an undirected and connected
graph Gk = (V, Ek), where Vk denotes the set of edge devices in the cluster, and edge (i, j) âˆˆEk
denotes that the pair of devices i, j âˆˆVk can communicate directly using D2D as determined by the
communication range of D2D links. Besides, each device can directly communicate with the central
server using D2S links. Denote the set of all devices in the system as := _k_ [K]
_V_ _âˆˆ_ _[V][k][, the number]_

of devices in each cluster k [K] as n := _k_, and the total number of devices in the system as
_âˆˆ_ _|V_ _|_
_N :=_ _k_ [K] _[n][ 1][.]_ [S]

_âˆˆ_

The FL goal of the system is to solve an optimization problem of the form:

[P]

min _fi(x) := [1]_ _fÂ¯k(x),_ (1)
_x_ R[d][ f] [(][x][) := 1]N _K_
_âˆˆ_ XiâˆˆV _kXâˆˆ[K]_

where fi(x) := Ez _i_ [â„“i(x; z)] is the local objective function of device i, _fÂ¯k(x)_ :=
_âˆ¼D_
(1/n) _i_ _k_ _[f][i][(][x][)][ is the local objective function of cluster][ k][, and][ D][i][ is the data distribution of]_
_âˆˆV_
device i. Here â„“i is the (non-convex) loss function defined by the learning model and z represents a
data sample from data distribution _i._

[P] _D_

When applying local SGD to (1) in FL with heterogeneous communications, the communications
between the server and devices in FL are all through D2S links that are bandwidth-limited, particularly for the uplink transmissions. Therefore, the incurred communication delay is high. Due to the
existing of high-bandwidth D2D links that are much more efficient than low-bandwidth D2S links,
it would be highly beneficial if we can leverage D2D links to reduce the usage of D2S links such
that the total training time can be reduced. This motivates us to design a new learning algorithm for
FL with heterogeneous communications.

4 HYBRID LOCAL SGD

In this section, we present our HL-SGD algorithm suitable for the FL setting with heterogeneous
communications. Algorithm 1 provides pseudo-code for our algorithm.

At the beginning of r-th global communication round, the server broadcasts the current global model
_x[r]_ to all devices in the system via cellular links (Line 4). Note that in typical FL systems, the down
1For presentation simplicity, we assume each cluster contains the same number of devices here. The results
of this paper can be extended to the case of clusters with different device numbers as well.


-----

**Algorithm 1 HL-SGD: Hybrid Local SGD**
**Input: initial global model x[0], learning rate Î·, communication graph Gk and mixing matrix Wk for**
all clusters k âˆˆ [K], and fraction of sampled devices in each cluster p.
**Output: final global model x[R]**

1: for each round r = 0, . . ., R âˆ’ 1 do
2: **for each cluster k âˆˆ** [K] in parallel do

3: **for each device i âˆˆVk in parallel do**

4: _x[r,]i_ [0] = x[r]

5: **for s = 0, . . ., Ï„ âˆ’** 1 do

6:7:8: Compute a stochastic gradientxxir,s[r,s]i +[+1][1]2 == x[r,s]ijâˆˆNâˆ’i[k]Î·g[(][W]i([k]x[)][r,s]i[i,j][)][x]r,sj + g[1]2i over a mini-batch Î¾i sampled fromâ–· gossip averagingâ–· local update Di

9: **end for**

10: **end for** [P]

11: **end for**

12: **for each cluster k âˆˆ** [K] do

13: _m â†_ max(p Â· n, 1)

14: _Sk[r]_ _â–·_ device sampling

15: **end for[â†]** [(][random set of][ m][ clients in][ V][k][)]

16: _x[r][+1]_ = _K[1]_ _kâˆˆ[K]_ _m1_ _iâˆˆSk[r]_ _[x]i[r,Ï„]_ _â–·_ global aggregation

17: end for

P P

18: return x[R]

link communication is much more efficient than uplink communication due to the larger bandwidth
allocation and higher data rate. Therefore, devices only consume a smaller amount of energy when
receiving data from the server compared with transmitting data to the server.

After that, devices in each cluster initialize their local models to be the received global model and
run Ï„ iterations of gossip-based SGD via D2D links to update their local models in parallel (lines 5â€“
9). Let x[r,s]i denote the local model of device i at the r-th local iteration of s-th round. Here
each gossip-based SGD iteration consists of two steps: (i) SGD update, performed locally on each
device (lines 6â€“7), followed by a (ii) gossip averaging, where devices average their models with
their neighbors (line 8). In the gossip averaging protocol, Ni[k] [denotes the neighbors of device][ i][,]
including itself, on the D2D communication graphthe mixing matrix of cluster k with each element (W Gkk) of clusteri,j being the weight assigned by device k, and Wk âˆˆ [0, 1][n][Ã—][n] denotes i to
device j. Note that (Wk)i,j > 0 only if devices i and j are directly connected via D2D links.

Next, a set Sk[r] [of][ m][ devices are sampled uniformly at random (u.a.r.) with probability][ p][ without]
replacement from each cluster k âˆˆ [K] by the server (lines 13â€“14), and their final updated local
models {x[r,Ï„]i _, âˆ€i âˆˆ_ _Sk[r][}][ are sent to the server via D2S links. After that, the server updates the global]_
model x[r][+1] by averaging the received local models from all sampled devices (line 16). Note that
only m devices per cluster will upload their models to the server in each round to save the usage of
expensive D2S uplink transmissions. The intuition is that after multiple iterations of gossip-based
SGD, devices have already reached approximate consensus within each cluster, and the sampled
average can well represent the true average. By trading D2D local aggregation for D2S global
aggregation, the total communication cost can be reduced. We will empirically validate such benefits
later in the experiments.

It is worth noting that HL-SGD inherits the privacy benefits of classic FL schemes by keeping the
raw data on device and sharing only model parameters. Moreover, HL-SGD is compatible with
existing privacy-preserving techniques in FL such as secure aggregation (Bonawitz et al., 2017; Guo
& Gong, 2018), differential privacy (McMahan et al., 2018; Hu et al., 2020; 2021), and shuffling
(Girgis et al., 2021) since only the sum rather than individual values is needed for the local and
global model aggregation steps.

**Runtime analysis of HL-SGD. We now present a runtime analysis of HL-SGD. Here we ignore**
the communication time of downloading models from the server by each device since the download
bandwidth is often much larger than upload bandwidth for the D2S communication in practice (?).
In each round of HL-SGD, we denote the average time taken by a device to compute a local update,


-----

perform one round of D2D communication and one round of D2S communication as ccp, cd2d and
_cd2s, respectively. Assume the uplink bandwidth between the server and devices is fixed and evenly_
shared among the sampled devices in each round, then cd2s is linearly proportional to the sampling
ratio p. Similarly, ccp depends on the D2D network topology Gk and typically increases with the
maximum node degree âˆ†( _k). The total runtime of HL-SGD after R communication rounds is_
_G_

_R Ã— [Ï„ Ã— (ccp + cd2d) + cd2s] ._ (2)

The specific values of ccp, cd2d and cd2s depend on the system configurations and applications. In
comparison, the total runtime of local SGD after R communication rounds is R Ã— [Ï„ Ã— ccp + cd2s].

**Previous algorithms as special cases. When devices do not communicate with each other, i.e.,**
_Wk = I, âˆ€k âˆˆ_ [K], and sampling ratio p = 1, HL-SGD reduces to distributed SGD (when Ï„ = 1)
or local SGD (when Ï„ > 1) where each device only directly communicates with the server with D2S
links. Also, when Ï„ â†’âˆ, HL-SGD reduces to gossip SGD where devices only cooperate with their
neighboring devices through a gossip-based communication protocol with D2D links to update their
models without relying on the server. Therefore, HL-SGD subsumes existing algorithms and enables
us to strike the best balance between runtime and model accuracy by tuning Ï„, Wk, and p. However,
due to the generality of HL-SGD, there exist significantly new challenges in its convergence analysis,
which constitutes one of the main contributions of this paper as elaborated in the following section.

5 CONVERGENCE ANALYSIS OF HL-SGD

In this section, we analyze the convergence of HL-SGD with respect to the gradient norm of the
objective function f (Â·), specifically highlighting the effects of Ï„ and p. Before stating our results,
we make the following assumptions:

**Assumption 1 (Smoothness). Each local objective function fi : R[d]** _â†’_ R is L-smooth for all i âˆˆV,
_i.e., for all x, y âˆˆ_ R[d],
_fi(x)_ _fi(y)_ _L_ _x_ _y_ _,_ _i_ _._
_âˆ¥âˆ‡_ _âˆ’âˆ‡_ _âˆ¥â‰¤_ _âˆ¥_ _âˆ’_ _âˆ¥_ _âˆ€_ _âˆˆV_

**Assumption 2 (Unbiased Gradient and Bounded Variance). The local mini-batch stochastic gra-**
_dient in Algorithm 1 is unbiased, i.e., EÎ¾i_ [gi(x)] = âˆ‡fi(x), and has bounded variance, i.e.,
EÎ¾i _âˆ¥gi(x) âˆ’âˆ‡fi(x)âˆ¥[2]_ _â‰¤_ _Ïƒ[2], âˆ€x âˆˆ_ R[d], i âˆˆV, where the expectation is over all the local mini_batches._

**Assumption 3 (Mixing Matrix). For any cluster k âˆˆ** [K], the D2D network is strongly connected
_We also assumeand the mixing matrix ||Wk âˆ’ W(1k âˆˆ/n)[011, 1][âŠ¤][n]||[Ã—]2[n] â‰¤satisfiesÏk for some Wk1 Ï =k 1 âˆˆ, 1[0[âŠ¤], 1)W.k = 1[âŠ¤], null(I âˆ’_ _Wk) = span(1)._

**Assumption 4(1/n)** _iâˆˆVk_ (Bounded Intra-Cluster Dissimilarity)fk(x)âˆ¥[2] _â‰¤_ _Ïµ[2]k_ _[for any][ x][ âˆˆ]. There exists a constant[R][d][ and][ k][ âˆˆ]_ [[][K][]][. If local functions are] Ïµk â‰¥ 0 such that
_identical to each other within a cluster, then we have[âˆ¥âˆ‡][f][i][(][x][)][ âˆ’âˆ‡]_ [Â¯] _Ïµk = 0._

**Assumption 5[P]** (Bounded Inter-Cluster Dissimilarity). There exist constants Î± â‰¥ 1, Ïµ â‰¥ 0 such that
(1each other across all clusters, then we have/K) _kâˆˆ[K]_ _[âˆ¥âˆ‡]f[Â¯]k(x)âˆ¥[2]_ _â‰¤_ _Î±[2]_ _âˆ¥âˆ‡f_ (x)âˆ¥[2] Î±+ = 1 Ïµ[2]g _[for any], Ïµg = 0[ x].[ âˆˆ]_ [R][d][. If local functions are identical to]

[P]

Assumptions 1â€“3 are standard in the analysis of SGD and decentralized optimization (Bottou et al.,
2018; Koloskova et al., 2019). Assumptions 4â€“5 are commonly used in the federated optimization
literature to capture the dissimilarities of local objectives (Koloskova et al., 2020; Wang et al., 2020).

5.1 MAIN RESULTS

We now provide the main theoretical results of the paper in Theorem 1 and Theorem 2. The detailed
proofs are provided in the appendices. Define the following constants:


1

_, Ï„_ _,_ _ÏµÂ¯[2]L_ [= 1]
1 _Ïmax_ _K_
_âˆ’_ 


_Ïµ[2]k_ (3)
_k=1_

X


_Ïmax = max_ _DÏ„,Ï = min_
_k_ [K] _[Ï][k][,]_
_âˆˆ_


-----

and let

_Ïƒ2_
_r0 = 8(f_ (x[0]) _f_ (x[â‹†])), r1 = 16L _,_
_âˆ’_ _N_
  1 (4)

_r2 = 16C1L[2]Ï„_ [2]Ïµ[2]g [+ 16][C][1][L][2] _Ï„Ï[2]max[D][Ï„,Ï]Ïµ[Â¯][2]L_ [+][ Ï„Ïƒ][2] max _._

_n_ [+][ Ï][2]

  

**Theorem 1 (Full device participation). Let Assumptions 1â€“5 hold, and let L, Ïƒ, Â¯ÏµL, Ïµg, DÏ„,Ï, Ïmax,**
_r0, r1, and r2 be as defined therein. If the learning rate Î· satisfies_


1

2




1

3




1 _r0_

4C1Î± _Ï„L_ _[,]_ _r1Ï„R_

_[Â·][ 1]_ 


_r0_

_r2Ï„R_


(5)


_Î· = min_


_then for any R > 0, the iterates of Algorithm 1 with full device participation for HL-SGD satisfy_


 Ï„ [2]Ïµ[2]g [+][ Ï„Ï]max[2] _[D]Ï„,Ï(Ïµ[Â¯]Ï„R[2]L_ [+]) 23[ Ï„]   1n [+][ Ï]max[2]  _Ïƒ[2][][ 1]3_ + R[1]


min _x[r,s])_ = O
_r,s_ [E][âˆ¥âˆ‡][f] [(Â¯] _âˆ¥[2]_


_,_ (6)

ï£¶

ï£¸


_NÏ„R_


_where Â¯x[r,s]_ = _N[1]_ _Ni=1_ _[x]i[r,s][.]_

In the following, we analyze the iteration complexity of HL-SGD and compare it with those ofP
some classic and state-of-the-art algorithms relevant to our setting in Table 1. First, we consider
two extreme cases of HL-SGD where Ïmax = 0 and Ïk = 1, âˆ€k âˆˆ [K], and show that our analysis
recovers the best known rate of local SGD.

**Fully Connected D2D networks. In this case, Ïmax = 0, and each cluster can be viewed as a single**
device, and thus HL-SGD reduces to local SGD with K devices. Substuting Ïmax = 0 into (6),

1/3 2/3

the iteration complexity of HL-SGD reduces to O(Ïƒ/âˆšNÏ„R + _Ï„_ [2]Ïµ[2]g [+][ Ï„][ Â·][ (][Ïƒ][2][/n][)] _/(Ï„R)_ +

1/R). This coincides with the complexity of local SGD provided in Table 1 with device number K
  
and stochastic gradient variance Ïƒ[2]/n thanks to the fully intra-cluster averaging.

**Disconnected D2D networks.** In this case, HL-SGD reduces to local SGD with N devices.
Substituting Ïmax = 1 into (6), the iteration complexity of HL-SGD becomes O(Ïƒ/âˆšNÏ„R +

_Ï„_ [2](Ïµ[2]g [+ Â¯]Ïµ[2]L[) +][ Ï„Ïƒ][2][][1][/][3][ /][(][Ï„R][)][2][/][3][ + 1][/R][)][. This coincides with the complexity of local SGD with]
_N devices, stochastic gradient variance Ïƒ[2], and gradient heterogeneity of order Ïµ[2]g_ [+ Â¯]Ïµ[2]L[.]
 


Table 1: Comparison of Iteration Complexity. [2]


_Ïƒ_

_NÏ„R_ [+ (][Ï„][ 2][Ïµ]([2]Ï„R[+][Ï„Ïƒ]) 23 [2][)]


Local SGD

Gossip SGD

Gossip PGA (Chen et al., 2021)

HL-SGD (this work)


_Ï„R_


_Ïƒ_ _Ï_ 3 Ïµ 3 _Ï_ 3 Ïƒ 3

_NÏ„R_ [+] (Ï„R1 ) 23 (11 _âˆ’Ï)_ 23 [+] (Ï„R) 32 (1âˆ’Ï) 13 [+]


(1âˆ’Ï)Ï„R


_NÏ„RÏƒ_ [+] _CÏ„,Ï3_ _[D](Ï„RÏ„,Ï3_ )[â€²]23[Ï] 3 Ïµ 3 + _[C]Ï„,Ï(13Ï„R[Ï]_ )32 Ïƒ23 32 + _ÏDÏ„RÏ„,Ïâ€²_

!

1

_Ïƒ_ _g[+][Ï„Ï]max[2]_ _[D][Ï„,Ï]Ïµ[Â¯][2]L[)]_ 3 _n_ [+][Ï]max[2] [)][Ïƒ][2][)]

_NÏ„R_ [+ (][Ï„][ 2][Ïµ][2] (Ï„R) 23 + [(][Ï„][(][ 1] (Ï„R) 23


_Ï„R_


Next, we compare the complexities of HL-SGD, local SGD, gossip SGD and gossip PGA.

**Comparison to Local SGD. Comparing (6) and the complexity of local SGD, we can see the intra-**
cluster D2D communication provably improves the iteration complexity by reducing the transient
iterations. This is reflected in the smaller coefficient associated with the O((Ï„R)[âˆ’][2][/][3]) term. In particular, improving D2D communication connectivity will lead to a smaller Ïmax and consequently,
mitigate the impact of both local data heterogeneity and stochastic noise on the convergence rate.

1The convergence rates for gossip SGD and local SGD are from (Koloskova et al. (2020)). The parameters
in the table are given by the following: Ïƒ[2]: stochastic gradient variance; Ï: network connectivity; Ïµ[2]: data
heterogeneity of order Ïµ[2]g [+ Â¯]Ïµ[2]L[;][ C][Ï„,Ï] [â‰œ] [P][Ï„]k=0[âˆ’][1] _[Ï][k][,][ D][Ï„,Ï][â€²][ = min][{][1][/][(1][ âˆ’]_ _[Ï][)][, Ï„]_ _[}][. Note that][ D][Ï„,Ï][ Ì¸][=][ D][Ï„,Ï][â€²]_ [.]


-----

**Comparison to Gossip SGD. Under the condition that Ï = Ïmax, i.e., the connectivity of D2D**
network in gossip SGD is the same as that of HL-SGD, Table 1 shows HL-SGD outperforms gossip
SGD when Ï„/n â‰¤ _Ï[2]/(1âˆ’Ï). In other words, HL-SGD is beneficial for weakly connected networks,_
which is the case in FL settings where a large number of devices are often loosely connected or
disconnected into several disjoint clusters via D2D communications only.

**Comparison to Gossip PGA. Gossip PGA improves local SGD by integrating gossiping among all**
devices in one round using a connected network. Compared to gossip SGD, gossip PGA has one
extra full averaging step with period Ï„ . The complexity of gossip PGA improves both by reducing the transient iterations. HL-SGD (full participation) differs from gossip PGA in the sense that
gossiping is performed within multiple clusters instead of a single one. The benefit comes from the
fact that for many commonly used D2D network topologies, the spectral gap 1 âˆ’ _Ï decreases as the_
network size decreases, see Table 2. Therefore, when employing the same D2D network topology,
HL-SGD enjoys a smaller connectivity number Ïmax than Ï. Considering the scenario where Ï„ and
_n are fixed while the cluster number K grows, the total device number N = nK grows and hence_
_Ï_ 1 for gossip PGA. In the case when Ï„ = DÏ„,Ï[â€²] _CÏ„,Ï, the fastest decaying O(1/Ï„R) terms_
_â†’_ _â‰ˆ_
are comparable for both algorithms. However, the O((Ï„R)[âˆ’][2][/][3]) term of gossip GPA can be larger
than that of HL-SGD since Ï increases with N . This observation shows for large-scale networks,
it is advantageous to use HL-SGD with multiple connected clusters instead of gossip GPA with a
single cluster under the D2D network topology.

Our next result shows the iteration complexity of HL-SGD with partial device participation. We
assume the devices participate in synchronizing their models at the end of each FL round following
the sampling rule given by Assumption 6.
**Assumption 6 (Sampling strategy). Each Sk[r]** _[contains a subset of][ m][ indices uniformly sampled from]_
_{1, . . ., n} without replacement. Furthermore, Sk[r]_ _[is independent of][ S]k[r][â€²][â€²][ for all][ (][k, r][)][ Ì¸][= (][k][â€²][, r][â€²][)][.]_

**Theorem 2 (Partial device participation). Let Assumptions 1â€“6 hold, and let L, Ïƒ, Â¯ÏµL, Ïµg, DÏ„,Ï,**
_Ïmax, r0, r1, and r2 be as defined therein. If the network connectivity satisfies_
_Ïmax_ 1 1/Ï„, (7)
_â‰¤_ _âˆ’_
_then for suitably chosen learning rate Î·, the iterates of Algorithm 1 with partial device participation_
_for HL-SGD satisfy_

min _x[r,s])_
_r,s_ [E][âˆ¥âˆ‡][f] [(Â¯] _âˆ¥[2]_

_ÏµL, Ïƒ, Ïmax)_ _Ï„_ [2]Ïµ[2]g [+][ Ï„Ï]max[2] _[D]Ï„,ÏÏµ[Â¯][2]L_ [+][Ï„] 1n [+][ Ï]max[2] _Ïƒ[2][][ 1]3_

=Oï£« _âˆšNÏ„R_ +   (Ï„R) 23    + [max][{][1][, G][p]R[D][Ï„,Ï][Ï][max][}] ï£¶ _,_

ï£­ _[Ïƒ][ +][ E][(][Ïµ][g][,][ Â¯]_ ï£¸(8)

_where Â¯x[r,s]_ = _N[1]_ _Ni=1_ _[x]i[r,s][,]_

(Ïµg, Â¯ÏµL, Ïƒ, ÏPmax) = (Ïµ[2]g[D][Ï„,Ï] [+][ Ï][max][D][Ï„,Ï]Ïµ[Â¯][2]L [+][ Ïƒ][2][)][ Â·][ G]p[â€²] _[D][Ï„,Ï][Ï][max][N][ +][ n]_ max[Ïƒ][2][.] (9)
_E_ [2] _m_ _[Â·][ 1]Ï„ [Ï][2]_

_and_

_n_ _m_
_Gp =_ _m( âˆ’n âˆ’_ 1) _[,]_ _G[â€²]p_ [=][ G][p] [+ 1]Ï„ [2][ .] (10)

Compared to Theorem 1, Theorem 2 shows partial device participation deteriorates the rate
by O( (Ïµg, Â¯ÏµL, Ïƒ, Ïmax)/âˆšNÏ„R). From the expression of, we observe that as Ïmax 0,

(Ïµg, Â¯ÏµEL, Ïƒ, Ïmax) vanishes, which indicates that the loss caused by device sampling can be com- E _â†’_
_E_
pensated by increasing network connectivity uniformly for all clusters.

The next corollary finds the critial Ïmax so that E [2] = O(1), and the order of convergence rate of
partial device participation matches that of the full participation case.
**Corollary 1. Under the same assumptions as Theorem 2, if the network connectivity satisfies**


1

(11)
4N [min][{][m, Ï„][ âˆ’] [1][}][.]


_Ïmax_
_â‰¤_

ï£« _âˆšNÏ„RÏµL_ +

ï£­ _[Ïƒ][ +][ Ïµ][g][ + Â¯]_


_then_

min _x[r,s])_ = O
_r,s_ [E][âˆ¥âˆ‡][f] [(Â¯] _âˆ¥[2]_


_Ï„_ [2]Ïµ[2]g [+][ Ï„Ï]max[2] _[D]Ï„,Ï(Ïµ[Â¯]Ï„R[2]L_ [+]) 23[ Ï„]   1n [+][ Ï]max[2]  _Ïƒ[2][][ 1]3_ + R[1]


_._ (12)

ï£¶

ï£¸


-----

Corollary 1 reveals the tradeoff between sampling intensity and network connectivity. More connected D2D networks result in smaller Ïmax, and thus (11) can be satisfied by a smaller m. This
means we can sample fewer devices at the end of each round and reduce the D2S communication
delay when the D2D network is more connected.

6 EXPERIMENTAL EVALUATION

6.1 EXPERIMENTAL SETTINGS

We use two common datasets in FL literature (McMahan et al., 2017; Reddi et al., 2021; Wang et al.,
2020): Federated Extended MNIST (Caldas et al., 2019) (FEMNIST) and CIFAR-10 (Krizhevsky
et al., 2009). The 62-class FEMNIST is built by partitioning the data in Extended MNIST (Cohen
et al., 2017) based on the writer of the digit/character and has a naturally-arising device partitioning. CIFAR-10 is partitioned across all devices using a Dirichlet distribution Dir(0.1) as done in
(Hsu et al., 2019; Yurochkin et al., 2019; Reddi et al., 2021; Wang et al., 2020). We evaluate our
algorithms by training CNNs on both datasets, and the CNN models for FEMNIST and CIFAR-10
were taken from (Caldas et al., 2019) and (McMahan et al., 2017) with around 6.5 and 1 million
parameters, respectively. For each dataset, the original testing set (without partitioning) is used to
evaluate the generalization performances of the trained global model.

We consider a FL system consisting of a central server and 32 devices. The devices are evenly
divided into four clusters, and each cluster has a ring topology by default, which provides a conservative estimation for the cluster connectivity and convergence speed. In our experiments, the
mixing matrix of each cluster Wk is set according to the Metropolis-Hastings weights (NediÂ´c et al.,
2018). According to the real-world measurements in (Yuan et al., 2020; Yang et al., 2021), we set
the average time for a device to perform a local update, a round of D2D communication under ring
topology, and a round of D2S communication with one device sampled per cluster to be ccp = 0.01h,
_cd2d(âˆ†= 2) = 0.005h and cd2s(p = 1/8) = 0.05h, respectively, in the runtime model (2). For ar-_
bitrary device sampling ratio and D2D network topology, we consider a linear-scaling rule (Wang
et al., 2019) and let cd2d(âˆ†) = (âˆ†/2) Ã— 0.005h and cd2s(p) = 8p Ã— 0.05h.

We compare HL-SGD with local SGD in the experiments. For local SGD, devices will only communicate with the central server periodically. In all experiments, we let the local iteration period
_Ï„ to be the same for both local SGD and HL-SGD to have a fair comparison. On the FEMNIST_
dataset, we fix the batch size as 30 and tune the learning rate Î· from {0.005, 0.01, 0.02, 0.05, 0.08}
for each algorithm separately. On the CIFAR-10 dataset, we fix the batch size as 50 and tune Î· from
_{0.01, 0.02, 0.05, 0.08, 0.1} for each algorithm separately. We run each experiment with 3 random_
seeds and report the average. All experiments in this paper are conducted on a Linux server with 4
NVIDIA RTX 8000 GPUs. The algorithms are implemented by PyTorch. More details are provided
in Appendix F.

6.2 EXPERIMENTAL RESULTS

(a) FEMNIST (b) FEMNIST (c) CIFAR-10 (d) CIFAR-10

Figure 1: Convergence rate and runtime comparisons of HL-SGD and local SGD under ring topology when
_Ï„ = 50 and p = 1 for FEMNIST and CIFAR-10 datasets. (a) and (c) show how the accuracy changes over_
communication round; (b) and (d) show how the accuracy changes over runtime.

We first compare the convergence speed and runtime of HL-SGD and local SGD while fixing Ï„ = 50
and p = 1. We measure the best test accuracy of the global model on the server in every FL round.
Figure 1 shows the convergence process. From the figure, we can observe that HL-SGD can largely
accelerate the model convergence while improving model accuracy in FL. On FEMNIST, the best
accuracy of HL-SGD achieved over 100 rounds is 4.78% higher than that of local SGD (i.e., 83.76%


-----

vs. 79.94%), and its runtime necessary to achieve a target test accuracy of 75% is only 17.64% of
that of the baseline (i.e., 5.67Ã— speedup). On CIFAR-10, the best accuracy of HL-SGD achieved
over 100 rounds is 9.32% higher than that of local SGD (i.e., 68.71% vs. 63.68%), and its runtime
necessary to achieve a target test accuracy of 60% is 15.67% less than that of local SGD (i.e., 1.186Ã—
speedup).

Figure 2: Convergence rate (left) and runtime (right) comparisons of HL-SGD and local SGD on CIFAR-10
under different Ï„ and ring topology when p = 1.

Next, to give a more comprehensive analysis on the runtime benefits of HL-SGD, we vary Ï„ from
_{5, 10, 20, 50} and compare the performances of HL-SGD and local SGD on CIFAR-10 in Figure 2._
From the figure, we can observe that HL-SGD can consistently outperform local SGD across a wide
range of Ï„ . In particular, on CIFAR-10, the best accuracy of HL-SGD achieved over 100 rounds is
2.49%, 3.99%, 4.05%, and 7% higher than that of local SGD, respectively, as Ï„ increases from 5 to
50. At the same time, the runtime of HL-SGD needed to achieve a target test accuracy of 60% is
9.66%, 19.76%, 33.46%, and 45.88% less than that of local SGD, respectively.

(a) FEMNIST (b) FEMNIST (c) CIFAR-10 (d) CIFAR-10

Figure 3: Effect of sampling ratio p on the convergence rate and runtime of HL-SGD under ring topology when
_Ï„ = 50 for FEMNIST and CIFAR-10 datasets. (a) and (c) show how the accuracy changes over communication_
round in HL-SGD; (b) and (d) show how the accuracy changes over runtime.

Finally, we investigate how the sampling ratio p affects the performance of HL-SGD. We select
_p from {0.125, 0.25, 0.5, 1}, corresponding to sampling {1, 2, 4, 8} devices from each cluster to_
upload models to the server. Figure 3 depicts the best value of test accuracy achieved over all prior
rounds. As can be observed from the figures, sampling one device per cluster only results in slightly
lower model accuracy, e.g., neligible and 1.92% drop compared to full participation on FEMNIST
and CIFAR-10, respectively. This matches the theoretical result in Corollary 1 that device sampling
does not affect the order of convergence rate under certain conditions. However, decreasing p can
lead to faster training speed due to its shorter D2S communication delay as observed in Figures 3b
and 3d. In practice, the optimal value of p needs to be tuned to strike a good balance between model
accuracy and runtime.

7 CONCLUSION

In this paper, we have proposed a new optimization algorithm called HL-SGD for FL with heterogeneous communications. Our algorithm leverages the D2D communication capabilities among edge
device to accelerate the model convergence while improving model accuracy in FL. We have provided the theoretical convergence analysis of HL-SGD and conducted experiments to demonstrate
the benefits of HL-SGD. In the future, we plan to extend HL-SGD to handle straggler issues under
device heterogeneity and provide rigorous privacy protection for HL-SGD.


-----

ACKNOWLEDGMENTS

The work of Y. Guo was supported in part by NSF under the Grant CNS-2106761. The work of Y.
Sun was partially supported by the Office of Naval Research under the Grant N00014-21-1-2673.
The work of R. Hu and Y. Gong was supported in part by NSF under the Grants CNS-2047761 and
CNS-2106761.

REFERENCES

Mehdi Salehi Heydar Abad, Emre Ozfatura, Deniz Gunduz, and Ozgur Ercetin. Hierarchical federated learning across heterogeneous cellular networks. In IEEE International Conference on
_Acoustics, Speech and Signal Processing (ICASSP), pp. 8866â€“8870. IEEE, 2020._

Arash Asadi, Qing Wang, and Vincenzo Mancuso. A survey on device-to-device communication in
cellular networks. IEEE Communications Surveys & Tutorials, 16(4):1801â€“1819, 2014.

Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacypreserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer
_and Communications Security, pp. 1175â€“1191, 2017._

LÂ´eon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223â€“311, 2018.

Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms.
_IEEE Transactions on Information Theory, 52(6):2508â€“2530, 2006._

Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub KoneË‡cn`y, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. LEAF: A benchmark for federated settings. In Work_shop on Federated Learning for Data Privacy and Confidentiality, 2019._

Timothy Castiglia, Anirban Das, and Stacy Patterson. Multi-Level Local SGD: Distributed SGD for
heterogeneous hierarchical networks. In International Conference on Learning Representations,
2021.

Yiming Chen, Kun Yuan, Yingya Zhang, Pan Pan, Yinghui Xu, and Wotao Yin. Accelerating gossip
SGD with periodic global averaging. In Proceedings of the 38th International Conference on
_Machine Learning, pp. 1791â€“1802. PMLR, 2021._

Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. EMNIST: Extending
MNIST to handwritten letters. In 2017 International Joint Conference on Neural Networks
_(IJCNN), pp. 2921â€“2926. IEEE, 2017._

Antonious Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and Ananda Theertha Suresh. Shuffled model of differential privacy in federated learning. In International Conference on Artificial
_Intelligence and Statistics, pp. 2521â€“2529. PMLR, 2021._

Yuanxiong Guo and Yanmin Gong. Practical collaborative learning for crowdsensing in the internet
of things with differential privacy. In 2018 IEEE Conference on Communications and Network
_Security (CNS), pp. 1â€“9. IEEE, 2018._

Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Local
SGD with periodic averaging: Tighter analysis and adaptive synchronization. In Advances in
_Neural Information Processing Systems, pp. 11082â€“11094, 2019._

Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.

Rui Hu, Yuanxiong Guo, Hongning Li, Qingqi Pei, and Yanmin Gong. Personalized federated
learning with differential privacy. IEEE Internet of Things Journal, 7(10):9530â€“9539, 2020.

Rui Hu, Yanmin Gong, and Yuanxiong Guo. Federated learning with sparsification-amplified privacy and adaptive optimization. In Proceedings of the Thirtieth International Joint Conference
_on Artificial Intelligence, IJCAI-21, pp. 1463â€“1469, 2021._


-----

Peter Kairouz, H Brendan McMahan, Brendan Avent, AurÂ´elien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and TrendsÂ® in Machine Learning,
14(1â€“2):1â€“210, 2021.

Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning.
In International Conference on Machine Learning, pp. 5132â€“5143. PMLR, 2020.

Anastasia Koloskova, Sebastian Stich, and Martin Jaggi. Decentralized stochastic optimization and
gossip algorithms with compressed communication. In International Conference on Machine
_Learning, pp. 3478â€“3487, 2019._

Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified
theory of decentralized SGD with changing topology and local updates. In International Confer_ence on Machine Learning, pp. 5381â€“5393. PMLR, 2020._

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.

Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Donâ€™t use large mini-batches,
use local SGD. In International Conference on Learning Representations, 2019.

Lumin Liu, Jun Zhang, SH Song, and Khaled B Letaief. Client-edge-cloud hierarchical federated
learning. In IEEE International Conference on Communications, pp. 1â€“6. IEEE, 2020.

Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelli_gence and statistics, pp. 1273â€“1282. PMLR, 2017._

H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private
recurrent language models. In International Conference on Learning Representations, 2018.

Angelia NediÂ´c, Alex Olshevsky, and Michael G Rabbat. Network topology and communicationcomputation tradeoffs in decentralized optimization. Proceedings of the IEEE, 106(5):953â€“976,
2018.

Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub KoneË‡cnÂ´y,
Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International
_Conference on Learning Representations, 2021._

Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica, Vladimir Braverman,
Joseph Gonzalez, and Raman Arora. FetchSGD: Communication-efficient federated learning
with sketching. In International Conference on Machine Learning, pp. 8253â€“8265. PMLR, 2020.

Sebastian Urban Stich. Local SGD converges fast and communicates little. In International Con_ference on Learning Representations, 2019._

John Nikolas Tsitsiklis. Problems in decentralized decision making and computation. Technical
report, Massachusetts Inst of Tech Cambridge Lab for Information and Decision Systems, 1984.

Jianyu Wang and Gauri Joshi. Cooperative SGD: A unified framework for the design and analysis
of local-update SGD algorithms. Journal of Machine Learning Research, 22(213):1â€“50, 2021.

Jianyu Wang, Anit Kumar Sahu, Zhouyi Yang, Gauri Joshi, and Soummya Kar. MATCHA: Speeding up decentralized SGD via matching decomposition sampling. In 2019 Sixth Indian Control
_Conference, pp. 299â€“300. IEEE, 2019._

Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective
inconsistency problem in heterogeneous federated optimization. Advances in Neural Information
_Processing Systems, 33:7611â€“7623, 2020._

Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021.


-----

Chengxu Yang, Qipeng Wang, Mengwei Xu, Zhenpeng Chen, Kaigui Bian, Yunxin Liu, and Xuanzhe Liu. Characterizing impacts of heterogeneity in federated learning upon large-scale smartphone data. In Proceedings of the Web Conference 2021, pp. 935â€“946, 2021.

Bicheng Ying, Kun Yuan, Yiming Chen, Hanbin Hu, Pan Pan, and Wotao Yin. Exponential graph
is provably efficient for decentralized deep training. Advances in Neural Information Processing
_Systems, 34, 2021._

Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted SGD with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
_the AAAI Conference on Artificial Intelligence, volume 33, pp. 5693â€“5700, 2019._

Zhe Yu, Yanmin Gong, Shimin Gong, and Yuanxiong Guo. Joint task offloading and resource
allocation in UAV-enabled mobile edge computing. IEEE Internet of Things Journal, 7(4):3147â€“
3159, 2020.

Jinliang Yuan, Mengwei Xu, Xiao Ma, Ao Zhou, Xuanzhe Liu, and Shangguang Wang. Hierarchical
federated learning through LAN-WAN orchestration. arXiv preprint arXiv:2010.11612, 2020.

Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and
Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In Interna_tional Conference on Machine Learning, pp. 7252â€“7261. PMLR, 2019._

Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.

A PRELIMINARIES

**Intra-cluster dynamics.R[n][Ã—][d]** constructed by stacking respectively To facilitate the analysis, we introduce matrices xi and gi for i âˆˆVk row-wise. Similarly, we define Xk âˆˆ R[n][Ã—][d] and Gk âˆˆ
the pseudo-gradient âˆ‡Fk(Xk) âˆˆ R[n][Ã—][d] associated to cluster k by stacking âˆ‡fi(xi) for i âˆˆVk
row-wise. In addition, define the following intra-cluster averages for each cluster k:

_xÂ¯k â‰œ_ [1] _xi_ and _gÂ¯k â‰œ_ [1] _gi._ (13)

_n_ _n_

_iXâˆˆVk_ _iXâˆˆVk_

The update within each cluster then can be written compactly in matrix form as

_Xk[r,s][+1]_ = Wk(Xk[r,s] _âˆ’_ _Î·G[r,s]k_ [)][,] _âˆ€k = 1, . . ., K._ (14)

Since each Wk is bi-stochastic, we obtain the following update of the intra-cluster average

_xÂ¯[r,s]k_ [+1] = Â¯x[r,s]k _âˆ’_ _Î· Â· Â¯gk[r,s][.]_ (15)

We proceed to derive the update of the intra-cluster consensus error. Define the averaging matrix

_J = [1]_ with **1 = [1, . . ., 1].** (16)

_n_ **[1][ Â·][ 1][âŠ¤]**
_n_

Multiplying both sides of (14) from the left by (I _J) leads to the following update of the consensus_

| {z }

_âˆ’_
error:


(I _J)Xk[r,s][+1]_
_âˆ’_

_Xk,[r,s]âŠ¥[+1]_

| {z }


= (I âˆ’ _J)Wk(Xk[r,s]_ _âˆ’_ _Î·G[r,s]k_ [)]

= (Wk âˆ’ _J)(Xk[r,s]_ _âˆ’_ _Î·G[r,s]k_ [)]

= (Wk _J)(Xk,[r,s]_ _k_ [)][.]
_âˆ’_ _âŠ¥_ _[âˆ’]_ _[Î·G][r,s]_


(17)


**Global average dynamics. Define the global average among all xiâ€™s as**


_xÂ¯ â‰œ_ [1]


_xi._ (18)
_i=1_

X


-----

Then accordingly to (15) we have the following update of Â¯x for all s = 0, . . ., Ï„ âˆ’ 1:


_xÂ¯[r,s][+1]_ = [1]

_N_

= [1]


_K_

_nxÂ¯[r,s]k_ [+1] = N[1]
_k=1_

X


_n (Â¯x[r,s]k_ _âˆ’_ _Î·gÂ¯k[r,s][)]_
_k=1_

X


(19)


_N_

(x[r,s]i _Î·gi[r,s][) = Â¯]x[r,s]_ _Î·_ [1]
_âˆ’_ _âˆ’_ _N_
_i=1_

X


_gi[r,s][.]_
_i=1_

X


**Filtration. Let G = [G1; . . . ; GK] âˆˆ** R[N] _[Ã—][d]_ be the matrix constructed by stacking all the stochastic
gradients. We introduce the following filtration

_r,s = Ïƒ_ _G[0][,][0], . . ., G[0][,Ï„]_ _[âˆ’][1], S0, G[1][,][0], . . ., G[1][,Ï„]_ _[âˆ’][1], . . ., Sr_ 1, G[r,][0], . . ., G[r,s][]
_F_ _âˆ’_ (20)

_Fr = Ïƒ_  G[0][,][0], . . ., G[0][,Ï„] _[âˆ’][1], S0, G[1][,][0], . . ., G[1][,Ï„]_ _[âˆ’][1], . . ., Srâˆ’1_ _._
  

Therefore we have x[r,]i [0] = x[r] _âˆˆFr for r â‰¥_ 1, and x[r,s]i _âˆˆFr,sâˆ’1 for 1 â‰¤_ _s â‰¤_ _Ï„_ . For simplicity
the conditional expectation E( Â· |Fr,s) is denoted as Er,s, and we define the noise in the stochastic
gradient as

_Î¾i[r,s]_ â‰œ _gi[r,s]_ _âˆ’âˆ‡fi(x[r,s]i_ [)][.] (21)

Since at the end of round r all nodes are picked with equal probability, the sampling procedure
preserves average in expectation:

Er,Ï„ 2x[r][+1] = E(E(x[r][+1] _r,Ï„_ 1) _r,Ï„_ 2)
_âˆ’_ _|F_ _âˆ’_ _|F_ _âˆ’_


ï£« _K_

ï£­ [1]


E

ï£«

ï£­


_x[r,Ï„]i_ _|Fr,Ï„_ _âˆ’1ï£¶_ _|Fr,Ï„_ _âˆ’2_
_iXâˆˆSk[r]_

ï£¸

I(i âˆˆ _Sk[r][)][x][r,Ï„]i_ _|Fr,Ï„_ _âˆ’1_
_iXâˆˆVk_


= E

= E


_k=1_

_K_

_k=1_

X


(22)


_|Fr,Ï„_ _âˆ’2_


= Er,Ï„ 2(Â¯x[r,Ï„] )
_âˆ’_


where the last equality holds since P _i âˆˆ_ _Sk[r][|][i][ âˆˆV][k]_ = _[m]n_ [.]
  

B CONVERGENCE ANALYSIS

To prove the convergence we first establish in Sec. B.1 that the objective value Ef (x[r]) is descending
at each round r, up to some consensus error terms. Subsequently, bounds on the error terms are
provided in Sec. B.2-B.4. Based on these results, the proof of convergence of Algorithm 1 with full
and partial device participation are given in Sec. B.5 and B.6, respectively. The proofs of the main
propositions are given in Sec. C and that of the supporting lemmas are deferred to Sec. D.


-----

B.1 OBJECTIVE DESCENT

**Lemma 1. Let** _x[r,s]i_
_then the following inequality holds for all {_ _[}][ be the sequence generated by Algorithm 1 under Assumptions 1-6. If] r âˆˆ_ N+: _[ Î· >][ 0][,]_

Ef (x[r][+1])


_Ï„_ _âˆ’1_

E _f_ (Â¯x[r,s])
_âˆ¥âˆ‡_ _âˆ¥[2]_ _âˆ’_ _[Î·]4_
_s=0_

X

_K_

2

[1] _fk(Â¯x[r,s]k_ [)]

_K_ _âˆ‡_ [Â¯]

_k=1_

X


_Ï„_ _âˆ’1_

_s=0_

X


Ef (x[r])
_â‰¤_ _âˆ’_ _[Î·]4_


_âˆ‡fi(x[r,s]i_ [)]
_i=1_

X


_Ï„_ _âˆ’1_

_s=0_

X


_âˆ’_ _[Î·]4_

+ _[Î·]_

4

+ _[Î·]_


_Ï„_ 1 _N_ _Ï„_ 1
_âˆ’_ 2 _âˆ’_

_[Î·]_ E _f_ (Â¯x[r,s]) _fi(x[r,s]i_ [)] + _[Î·]_ E _f_ (Â¯x[r,s])

4 _âˆ‡_ _âˆ’_ _N[1]_ _âˆ‡_ 4 _âˆ‡_ _âˆ’_ _K[1]_

_s=0_ _i=1_ _s=0_

X X X

_Ï„_ 1 _N_ _K_
_âˆ’_ 2

_[Î·]_ E [1] _fi(x[r,s]i_ [)][ âˆ’] [1] _fk(Â¯x[r,s]k_ [)]

4 _N_ _âˆ‡_ _K_ _âˆ‡_ [Â¯]

_s=0_ _i=1_ _k=1_

X X X

_Ï„_ _âˆ’2_ _L_ _L_

E _x[r,s][+1]_ _xÂ¯[r,s]_ + E _x[r,Ï„]_ _[âˆ’][1]_ _._

2 _âˆ’_ _âˆ¥[2]_ 2 _âˆ¥[2]_

_s=0_  _[âˆ¥][Â¯]_   _[âˆ¥][x][r][+1][ âˆ’]_ [Â¯] 

X


(23)


_âˆ‡f[Â¯]k(Â¯x[r,s]k_ [)]
_k=1_

X


_Proof. The proof is a standard application of the descent lemma and the sampling rule applied at_
iteration Ï„ to obtain x[r][+1]. See Appendix D.1.

Lemma 1 shows the objective value f (x[r]) is descending in expectation up to the following error
terms:


_N_

2

_T1 = E_ _f_ (Â¯x[r,s]) _fi(x[r,s]i_ [)] _, T2 = E_ _f_ (Â¯x[r,s])
_âˆ‡_ _âˆ’_ _N[1]_ _âˆ‡_ _âˆ‡_ _âˆ’_ _K[1]_

_i=1_

X

_K_ _N_

2

_T3 = E_ [1] _fk(Â¯x[r,s]k_ [)][ âˆ’] [1] _fi(x[r,s]i_ [)]

_K_ _âˆ‡_ [Â¯] _N_ _âˆ‡_

_k=1_ _i=1_

X X

_T4 = Eâˆ¥xÂ¯[r,s][+1]_ _âˆ’_ _xÂ¯[r,s]âˆ¥[2], T5 = Eâˆ¥x[r][+1]_ _âˆ’_ _xÂ¯[r,Ï„]_ _[âˆ’][1]âˆ¥[2]._


_âˆ‡f[Â¯]k(Â¯x[r,s]k_ [)]
_k=1_

X


(24)


In the sequel, we will show these quantities can be bounded by the optimality gap measured in terms
of the gradient norms âˆ¥âˆ‡f (Â¯x[r,s])âˆ¥[2], âˆ¥(1/K) _k=1_ _[âˆ‡]f[Â¯]k(Â¯x[r,s]k_ [)][âˆ¥][2][, and][ âˆ¥][(1][/N] [)][ P]i[N]=1 _[âˆ‡][f][i][(][x]i[r,s][)][âˆ¥][2][.]_

[P][K]

B.2 BOUNDING T1, T2 AND T3.

Define

_Ïmax =_ max (25)
_k=1,...,K_ _[Ï][k][.]_

Therefore it holds 0 _Ïmax_ 1 by Assumption 3.
_â‰¤_ _â‰¤_


-----

Since each fi is L-smooth by Assumption 1, we have _f[Â¯]k and f are also L-smooth. Using this fact_
and the convexity of âˆ¥Â· âˆ¥[2] we can bound T1, T2 and T3 as


_K_

_T1 = E_ _f_ (Â¯x[r,s]) [1] _fk(Â¯x[r,s]k_ [)][ âˆ’] [1]
_âˆ‡_ _Â±_ _K_ _âˆ‡_ [Â¯] _N_

_k=1_

X

_K_ _K_

1

_â‰¤_ 2 K[1] _k=1_ _L[2]Eâˆ¥xÂ¯[r,s]_ _âˆ’_ _xÂ¯[r,s]k_ _[âˆ¥][2][ + 2]_ _k=1_ _N_

X X


_âˆ‡fi(x[r,s]i_ [)]
_i=1_

X

_L[2]E_ _xÂ¯[r,s]k_ _x[r,s]i_
_iXâˆˆVk_ _âˆ¥_ _âˆ’_ _[âˆ¥][2][,]_


(26)


_K_

2

_T2 = E_ _f_ (Â¯x[r,s]) _fk(Â¯x[r,s]k_ [)]
_âˆ‡_ _âˆ’_ _K[1]_ _âˆ‡_ [Â¯] _â‰¤_ _K[1]_

_k=1_

X

_K_ _N_

_T3 = E_ [1] _fk(Â¯x[r,s]k_ [)][ âˆ’] [1] _fi(x[r,s]i_ [)]

_K_ _âˆ‡_ [Â¯] _N_ _âˆ‡_

_k=1_ _i=1_

X X


_L[2]E_ _xÂ¯[r,s]_ _xÂ¯[r,s]k_
_âˆ’_
_k=1_

X


_L[2]E_ _xÂ¯[r,s]k_ _x[r,s]i_
_iXâˆˆVk_ _âˆ¥_ _âˆ’_ _[âˆ¥][2][.]_


_k=1_


Clearly, in order to bound T1,2,3 we first need to bound the inter-cluster consensus error _xÂ¯[r,s]_ _xÂ¯[r,s]k_
and the intra-cluster consensus error _xÂ¯[r,s]k_ _x[r,s]i_ _âˆ¥_ _âˆ’_ _[âˆ¥]_
_âˆ¥_ _âˆ’_ _[âˆ¥][.]_

**Lemma 2 (Inter-Cluster Consensus Error Bound). Let** _x[r,s]i_
_rithm 1 under Assumptions 1, 2, 3, and 5. If the learning rate {_ _Î· >[}][ be the sequence generated by Algo-] 0 satisfies_


_Î·[2]_ _â‰¤_


1

(27)
24Ï„ (4Ï„ 1)L[2][,]
_âˆ’_


_then for all s = 0, . . ., Ï„ âˆ’_ 1 it holds

_K_

1

_K_ Eâˆ¥xÂ¯[r,s][+1] _âˆ’_ _xÂ¯[r,s]k_ [+1]âˆ¥[2] _â‰¤_ _CÏ„_

_k=1_

X


E _xÂ¯[r,s]_ _xÂ¯[r,s]k_
_k=1_ _âˆ¥_ _âˆ’_ _[âˆ¥][2]_

X

! + 12Ï„Î·[2][  ]Î±[2]Eâˆ¥âˆ‡f (Â¯x[r,s])âˆ¥[2] + Ïµ[2]g + Î·[2][ K][ âˆ’]N [1] _Ïƒ[2]_ (28)




E _Xk,[r,s]âŠ¥_
_K=1_

X


+ 12Ï„Î·[2]L[2]


_where_


_CÏ„ â‰œ_ 1 + [3]

2

_[Â·]_


1

(29)
4Ï„ 1 _[.]_
_âˆ’_


_Proof. See Appendix D.2._

**Lemma 3 (Intra-Cluster Consensus Error Bound). Let** _x[r,s]i_
_rithm 1 under Assumptions 1-5. If Î· > 0, then for all s = 0 {_ _, . . ., Ï„[}][ be the sequence generated by Algo-] âˆ’_ 1 it holds


_K_

1
E _Xk,[r,s][+1]_ max _k[(1 +][ Î¶]k[âˆ’][1][) +][ Î·][2][Ï][L][ Â·][ 4][L][2]_
_âˆ¥_ _âŠ¥_ _âˆ¥[2]_ _â‰¤_ _k_ [K] _[Ï][2]_ _N_
_k=1_  _âˆˆ_ 

X


E _Xk,[r,s]_
_âˆ¥_ _âŠ¥[âˆ¥][2]_
_k=1_

X


+ 4Î·[2]ÏLL[2][ 1]


Eâˆ¥xÂ¯[r,s]k _âˆ’_ _xÂ¯[r,s]âˆ¥[2]_ + 4Î·[2]ÏL(Î±[2]Eâˆ¥âˆ‡f (Â¯x[r,s])âˆ¥[2] + Ïµ[2]g[) + 4][Î·][2][Ï][L]Ïµ[Â¯][2]L [+][ Î·][2][Ïƒ][2][Ï]max[2] _[,]_
_k=1_

X

(30)


_where Ïmax is defined in (25) and_

_ÏL â‰œ_ _k=1max,...,K_ _Ï[2]k[(1 +][ Î¶][k][)]_ _,_ _ÏµÂ¯[2]L_ [â‰œ] _K[1]_



_Ïµ[2]k_ (31)
_k=1_

X


_with Î¶k > 0 being a free parameter to be chosen properly for all k = 1, . . ., K._

_Proof. See Appendix D.3._


-----

Combining Lemma 2 and 3 we can obtain the following bound on the sum of intra- and interconsensus errors using gradient âˆ¥âˆ‡f (Â¯x[r,s])âˆ¥[2].

**Proposition 1. Let** _x[r,s]i_
_learning rate Î· > 0 { satisfies[}][ be the sequence generated by Algorithm 1 under Assumptions 1-5. If the]_


_Î·[2]_ _â‰¤_


1

(32)
24Ï„ (4Ï„ 1)L[2][,]
_âˆ’_


_then for all s = 0, . . ., Ï„ âˆ’_ 1 it holds


E _xÂ¯[r,s][+1]_ _xÂ¯[r,s]k_ [+1] +
_âˆ¥_ _âˆ’_ _âˆ¥[2]_
_k=1_ _k=1_

X X


1

_N_ _k,âŠ¥_ _âˆ¥[2]_

_[âˆ¥][X]_ _[r,s][+1]_


_s_

_C1Î·[2][  ]Ï„ + Ï[2]max[D][Ï„,Ï]_ (Î±[2]E _f_ (Â¯x[r,â„“]) + Ïµ[2]g[) +][ C][1][Î·][2][Ï„Ï][2]max[D][Ï„,Ï]Ïµ[Â¯][2]L
_âˆ¥âˆ‡_ _âˆ¥[2]_
_â„“=0_

X 

+ C1Ï„Î·[2]Ï[2]max[Ïƒ][2][ +][ C][1][(][Ï„][ +][ D]Ï„,Ï[2] _[Ï„][ âˆ’][1][Ï][2]max[)][Î·][2][ 1]_

_n_ _[Ïƒ][2]_


(33)

(34)

(35)

(36)


_where_

_DÏ„,Ï â‰œ_ min _Ï„,_


_and C1 > 0 is some universal constant._

_Proof. See Appendix C.1._


1 _Ïmax_
_âˆ’_


Notice that according to (26) the gradient difference terms in Lemma 1 can be bounded as


_N_

_Î·_ 2
_f_ (Â¯x[r,s]) _fi(x[r,s]i_ [)] + _[Î·]_ _f_ (Â¯x[r,s])
4 [E] _âˆ‡_ _âˆ’_ _N[1]_ _âˆ‡_ 4 [E] _âˆ‡_ _âˆ’_ _K[1]_

_i=1_

X

_N_ _K_

2

+ _[Î·]_ [1] _fi(x[r,s]i_ [)][ âˆ’] [1] _fk(Â¯x[r,s]k_ [)]

4 [E] _N_ _âˆ‡_ _K_ _âˆ‡_ [Â¯]

_i=1_ _k=1_

X X


_âˆ‡f[Â¯]k(Â¯x[r,s]k_ [)]
_k=1_

X


_â‰¤_ _[Î·]4_

+ _[Î·]_

4

_â‰¤Î·L[2]_


2

_kX=1_ _L[2]Eâˆ¥xÂ¯[r,s]_ _âˆ’_ _xÂ¯[r,s]k_ _[âˆ¥][2][ +]_ _kX=1_ _N_ _iXâˆˆVk_ _L[2]Eâˆ¥xÂ¯[r,s]k_ _âˆ’_ _x[r,s]i_ _[âˆ¥][2]!_

_K_ _K_

1 2 1

_K_ _kX=1_ _L[2]E_ _xÂ¯[r,s]_ _âˆ’_ _xÂ¯[r,s]k_ + _kX=1_ _N_ _iXâˆˆVk_ _L[2]Eâˆ¥xÂ¯[r,s]k_ _âˆ’_ _x[r,s]i_ _[âˆ¥][2]_

_K_ _K_

1 1

_K_ _k=1_ Eâˆ¥xÂ¯[r,s] _âˆ’_ _xÂ¯[r,s]k_ _[âˆ¥][2][ +]_ _k=1_ _N_ _[âˆ¥][X]k,[r,s]âŠ¥[âˆ¥][2]!_

X X


for all s = 1, . . ., Ï„ . Therefore Proposition 1 immediately leads to the following result.

**Corollary 2. Under the same setting as Proposition 1, it holds**


_Ï„_ _N_

_Î·_
_f_ (Â¯x[r,s]) _fi(x[r,s]i_ [)]
4 [E] _âˆ‡_ _âˆ’_ _N[1]_ _âˆ‡_

_s=0_ _i=1_

X X

_Ï„_ _Î·_ _Ï„_ _âˆ’1_ _N_

+ E [1] _fi(x[r,s]i_ [)][ âˆ’] [1]

4 _N_ _âˆ‡_ _K_

_s=0_ _s=0_ _i=1_

X X X


_Ï„_

2 _Î·_
+ _f_ (Â¯x[r,s])

4 [E] _âˆ‡_ _âˆ’_ _K[1]_

_s=0_

X

_K_

2
_âˆ‡f[Â¯]k(Â¯x[r,s]k_ [)]
_k=1_

X


_âˆ‡f[Â¯]k(Â¯x[r,s]k_ [)]
_k=1_

X


_Ï„_ _âˆ’1_

_C1L[2]Î·[3][  ]Ï„_ [2] + Ï„Ï[2]max[D][Ï„,Ï] (Î±[2]E _f_ (Â¯x[r,s]) + Ïµ[2]g[) +][ C][1][L][2][Î·][3][Ï„][ 2][Ï][2]max[D][Ï„,Ï]Ïµ[Â¯][2]L
_âˆ¥âˆ‡_ _âˆ¥[2]_
_s=0_

X 

+ C1Ï„ [2]L[2]Î·[3]Ï[2]max[Ïƒ][2][ +][ C][1][L][2][(][Ï„][ 2][ +][ D]Ï„,Ï[2] _[Ï][2]max[)][Î·][3][ Ïƒ][2]_

_n [.]_


-----

We conclude this section by providing a separate bound on the consensus error

_N1_ _Kk=1_ [E][r][âˆ¥][X]k,[r,Ï„]âŠ¥[âˆ’][1]âˆ¥[2] that will be useful in bounding T5.

**Proposition 2.P** _Under the same setting as Proposition 1, if Ïmax â‰¤_ 1 âˆ’ _Ï„[1]_ _[,][ then we have]_

1 _K_ _Ï„_ _âˆ’2_

_N_ [E] _k=1_ _âˆ¥Xk,[r,Ï„]âŠ¥[âˆ’][1]âˆ¥[2]_ _â‰¤_ 2C2 _s=0_ _DÏ„,Ï[2]_ _[Ï][2]max[Î·][2][(][Î±][2][âˆ¥âˆ‡][f]_ [(Â¯]x[r,s])âˆ¥[2] + Ïµ[2]g[)]

X X

1 _DÏ„,Ï_

+ C2DÏ„,Ï[2] _[Ï„Ï][2]max[Î·][2]Ïµ[Â¯][2]L_ [+][ C][2] + 1 _Ï[2]max[Ï„D][Ï„,Ï][Î·][2][Ïƒ][2][.]_ (37)

_n_ _Ï„_

 

_for some universal constant C2 > 0._

_Proof. See Appendix C.2._

Proposition 2 shows that the average intra-cluster consensus error _N[1]_ _Kk=1_ _k,_ decreases as

the network connectivity improves, and vanishes if Ïmax goes to zero. _[âˆ¥][X]_ _[r,Ï„]âŠ¥[âˆ’][1]âˆ¥[2]_
P

B.3 BOUNDING T4

**Proposition 3. Under the same setting as Lemma 1, we have**


+ Î·[2][ Ïƒ][2]


Eâˆ¥xÂ¯[r,s][+1] _âˆ’_ _xÂ¯[r,s]âˆ¥[2]_ = Î·[2]E

_for s = 0, . . ., Ï„ âˆ’_ 1 and r âˆˆ N+.


_âˆ‡fi(x[r,s]i_ [)]
_i=1_

X


(38)


_Proof. Recall the algorithmic update at iteration s for all s = 0, . . ., Ï„ âˆ’_ 1:

_Xk[r,s][+1]_ = WkXk[r,s] _Î·WkG[r,s]k_
_âˆ’_ (39)
_xÂ¯[r,s]k_ [+1] = Â¯x[r,s]k _âˆ’_ _Î·gÂ¯k[r,s][.]_

Therefore, it holds under Assumption 2 that

Eâˆ¥xÂ¯[r,s][+1] _âˆ’_ _xÂ¯[r,s]âˆ¥[2]_


(40)

+ Î·[2][ Ïƒ][2]

_N [.]_


(gi[r,s] _Â± âˆ‡fi(x[r,s]i_ [)]
_i=1_

X


_âˆ‡fi(x[r,s]i_ [)]
_i=1_

X


=E

B.4 BOUNDING T5


= E


We provide the bound on T5 separately for the full device participation and partial participation
cases.

**Full participation.**

When the sampling probability p = 1, we have


_x[r][+1]_ = [1]

_N_

In this case, it follows from Proposition 3 that

Eâˆ¥x[r][+1] _âˆ’_ _xÂ¯[r,Ï„]_ _[âˆ’][1]âˆ¥[2]_ = Î·[2]E

**Partial participation.**


_x[r,Ï„]i_ = Â¯x[r,Ï„] _._
_i=1_

X

_N_

1

_N_ _âˆ‡fi(xi[r,Ï„]_ _[âˆ’][1]_

_i=1_

X


+ Î·[2][ Ïƒ]N [2] _[.]_ (41)


-----

We proceed to bound T5 for

1 â‰¤ _m â‰¤_ _n âˆ’_ 1. (42)

Define p = m/n. Recall the algorithmic update at iteration Ï„ âˆ’ 1:

_Xk[r,Ï„]_ = WkXk[r,Ï„] _[âˆ’][1]_ _Î·WkGk[r,Ï„]_ _[âˆ’][1]_ (43)
_âˆ’_

and


_x[r][+1]_ = [1]


_x[r,Ï„]i_
_iXâˆˆSk[r]_


_x[r,Ï„]i_ _._ (44)
_iXâˆˆSk[r]_


_Np_


_k=1_


_k=1_


Therefore, with (Wk)i,j being the ij-th element of matrix Wk we have under Assumption 2:


Eâˆ¥x[r][+1] _âˆ’_ _xÂ¯[r,Ï„]_ _[âˆ’][1]âˆ¥[2]_


2[ï£¶]

ï£·


_x[r,Ï„]i_ _xÂ¯[r,Ï„]_ _[âˆ’][1]_
_âˆ’_
_iXâˆˆSk[r]_


=E

=E

= E


_Np_


_k=1_


(Wk)i,j(xj[r,Ï„] _[âˆ’][1]_ _Î·gj[r,Ï„]_ _[âˆ’][1])_ _xÂ¯[r,Ï„]_ _[âˆ’][1]_
_âˆ’_ _âˆ’_

  j[X]âˆˆVk 


_Np_

1

_Np_


_k=1_ _iâˆˆSk[r]_ _jâˆˆVk_

_K_ 2 (45)

1

= E _Np_ I(i âˆˆ _Sk[r][)]_ (Wk)i,j(x[r,Ï„]j _[âˆ’][1]_ _âˆ’_ _Î·âˆ‡fj(xj[r,Ï„]_ _[âˆ’][1])_ _âˆ’_ _xÂ¯[r,Ï„]_ _[âˆ’][1]_

_kX=1_ _iXâˆˆVk_   j[X]âˆˆVk 

_A2,1_

2

| _K_ {z }

1

+ Î·[2] E _Np_ I(i âˆˆ _Sk[r][)]_ (Wk)i,j(âˆ‡fj(x[r,Ï„]j _[âˆ’][1]) âˆ’_ _gj[r,Ï„]_ _[âˆ’][1])_ _._

_kX=1_ _iXâˆˆVk_   j[X]âˆˆVk 

_A2,2_

**Proposition 4. Let|** _x[r,s]i_ {z }
_learning rate Î· > 0 { satisfies[}][ be the sequence generated by Algorithm 1 under Assumptions 1-6. If the]_


1
_Î·[2]_ (46)
_â‰¤_ 24Ï„ (4Ï„ 1)L[2][,]

_âˆ’_

_then we have the following bounds on A2,1:_


+ 8 _Gp + [1]_

_Ï„_ [2]




_fk(Â¯x[r,Ï„]k_ _[âˆ’][1]_
_âˆ‡_ [Â¯]
_k=1_

X


E _Xk,[r,Ï„]_ _[âˆ’][1]_
_âˆ¥_ _âŠ¥_ _âˆ¥[2]_
_k=1_

X


1

_A2,1_ 2Î·[2]E
_â‰¤_ _K_

_where_

_Proof. See Appendix C.3._


(47)


_n_ _m_
_Gp â‰œ_ _âˆ’_ (48)

_m(n_ 1) _[.]_
_âˆ’_


**Proposition 5. Under the same setting as Proposition 4, A2,2 can be bounded as**


_A2,2_
_â‰¤_ _[Ïƒ]N[2]_


2 + _m[n]_ max _._ (49)

_[Â·][ Ï][2]_ 


_Proof. See Appendix C.4_


-----

B.5 PROOF OF THEOREM 1 (FULL PARTICIPATION)

We first prove the descent of the objective value under suitable choice of Î·.
**Proposition 6. If the learning rate satisfies**


1

(50)

4C1Î± _Ï„L_ _[,]_

_[Â·][ 1]_


_Î· â‰¤_


_then we have_


_Ï„_ _âˆ’1_

Eâˆ¥âˆ‡f (Â¯x[r,s])âˆ¥[2] + Rfull(Î·), (51)
_s=0_

X


Ef (x[r][+1]) Ef (x[r])
_â‰¤_ _âˆ’_ _[Î·]8_


_where_

1
full(Î·) =C1L[2]Î·[3]Ï„ [2][  ]Ï„ + Ï[2]max[D][Ï„,Ï] _Ïµ[2]g_ [+ 2][C][1][L][2][Î·][3] _Ï„_ [2]Ï[2]max[D][Ï„,Ï]Ïµ[Â¯][2]L [+][ Ï„][ 2][Ïƒ][2] max
_R_ _n_ [+][ Ï][2]
  


+ Î·[2]LÏ„ [Ïƒ][2]

_N [.]_

(52)

_C1 > 0 is some universal constant._

_Proof. See Appendix C.5._

To attain the expression of the convergence rate, we sum (51) over r = 0, . . ., R:

min min _x[r,s])_
_râˆˆ[R]_ _s=0,...,Ï„_ _âˆ’1_ [E][âˆ¥âˆ‡][f] [(Â¯] _âˆ¥[2]_

+ [8][R][full][(][Î·][)]

_â‰¤_ [8(][f]Î·Ï„[(][x][0]([)]R[ âˆ’] + 1)[f] [(][x][â‹†][))] _Î·Ï„_

= [8(][f] [(][x][0][)][ âˆ’] _[f]_ [(][x][â‹†][))] + 16Î·L _[Ïƒ][2]_ (53)

_Î·Ï„_ (R + 1) _N_

centralized SGD

1

|+ 16C1L[2]Ï„ [2]Î·{z[2]Ïµ[2]g [+ 16][C][1][L][2][Î·]}[2] _Ï„Ï[2]max[D][Ï„,Ï]Ïµ[Â¯][2]L_ [+][ Ï„Ïƒ][2] max _._

_n_ [+][ Ï][2]

  

network effect

The first two terms of (53) corresponds to the impact of stochastic noise and is of the same order| {z }
as the centralized SGD algorithm. The last term is of order Î·[2] and corresponds to the deterioration
of convergence rate due to the fact that we are not computing the average gradients of all devices at
each iteration.


Denote

_Ïƒ2_
_r0 = 8(f_ (x[0]) _f_ (x[â‹†])), r1 = 16L _,_
_âˆ’_ _N_
  1 (54)

_r2 = 16C1L[2]Ï„_ [2]Ïµ[2]g [+ 16][C][1][L][2] _Ï„Ï[2]max[D][Ï„,Ï]Ïµ[Â¯][2]L_ [+][ Ï„Ïƒ][2] max _._

_n_ [+][ Ï][2]

  

The rest of the proof follows the same argument as (?, Appendix B.5) and thus we omit the details.


B.6 PROOF OF THEOREM 2 AND COROLLARY 1 (PARTIAL PARTICIPATION)

**Proposition 7. Let** _x[r,s]i_
_learning rate Î· and the network connectivity satisfies {_ _[}][ be the sequence generated by Algorithm 1 under Assumption 1-5. If the]_


_and_ _Ïmax_ 1 (55)
_â‰¤_ _âˆ’_ _Ï„ [1]_ _[,]_


_Î· â‰¤_


1,
_C3Î±Ï„L_

_[Â·][ min]_ 


_Î±GpDÏ„,Ï Ïmax_


-----

_then_


_Ï„_ _âˆ’1_

Eâˆ¥âˆ‡f (Â¯x[r,s])âˆ¥[2] + Rpart[(1)] [(][Î·][) +][ R][(2)]part[(][Î·][)] (56)
_s=0_

X


Ef (x[r][+1]) Ef (x[r])
_â‰¤_ _âˆ’_ _[Î·]8_


_with_

1
part[(][Î·][) =][C][1][L][2][Î·][3][Ï„][ 2][  ]Ï„ + Ï[2]max[D][Ï„,Ï] _Ïµ[2]g_ [+ 2][C][1][L][2][Î·][3] _Ï„_ [2]Ï[2]max[D][Ï„,Ï]Ïµ[Â¯][2]L [+][ Ï„][ 2][Ïƒ][2] max
_R[(1)]_ _n_ [+][ Ï][2]
 


+ LÏ„Î·[2][ Ïƒ][2]

_N [,]_




(57)


part[(][Î·][) =] 8C2G[â€²]p[LD]Ï„,Ï[2] _[Ï„Ï][2]max_ _Î·[2]Ïµ[2]g_
_R[(2)]_

1

  

+ 4C2LG[â€²]p[Î·][2] _DÏ„,Ï[2]_ _[Ï„Ï][2]maxÏµ[Â¯][2]L_ [+]

_n_

 

_C1, C3 > 0 are some universal constants, and_


_n_

_m_ _[Ï]max[2]_ _Î·[2][ Ïƒ]N [2]_ _[.]_

 


_DÏ„,Ï_


+ 1 _Ï[2]max[Ï„D][Ï„,Ï][Ïƒ][2]_ + _[L]_
 


_n_ _m_

_G[â€²]p_ [=][ G][p] [+ 1]Ï„ [2][,] _with_ _Gp =_ _m( âˆ’n_ 1) _[.]_ (58)

_âˆ’_

_Proof. See Appendix C.6_

Comparing (56) to (51) we can see that Rpart[(1)] [(][Î·][)][ is of the same order as][ R][(1)]full[(][Î·][)][, while][ R]part[(2)] [(][Î·][)][ is]
an extra loss term introduced by sampling.

Following the same steps as the proof of Theorem 1 gives


min min _x[r,s])_
_râˆˆ[R]_ _s=0,...,Ï„_ _âˆ’1_ [E][âˆ¥âˆ‡][f] [(Â¯] _âˆ¥[2]_

1 3

_Ïƒ +_ (Ïµg, Â¯ÏµL, Ïƒ, Ïmax) _Ï„_ [2]Ïµ[2]g [+][ Ï„Ï]max[2] _[D][Ï„,Ï]Ïµ[Â¯][2]L_ [+][ Ï„] _n_ [+][ Ï]max[2] _Ïƒ[2][][ 1]_
=O _E_ + 2
 _âˆšNÏ„R_   (Ï„R) 3   

+ [1] _,_

_R_ [max][{][1][, G][p][D][Ï„,Ï][Ï][max][}]



(59)


where

(Ïµg, Â¯ÏµL, Ïƒ, Ïmax) = (Ïµ[2]g[D][Ï„,Ï] [+][ D][Ï„,Ï]Ïµ[Â¯][2]L [+][ Ïƒ][2][)][ Â·][ G]p[â€²] _[D][Ï„,Ï][Ï]max[2]_ _[N][ +][ n]_ max[Ïƒ][2][.] (60)
_E_ [2] _m_ _[Â·][ 1]Ï„ [Ï][2]_


Our last step simplifies the overall conditions on Ïmax so that E [2](Ïµg, Â¯ÏµL, Ïƒ, Ïmax) = O(1):

_Ïmax_ 1 _G[â€²]p[D]Ï„,Ï[2]_ _[Ï][2]max_ _Ïmax_ (61)
_â‰¤_ _âˆ’_ _Ï„ [1]_ _[,]_ _[â‰¤]_ _N [1]_ _[,]_ _â‰¤_ _[m]n_

_[Â·][ Ï„.]_

We claim to fulfill (61) it suffices to require

1
_Ïmax_ (62)
_â‰¤_ 4N [min][{][m, Ï„][ âˆ’] [1][}][.]

When Ï„ = 1, the condition trivially requires Ïmax = 0. We then consider the case for Ï„ â‰¥ 2. By
definition, it can be verified that


_G[â€²]p_ _[â‰¤]_ _m[1]_ [+ 1]Ï„ [2][ .] (63)


First notice that


_m_

4N 4 _Ï„_

_[â‰¤]_ [1] _[â‰¤]_ [1][ âˆ’] [1]

Therefore, it remains to prove (62) implies


_Ï„_

(64)

4N _n_

_[â‰¤]_ _[m]_ _[Â·][ Ï„.]_


and


_G[â€²]p[D]Ï„,Ï[2]_ _[Ï][2]max_ (65)

_[â‰¤]_ _N [1]_ _[.]_


-----

Using the fact that under (62) Ïmax 1/4 we have
_â‰¤_


1
_G[â€²]p[D]Ï„,Ï[2]_ _[Ï][max]_ [=][ G][â€²]p[Ï][max]

_[Â·]_ (1 _Ïmax)[2]_

1 _âˆ’ 1_ 1 (66)

_â‰¤_ [16]9 _[Ï][max]_ _m_ [+ 1]Ï„ [2] _â‰¤_ [16]9 _m_ [+ 1]Ï„ [2] 4N [min][{][m, Ï„] _[} â‰¤]_ _N [1]_ _[.]_

   

This proves the claim.

C PROOF OF MAIN PROPOSITIONS

C.1 PROOF OF PROPOSITION 1

Denote for short

_K_

1

_N_ Eâˆ¥Xk,[r,s]âŠ¥[âˆ¥][2]

_M_ _[r,s]_ â‰œ ï£« _KkX=1_ ï£¶ _._ (67)

1

ï£¬ ï£·
ï£¬ E _xÂ¯[r,s]k_ _xÂ¯[r,s]_ ï£·
ï£¬ _K_ _k=1_ _âˆ¥_ _âˆ’_ _âˆ¥[2]ï£·_
ï£¬ X ï£·
ï£­ ï£¸

Invoking Lemma 2 and Lemma 3 we obtain that under the condition that the learning rate Î· > 0
satisfies

1
_Î·[2]_ (68)
_â‰¤_ 24Ï„ (4Ï„ 1)L[2][,]

_âˆ’_

the following inequality is satisfied for all s = 0, . . ., Ï„ âˆ’ 1:

_M_ _[r,s][+1]_ _â‰¤_ _G Â· M_ _[r,s]_ + B[r,s], (69)

where


_G =_ maxkâˆˆ[K] Ï[2]k[(1 +][ Î¶]k[âˆ’][1][) +][ Î·][2][Ï][L][ Â·][ 4][L][2] _Î·[2]ÏL Â· 4L[2]_

12Ï„Î·[2]L[2] _CÏ„_

 

_B[r,s]_ = 4ÏLÎ·2(Î±2Eâˆ¥âˆ‡f (Â¯x[r,s])âˆ¥[2] + Ïµ[2]g[) + 4][Î·][2][Ï][L]Ïµ[Â¯][2]L [+][ Î·][2][Ï]max[2] _[Ïƒ][2]_
 12Ï„Î·[2](Î±[2]Eâˆ¥âˆ‡f (Â¯x[r,s])âˆ¥[2] + Ïµ[2]g[) +][ Î·][2][ Ïƒ]n[2] _[.]_


(70)

(71)


The inequality in (69) is defined elementwise.

Unrolling (69) yields

_M_ _[r,s][+1]_ _â‰¤_


_G[â„“]B[r,s][âˆ’][â„“],_ (72)

_â„“=0_

X


where we have used the fact that M _[r,][0]_ = 0 due to full synchronization of the xiâ€™s at the beginning
of each round r.

We first provide a bound on the sum of the two elements of G[â„“]B[r,s][âˆ’][â„“]. For simplicity we omit the
round index r in the superscript for the rest of this section.

**Lemma 4. Let b[s]1[âˆ’][â„“]** _and b[s]2[âˆ’][â„“]_ _be the first and second element of B[s][âˆ’][â„“], respectively. Suppose the_
_learning rate Î· > 0 then_

(1, 1)G[â„“]B[s][âˆ’][â„“] _Î»[â„“]2[(][b][s]1[âˆ’][â„“]_ + b[s]2[âˆ’][â„“]) + _[Î»]2[â„“]_ _[âˆ’]_ _[Î»]1[â„“]_ _Î·[2]_ 12Ï„L[2]b[s]1[âˆ’][â„“] + 4ÏLL[2]b[s]2[âˆ’][â„“] (73)
_â‰¤_ _Î»2_ _Î»1_ _Â·_

_âˆ’_
  

_where Î»1 â‰¤_ _Î»2 are the eigenvalues of G; and ÏL is defined in (31)._

_Proof. See Appendix D.4._


-----

From Lemma 4 we immediately get

_s_

(1, 1) Â· G[â„“]B[s][âˆ’][â„“]
_â„“=0_

X


(74)


_s_ (74)

_Î»[â„“]2[(][b][s]1[âˆ’][â„“]_ + b[s]2[âˆ’][â„“]) + _[Î»]2[â„“]_ _[âˆ’]_ _[Î»]1[â„“]_ _Î·[2]_ 12Ï„L[2]b[s]1[âˆ’][â„“] + 4ÏLL[2]b[s]2[âˆ’][â„“] _._

_â‰¤_ _Î»2_ _Î»1_ _Â·_

Xâ„“=0  _âˆ’_   []

Since Î»2 â‰¥ _CÏ„ > 1, we have_

_Î»[â„“]2_ _[âˆ’]_ _[Î»]1[â„“]_ = Î»[â„“]2[âˆ’][1] _â„“âˆ’1_ _Î»1_ _s_ _Î»[â„“]2[âˆ’][1]_ min _Î»2_ _, â„“_ _Î»[â„“]2_ [min] 1 _, â„“_ (75)

_Î»2_ _Î»1_ _Î»2_ _â‰¤_ _Î»2_ _Î»1_ _â‰¤_ _Î»2_ _Î»1_
_âˆ’_ Xs=0    _âˆ’_   _âˆ’_ 


and thus

_s_

(1, 1) Â· G[â„“]B[s][âˆ’][â„“]
_â„“=0_

X _s_

_Î»[â„“]2[(][b][s]1[âˆ’][â„“]_ + b2[s][âˆ’][â„“]) +

_â‰¤_

_â„“=0_

X


(76)


1

_Î»2_ _Î»1_ _, â„“_ _Î·[2]_ _Â·_ 12Ï„L[2]b[s]1[âˆ’][â„“] + 4ÏLL[2]b[s]2[âˆ’][â„“]
_âˆ’_ 
 


_Î»[â„“]2_ [min]


_â„“=0_


Recall the definition of ÏL given by (31):

_ÏL =_ max _k[(1 +][ Î¶][k][)][.]_ (77)
_k=1,...,K_ _[Ï][2]_

By the Gershgorinâ€™s theorem, since Î· > 0, we can upperbound Î»2 as


_Î»2_ max max _k[(1 +][ Î¶]k[âˆ’][1][) +][ Î·][2][Ï][L][ Â·][ 8][L][2][, C][Ï„][ + 12][Ï„Î·][2][L][2]_
_â‰¤_ kâˆˆ[K] _[Ï][2]_


(78)


_ÏL_
max max _k[(1 +][ Î¶]k[âˆ’][1][) +]_
_â‰¤_ _k_ [K] _[Ï][2]_ (4Ï„ 1)3Ï„ [,][ 1 +]
 _âˆˆ_ _âˆ’_

where the last inequality is due to the bound on Î·:


4Ï„ âˆ’ 1


_Î·[2]_ _â‰¤_


1

(79)
24Ï„ (4Ï„ 1)L[2][ .]
_âˆ’_


Define constant

We consider two cases.

_â€¢ Case 1:_


(80)


_DÏ„,Ï = min_ _Ï„,_



1 _Ïmax_
_âˆ’_


1

_Ïmax_ 1 _Ï„._ (81)
_â‰¤_ _âˆ’_ _Ï„[1]_ _[â‡’]_ 1 âˆ’ _Ïmax_ _â‰¤_

Thus DÏ„,Ï = 1/(1 âˆ’ _Ïmax). We let Î¶k = Ïk/(1 âˆ’_ _Ïk) and it gives_


_Ï[2]k_

1 _Ïk_
_âˆ’_


_Ï[2]max_ = Ï[2]max[D][Ï„,Ï][.] (82)

1 _Ïmax_
_âˆ’_


_kmax[K]_ _[Ï]k[2][(1 +][ Î¶]k[âˆ’][1][) =][ Ï][max][,]_ _ÏL =_ _k=1max,...,K_
_âˆˆ_

Substituting into the bound of Î»2 [cf. (78)] gives


_Î»2_ max _Ïmax +_ _Ï[2]max_ 2
_â‰¤_ (1 _Ïmax)3Ï„_ (4Ï„ 1) _[,][ 1 +]_ 4Ï„ 1
 _âˆ’_ _âˆ’_ _âˆ’_ 

2 (83)

1 _Ï„_ 2 3
_âˆ’_ [1]

max 1 _< 1 +_
_â‰¤_ ï£± _âˆ’_ _Ï„[1]_ [+] 3(4Ï„ 1) _[,][ 1 +]_ 4Ï„ 1 ï£¼ 4Ï„ 1 _[,]_

ï£´ï£² _âˆ’_ _âˆ’_ ï£´ï£½ _âˆ’_


-----

where in the second inequality we used the condition (81).

Since s _Ï„ and Î»2_ 1, we obtain the following bound
_â‰¤_ _â‰¥_


_s_

_Î»[â„“]2[b][s]1[âˆ’][â„“]_ 1 +
_â‰¤_
_â„“=0_ 

X


_Ï„_
 


_b[â„“]1_
_â„“=0_

X


_b[â„“]1_
_â„“=0_

X


(84)


_â‰¤_ 3 Â·


4Ï„ âˆ’ 1


Moreover, since

_Ïmax + Î·[2]ÏL_ 4L[2] _Ïmax +_ _Ï[2]max_
_Â·_ _â‰¤_ (1 _Ïmax)(4Ï„_ 1)6Ï„

_âˆ’_ _âˆ’_

we can bound Î»2 âˆ’ _Î»1 as_


2

(81) 1 _Ï„_

_âˆ’_ [1]

1 (85)
_â‰¤_ _âˆ’_ _Ï„[1]_ [+] 6(4Ï„ 1)

_âˆ’_ _[â‰¤]_ _[C][Ï„]_ _[,]_


_Î»2_ _Î»1_ _CÏ„_ _Ïmax_ _Î·[2]ÏL_ 4L[2]
_âˆ’_ _â‰¥_ _âˆ’_ _âˆ’_ _Â·_

_CÏ„_ _Ïmax +_ _Ï[2]max_
_â‰¥_ _âˆ’_ (1 _Ïmax)(4Ï„_ 1)6Ï„
 _âˆ’_ _âˆ’_

(81) 1 _Ï„_
_CÏ„_ _Ïmax + Ïmax_ _âˆ’_ [1]
_â‰¥_ _âˆ’_ _Â·_ 6(4Ï„ 1)
 _âˆ’_ 


(86)

(87)


1 + _Ïmax + Ïmax_
_â‰¥_ 4Ï„ 1 _Â·_

_âˆ’_ _[âˆ’]_ 


4Ï„ âˆ’ 1


= (1 _Ïmax)_ 1 +
_âˆ’_


Collecting (84) and (86) we can bound (76) as

_s_

(1, 1) Â· G[â„“]B[s][âˆ’][â„“]
_â„“=0_

X


1 _Ïmax._
_â‰¥_ _âˆ’_


4Ï„ âˆ’ 1


1

_, Ï„_
_Î»2_ _Î»1_
_âˆ’_ 


_Î·[2]_ _Â·_ 12Ï„L[2]b[â„“]1 [+ 4][Ï][L][L][2][b]2[â„“] _Â· 3_ min
_â„“=0_ 

X   


(b[â„“]1 [+][ b]2[â„“] [)][ Â·][ 3 +]
_â„“=0_

Xs

(b[â„“]1 [+][ b]2[â„“] [)][ Â·][ 3 +]
_â„“=0_

X


_Î·[2]_ _Â·_ 12Ï„L[2]b[â„“]1 [+][ D][Ï„,Ï][Ï]max[2] [4][L][2][b][â„“]2 _Â· 3DÏ„,Ï_
_â„“=0_

X   


(79)


(b[â„“]1 [+][ b]2[â„“] [)][ Â·][ 3 +]
_â„“=0_

X


1
12Ï„b[â„“]1 [+][ D][Ï„,Ï][Ï]max[2] [4][b][â„“]2 3DÏ„,Ï

_Â·_ (4Ï„ 1)24Ï„

_âˆ’_




_â„“=0_


12Ï„b[â„“]1 [+][ D][Ï„,Ï][Ï]max[2] [4][b][â„“]2 [1]

_Â·_ 8


12Ï„b[â„“]1 [+][ D][Ï„,Ï][Ï]max[2] [4][b][â„“]2 [1]

_Â·_ 8



_DÏ„,Ï_

_Ï„_ [2]

_DÏ„,Ï_

_Ï„_ [2][ .]


(b[â„“]1 [+][ b]2[â„“] [)][ Â·][ 3 +]
_â„“=0_

Xs

(b[â„“]1 [+][ b]2[â„“] [)][ Â·][ 3 +]
_â„“=0_

X


_â„“=0_

_s_

_â„“=0_

X


-----

Substituting the expression of b[â„“]1 [and][ b]2[â„“] [gives]


_DÏ„,Ï[2]_

_Ï„_ [2] _Â· Ï[2]max[b][â„“]2_


(1, 1) Â· G[â„“]B[s][âˆ’][â„“] _â‰¤_
_â„“=0_

X


(b[â„“]1 [+][ b]2[â„“] [)][ Â·][ 5 +]
_â„“=0_

X


_â„“=0_


_s_

5 4Î·[2](ÏL + 3Ï„ )(Î±[2]E _f_ (Â¯x[r,â„“]) + Ïµ[2]g[) + 4][Î·][2][Ï][L]Ïµ[Â¯][2]L [+][ Î·][2][Ï]max[2] _[Ïƒ][2][ +][ Î·][2][ 1]_
_âˆ¥âˆ‡_ _âˆ¥[2]_ _n_ _[Ïƒ][2]_
_â„“=0_ 

X


_DÏ„,Ï[2]_

_Ï„_ [2] _Â· Ï[2]max_


12Ï„Î·[2](Î±[2]E _f_ (Â¯x[r,â„“]) + Ïµ[2]g[) +][ Î·][2][ 1]
_âˆ¥âˆ‡_ _âˆ¥[2]_ _n_ _[Ïƒ][2]_


_â„“=0_


(88)


_C1_

2 _[Î·][2][(][Ï][L][ +][ Ï„][ +][ Ï]max[2]_ _[D]Ï„,Ï[2]_ _[Ï„][ âˆ’][1][)(][Î±][2][E][âˆ¥âˆ‡][f]_ [(Â¯]x[r,â„“])âˆ¥[2] + Ïµ[2]g[) +][ C]2[1] _[Î·][2][Ï„Ï][L]Ïµ[Â¯][2]L_


_â„“=0_


+ _[C][1]_ max[Ïƒ][2][ +][ C][1] _Ï„,Ï[Ï„][ âˆ’][1][Ï][2]max[)][Î·][2][ 1]_

2 _[Ï„Î·][2][Ï][2]_ 2 [(][Ï„][ +][ D][2] _n_ _[Ïƒ][2]_


_s_

_C1Î·[2][  ]Ï„ + Ï[2]max[D][Ï„,Ï]_ (Î±[2]E _f_ (Â¯x[r,â„“]) + Ïµ[2]g[) +][ C][1][Î·][2][Ï„Ï][2]max[D][Ï„,Ï]Ïµ[Â¯][2]L

_â‰¤_ _âˆ¥âˆ‡_ _âˆ¥[2]_

_â„“=0_

X 

+ C1Ï„Î·[2]Ï[2]max[Ïƒ][2][ +][ C][1][(][Ï„][ +][ D]Ï„,Ï[2] _[Ï„][ âˆ’][1][Ï]max[2]_ [)][Î·][2][ 1]

_n_ _[Ïƒ][2]_

where C1 is some universal constant. The last inequality holds since ÏL = Ï[2]max[D][Ï„,Ï] [and][ D][Ï„,Ï]

_[â‰¤]_ _[Ï„]_ [.]

_â€¢ Case 2:_

_Ïmax > 1_ (89)
_âˆ’_ _Ï„[1]_

_[â‡’]_ _[D][Ï„,Ï][ =][ Ï„.]_

In such a case, we let Î¶k = (4Ï„ âˆ’ 1) and thus

max _k[(1 +][ Î¶]k[âˆ’][1][) =][ Ï]max[2]_ [(1 + (4][Ï„][ âˆ’] [1)][âˆ’][1][)][,] _ÏL = 4Ï„Ï[2]max_ [= 4][Ï]max[2] _[D][Ï„,Ï][.]_ (90)
_k_ [K] _[Ï][2]_
_âˆˆ_

Substituting into the bound of Î»2 given in (78), applying again the learning rate condition (79) and
using the fact that DÏ„,Ï = Ï„ :

_Î»2_ max _Ï[2]max[(1 + (4][Ï„][ âˆ’]_ [1)][âˆ’][1][) +] 4Ï[2]max 2
_â‰¤_ 3(4Ï„ 1) _[,][ 1 +]_ (4Ï„ 1)
 _âˆ’_ _âˆ’_  (91)

3
1 +
_â‰¤_ 4Ï„ 1 _[.]_

_âˆ’_

Therefore by (76), (79), (84), and the fact that


1

_, â„“_ _Ï„ = DÏ„,Ï_ (92)
_Î»2_ _Î»1_ _â‰¤_
_âˆ’_ 


min

_s_

(1, 1) Â· G[â„“]B[s][âˆ’][â„“]
_â„“=0_

X


we obtain


_Î·[2]_ _Â·_ 12Ï„L[2]b[â„“]1 [+ 16][Ï]max[2] _[D][Ï„,Ï][L][2][b][â„“]2_ _Â· 3DÏ„,Ï_
_â„“=0_

X   


(b[â„“]1 [+][ b]2[â„“] [)][ Â·][ 3 +]
_s=0_

Xs

(b[â„“]1 [+][ b]2[â„“] [)][ Â·][ 3 +]
_â„“=0_

Xs

(b[â„“]1 [+][ b]2[â„“] [)][ Â·][ 5 +]
_â„“=0_

X


(93)


_s_ 1

12Ï„b[â„“]1 [+ 16][Ï]max[2] _[D][Ï„,Ï][b][â„“]2_

8

_â„“=0_

Xs   

_Ï„,Ï_
2 _[D][2]_ max[b][â„“]2[.]

_Ï„_ [2][ Ï][2]

_â„“=0_

X


_DÏ„,Ï_

_Ï„_ [2]


Substituting the expression of b1 and b2 and using the fact that

_ÏL = 4Ï„Ï[2]max_ [= 4][Ï]max[2] _[D][Ï„,Ï]_
we arrive at the same bound as in Case 1, possibly with a different constant C1.


-----

C.2 PROOF OF PROPOSITION 2

We are in Case 1 described in the proof of Proposition 1. By letting Î¶k = Ïk/(1 âˆ’ _Ïk) we have_


_Ïmax +_ 1âˆ’Ï[2]maxÏmax 1âˆ’Ï[2]maxÏmax

12Ï„Î·[2]L[Â·][2][ Î·][2][ Â·][ 4][L][2] _C[Â·][ Î·]Ï„_ [2][ Â·][ 4][L][2]


(94)


_G =_


and the following bound on the difference of the eigenvalues of G:

_Î»2_ _Î»1_ 1 _Ïmax._ (95)
_âˆ’_ _â‰¥_ _âˆ’_

Notice that according to (72) and (148)


_Ï„_ _âˆ’2_

_t[Ï„]1[âˆ’][2][âˆ’][â„“]_
_â„“=0_

X


_Xk,[r,Ï„]_ _[âˆ’][1]_ =
_âˆ¥_ _âŠ¥_ _âˆ¥[2]_
_k=1_

X


det(T )


Therefore

_K_

1

_N_ _âˆ¥Xk,[r,Ï„]âŠ¥[âˆ’][1]âˆ¥[2]_

_k=1_

X

1
=

12Ï„Î·[2]L[2](Î»2 _Î»1)_
_âˆ’_


_Ï„_ _âˆ’2_

_â„“=0_

X


1 _Ï„_ _âˆ’2_
= 12Ï„Î·[2]L[2](Î»2 _Î»1)_ _â„“=0_ (CÏ„ âˆ’ _Î»1)_ _Î»[â„“]1[12][Ï„Î·][2][L][2][b][Ï„]1[âˆ’][2][âˆ’][â„“]_ _âˆ’_ _Î»[â„“]1[(][Î»][2]_ _[âˆ’]_ _[C][Ï„]_ [)][b]2[Ï„] _[âˆ’][2][âˆ’][â„“]_

1 âˆ’ XÏ„ âˆ’2   
+ 12Ï„Î·[2]L[2](Î»2 _Î»1)_ _â„“=0_ (Î»2 âˆ’ _CÏ„_ ) _Î»[â„“]2[12][Ï„Î·][2][L][2][b][Ï„]1[âˆ’][2][âˆ’][â„“]_ + Î»[â„“]2[(][C][Ï„] _[âˆ’]_ _[Î»][1][)][b]2[Ï„]_ _[âˆ’][2][âˆ’][â„“]_ (96)

1 _âˆ’_ _Ï„_ _âˆ’X2_     
_â‰¤_ 12Î·[2]L[2]Ï„ (1 âˆ’ _Ïmax)_ _â„“=0_ (CÏ„ âˆ’ _Î»1) Î»[â„“]1_ 12Ï„Î·[2]L[2]b[Ï„]1[âˆ’][2][âˆ’][â„“] _âˆ’_ (Î»2 âˆ’ _CÏ„_ )b[Ï„]2[âˆ’][2][âˆ’][â„“]

1 XÏ„ âˆ’2   
+ 12Î·[2]L[2]Ï„ (1 âˆ’ _Ïmax)_ _â„“=0_ (Î»2 âˆ’ _CÏ„_ ) Î»[â„“]2 12Ï„Î·[2]L[2]b[Ï„]1[âˆ’][2][âˆ’][â„“] + (CÏ„ âˆ’ _Î»1)b[Ï„]2[âˆ’][2][âˆ’][â„“]_ _._

X     

In the following, we boundthe subscript of Ïmax in the rest of the proof. Further, we introduce the following shorthand notation Î»1 and Î»2 âˆ’ _CÏ„ as a function of Ïmax. For notation simplicity we omit_
for the elements of G :

_Ï[2]_ _Ï[2]_
_f_ (Ï) = Ï + _g(Ï) =_ and h(Ï„ ) = 12Ï„Î·[2]L[2]. (97)

1 âˆ’ _Ï_ _[Â·][ Î·][2][ Â·][ 4][L][2][,]_ 1 âˆ’ _Ï_ _[Â·][ Î·][2][ Â·][ 4][L][2][,]_

Applying the Gershgorinâ€™s theorem we obtain

_Î»1_ min _Ï, CÏ„_ 12Ï„Î·[2]L[2] _Ï_ 0, (98)
_â‰¥_ _âˆ’_ _â‰¥_ _â‰¥_

and 

_Î»2_ max _f_ (Ï) + h(Ï), g(Ï) + CÏ„ _._ (99)
_â‰¤_ _{_ _}_

Under the learning rate condition (79) we can show


_g(Ï) + CÏ„_ (f (Ï) + h(Ï))
_âˆ’_

_Ï[2]_

= _[Ï][2]_

1 âˆ’ _Ï_ _[Â·][ Î·][2][ Â·][ 4][L][2][ +][ C][Ï„][ âˆ’]_ _[Ï][ âˆ’]_ 1 âˆ’ _Ï_ _[Â·][ Î·][2][ Â·][ 4][L][2][ âˆ’]_ [12][Ï„Î·][2][L][2]

=1 + [3] 1

2 4Ï„ 1

_[Â·]_ _âˆ’_ _[âˆ’]_ _[Ï][ âˆ’]_ [12][Ï„Î·][2][L][2][ â‰¥] [0][.]

Therefore,
_Î»2_ _CÏ„_ _g(Ï)._
_âˆ’_ _â‰¤_


(100)


-----

Substituting the bounds into (96) gives

_K_

1

_N_ _âˆ¥Xk,[r,Ï„]âŠ¥[âˆ’][1]âˆ¥[2]_

_k=1_

X


_Ï„_ _âˆ’2_

_â„“=0_

X


1 _Ï„_ _âˆ’2_
_â‰¤_ 12Î·[2]L[2]Ï„ (1 âˆ’ _Ï)_ _â„“=0_ _CÏ„_ _Î»[â„“]1_ 12Ï„Î·[2]L[2]b[Ï„]1[âˆ’][2][âˆ’][â„“] _âˆ’_ (Î»2 âˆ’ _CÏ„_ )b[Ï„]2[âˆ’][2][âˆ’][â„“]

1 XÏ„ âˆ’2   
+ 12Î·[2]L[2]Ï„ (1 âˆ’ _Ï)_ _â„“=0_ (Î»2 âˆ’ _CÏ„_ ) Î»[â„“]2 12Ï„Î·[2]L[2]b[Ï„]1[âˆ’][2][âˆ’][â„“] + CÏ„ _b[Ï„]2[âˆ’][2][âˆ’][â„“]_

1 _Ï„_ _âˆ’2X_     
_â‰¤_ 12Î·[2]L[2]Ï„ (1 _Ï)_ _CÏ„_ _f_ (Ï)[â„“] [ ]12Ï„Î·[2]L[2]b[Ï„]1[âˆ’][2][âˆ’][â„“]

_âˆ’_ _â„“=0_

1 XÏ„ âˆ’2 
+ 12Î·[2]L[2]Ï„ (1 _Ï)_ _g(Ï)Î»[â„“]2_ 12Ï„Î·[2]L[2]b[Ï„]1[âˆ’][2][âˆ’][â„“] + CÏ„ _b[Ï„]2[âˆ’][2][âˆ’][â„“]_

_âˆ’_ _â„“=0_

1 _Ï„_ _âˆ’2_ X  1   _Ï„_ _âˆ’2_ 
_â‰¤_ 1 âˆ’ _Ï_ _[C][Ï„]_ _â„“=0_ _b[â„“]1!_ + 1 âˆ’ _Ï_ _[g][(][Ï][)][Î»]2[Ï„]_ _â„“=0_ _b[â„“]1!_

X X

_CÏ„_ _Ï„_ _âˆ’2_
+ 12Î·[2]L[2]Ï„ (1 _Ï)_ _[g][(][Ï][)][Î»]2[Ï„]_ _b[â„“]2_

_âˆ’_ _â„“=0_ !

X

_Ï„_ _âˆ’2_ _Ï„_ _âˆ’2_ _Ï„_ _âˆ’2_

_DÏ„,ÏCÏ„_ _b[â„“]1_ + 12DÏ„,Ï[2] _[Î·][2][L][2][Ï][2]_ _b[â„“]1_ + _[C][Ï„]_ _Ï„,Ï_ _b[â„“]2_ _._
_â‰¤_ _â„“=0_ ! _â„“=0_ ! _Ï„ [Ï][2][D][2]_ _â„“=0_ !

X X X


12Î·[2]L[2]Ï„ (1 âˆ’ _Ï)_


(101)


where we have used the bound Î»1 â‰¤ _f_ (Ï) < 1, Î»2 > 1 and Î»[Ï„]2 _[<][ 3][.]_

Plug in the expression of b1 and b2 and using the fact that CÏ„ < 2, ÏL = Ï[2]DÏ„,Ï gives

_K_

1

_N_ _âˆ¥Xk,[r,Ï„]âŠ¥[âˆ’][1]âˆ¥[2]_

_k=1_

X


_Ï„_ _âˆ’2_
2DÏ„,Ï + 12DÏ„,Ï[2] _[Î·][2][L][2][Ï][2][]Ï[2]_ 4Î·[2]DÏ„,Ï(Î±[2] _f_ (Â¯x[r,s]) + Ïµ[2]g[) + 4][Î·][2][D][Ï„,Ï]Ïµ[Â¯][2]L [+][ Î·][2][Ïƒ][2][]

_âˆ¥âˆ‡_ _âˆ¥[2]_

_s=0_

  _Ï„,Ï[C][Ï„]_ _Ï„_ _âˆ’2_ X  

+ _[D][2]_ _Ï[2]_ (12Ï„Î·[2](Î±[2] _f_ (Â¯x[r,s]) + Ïµ[2]g[) +][ Î·][2][ Ïƒ][2]

_Ï„_ _âˆ¥âˆ‡_ _âˆ¥[2]_ _n_ [)]

_s=0_

X

2DÏ„,Ï + _[D]Ï„,Ï[2]_ _Ï[2][  ]4Î·[2]DÏ„,Ï(Î±[2]_ _f_ (Â¯x[r,s]) + Ïµ[2]g[) + 4][Î·][2][D][Ï„,Ï]Ïµ[Â¯][2]L [+][ Î·][2][Ïƒ][2][]

_Ï„_ [2][ Ï][2][][ Ï„] _[âˆ’][2]_ _âˆ¥âˆ‡_ _âˆ¥[2]_

_s=0_

 + 2 _[D]Ï„,Ï[2]_ _Ï[2]_ _Ï„_ _âˆ’2(12Ï„Î·X[2](Î±[2]_ _f_ (Â¯x[r,s]) + Ïµ[2]g[) +][ Î·][2][ 1]

_Ï„_ _âˆ¥âˆ‡_ _âˆ¥[2]_ _n_ _[Ïƒ][2][)]_

_s=0_

X


_Ï„_ _âˆ’2_
3DÏ„,ÏÏ[2] 4Î·[2]DÏ„,Ï(Î±[2] _f_ (Â¯x[r,s]) + Ïµ[2]g[) + 4][Î·][2][D][Ï„,Ï]Ïµ[Â¯][2]L [+][ Î·][2][Ïƒ][2][]
_â‰¤_ _âˆ¥âˆ‡_ _âˆ¥[2]_

_s=0_

X   _Ï„_ _âˆ’2_

+ 2DÏ„,Ï[2] _[Ï][2][Ï„][ âˆ’][1]_ (12Ï„Î·[2](Î±[2] _f_ (Â¯x[r,s]) + Ïµ[2]g[) +][ Î·][2][ 1]

_âˆ¥âˆ‡_ _âˆ¥[2]_ _n_ _[Ïƒ][2][)][.]_
_s=0_

X


(102)


-----

Tidy up the expression gives

1 _K_ _Ï„_ _âˆ’2_

_Xk,[r,Ï„]_ _[âˆ’][1]_ 3DÏ„,ÏÏ[2][  ]4Î·[2]DÏ„,Ï(Î±[2] _f_ (Â¯x[r,s]) + Ïµ[2]g[) + 4][Î·][2][D][Ï„,Ï]Ïµ[Â¯][2]L [+][ Î·][2][Ïƒ][2][]

_N_ _âˆ¥_ _âŠ¥_ _âˆ¥[2]_ _â‰¤_ _âˆ¥âˆ‡_ _âˆ¥[2]_

_k=1_ _s=0_

X X

_Ï„_ _âˆ’2_

+ 2DÏ„,Ï[2] _[Ï][2][Ï„][ âˆ’][1][(12][Ï„Î·][2][(][Î±][2][âˆ¥âˆ‡][f]_ [(Â¯]x[r,s]) + Ïµ[2]g[) +][ Î·][2][ 1]

_âˆ¥[2]_ _n_ _[Ïƒ][2][)]_
_s=0_

X


_Ï„_ _âˆ’2_

_s=0_

X


_Ï„_ _âˆ’2_

_C2_ _DÏ„,Ï[2]_ _[Ï][2][]_ _Î·[2](Î±[2]âˆ¥âˆ‡f_ (Â¯x[r,s])âˆ¥[2] + Ïµ[2]g[)]

_s=0_

X  

+ C2(DÏ„,Ï)[2]Ï„Ï[2]Î·[2]ÏµÂ¯[2]L [+][ C][2][Ï„D][Ï„,Ï][Ï][2][Î·][2][Ïƒ][2][ +][ C][2][(][D][Ï„,Ï][)][2][Ï][2][Î·][2][ 1]

_n_ _[Ïƒ][2]_


_C2_
_â‰¤_


(103)

(104)


_Ï„_ _âˆ’2_

2C2 _DÏ„,Ï[2]_ _[Ï][2][Î·][2][(][Î±][2][âˆ¥âˆ‡][f]_ [(Â¯]x[r,s]) + Ïµ[2]g[) +][ C][2][(][D][Ï„,Ï][)][2][Ï„Ï][2][Î·][2]Ïµ[Â¯][2]L
_â‰¤_ _âˆ¥[2]_

_s=0_

X

1 _DÏ„,Ï_

+ C2 + 1 _Ï[2]Ï„DÏ„,ÏÎ·[2]Ïƒ[2]_

_n_ _Ï„_

 

for some C2 > 0.

C.3 PROOF OF PROPOSITION 4

We prove (47) by splitting the terms A2,1 follows:


I(i _Sk[r][)]_ (Wk)i,j(xj[r,Ï„] _[âˆ’][1]_ _Î·_ _fj(xj[r,Ï„]_ _[âˆ’][1])_ _xÂ¯k[r,Ï„]_ _[âˆ’][1]_
_âˆˆ_ _âˆ’_ _âˆ‡_ _âˆ’_
_iXâˆˆVk_   j[X]âˆˆVk


(a)
_A2,1_ = E

_â‰¤2E_

where


_Np_


_k=1_


1

_Np_ _p Â· nÎ·âˆ‡f[Â¯]k(Â¯x[r,Ï„]k_ _[âˆ’][1]_

_k=1_

X


I(i _Sk[r][)][r][ik]_
_âˆˆ_
_iXâˆˆVk_


+ 2E


_Np_


_k=1_


_rik â‰œ_ (Wk)i,j(x[r,Ï„]j _[âˆ’][1]_ _âˆ’_ _xÂ¯k[r,Ï„]_ _[âˆ’][1]_ _âˆ’_ _Î·(âˆ‡fj(xj[r,Ï„]_ _[âˆ’][1]) âˆ’âˆ‡f[Â¯]k(Â¯x[r,Ï„]k_ _[âˆ’][1]))._ (105)

_jXâˆˆVk_

Equality (a) holds since


(Wk)i,j Â¯x[r,Ï„]k _[âˆ’][1]_
_jXâˆˆVk_


_xÂ¯k[r,Ï„]_ _[âˆ’][1]_
_iXâˆˆSk[r]_


_m_ Â¯xk[r,Ï„] _[âˆ’][1]_
_Â·_
_k=1_

X


_k=1_ _iâˆˆSk[r]_


_k=1_

_K_

_k=1_

X


(106)


_x[r,Ï„]i_ _[âˆ’][1]_ = NpxÂ¯[r,Ï„] _[âˆ’][1],_
_iXâˆˆVk_


and similarly,


(Wk)i,j _fk(Â¯xk[r,Ï„]_ _[âˆ’][1]) = np_ _fk(Â¯xk[r,Ï„]_ _[âˆ’][1])._ (107)
_âˆ‡_ [Â¯] _âˆ‡_ [Â¯]
_jXâˆˆVk_


_iâˆˆSk[r]_


Since samples are taken according to the rule specified by Assumption 6, the following probabilities
hold:

P _i âˆˆ_ _Sk[r]_ _[|][ i][ âˆˆV][k]_ = p, P(i, j âˆˆ _Sk[r]_ _[|][, i, j][ âˆˆV][k][) =][ p][ Â·][ np]n_ _[ âˆ’]1[1]_ _[,]_ (108)

_âˆ’_

P (i âˆˆ _Sk[r][, j][ âˆˆ]_ _[S]â„“[r]_ [|][ i][ âˆˆV][k][, j][ âˆˆV][â„“][, k][ Ì¸][=][ â„“][) =][ p][2][.] (109)

Consequently, we can evaluate the second term in (104) and obtain


_Î·_ _fk(Â¯x[r,Ï„]k_ _[âˆ’][1]_
_âˆ‡_ [Â¯]
_k=1_

X


_A2,1 =2E_


-----

_K_

2

_p_ _rik_ + p _[np][ âˆ’]_ [1]

(Np)[2][ E] ï£« _âˆ¥_ _âˆ¥[2]_ _Â·_ _n_ 1

_kX=1_ _iXâˆˆVk_ _âˆ’_

2 ï£­

E(rik[âŠ¤] _[r][jâ„“][)]_

(Np)[2][ Â·][ p][2][ X]

_k=Ì¸_ _â„“_ _iXâˆˆVk_ _jXâˆˆVâ„“_

2

_K_

1

_K_ _Î·âˆ‡f[Â¯]k(Â¯x[r,Ï„]k_ _[âˆ’][1])_

_k=1_

X


_rik[âŠ¤]_ _[r][jk]_
_i,jXâˆˆVk_


_k=1_


=2E

+

+

_â‰¤2E_

+

+

_â‰¤2E_


_p_ _[np][ âˆ’]_ [1]
_Â·_ _n_ 1

_âˆ’_


2
_rik_ + _[p][(1][ âˆ’]_ _[p][)][n]_

_n_ 1

_iXâˆˆVk_ _âˆ’_


2

(Np)[2][ E]


_rik_
_âˆ¥_ _âˆ¥[2]_
_iXâˆˆVk_

_rik_
_âˆ¥_ _âˆ¥[2]_
_iXâˆˆVk_

_rik_
_âˆ¥_ _âˆ¥[2]_
_iXâˆˆVk_


_k=1_


_k=1_

_K_

_k=1_

X


ï£«

_k=Ì¸_ _â„“_

ï£­[X]


2 _p(1 âˆ’_ _p)_

(Np)[2] _n_ 1 [E]

_âˆ’_


_rik[âŠ¤]_ _[r][jâ„“]_
_jXâˆˆVâ„“_


_iâˆˆVk_


_Î·_ _fk(Â¯x[r,Ï„]k_ _[âˆ’][1]_
_âˆ‡_ [Â¯]
_k=1_

X


_p_ _[np][ âˆ’]_ [1]
_Â·_ _n_ 1

_âˆ’_


2
_rik_ + _[p][(1][ âˆ’]_ _[p][)][n]_

_n_ 1

_iXâˆˆVk_ _âˆ’_


2

(Np)[2][ E]


_k=1_


ï£«

_k=Ì¸_ _â„“_

ï£­[X]


2 _p(1 âˆ’_ _p)_

(Np)[2] _n_ 1 [E]

_âˆ’_


1
2 _[âˆ¥][r][ik][âˆ¥][2][ + 1]2_ _[âˆ¥][r][jâ„“][âˆ¥][2]_

_jXâˆˆVâ„“_


_iâˆˆVk_


_Î·_ _fk(Â¯x[r,Ï„]k_ _[âˆ’][1]_
_âˆ‡_ [Â¯]
_k=1_

X


_p_ _[np][ âˆ’]_ [1]
_Â·_ _n_ 1

_âˆ’_


2
_rik_ + _[p][(1][ âˆ’]_ _[p][)][n]_

_n_ 1

_iXâˆˆVk_ _âˆ’_


2

(Np)[2][ E]


_k=1_


_k=1_


_K_

2 _p(1_ _p)_
+ _âˆ’_ E _rik_ _._ (110)

(Np)[2] _n_ 1 [(][K][ âˆ’] [1)][n] _âˆ¥_ _âˆ¥[2]_

_âˆ’_ _kX=1_ _iXâˆˆVk_

ByK substituting the expression of _rik_ we can bound terms _âˆ¥_ [P]k[K]=1 _iâˆˆVk_ _[r][ik][âˆ¥][2]_ and

_k=1_ _iâˆˆVk_ P

_[âˆ¥][r][ik][âˆ¥][2][ as]_

P _K_ P 2 _K_ 2

_rik_ = (Wk)i,j(xj[r,Ï„] _[âˆ’][1]_ _xÂ¯k[r,Ï„]_ _[âˆ’][1]_ _Î·(_ _fj(xj[r,Ï„]_ _[âˆ’][1])_ _fk(Â¯x[r,Ï„]k_ _[âˆ’][1]))_

_âˆ’_ _âˆ’_ _âˆ‡_ _âˆ’âˆ‡_ [Â¯]

_kX=1_ _iXâˆˆVk_ _kX=1_ _iXâˆˆVk_ _jXâˆˆVk_

2

_K_

= Î·[2] (Wk)i,j( _fj(x[r,Ï„]j_ _[âˆ’][1])_ _fk(Â¯xk[r,Ï„]_ _[âˆ’][1]))_

_âˆ‡_ _âˆ’âˆ‡_ [Â¯]

_kX=1_ _iXâˆˆVk_ _jXâˆˆVk_

2

_K_

= Î·[2] (Wk)i,j( _fj(x[r,Ï„]j_ _[âˆ’][1])_ _fj(Â¯xk[r,Ï„]_ _[âˆ’][1]))_

_âˆ‡_ _âˆ’âˆ‡_

_kX=1_ _iXâˆˆVk_ _jXâˆˆVk_


(Wk)i,jL[2] _xj[r,Ï„]_ _[âˆ’][1]_ _xÂ¯[r,Ï„]k_ _[âˆ’][1]_
_âˆ¥_ _âˆ’_ _âˆ¥[2]_
_jXâˆˆVk_


_â‰¤_ _Î·[2]N_

_â‰¤_ _Î·[2]N_


_iâˆˆVk_


_k=1_

_K_

_k=1_

X


_L[2]_ _x[r,Ï„]i_ _[âˆ’][1]_ _xÂ¯k[r,Ï„]_ _[âˆ’][1]_ = Î·[2] _NL[2]_ _Xk,[r,Ï„]_ _[âˆ’][1]_
_âˆ¥_ _âˆ’_ _âˆ¥[2]_ _Â·_ _âˆ¥_ _âŠ¥_ _âˆ¥[2]_
_iXâˆˆVk_ _kX=1_


(111)


-----

and


_rik_
_âˆ¥_ _âˆ¥[2]_
_iXâˆˆVk_


_k=1_


(Wk)i,j(x[r,Ï„]j _[âˆ’][1]_ _xÂ¯[r,Ï„]k_ _[âˆ’][1]_ _Î·(_ _fj(x[r,Ï„]j_ _[âˆ’][1])_ _fk(Â¯xk[r,Ï„]_ _[âˆ’][1]))_
_âˆ’_ _âˆ’_ _âˆ‡_ _âˆ’âˆ‡_ [Â¯]
_jXâˆˆVk_


_iâˆˆVk_

_iXâˆˆVk_

_iXâˆˆVk_


_k=1_

_K_

_k=1_

X

_K_

_k=1_

X

_K_

_k=1_

X


2
(Wk)i,j _x[r,Ï„]j_ _[âˆ’][1]_ _xÂ¯k[r,Ï„]_ _[âˆ’][1]_ _Î·(_ _fj(x[r,Ï„]j_ _[âˆ’][1])_ _fj(Â¯xk[r,Ï„]_ _[âˆ’][1]))_
_âˆ’_ _âˆ’_ _âˆ‡_ _âˆ’âˆ‡_

_kX=1_ _iXâˆˆVk_ _jXâˆˆVk_

_K_

2 2[]

(Wk)i,j 2 _xj[r,Ï„]_ _[âˆ’][1]_ _xÂ¯k[r,Ï„]_ _[âˆ’][1]_ + 2Î·[2] _fj(x[r,Ï„]j_ _[âˆ’][1])_ _fj(Â¯xk[r,Ï„]_ _[âˆ’][1])_

_âˆ’_ _âˆ‡_ _âˆ’âˆ‡_

_k=1_ _i_ _k_ _j_ _k_ 

X XâˆˆV XâˆˆV

_K_ _K_

2
2 _x[r,Ï„]i_ _[âˆ’][1]_ _xÂ¯[r,Ï„]k_ _[âˆ’][1]_ + 2Î·[2]L[2] _xi[r,Ï„]_ _[âˆ’][1]_ _xÂ¯k[r,Ï„]_ _[âˆ’][1]_
_âˆ’_ _âˆ¥_ _âˆ’_ _âˆ¥[2]_

_kX=1_ _iXâˆˆVk_ _kX=1_ _iXâˆˆVk_

_K_

2(1 + Î·[2]L[2]) _Xk,[r,Ï„]_ _[âˆ’][1]_ _._ (112)
_âˆ¥_ _âŠ¥_ _âˆ¥[2]_
_k=1_

X


Tidy up the expression leads to the following bound of A2,1:


_p_ _[np][ âˆ’]_ [1]
_Â·_ _n_ 1 [E]

_âˆ’_


_Î·_ _fk(Â¯x[r,Ï„]k_ _[âˆ’][1]_
_âˆ‡_ [Â¯]
_k=1_

X


_A2,1 â‰¤2E_

+

_â‰¤2E_


_rik_
_iXâˆˆVk_


(Np)[2]


_k=1_


Eâˆ¥rikâˆ¥[2]
_iXâˆˆVk_


(Np)[2][ p][(1][ âˆ’] _[p][)]_


_n âˆ’_ 1


_k=1_


_p_ _[np][ âˆ’]_ [1]
_Â·_ _n_ 1 _[Î·][2][ Â·][ NL][2]_

_âˆ’_


_Î·_ _fk(Â¯x[r,Ï„]k_ _[âˆ’][1]_
_âˆ‡_ [Â¯]
_k=1_

X


E _Xk,[r,Ï„]_ _[âˆ’][1]_
_âˆ¥_ _âŠ¥_ _âˆ¥[2]_
_k=1_

X


(Np)[2]


2(1 + Î·[2]L[2])E _Xk,[r,Ï„]_ _[âˆ’][1]_
_âˆ¥_ _âŠ¥_ _âˆ¥[2]_
_k=1_

X


(Np)[2][ p][(1][ âˆ’] _[p][)]_


_n âˆ’_ 1


+ [2]

_N [Î·][2][L][2]_


_fk(Â¯x[r,Ï„]k_ _[âˆ’][1]_
_âˆ‡_ [Â¯]
_k=1_

X


E _Xk,[r,Ï„]_ _[âˆ’][1]_
_âˆ¥_ _âŠ¥_ _âˆ¥[2]_
_k=1_

X


_â‰¤2Î·[2]E_

+ [4]

_N_

_â‰¤2Î·[2]E_


1 âˆ’ _p_

_p(n âˆ’_ 1)


(1 + Î·[2]L[2])E _Xk,[r,Ï„]_ _[âˆ’][1]_
_âˆ¥_ _âŠ¥_ _âˆ¥[2]_
_k=1_

X


_K_

E _Xk,[r,Ï„]_ _[âˆ’][1]_
_âˆ¥_ _âŠ¥_ _âˆ¥[2]_

 _k=1_
X


+ [8]


1 âˆ’ _p_

_p(n âˆ’_ 1) [+ 1]Ï„ [2]


_fk(Â¯x[r,Ï„]k_ _[âˆ’][1]_
_âˆ‡_ [Â¯]
_k=1_

X


(113)


where the last inequality holds under the learning rate condition


_Î·[2]_ _â‰¤_


1

(114)
24Ï„ (4Ï„ 1)L[2][ .]
_âˆ’_


This completes the proof of (47).


-----

C.4 PROOF OF PROPOSITION 5

We bound A2,2 in following the same rationale as Proposition 4.

_K_

_A2,2 =E_ [1] I(i _Sk[r][)]_ (Wk)i,j( _fj(x[r,s]j_ [)][ âˆ’] _[g]j[r,s][)]_

_Np_ _âˆˆ_ _âˆ‡_

_kX=1_ _iXâˆˆVk_   j[X]âˆˆVk 

_eik_

_K_ _K_

1 | 2 {z _n_ }
= _p_ _[np][ âˆ’]_ [1] _eik_ + p(1 _p)_

(Np)[2] _Â·_ _n âˆ’_ 1 [E] _kX=1_ _iXâˆˆVk_ _âˆ’_ _n âˆ’_ 1 _kX=1_

2 _p(1_ _p)_
+ _âˆ’_ _e[âŠ¤]ik[e][jâ„“]_

(Np)[2] _n âˆ’_ 1 [E] ï£«k=Ì¸ _â„“_ _iXâˆˆVk_ _jXâˆˆVâ„“_ ï£¶

1 ï£­[X]K 2 ï£¸ _n_ _K_
= _p_ _[np][ âˆ’]_ [1] _eik_ + p(1 _p)_

(Np)[2] _Â·_ _n âˆ’_ 1 [E] _kX=1_ _iXâˆˆVk_ _âˆ’_ _n âˆ’_ 1 _kX=1_


Eâˆ¥eikâˆ¥[2]
_iXâˆˆVk_

Eâˆ¥eikâˆ¥[2]
_iXâˆˆVk_


(115)


where the last equality is due to fact that the inter-cluster stochastic noise is zero mean and independent.

Recall the definition Î¾i[r,s] â‰œ _gi[r,s]_ _âˆ’âˆ‡fi(x[r,s]i_ [)][. Using again the independence of the][ Î¾][i][â€™s we get]


(Wk)i,jÎ¾j[r,s] = E

_kX=1_ _iXâˆˆVk_ _jXâˆˆVk_

_K_

2

E (Wk)i,jÎ¾j[r,s]

_kX=1_ _iXâˆˆVk_ _jXâˆˆVk_


_Î¾i[r,s]_ = NÏƒ[2], (116)
_i=1_

X

_K_

_Wk_ _Ïƒ[2]_
_âˆ¥_ _âˆ¥[2]_
_k=1_

X

(117)


_eik_ = E
_iXâˆˆVk_


_k=1_


and


Eâˆ¥eikâˆ¥[2] =
_iXâˆˆVk_


_k=1_


_Ïƒ[2]_

_k=1_

X


_d[2]j_
_j=1_

X


_K_

_â‰¤_ _Ïƒ[2][]1 +_ _n âˆ’_ 1 _Ï[2]k_ _â‰¤_ _KÏƒ[2]_ + (n âˆ’ 1) KÏ[2]max[Ïƒ][2]

_k=1_

X    

= 1 + (n 1) Ï[2]max _KÏƒ[2]._
_âˆ’_
  

where d1 â‰¤ _d2 â‰¤Â· Â· Â· â‰¤_ _dn = 1 are the singular values of Wk. Therefore,_


1 _n_
_A2,2 â‰¤_ (Np)[2] _p Â·_ _[np]n_ _[ âˆ’]1[1]_ _n_ 1 1 + (n âˆ’ 1) Ï[2]max _KÏƒ[2]_

 _âˆ’_ _[Â·][ NÏƒ][2][ +][ p][(1][ âˆ’]_ _[p][)]_ _âˆ’_

1 _np_ 1 1 1 _p_   
= _Np_ _n_ _âˆ’1_ _[Ïƒ][2]_ + _Np_ _n âˆ’_ 1 1 + (n âˆ’ 1) Ï[2]max _Ïƒ[2]_

 _âˆ’_   _âˆ’_ 

_p[âˆ’][1]_ 1   

_â‰¤_ _[Ïƒ]N[2]_ [+][ Ïƒ]N[2] _n_ _âˆ’1_ 1 + (n âˆ’ 1) Ï[2]max _â‰¤_ _[Ïƒ]N[2]_ 2 + p[âˆ’][1]Ï[2]max _._

_âˆ’_
     


(118)


The last inequality is due to p â‰¥ 1/n.


-----

C.5 PROOF OF PROPOSITION 6

Invoking the descent inequality Lemma 1 and the error bound for T1-T5 given by Corollary 2,
Proposition 3 and Eq. (41):

Ef (x[r][+1])


_Ï„_ _âˆ’1_

E _f_ (Â¯x[r,s])
_âˆ¥âˆ‡_ _âˆ¥[2]_ _âˆ’_ _[Î·]4_
_s=0_

X

_K_

2

[1] _fk(Â¯x[r,s]k_ [)]

_K_ _âˆ‡_ [Â¯]

_k=1_

X


_Ï„_ _âˆ’1_

_s=0_

X


Ef (x[r])
_â‰¤_ _âˆ’_ _[Î·]4_


_âˆ‡fi(x[r,s]i_ [)]
_i=1_

X


_Ï„_ _âˆ’1_

_s=0_

X


_âˆ’_ _[Î·]4_



_[Ï„]_ _[âˆ’][1]_
+ C1L[2]Î·[3]Ï„ _Ï„ + Ï[2]max[D][Ï„,Ï]_ (Î±[2] _f_ (Â¯x[r,s]) + Ïµ[2]g[) +][ C][1][L][2][Î·][3][Ï„][ 2][Ï][2]max[D][Ï„,Ï]Ïµ[Â¯][2]L

_âˆ¥âˆ‡_ _âˆ¥[2]_
_s=0_

   X

+ C1Ï„ [2]L[2]Î·[3]Ï[2]max[Ïƒ][2][ +][ C][1][L][2][(][Ï„][ 2][ +][ D]Ï„,Ï[2] _[Ï][2]max[)][Î·][3][ Ïƒ][2]_


(119)


_Ï„_ _âˆ’1_

_s=0_

X


+ _[Î·][2][L]_


+ _[Î·][2][L]_

2 _[Ï„ Ïƒ]N[2]_


1

E _fi(x[r,s]i_ [)]

_N_ _âˆ‡_

_i=1_

X

_Ï„_ _âˆ’1_

Eâˆ¥âˆ‡f (Â¯x[r,s])âˆ¥[2]
_s=0_

X


Ef (x[r])
_â‰¤_ _âˆ’_ _[Î·]4_



_[Ï„]_ _[âˆ’][1]_
+ C1L[2]Î·[3]Ï„ _Ï„ + Ï[2]max[D][Ï„,Ï]_ (Î±[2]E _f_ (Â¯x[r,s]) + Ïµ[2]g[) +][ C][1][L][2][Î·][3][Ï„][ 2][Ï]max[2] _[D][Ï„,Ï]Ïµ[Â¯][2]L_

_âˆ¥âˆ‡_ _âˆ¥[2]_
_s=0_

   X

+ C1Ï„ [2]L[2]Î·[3]Ï[2]max[Ïƒ][2][ +][ C][1][L][2][(][Ï„][ 2][ +][ D]Ï„,Ï[2] _[Ï]max[2]_ [)][Î·][3][ Ïƒ][2]

_n_ [+][ Î·][2][LÏ„ Ïƒ]N [2] _[.]_


The last inequality holds under the condition that

_Î·_ (120)
_â‰¤_ 2[1]L _[.]_


If we further enforce

_C1L[2]Î·[3]Ï„_ _Ï„ + Ï[2]max[D][Ï„,Ï]_ _Î±[2]_
   _â‰¤_ _[Î·]8_

then
Ef (x[r][+1])


_Î·[2]_ _â‰¤_


1

(121)
8C1L[2]Ï„ (Ï„ + Ï[2]max[D][Ï„,Ï][)][ Î±][2][,]


_Ï„_ _âˆ’1_

Ef (x[r]) E _f_ (Â¯x[r,s])
_â‰¤_ _âˆ’_ _[Î·]8_ _âˆ¥âˆ‡_ _âˆ¥[2]_

_s=0_

X

+ C1L[2]Î·[3]Ï„ [2][  ]Ï„ + Ï[2]max[D][Ï„,Ï] _Ïµ[2]g_ [+][ C][1][L][2][Î·][3][Ï„][ 2][Ï]max[2] _[D][Ï„,Ï]Ïµ[Â¯][2]L_

+ C1Ï„ [2]L[2]Î·[3]Ï[2]max[Ïƒ][2][ +][ C][1][L][2][(][Ï„][ 2][ +][ D]Ï„,Ï[2] _[Ï][2]max[)][Î·][3][ Ïƒ][2]_

_n_ [+][ Î·][2][LÏ„ Ïƒ]N[2]


_Ï„_ _âˆ’1_

=Ef (x[r]) âˆ’ _[Î·]8_ Eâˆ¥âˆ‡f (Â¯x[r,s])âˆ¥[2] + C1L[2]Î·[3]Ï„ [2][  ]Ï„ + Ï[2]max[D][Ï„,Ï] _Ïµ[2]g_

_s=0_

X 

+ C1L[2]Î·[3] _Ï„_ [2]Ï[2]max[(][D][Ï„,Ï]Ïµ[Â¯][2]L [+][ Ïƒ][2][) + (][Ï„][ 2][ +][ D]Ï„,Ï[2] _[Ï][2]max[)]_ _[Ïƒ][2]_ + Î·[2]LÏ„ [Ïƒ][2]

_n_ _N_

 

_Ï„_ _âˆ’1_

Ef (x[r]) E _f_ (Â¯x[r,s]) + Î·[2]LÏ„ [Ïƒ][2]
_â‰¤_ _âˆ’_ _[Î·]8_ _âˆ¥âˆ‡_ _âˆ¥[2]_ _N_

_s=0_

X

1
+ C1L[2]Î·[3]Ï„ [2][  ]Ï„ + Ï[2]max[D][Ï„,Ï] _Ïµ[2]g_ [+ 2][C][1][L][2][Î·][3] _Ï„_ [2]Ï[2]max[D][Ï„,Ï]Ïµ[Â¯][2]L [+][ Ï„][ 2][Ïƒ][2] max

_n_ [+][ Ï][2]

  

the last inequality is due to DÏ„,Ï  _Ï„ and Ïmax_ 1.
_â‰¤_ _â‰¤_


(122)


-----

C.6 PROOF OF PROPOSITION 7

Invoking the descent inequality Lemma 1 and the error bound for T1-T5 given by Corollary 2,
Proposition 3, and 4:

Ef (x[r][+1])


_Ï„_ _âˆ’1_

E _f_ (Â¯x[r,s])
_âˆ¥âˆ‡_ _âˆ¥[2]_ _âˆ’_ _[Î·]4_
_s=0_

X

_K_

2

[1] _fk(Â¯x[r,s]k_ [)]

_K_ _âˆ‡_ [Â¯]

_k=1_

X


_Ï„_ _âˆ’1_

_s=0_

X


Ef (x[r])
_â‰¤_ _âˆ’_ _[Î·]4_


_âˆ‡fi(x[r,s]i_ [)]
_i=1_

X


_Ï„_ _âˆ’1_

_s=0_

X


_âˆ’_ _[Î·]4_

+ _[Î·]_

4

+ _[Î·]_


_Ï„_ 1 _N_ _Ï„_ 1
_âˆ’_ 2 _âˆ’_

_[Î·]_ E _f_ (Â¯x[r,s]) _fi(x[r,s]i_ [)] + _[Î·]_ E _f_ (Â¯x[r,s])

4 _âˆ‡_ _âˆ’_ _N[1]_ _âˆ‡_ 4 _âˆ‡_ _âˆ’_ _K[1]_

_s=0_ _i=1_ _s=0_

X X X

_Ï„_ 1 _N_ _K_
_âˆ’_ 2

_[Î·]_ E [1] _fi(x[r,s]i_ [)][ âˆ’] [1] _fk(Â¯x[r,s]k_ [)]

4 _N_ _âˆ‡_ _K_ _âˆ‡_ [Â¯]

_s=0_ _i=1_ _k=1_

X X X

_Ï„_ _âˆ’2_ _L_ _L_

E _x[r,s][+1]_ _xÂ¯[r,s]_ + E _x[r,Ï„]_ _[âˆ’][1]_

2 _âˆ’_ _âˆ¥[2]_ 2 _âˆ¥[2]_

_s=0_  _[âˆ¥][Â¯]_   _[âˆ¥][x][r][+1][ âˆ’]_ [Â¯] 

X


_âˆ‡f[Â¯]k(Â¯x[r,s]k_ [)]
_k=1_

X


_Ï„_ _âˆ’1_

E _f_ (Â¯x[r,s])
_âˆ¥âˆ‡_ _âˆ¥[2]_ _âˆ’_ _[Î·]4_
_s=0_

X

_K_

2

[1] _fk(Â¯x[r,s]k_ [)]

_K_ _âˆ‡_ [Â¯]

_k=1_

X


_Ï„_ _âˆ’1_

_s=0_

X


Ef (x[r])
_â‰¤_ _âˆ’_ _[Î·]4_


_âˆ‡fi(x[r,s]i_ [)]
_i=1_

X


_Ï„_ _âˆ’1_

_s=0_

X


_âˆ’_ _[Î·]4_



_[Ï„]_ _[âˆ’][1]_
+ C1L[2]Î·[3]Ï„ _Ï„ + Ï[2]max[D][Ï„,Ï]_ (Î±[2] _f_ (Â¯x[r,s]) + Ïµ[2]g[) +][ C][1][L][2][Î·][3][Ï„][ 2][Ï]max[2] _[D][Ï„,Ï]Ïµ[Â¯][2]L_

_âˆ¥âˆ‡_ _âˆ¥[2]_
_s=0_

   X

+ C1Ï„ [2]L[2]Î·[3]Ï[2]max[Ïƒ][2][ +][ C][1][L][2][(][Ï„][ 2][ +][ D]Ï„,Ï[2] _[Ï]max[2]_ [)][Î·][3][ Ïƒ][2]


_Ï„_ _âˆ’2_

+ _[L]_

2 _[Î·][2]_

_s=0_

X

1

+ LÎ·[2]E


+ _[L]_

2 _[Î·][2][(][Ï„][ âˆ’]_ [1)] _[Ïƒ]N[2]_


1

_fi(x[r,s]i_ [)]

_N_ _âˆ‡_

_i=1_

X

_K_

_fk(Â¯x[r,Ï„]k_ _[âˆ’][1])_
_âˆ‡_ [Â¯]
_k=1_

X


+ _[L]_

2 _[Î·][2][ Ïƒ]N[2]_


+ _[L]_


_Gp + [1]_

_Ï„_ [2]


2 + _m[n]_ _[Ï]max[2]_ _._ (123)




E _Xk,[r,Ï„]_ _[âˆ’][1]_
_âˆ¥_ _âŠ¥_ _âˆ¥[2]_
_k=1_

X


Denote for short

_G[â€²]p_ [â‰œ] _[G][p]_ [+ 1]Ï„ [2][ .] (124)

Further applying the bounds on the consensus error derived in Proposition 2:

Ef (x[r][+1])


_Ï„_ _âˆ’1_

E _f_ (Â¯x[r,s])
_âˆ¥âˆ‡_ _âˆ¥[2]_ _âˆ’_ _[Î·]4_
_s=0_

X

_K_

2

[1] _fk(Â¯x[r,s]k_ [)]

_K_ _âˆ‡_ [Â¯]

_k=1_

X


_Ï„_ _âˆ’1_

_s=0_

X


Ef (x[r])
_â‰¤_ _âˆ’_ _[Î·]4_


_âˆ‡fi(x[r,s]i_ [)]
_i=1_

X


_Ï„_ _âˆ’1_

_âˆ’_ _[Î·]4_

_s=0_

X



_[Ï„]_ _[âˆ’][1]_
+ C1L[2]Î·[3]Ï„ _Ï„ + Ï[2]max[D][Ï„,Ï]_ (Î±[2] _f_ (Â¯x[r,s]) + Ïµ[2]g[) +][ C][1][L][2][Î·][3][Ï„][ 2][Ï][2]max[D][Ï„,Ï]Ïµ[Â¯][2]L

_âˆ¥âˆ‡_ _âˆ¥[2]_
_s=0_

   X


-----

+ C1Ï„ [2]L[2]Î·[3]Ï[2]max[Ïƒ][2][ +][ C][1][L][2][(][Ï„][ 2][ +][ D]Ï„,Ï[2] _[Ï][2]max[)][Î·][3][ Ïƒ][2]_


_Ï„_ _âˆ’2_

+ _[L]_

2 _[Î·][2]_

_s=0_

X


+ _[L]_

2 _[Î·][2][(][Ï„][ âˆ’]_ [1)] _[Ïƒ]N[2]_


1

_fi(x[r,s]i_ [)]

_N_ _âˆ‡_

_i=1_

X

_K_

_fk(Â¯x[r,Ï„]k_ _[âˆ’][1])_
_âˆ‡_ [Â¯]
_k=1_

X


+ LÎ·[2]E

+ 4LG[â€²]p

+ 4LG[â€²]p

+ _[L]_

2 _[Î·][2][ Ïƒ]N[2]_


_Ï„_ _âˆ’2_

2C2 _s=0_ _DÏ„,Ï[2]_ _[Ï][2]max[Î·][2][(][Î±][2][âˆ¥âˆ‡][f]_ [(Â¯]x[r,s])âˆ¥[2] + Ïµ[2]g[)]!

X

1 _DÏ„,Ï_

_C2DÏ„,Ï[2]_ _[Ï„Ï][2]max[Î·][2]Ïµ[Â¯][2]L_ [+][ C][2] + 1 _Ï[2]max[Ï„D][Ï„,Ï][Î·][2][Ïƒ][2]_

_n_ _Ï„_

  

2 + _m[n]_ _[Ï]max[2]_ (125)
 


Rearranging terms and tidy up the expression we have

Ef (x[r][+1])


_Ï„_ _âˆ’1_

E _f_ (Â¯x[r,s])
_âˆ¥âˆ‡_ _âˆ¥[2]_ _âˆ’_ _[Î·]4_
_s=0_

X


_Ï„_ _âˆ’1_

_s=0_

X


Ef (x[r])
_â‰¤_ _âˆ’_ _[Î·]4_


_âˆ‡fi(x[r,s]i_ [)]
_i=1_

X

_N_

1

_fi(x[r,s]i_ [)]

_N_ _âˆ‡_

_i=1_

X


_Ï„_ _âˆ’1_

_s=0_

X


_K_ _Ï„_ 2

2 _âˆ’_

[1] _fk(Â¯x[r,s]k_ [)] + _[L]_

_âˆ‡_ [Â¯] 2 _[Î·][2]_
_k=1_ _s=0_

X X

2

_K_

_fk(Â¯x[r,Ï„]k_ _[âˆ’][1])_
_âˆ‡_ [Â¯]
_k=1_

X


_âˆ’_ _[Î·]4_


+ LÎ·[2]E



_[Ï„]_ _[âˆ’][1]_
+ C1L[2]Î·[3]Ï„ _Ï„ + Ï[2]max[D][Ï„,Ï]_ (Î±[2] _f_ (Â¯x[r,s]) + Ïµ[2]g[)] (126)

_âˆ¥âˆ‡_ _âˆ¥[2]_
_s=0_

  _Ï„_ _âˆ’2X_

+ 8C2G[â€²]p[LD]Ï„,Ï[2] _[Ï][2]max[Î·][2]_ _s=0(Î±[2]âˆ¥âˆ‡f_ (Â¯x[r,s])âˆ¥[2] + Ïµ[2]g[)]!
X

+ C1L[2]Î·[3] _Ï„_ [2]DÏ„,ÏÏ[2]maxÏµ[Â¯][2]L [+][ Ï„][ 2][Ï]max[2] _[Ïƒ][2][ + (][Ï„][ 2][ +][ D]Ï„,Ï[2]_ _[Ï][2]max[)]_ _[Ïƒ][2]_
 


+ _[L]_ 2 + _[n]_ max _Î·[2][ Ïƒ][2]_

2 [(][Ï„][ âˆ’] [1)][Î·][2][ Ïƒ]N[2] [+][ L]2 _m_ _[Ï][2]_ _N_

 1 _D_ _Ï„,Ï_

+ 4C2LG[â€²]p[Î·][2] _DÏ„,Ï[2]_ _[Ï„Ï][2]maxÏµ[Â¯][2]L_ [+] + 1 _Ï[2]max[Ï„D][Ï„,Ï][Ïƒ][2]_

_n_ _Ï„_

  


Notice that if the following conditions on the learning rate are satisfied

_Î·_

_Î·4_ _[â‰¥]_ _[Î·][2][L,]_ (127)
_Ï„ + Ï[2]max[D][Ï„,Ï]_ _Î±[2]_ + 8C2G[â€²]p[LD]Ï„,Ï[2] _[Ï][2]max[Î·][2][Î±][2][,]_
8

_[â‰¥]_ _[C][1][L][2][Î·][3][Ï„]_
  


-----

then the terms associated to the gradients will be negative and

Ef (x[r][+1])

_Ï„_ _âˆ’1_

Ef (x[r]) E _f_ (Â¯x[r,s])
_â‰¤_ _âˆ’_ _[Î·]8_ _âˆ¥âˆ‡_ _âˆ¥[2]_

_s=0_

X

1
+ C1L[2]Î·[3]Ï„ [2][  ]Ï„ + Ï[2]max[D][Ï„,Ï] _Ïµ[2]g_ [+ 2][C][1][L][2][Î·][3] _Ï„_ [2]Ï[2]max[D][Ï„,Ï]Ïµ[Â¯][2]L [+][ Ï„][ 2][Ïƒ][2] max

_n_ [+][ Ï][2]

  


+ LÏ„Î·[2][ Ïƒ]N[2] [+] 8C2G[â€²]p[LD]Ï„,Ï[2] _[Ï„Ï][2]max_ _Î·[2]Ïµ[2]g_

  1 _DÏ„,Ï_ _n_

+ 4C2LG[â€²]p[Î·][2] _DÏ„,Ï[2]_ _[Ï„Ï][2]maxÏµ[Â¯][2]L_ [+] + 1 _Ï[2]max[Ï„D][Ï„,Ï][Ïƒ][2]_ + _[L]_ max _Î·[2][ Ïƒ][2]_

_n_ _Ï„_ 2 _m_ _[Ï][2]_ _N [.]_

     


In the last step we clean the condition on the learning rate Î·. Collecting all the conditions on Î·:


1
_Î·[2]_ (128)
_â‰¤_ 24L[2]Ï„ (4Ï„ 1) _[,]_

_âˆ’_

_Î·_
(129)
4

_Î·_ _[â‰¥]_ _[Î·][2][L,]_
_Ï„ + Ï[2]max[D][Ï„,Ï]_ _Î±[2]_ + 8C2G[â€²]p[LD]Ï„,Ï[2] _[Ï]max[2]_ _[Î·][2][Î±][2][.]_ (130)
8

_[â‰¥]_ _[C][1][L][2][Î·][3][Ï„]_
  

Clearly, (128) implies (129). To ensure (130) it suffices to require


_Î·_

_Ï„ + Ï[2]max[D][Ï„,Ï]_ _Î±[2]_
16Î· _[â‰¥]_ _[C][1][L][2][Î·][3][Ï„]_ (131)
  

_p[LD]Ï„,Ï[2]_ _[Ï]max[2]_ _[Î·][2][Î±][2][.]_
16

_[â‰¥]_ [8][C][2][G][â€²]

_n_ _m_

_G[â€²]p_ [=][ G][p] [+ 1]Ï„ [2][,] and _Gp =_ _m( âˆ’n_ 1) _[.]_ (132)

_âˆ’_


Recall the definition of G[â€²]p[:]

It can be verified that if


1

(133)
_C3[â€²]_ _[Î±][2][Ï„L.]_


_Î· â‰¤_


for some C3[â€²] _[>][ 0][ large enough, then]_

_Î·_

_Ï„ + Ï[2]max[D][Ï„,Ï]_ _Î±[2]_
16

_[â‰¥]_ _[C][1][L][2][Î·][3][Ï„]_

_Î·_ 1   

_LDÏ„,Ï[2]_ _[Ï][2]max[Î·][2][Î±][2][.]_

32 _Ï„_ [2]

_[â‰¥]_ [8][C][2]  


It remains to guarantee

_Î·_

_Ï„,Ï[Ï][2]max[Î·][2][Î±][2][.]_ (134)
32

_[â‰¥]_ [8][C][2][G][p][LD][2]

Rearranging terms gives the condition


1

(135)
256C2GpDÏ„,Ï[2] _[Ï]max[2]_ _[Î±][2][L.]_


_Î· â‰¤_


Combining with (133) and using the fact that DÏ„,ÏÏmax _Ï„ provides the final condition on Î· as_
_â‰¤_


1

_._ (136)
_Î±GpDÏ„,Ï Ïmax_



_Î· â‰¤_


1,
_C3Î±Ï„L_

_[Â·][ min]_ 


-----

D SUPPORTING LEMMAS

D.1 PROOF OF LEMMA 1

Since the global average of the local copies follows the update [cf. (19)]:


_xÂ¯[r,s][+1]_ = Â¯x[r,s] _Î·_ [1]
_âˆ’_ _N_


_gi[r,s][,][ âˆ€][s][ = 0][, . . ., Ï„][ âˆ’]_ [1][.] (137)
_i=1_

X


Under Assumption 1, we can apply the descent lemma at points Â¯x[r,s][+1] and Â¯x[r,s] for s = 0, . . ., Ï„ âˆ’2,
conditioned on Fr,sâˆ’1:

_L_

Er,s 1f (Â¯x[r,s][+1]) _f_ (Â¯x[r,s]) + _f_ (Â¯x[r,s])[âŠ¤]Er,s 1 _xÂ¯[r,s][+1]_ _xÂ¯[r,s][]_ + Er,s 1 _x[r,s][+1]_ _xÂ¯[r,s]_
_âˆ’_ _â‰¤_ _âˆ‡_ _âˆ’_ _âˆ’_ _âˆ’_ 2 _âˆ’_ _âˆ¥[2]_

_N_   _[âˆ¥][Â¯]_

(a) _L_
= _f_ (Â¯x[r,s]) âˆ’ _Î·âˆ‡f_ (Â¯x[r,s])[âŠ¤][][ 1]N _âˆ‡fi(x[r,s]i_ [)] + Er,sâˆ’1 2 _x[r,s][+1]_ _âˆ’_ _xÂ¯[r,s]âˆ¥[2]_

Xi=1   _[âˆ¥][Â¯]_ 

_N_

=f (Â¯x[r,s]) _x[r,s])[âŠ¤][][ 1]_ _fi(x[r,s]i_ [)]
_âˆ’_ [1]2 _[Î·][âˆ‡][f]_ [(Â¯] _N_ _âˆ‡_

_i=1_

X 

_N_ _K_

_L_

_âˆ’_ 2[1] _[Î·][âˆ‡][f]_ [(Â¯]x[r,s])[âŠ¤][][ 1]N _âˆ‡fi(x[r,s]i_ [)][ Â±][ 1]K _âˆ‡f[Â¯]k(Â¯x[r,s]k_ [)] + Er,sâˆ’1 2 _x[r,s][+1]_ _âˆ’_ _xÂ¯[r,s]âˆ¥[2]_

Xi=1 _kX=1_   _[âˆ¥][Â¯]_ 


(b)
_f_ (Â¯x[r,s])
_â‰¤_ _âˆ’_ _[Î·]2_


_N_ _N_

(b) 1 2
_f_ (Â¯x[r,s]) _x[r,s])_ + [1] [1] _fi(x[r,s]i_ [)] _f_ (Â¯x[r,s]) _fi(x[r,s]i_ [)]
_â‰¤_ _âˆ’_ _[Î·]2_ 2 _[âˆ¥âˆ‡][f]_ [(Â¯] _âˆ¥[2]_ 2 _N_ _âˆ‡_ _âˆ’_ [1]2 _âˆ‡_ _âˆ’_ _N[1]_ _âˆ‡_

_i=1_ _i=1_

X X

_K_ _K_

1 2 2
_x[r,s])_ + [1] [1] _fk(Â¯x[r,s]k_ [)] _f_ (Â¯x[r,s]) _fk(Â¯x[r,s]k_ [)]

_âˆ’_ _[Î·]2_ 2 _[âˆ¥âˆ‡][f]_ [(Â¯] _âˆ¥[2]_ 2 _K_ _k=1_ _âˆ‡_ [Â¯] _âˆ’_ [1]2 _âˆ‡_ _âˆ’_ _K[1]_ _k=1_ _âˆ‡_ [Â¯] !

X X

_N_ _K_

+ _[Î·]_ _x[r,s])_ [1] _fi(x[r,s]i_ [)][ âˆ’] [1] _fk(Â¯x[r,s]k_ [)]

2 _[âˆ¥âˆ‡][f]_ [(Â¯] _âˆ¥_ _N_ _âˆ‡_ _K_ _âˆ‡_ [Â¯]

_i=1_ _k=1_

X X

_L_

+ Er,s 1 _x[r,s][+1]_ _xÂ¯[r,s]_
_âˆ’_ 2 _âˆ’_ _âˆ¥[2]_

 _[âˆ¥][Â¯]_ 

_N_ _K_

(c) 2 2
_f_ (Â¯x[r,s]) _x[r,s])_ [1] _fi(x[r,s]i_ [)] [1] _fk(Â¯x[r,s]k_ [)]
_â‰¤_ _âˆ’_ _[Î·]4_ _[âˆ¥âˆ‡][f]_ [(Â¯] _âˆ¥[2]_ _âˆ’_ _[Î·]4_ _N_ _âˆ‡_ _âˆ’_ _[Î·]4_ _K_ _âˆ‡_ [Â¯]

_i=1_ _k=1_

X X


1
_x[r,s])_ + [1]
2 _[âˆ¥âˆ‡][f]_ [(Â¯] _âˆ¥[2]_ 2


_N_

2
_fi(x[r,s]i_ [)]
_âˆ‡_ _âˆ’_ [1]2
_i=1_

X


_N_ _K_

2 2

+ _[Î·]_ _f_ (Â¯x[r,s]) _fi(x[r,s]i_ [)] + _[Î·]_ _f_ (Â¯x[r,s]) _fk(Â¯x[r,s]k_ [)]

4 _âˆ‡_ _âˆ’_ _N[1]_ _âˆ‡_ 4 _âˆ‡_ _âˆ’_ _K[1]_ _âˆ‡_ [Â¯]

_i=1_ _k=1_

X X

_N_ _K_

2 _L_

+ _[Î·]4_ _N[1]_ _âˆ‡fi(x[r,s]i_ [)][ âˆ’] _K[1]_ _âˆ‡f[Â¯]k(Â¯x[r,s]k_ [)] + Er,sâˆ’1 2 _x[r,s][+1]_ _âˆ’_ _xÂ¯[r,s]âˆ¥[2]_ _,_

_i=1_ _k=1_  _[âˆ¥][Â¯]_ 

X X

where (a) is due to Assumption 2, (b) is due to 2ab = âˆ¥aâˆ¥[2] + âˆ¥bâˆ¥[2] _âˆ’âˆ¥a âˆ’_ _bâˆ¥[2]_ and ab â‰¤âˆ¥aâˆ¥âˆ¥bâˆ¥,
and (c) is due to âˆ¥aâˆ¥âˆ¥bâˆ¥â‰¤ [1]2 _[âˆ¥][a][âˆ¥][2][ +][ 1]2_ _[âˆ¥][b][âˆ¥][2][. Notation][ a][ Â±][ b][ stands for adding and subtracting, i.e.,]_

_a Â± b = a + b âˆ’_ _b._

For the pair (Â¯x[r,Ï„] _[âˆ’][1], x[r][+1]) we have according to (19) and (22):_


_xÂ¯[r,Ï„]_ _[âˆ’][1]_ _Î·_ [1]
_âˆ’_ _N_


Er,Ï„ 2x[r][+1] = Er,Ï„ 2(Â¯x[r,Ï„] ) = Er,Ï„ 2
_âˆ’_ _âˆ’_ _âˆ’_


_gi[r,Ï„]_ _[âˆ’][1]_
_i=1_

X


Applying the descent lemma in the same way as before yields

Er,Ï„ 2f (x[r][+1])
_âˆ’_

_f_ (Â¯x[r,Ï„] _[âˆ’][1]) +_ _f_ (Â¯x[r,Ï„] _[âˆ’][1])[âŠ¤]Er,Ï„_ 2(x[r][+1] _xÂ¯[r,Ï„]_ _[âˆ’][1]) +_ _[L]_ _x[r,Ï„]_ _[âˆ’][1]_
_â‰¤_ _âˆ‡_ _âˆ’_ _âˆ’_ 2 [E][r,Ï„] _[âˆ’][2][âˆ¥][x][r][+1][ âˆ’]_ [Â¯] _âˆ¥[2]_


-----

_f_ (Â¯x[r,Ï„] _[âˆ’][1])_ _x[r,Ï„]_ _[âˆ’][1])_
_â‰¤_ _âˆ’_ _[Î·]4_ _[âˆ¥âˆ‡][f]_ [(Â¯] _âˆ¥[2]_ _âˆ’_ _[Î·]4_


_N_

2
_âˆ‡fi(xi[r,Ï„]_ _[âˆ’][1])_ _âˆ’_ _[Î·]4_
_i=1_

X


_fk(Â¯x[r,Ï„]k_ _[âˆ’][1]_
_âˆ‡_ [Â¯]
_k=1_

X

_K_

_fk(Â¯xk[r,Ï„]_ _[âˆ’][1]_
_âˆ‡_ [Â¯]
_k=1_

X


+ _[Î·]_

4

+ _[Î·]_


_N_ _K_

2

_âˆ‡f_ (Â¯x[r,Ï„] _[âˆ’][1]) âˆ’_ _N[1]_ _âˆ‡fi(xi[r,Ï„]_ _[âˆ’][1])_ + _[Î·]4_ _âˆ‡f_ (Â¯x[r,Ï„] _[âˆ’][1]) âˆ’_ _K[1]_ _âˆ‡f[Â¯]k(Â¯xk[r,Ï„]_ _[âˆ’][1]_

_i=1_ _k=1_

X X

_N_ _K_

2 _L_

_N[1]_ _âˆ‡fi(x[r,Ï„]i_ _[âˆ’][1]) âˆ’_ _K[1]_ _âˆ‡f[Â¯]k(Â¯xk[r,Ï„]_ _[âˆ’][1])_ + Er,Ï„ _âˆ’2_ 2 _x[r,Ï„]_ _[âˆ’][1]âˆ¥[2]_

_i=1_ _k=1_  _[âˆ¥][x][r][+1][ âˆ’]_ [Â¯]

X X


Taking expectation, summing over the iterations in round r over s = 0, . . ., Ï„ âˆ’ 1 and using the fact
that x[r] = Â¯x[r,][0] completes the proof.

D.2 PROOF OF LEMMA 2

Recall the average update of the k-th cluster and that of the global average given by (15) and (19),
respectively, for s = 0, . . ., Ï„ âˆ’ 1:

_xÂ¯[r,s]k_ [+1] = Â¯x[r,s]k _Î·_ Â¯gk[r,s] (138)
_âˆ’_ _Â·_


_xÂ¯[r,s][+1]_ = Â¯x[r,s] _Î·_ [1]
_âˆ’_ _Â·_ _N_


_gi[r,s][.]_ (139)
_i=1_

X


Taking the difference gives

E _xÂ¯[r,s][+1]_ _xÂ¯[r,s]k_ [+1]
_âˆ¥_ _âˆ’_ _âˆ¥[2]_

1

=E _x[r,s]_ _xÂ¯[r,s]k_ [)][ âˆ’] _[Î·]_

_âˆ’_ _n_

[(Â¯] 1

+ Î·[2]E _n_ _Î¾i[r,s]_ _âˆ’_ _N[1]_

_iXâˆˆVk_


_fi(x[r,s]i_ [)][ âˆ’] [1]
_âˆ‡_ _N_
_iXâˆˆVk_

2

_N_

_Î¾i[r,s]_
_i=1_

X


_âˆ‡fi(x[r,s]i_ [)]
_i=1_

X


(140)


_fi(x[r,s]i_ [)][ âˆ’] [1]
_âˆ‡_ _N_
_iXâˆˆVk_


(1 + Ïµ)E _xÂ¯[r,s]_ _xÂ¯[r,s]k_
_â‰¤_ _âˆ¥_ _âˆ’_ _[âˆ¥][2][ + (1 +][ Ïµ][âˆ’][1][)][Î·][2][E]_


_âˆ‡fi(x[r,s]i_ [)]
_i=1_

X


_Î¾i[r,s]_ _âˆ’_ _N[1]_
_iXâˆˆVk_


_Î¾i[r,s]_
_i=1_

X


+ Î·[2]E


where Ïµ > 0 is some constant to be chosen.

Averaging over k = 1, . . ., K:

_K_

1

_K_ Eâˆ¥xÂ¯[r,s][+1] _âˆ’_ _xÂ¯[r,s]k_ [+1]âˆ¥[2]

_k=1_

X


(1 + Ïµ) [1]
_â‰¤_ _K_

+ Î·[2][ 1]


_K_

_k=1_ Eâˆ¥xÂ¯[r,s] _âˆ’_ _xÂ¯[r,s]k_ _[âˆ¥][2][ + (1 +][ Ïµ][âˆ’][1][)][Î·][2][ 1]K_

X


1

_fi(x[r,s]i_ [)][ âˆ’] [1]

_n_ _âˆ‡_ _N_

_iXâˆˆVk_

_N_

2
_i=1_ _âˆ‡fi(x[r,s]i_ [)] !

X


_âˆ‡fi(x[r,s]i_ [)]
_i=1_

X


_k=1_


_Î¾i[r,s]_ _âˆ’_ _N[1]_
_iXâˆˆVk_


_Î¾i[r,s]_
_i=1_

X


_k=1_


(a)
=(1 + Ïµ) [1]


E _xÂ¯[r,s]_ _xÂ¯[r,s]k_
_k=1_ _âˆ¥_ _âˆ’_ _[âˆ¥][2]_

X


2
_âˆ‡fi(x[r,s]i_ [)] _âˆ’_ E
_iXâˆˆVk_


+ (1 + Ïµ[âˆ’][1])Î·[2]


_k=1_


-----

1
+ Î·[2]

_K_

(1 + Ïµ) [1]
_â‰¤_ _K_

=(1 + Ïµ) [1]


_Î¾i[r,s]_ E
_âˆ’_
_iXâˆˆVk_


_Î¾i[r,s]_
_i=1_

X


_k=1_


+ Î·[2][ K][ âˆ’] [1]


E _xÂ¯[r,s]_ _xÂ¯[r,s]k_
_k=1_ _âˆ¥_ _âˆ’_ _[âˆ¥][2][ + (1 +][ Ïµ][âˆ’][1][)][Î·][2]_

X

_K_

E _xÂ¯[r,s]_ _xÂ¯[r,s]k_
_k=1_ _âˆ¥_ _âˆ’_ _[âˆ¥][2]_

X


_âˆ‡fi(x[r,s]i_ [)]
_iXâˆˆVk_


_Ïƒ[2]_


_k=1_


+ Î·[2][ K][ âˆ’] [1]


_âˆ‡fi(x[r,s]i_ [)][ Â± âˆ‡]f[Â¯]k(Â¯x[r,s]k [)][ Â± âˆ‡]f[Â¯]k(Â¯x[r,s])
 


+ (1 + Ïµ[âˆ’][1])Î·[2]


_Ïƒ[2]_


_k=1_


_iâˆˆVk_


(1 + Ïµ) [1]
_â‰¤_ _K_


E _xÂ¯[r,s]_ _xÂ¯[r,s]k_
_k=1_ _âˆ¥_ _âˆ’_ _[âˆ¥][2][ + (1 +][ Ïµ][âˆ’][1][)][Î·][2]_

X


_âˆ‡fi(x[r,s]i_ [)][ âˆ’âˆ‡]f[Â¯]k(Â¯x[r,s]k [)]


_k=1_


_iâˆˆVk_


+ (1 + Ïµ[âˆ’][1])Î·[2][ 3]


_K_

2
E _fk(Â¯x[r,s]k_ [)][ âˆ’âˆ‡]f[Â¯]k(Â¯x[r,s]) + (1 + Ïµ[âˆ’][1])Î·[2][ 3]
_âˆ‡_ [Â¯] _K_
_k=1_

X


Eâˆ¥âˆ‡f[Â¯]k(Â¯x[r,s])âˆ¥[2]
_k=1_

X


+ Î·[2][ K][ âˆ’] [1] _Ïƒ[2]_


(b)
(1 + Ïµ) [1]
_â‰¤_ _K_


E _xÂ¯[r,s]_ _xÂ¯[r,s]k_
_k=1_ _âˆ¥_ _âˆ’_ _[âˆ¥][2][ + (1 +][ Ïµ][âˆ’][1][)][Î·][2]_

X


3

_â‰¤_ (1 + Ïµ) K[1] _k=1_ Eâˆ¥xÂ¯[r,s] _âˆ’_ _xÂ¯[r,s]k_ _[âˆ¥][2][ + (1 +][ Ïµ][âˆ’][1][)][Î·][2]_ _N_ _k=1_ _L[2]E_ _Xk,[r,s]âŠ¥_ !

X X

_K_ _K_

2 2

+ (1 + Ïµ[âˆ’][1])Î·[2][ 3]K _L[2]E_ _xÂ¯[r,s]k_ _âˆ’_ _xÂ¯[r,s]_ + (1 + Ïµ[âˆ’][1])Î·[2][ 3]K E _âˆ‡f[Â¯]k(Â¯x[r,s])_ + Î·[2][ K][ âˆ’]N [1] _Ïƒ[2]_

_k=1_ _k=1_

X X

_K_ _K_

1 3 2
= 1 + Ïµ + 3L[2]Î·[2](1 + Ïµ[âˆ’][1]) _K_ _k=1_ Eâˆ¥xÂ¯[r,s] _âˆ’_ _xÂ¯[r,s]k_ _[âˆ¥][2][ + (1 +][ Ïµ][âˆ’][1][)][Î·][2]_ _N_ _k=1_ _L[2]E_ _Xk,[r,s]âŠ¥_ !
   X X

_K_

2

+ (1 + Ïµ[âˆ’][1])Î·[2][ 3] E _fk(Â¯x[r,s])_ + Î·[2][ K][ âˆ’] [1] _Ïƒ[2]._ (141)

_K_ _âˆ‡_ [Â¯] _N_

_k=1_

X

In (a) we used the fact that


_fi(x[r,s]i_ [) = 1]
_âˆ‡_ _N_
_iXâˆˆVk_

_K_

_xi_ _xÂ¯_ =
_i=1_ _âˆ¥_ _âˆ’_ _âˆ¥[2]_

X


_Î¾i[r,s]_ = N[1]
_iXâˆˆVk_


_âˆ‡fi(x[r,s]i_ [)][,]
_i=1_

X


_Î¾i[r,s][.]_ (142)
_i=1_

X


_k=1_


_k=1_


and


_K_

_xi_ _K_ _xÂ¯_ with _xÂ¯ = [1]_
_âˆ¥_ _âˆ¥[2]_ _âˆ’_ _âˆ¥_ _âˆ¥[2]_ _K_
_i=1_

X


_xi._ (143)

_k=1_

X

_K_

1 2

_N_ _k=1_ E _Xk,[r,s]âŠ¥_ !

X


In (b) we applied the L-smoothness of fi and _f[Â¯]k._


Choosing Ïµ =

we have


1

4Ï„ 1 [and using the condition that]
_âˆ’_


_Î·[2]_ _â‰¤_


24Ï„ (4Ï„ âˆ’ 1)L[2]


E _xÂ¯[r,s][+1]_ _xÂ¯[r,s]k_ [+1]
_âˆ¥_ _âˆ’_ _âˆ¥[2]_
_k=1_

X

1 1 1
1 +

4Ï„ 1 [+] 2(4Ï„ 1) _K_
_âˆ’_ _âˆ’_ 


E _xÂ¯[r,s]_ _xÂ¯[r,s]k_
_k=1_ _âˆ¥_ _âˆ’_ _[âˆ¥][2][ + 12][Ï„Î·][2][L][2]_

X


-----

_K_

2

+ 12Ï„Î·[2][ 1] E _fk(Â¯x[r,s])_ + Î·[2][ K][ âˆ’] [1] _Ïƒ[2]_

_K_ _âˆ‡_ [Â¯] _N_

_k=1_

X

_K_ _K_

1 1 2
_â‰¤CÏ„_ _K_ _k=1_ Eâˆ¥xÂ¯[r,s] _âˆ’_ _xÂ¯[r,s]k_ _[âˆ¥][2][ + 12][Ï„Î·][2][L][2]_ _N_ _k=1_ E _Xk,[r,s]âŠ¥_ !

X X

+ 12Ï„Î·[2][  ]Î±[2]Eâˆ¥âˆ‡f (Â¯x[r,s])âˆ¥[2] + Ïµ[2]g + Î·[2][ K][ âˆ’]N [1] _Ïƒ[2]._

In the last inequality we applied Assumption 5 on the inter-cluster heterogeneity.

D.3 PROOF OF LEMMA 3

We follow the perturbed average consensus analysis. Recall the update equation of the consensus
error given in (17):

_Xk,[r,s][+1]_ = (Wk _J)(Xk,[r,s]_ _k_ [)][.] (144)
_âŠ¥_ _âˆ’_ _âŠ¥_ _[âˆ’]_ _[Î·G][r,s]_


Squaring both sides and conditioning:

E _Xk,[r,s][+1]_ = E E (Wk _J)(Xk,[r,s]_ _k_ [)][ âˆ’] _[Î·G]k[r,s][)][âˆ¥][2][|F][r,s][âˆ’][1]_
_âˆ¥_ _âŠ¥_ _âˆ¥[2]_ _âˆ¥_ _âˆ’_ _âŠ¥_ _[Â±][ Î·][âˆ‡][F][k][(][X]_ _[r,s]_

_â‰¤_ Eâˆ¥(W k âˆ’ _J)(Xk,[r,s]âŠ¥_ _[âˆ’]_ _[Î·][âˆ‡][F][k][(][X]k[r,s][))][âˆ¥][2][ +][ Î·][2][Ï]k[2][nÏƒ][2]_ []

_â‰¤_ _Ï[2]k[(1 +][ Î¶]k[âˆ’][1][)][ Â·][ E][âˆ¥][X]k,[r,s]âŠ¥[âˆ¥][2][ +][ Ï]k[2][(1 +][ Î¶][k][)][Î·][2][E][âˆ¥âˆ‡][F][k][(][X]k[r,s][)][âˆ¥][2][ +][ Î·][2][Ï]k[2][nÏƒ][2][,]_

where Î¶k > 0 is some free parameter to be properly chosen. Next, we bound the norm of the
pseudo-gradient âˆ‡Fk(Xk[r,s][)][.]

_Fk(Xk[r,s][)][âˆ¥][2][ =]_ _fi(x[r,s]i_ [)][âˆ¥][2]
_âˆ¥âˆ‡_ _âˆ¥âˆ‡_

_iXâˆˆVk_

= _âˆ¥âˆ‡fi(x[r,s]i_ [)][ Â± âˆ‡][f][i][(Â¯]x[r,s]k [)][ Â± âˆ‡]f[Â¯]k(Â¯x[r,s]k [)][ Â± âˆ‡]f[Â¯]k(Â¯x[r,s])âˆ¥[2]

_iXâˆˆVk_

_â‰¤_ (4âˆ¥âˆ‡fi(x[r,s]i [)][ âˆ’âˆ‡][f][i][(Â¯]x[r,s]k [)][âˆ¥][2][ + 4][âˆ¥âˆ‡][f][i][(Â¯]x[r,s]k [)][ âˆ’âˆ‡]f[Â¯]k(Â¯x[r,s]k [)][âˆ¥][2][ + 4][âˆ¥âˆ‡]f[Â¯]k(Â¯x[r,s]k [)][ âˆ’âˆ‡]f[Â¯]k(Â¯x[r,s])âˆ¥[2])

_iXâˆˆVk_

+ 4 _fk(Â¯x[r,s])_ (145)

_âˆ¥âˆ‡_ [Â¯] _âˆ¥[2]_
_iXâˆˆVk_

_â‰¤_ 4âˆ¥âˆ‡fi(Â¯x[r,s]k [)][ âˆ’âˆ‡]f[Â¯]k(Â¯x[r,s]k [)][âˆ¥][2][ + 4][L][2][âˆ¥][x]i[r,s] _âˆ’_ _xÂ¯[r,s]k_ _x[r,s]k_ _âˆ’_ _xÂ¯[r,s]âˆ¥[2]_ + 4âˆ¥âˆ‡f[Â¯]k(Â¯x[r,s])âˆ¥[2][]

_iXâˆˆVk_   _[âˆ¥][2][ + 4][L][2][âˆ¥][Â¯]_

4L[2] _Xk,[r,s]_ _x[r,s]k_ _xÂ¯[r,s]_ + 4n _fk(Â¯x[r,s])_ + 4nÏµ[2]k[.]
_â‰¤_ _âˆ¥_ _âŠ¥[âˆ¥][2][ + 4][L][2][n][âˆ¥][Â¯]_ _âˆ’_ _âˆ¥[2]_ _âˆ¥âˆ‡_ [Â¯] _âˆ¥[2]_

The last inequality is due to Assumption 4 on the intra-cluster heterogeneity.

Averaging over k = 1, . . ., K clusters:

_K_

1

_N_ Eâˆ¥Xk,[r,s]âŠ¥[+1]âˆ¥[2]

_k=1_

X


_â‰¤_ _N[1]_

_â‰¤_ _N[1]_


_K_

_Ï[2]k[(1 +][ Î¶]k[âˆ’][1][)][E][âˆ¥][X]k,[r,s]_
_âŠ¥[âˆ¥][2][ + 1]N_
_k=1_

X


_Ï[2]k[(1 +][ Î¶][k][)][Î·][2][E][âˆ¥âˆ‡][F][k][(][X]k[r,s][)][âˆ¥][2][ +][ Î·][2]_
_k=1_

X


_Ï[2]k_
_k=1_

X


_Ïƒ[2]_


_K_

_Ï[2]k[(1 +][ Î¶]k[âˆ’][1][)][E][âˆ¥][X]k,[r,s]_
_âŠ¥[âˆ¥][2][ +][ Î·][2][ 1]N_
_k=1_

X


_Ï[2]k[(1 +][ Î¶][k][)][ Â·][ 4][L][2][E][âˆ¥][X]k,[r,s]âŠ¥[âˆ¥][2]_
_k=1_

X


+ Î·[2][ 1]

_K_

+ Î·[2][ 1]


_K_

_Ï[2]k[(1 +][ Î¶][k][)4][L][2][E][âˆ¥]x[Â¯][r,s]k_ _âˆ’_ _xÂ¯[r,s]âˆ¥[2]_ + Î·[2][ 1]K
_k=1_

X


_Ï[2]k[(1 +][ Î¶][k][)4][E][âˆ¥âˆ‡]f[Â¯]k(Â¯x[r,s])âˆ¥[2]_
_k=1_

X


_Ï[2]k[(1 +][ Î¶][k][)4][Ïµ][2]k_ [+][ Î·][2][Ï]max[2] _[Ïƒ][2]_ (146)
_k=1_

X


-----

1

_N_




max _k[(1 +][ Î¶]k[âˆ’][1][) +][ Î·][2][ Â·][ 4][L][2][ max]_
_k_ [K] _[Ï][2]_ _k_ [K]
_âˆˆ_ _âˆˆ_


_Ï[2]k[(1 +][ Î¶][k][)]_


E _Xk,[r,s]_
_âˆ¥_ _âŠ¥[âˆ¥][2]_
_k=1_

X


_ÏL_

_K_

{z }

E _xÂ¯[r,s]k_ _xÂ¯[r,s]_
_âˆ¥_ _âˆ’_ _âˆ¥[2]_
_k=1_

X


_Ï[2]k[(1 +][ Î¶][k][)]_ 4L[2][ 1]

_Â·_ _K_


+ Î·[2] max
_kâˆˆ[K]_

+ Î·[2] max
_kâˆˆ[K]_


_Ï[2]k[(1 +][ Î¶][k][)]_ 4(Î±[2]E _f_ (Â¯x[r,s]) + Ïµ[2]g[) +][ Î·][2][ 1]

_Â·_ _âˆ¥âˆ‡_ _âˆ¥[2]_ _K_


_Ï[2]k[(1 +][ Î¶][k][)4][Ïµ][2]k_ [+][ Î·][2][Ï]max[2] _[Ïƒ][2][.]_
_k=1_

X


D.4 PROOF OF LEMMA 4

To simplify the notation we omit the superscript in B[r,s][âˆ’][â„“] in this section.

Let Î› = diag(Î»1, Î»2) and the eigendecomposition of G = T Î›T, we can obtain the closed form

_[âˆ’][1]_
expression of T as

_T =_ 12Î»1Ï„Î· âˆ’[2]CLÏ„[2] 12Î»2Ï„Î· âˆ’[2]CLÏ„[2]
 

and


12Ï„Î·[2]L[2] _âˆ’Î»2 + CÏ„_
12Ï„Î·[2]L[2] _Î»1_ _CÏ„_
_âˆ’_ _âˆ’_


_T_ _[âˆ’][1]_ =


det(T )


where

Consequently


det(T ) = 12Ï„Î·[2]L[2](Î»1 _Î»2)._ (147)
_âˆ’_


det(T ) Â· G[â„“]B = det(T ) Â· T Î›[â„“]T _[âˆ’][1]B_

_Î»1_ _CÏ„_ _Î»2_ _CÏ„_ _Î»[â„“]1_ 0 12Ï„Î·[2]L[2] _Î»2 + CÏ„_ _b1_
12Ï„Î· âˆ’[2]L[2] 12Ï„Î· âˆ’[2]L[2] 0 _Î»[â„“]2_ 12Ï„Î·[2]L[2] _âˆ’Î»1_ _CÏ„_ _b2_
    âˆ’ _âˆ’_  

_Î»1_ _CÏ„_ _Î»2_ _CÏ„_ _Î»[â„“]1_ 0 12Ï„Î·[2]L[2]b1 + ( _Î»2 + CÏ„_ )b2
12Ï„Î· âˆ’[2]L[2] 12Ï„Î· âˆ’[2]L[2] 0 _Î»[â„“]2_ 12Ï„Î·[2]L[2]b1 + (âˆ’Î»1 _CÏ„_ )b2
    âˆ’ _âˆ’_ 

_Î»1_ _CÏ„_ _Î»2_ _CÏ„_ _Î»[â„“]1[12][Ï„Î·][2][L][2][b][1]_ [+][ Î»][â„“]1[(][âˆ’][Î»][2] [+][ C][Ï„] [)][b][2]
12Ï„Î· âˆ’[2]L[2] 12Ï„Î· âˆ’[2]L[2] _Î»[â„“]2[12][Ï„Î·][2][L][2][b][1]_ [+][ Î»][â„“]2[(][Î»][1]
  âˆ’ _[âˆ’]_ _[C][Ï„]_ [)][b][2]

_t1_
_t2_
 


(148)


with
_t1 = (Î»1 âˆ’_ _CÏ„_ ) _Î»[â„“]1[12][Ï„Î·][2][L][2][b][1]_ [+][ Î»][â„“]1[(][âˆ’][Î»][2] [+][ C][Ï„] [)][b][2] (149)

+ (Î»2  CÏ„ ) _Î»[â„“]2[12][Ï„Î·][2][L][2][b][1]_ [+][ Î»][â„“]2[(][Î»][1]  _,_
_âˆ’_ _âˆ’_ _[âˆ’]_ _[C][Ï„]_ [)][b][2]

_t2 = 12Ï„Î·[2]L[2][  ]Î»[â„“]1[12] [Ï„Î·][2][L][2][b][1]_ [+][ Î»][â„“]1[(][âˆ’][Î»][2] [+][ C][Ï„] [)][b][2] 

(150)
+ 12Ï„Î·[2]L[2][  ] _Î»[â„“]2[12][Ï„Î·][2][L][2][b][1]_ [+][ Î»][â„“]2[(][Î»][1]  _._
_âˆ’_ _[âˆ’]_ _[C][Ï„]_ [)][b][2]

Therefore 
det(T )(1, 1)T Î›[â„“]T _[âˆ’][1]B = t1 + t2_
= (Î»1 _CÏ„_ ) _Î»[â„“]1[L][2][12][Ï„Î·][2][b][1]_ [+][ Î»][â„“]1[(][âˆ’][Î»][2] [+][ C][Ï„] [)][b][2]
_âˆ’_

+ (Î»2 _C Ï„_ ) _Î»[â„“]2[L][2][12][Ï„Î·][2][b][1]_ [+][ Î»][â„“]2[(][Î»][1]  (151)
_âˆ’_ _âˆ’_ _[âˆ’]_ _[C][Ï„]_ [)][b][2]

+ L[2]12Ï„Î·[2][  ]Î» [â„“]1[L][2][12][Ï„Î·][2][b][1] [+][ Î»][â„“]1[(][âˆ’][Î»][2] [+][ C][Ï„] [)][b][2] 

+ L[2]12Ï„Î·[2][  ] _Î»[â„“]2[L][2][12][Ï„Î·][2][b][1]_ [+][ Î»][â„“]2[(][Î»][1]  _._
_âˆ’_ _[âˆ’]_ _[C][Ï„]_ [)][b][2]

Substituting the expression of det(T ) and dividing both sides of the equality by 12Ï„Î·[2](Î»1 _Î»2) we_
have _âˆ’_

_L[2](1, 1)T_ Î›[â„“]T _[âˆ’][1]B_


-----

1
= _Î»[â„“]1[L][2][12][Ï„Î·][2][b][1]_ [+][ Î»][â„“]1[(][âˆ’][Î»][2] [+][ C][Ï„] [)][b][2]

12Ï„Î·[2](Î»1 _Î»2) [(][Î»][1][ âˆ’]_ _[C][Ï„]_ [)]
_âˆ’_

1   
+ _Î»[â„“]2[L][2][12][Ï„Î·][2][b][1]_ [+][ Î»][â„“]2[(][Î»][1]

12Ï„Î·[2](Î»1 âˆ’ _Î»2) [(][Î»][2][ âˆ’]_ _[C][Ï„]_ [)] _âˆ’_ _[âˆ’]_ _[C][Ï„]_ [)][b][2]

1   
+ _Î»[â„“]1[L][2][12][Ï„Î·][2][b][1]_ [+][ Î»][â„“]1[(][âˆ’][Î»][2] [+][ C][Ï„] [)][b][2]

(Î»1 _Î»2)_ _[L][2][  ]_
_âˆ’_

1 
+ _Î»[â„“]2[L][2][12][Ï„Î·][2][b][1]_ [+][ Î»][â„“]2[(][Î»][1]

(Î»1 âˆ’ _Î»2)_ _[L][2][  ]âˆ’_ _[âˆ’]_ _[C][Ï„]_ [)][b][2]

1 1 
= 1[L][2][b][1] [+] _âˆ’_ 2[L][2][b][1]

(Î»1 _Î»2) [(][Î»][1][ âˆ’]_ _[C][Ï„]_ [)][ Î»][â„“] (Î»1 _Î»2) [(][Î»][2][ âˆ’]_ _[C][Ï„]_ [)][ Î»][â„“]
_âˆ’_ _âˆ’_

1 1
+ 1[12][Ï„Î·][2][b][1] 2[12][Ï„Î·][2][b][1]

(Î»1 _Î»2)_ _[L][4][Î»][â„“]_ _[âˆ’]_ (Î»1 _Î»2)_ _[L][4][Î»][â„“]_
_âˆ’_ _âˆ’_

1 1
+ 1[(][âˆ’][Î»][2] [+][ C][Ï„] [)][b][2] [+] 2[(][Î»][1]

12Ï„Î·[2](Î»1 âˆ’ _Î»2) [(][Î»][1][ âˆ’]_ _[C][Ï„]_ [)][ Î»][â„“] 12Ï„Î·[2](Î»1 âˆ’ _Î»2) [(][Î»][2][ âˆ’]_ _[C][Ï„]_ [)][ Î»][â„“] _[âˆ’]_ _[C][Ï„]_ [)][b][2]

1 1
+ 1[(][âˆ’][Î»][2] [+][ C][Ï„] [)][b][2] [+] 2[(][Î»][1]

(Î»1 âˆ’ _Î»2)_ _[L][2][Î»][â„“]_ (Î»1 âˆ’ _Î»2)_ _[L][2][Î»][â„“]_ _[âˆ’]_ _[C][Ï„]_ [)][b][2]

1 1
= _Î»[â„“]2[(][Î»][2]_ 1[(][C][Ï„] _L[2]b1 +_ 1 2[)][L][4][b][1]

(Î»1 _Î»2)_ _âˆ’_ _[âˆ’]_ _[C][Ï„]_ [)][ âˆ’] _[Î»][â„“]_ _[âˆ’]_ _[Î»][1][)]_ (Î»1 _Î»2)_ [12][Ï„Î·][2][(][Î»][â„“] _[âˆ’]_ _[Î»][â„“]_
_âˆ’_ _âˆ’_

1   1
+ 1[(][âˆ’][Î»][2] [+][ C][Ï„] [)][b][2] [+] 2[(][Î»][1]

12Ï„Î·[2](Î»1 âˆ’ _Î»2) [(][Î»][1][ âˆ’]_ _[C][Ï„]_ [)][ Î»][â„“] 12Ï„Î·[2](Î»1 âˆ’ _Î»2) [(][Î»][2][ âˆ’]_ _[C][Ï„]_ [)][ Î»][â„“] _[âˆ’]_ _[C][Ï„]_ [)][b][2]

1 1
+ 1[(][âˆ’][Î»][2] [+][ C][Ï„] [)][b][2] [+] 2[(][Î»][1]

(Î»1 âˆ’ _Î»2)_ _[L][2][Î»][â„“]_ (Î»1 âˆ’ _Î»2)_ _[L][2][Î»][â„“]_ _[âˆ’]_ _[C][Ï„]_ [)][b][2]

_Î»[â„“]2[L][2][b][1]_ [+][ Î»]2[â„“] _[âˆ’]_ _[Î»]1[â„“]_ 12Ï„Î·[2]L[4]b1 + _[Î»]2[â„“]_ _[âˆ’]_ _[Î»]1[â„“]_ (Î»2 _CÏ„_ )(CÏ„ _Î»1)_ 1 2[L][2][b][2][,] (152)
_â‰¤_ _Î»2_ _Î»1_ _Â·_ _Î»2_ _Î»1_ _âˆ’_ _âˆ’_ 12Ï„Î·[2][ b][2][ +][ Î»][â„“]

_âˆ’_ _âˆ’_

where in the last inequality we used the fact that Î»1 _CÏ„_ _Î»2._
_â‰¤_ _â‰¤_

Note that


(Î»2 _CÏ„_ )(CÏ„ _Î»1)_
_âˆ’_ _âˆ’_

= _CÏ„[2]_ [+ (][Î»][1] [+][ Î»][2][)][C][Ï„]
_âˆ’_ _[âˆ’]_ _[Î»][1][Î»][2]_
= _CÏ„[2]_
_âˆ’_ _[âˆ’]_ [det(][G][) +][ Tr][(][G][)][C][Ï„]
= _CÏ„[2]_ _CÏ„_ (max _k[(1 +][ Î¶]k[âˆ’][1][) +][ Î·][2][Ï][L][ Â·][ 4][L][2][)][ âˆ’]_ [48][Ï][L][Ï„Î·][4][L][4][]
_âˆ’_ _[âˆ’]_ _kâˆˆ[K]_ _[Ï][2]_
 

+ CÏ„ (max _k[(1 +][ Î¶]k[âˆ’][1][) +][ Î·][2][Ï][L][ Â·][ 4][L][2][ +][ C][Ï„]_ [)]
_k_ [K] _[Ï][2]_
_âˆˆ_

= 48ÏLÏ„Î·[4]L[4].

Therefore, we further obtain


(153)


_L[2](1, 1)T_ Î›[â„“]T _[âˆ’][1]B_

(154)

_Î»[â„“]2[L][2][(][b][1]_ [+][ b][2][) +][ Î»]2[â„“] _[âˆ’]_ _[Î»]1[â„“]_ _Î·[2]_ 12Ï„L[4]b1 + 4ÏLL[4]b2
_â‰¤_ _Î»2_ _Î»1_ _Â·_

_âˆ’_
  

Dividing both sides by L[2] completes the proof.

E NETWORK CONNECTIVITY CONDITIONS IN THEOREMS AND COROLLARY

Both Theorem 2 and Corollary 1 impose some sufficient conditions on the network connectivity
_Ïmax for convergence. This can be satisfied in practice as follows. For Theorem 2, as long as_
_Ïmax < 1, we can choose Ï„ large enough so that (7) is fulfilled. Corollary 1 strengthens the result of_
Theorem 2 by requiring no loss in the order of convergence rate compared to full device participation. This naturally leads to a more stringent condition on Ïmax given by (11). For any given D2D
network topology, this can be satisfied by running multiple D2D gossip averaging steps per SGD


-----

update in Algorithm 1. Since the right hand side of (11) depends only on the algorithmic parameters, we can choose the suitable gossip averaging steps to fulfill this condition before launching the
algorithm.

F MORE EXPERIMENTAL DETAILS

In this section, we provide additional experimental results on CIFAR-10 dataset. We follow the
same CNN model and non-iid data partition strategy as before and run each experiments for 3 times
with different random seeds to report the mean values of best test accuracy. Instead of using a
constant learning rate, we decay the local learning rate Î· by half after finishing 50% and 75% of the
communication rounds and tune the initial learning rate from {0.01, 0.02, 0.05, 0.08, 0.1} for each
algorithm.

(a) (b)

Figure 4: Convergence rate and runtime comparisons of HL-SGD and local SGD on CIFAR-10 under ER
random D2D network topology. The device sampling ratio p = 1/8 and local iteration period Ï„ = 50.

(a) (b)

Figure 5: Convergence rate and runtime comparisons of HL-SGD and local SGD on CIFAR-10 under ring
topology and multiple SGD updates before gossip averaging. The device sampling ratio p = 1, and the local
iteration period Ï„ = 50.

First, we evaluate the convergence processes of HL-SGD and local SGD under varying D2D network
topologies in Figure 4. We generate random network topologies by ErdËos-RÂ´enyi model with edge
probability from 0.2, 0, 5, 0.8, 1 and use Metropolis-Hastings weights to set Wk, corresponding to
_{_ _}_
spectral norm Ïmax = {0.9394, 0.844, 0.5357, 0}. As observed in Figure 4a, a more connected D2D
network topology (i.e., a smaller value of Ïmax) generally accelerates the convergence and leads to
a higher model accuracy achieved over 100 communication rounds in HL-SGD. However, in terms
of runtime, a more connected D2D network topology corresponds to a larger D2D communication
delay cd2d per round, and hence the total runtime is larger as well, which can be clearly observed
in Figure 4b. Therefore, to achieve a target level of model accuracy within the shortest time in HLSGD, a sparse D2D network topology could work better than the fully connected one in practice.


-----

Second, we consider an extension of HL-SGD by allowing each device to perform multiple SGD
updates before the gossip averaging step in Algorithm 1 and empirically evaluate its performance.
Specifically, each device performs l = {1, 5, 10} steps of SGD update before aggregating models
with their neighbors in the same cluster. Note that l = 1 corresponds to the original version of HLSGD in Algorithm 1. As observed in Figure 5a, when communicating and aggregating models with
neighbors more frequently, HL-SGD with l = 1 has the best convergence speed and will converge
to the highest level of test accuracy. In terms of runtime, choosing a value of l > 1 might be
favorable in some cases due to the reduced D2D communication delay per round. For instance, to
achieve a target level of 60% test accuracy, HL-SGD with l = 5 needs 5.22% less amount of time
than l = 1. It is an interesting direction to rigorously analyze the convergence properties of HLSGD with arbitrary l and find the best hyperparameter tuning method for minimizing the runtime to
achieve a target level of model accuracy in the future.

G RELATIONSHIP BETWEEN SPECTRAL GAP AND NETWORK TOPOLOGY

ring/path 2D-grid 2-D torus ErdËos-RÂ´enyi exponential

1 âˆ’ _Ï_ _O(1/n[2])_ _O(1/(n log n))_ _O(1/n)_ _O(1)_ _O(1/ log(n))_

Table 2: Spectral gap of some commonly used graphs, where n denotes the number of nodes. Results are taken
from (NediÂ´c et al. (2018); Ying et al. (2021)).


-----

