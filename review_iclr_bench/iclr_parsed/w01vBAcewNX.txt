Published as a conference paper at ICLR 2022
ON COVARIATE SHIFT OF LATENT CONFOUNDERS
IN IMITATION AND REINFORCEMENT LEARNING
Guy Tennenholtz
Nvidia Research
Technion
Assaf Hallak
Nvidia Research
Gal Dalal
Nvidia Research
Shie Mannor
Nvidia Research
Technion
Gal Chechik
Nvidia Research
Uri Shalit
Technion
ABSTRACT
We consider the problem of using expert data with unobserved confounders for
imitation and reinforcement learning. We begin by deﬁning the problem of learn-
ing from confounded expert data in a contextual MDP setup. We analyze the
limitations of learning from such data with and without external reward, and pro-
pose an adjustment of standard imitation learning algorithms to ﬁt this setup. We
then discuss the problem of distribution shift between the expert data and the on-
line environment when the data is only partially observable. We prove possibility
and impossibility results for imitation learning under arbitrary distribution shift of
the missing covariates. When additional external reward is provided, we propose
a sampling procedure that addresses the unknown shift and prove convergence to
an optimal solution. Finally, we validate our claims empirically on challenging
assistive healthcare and recommender system simulation tasks.
1
INTRODUCTION
Reinforcement Learning (RL) is increasingly used across many ﬁelds to create agents that learn via
interaction and reward feedback. Imitation Learning (IL, Hussein et al. (2017)) is concerned with
learning via expert demonstrations without access to a reward function. Similarly, RL settings often
use expert data to boost performance, eliminating the need to learn from scratch. In this work we
consider the IL and RL paradigms in the presence of partially observable expert data.
While expert demonstration data is useful, in many realistic settings such data may be prone to
hidden confounding (Gottesman et al., 2019), i.e., there may be features used by the expert which
are not observed by the learning agent. This can occur due to, e.g., privacy constraints, continually
changing features in ongoing production pipelines, or when not all information available to the
human expert was recorded. As we show in our work, covariate shift of unobserved factors between
the expert data and the real world may lead to signiﬁcant negative impact on performance, frequently
rendering the data useless for imitation (see Figure 1 and Theorem 2).
In this paper we deﬁne the tasks of imitation and reinforcement learning using expert data with un-
observed confounders and possible covariate shift. We focus on a contextual MDP setting, where a
context is sampled at every episode from some distribution, affecting both the reward and the transi-
tion between states. We assume that the agent has access to additional expert data, generated by an
optimal policy, for which the sampled context is missing, yet is observed in the online environment.
We begin by analyzing the imitation-learning problem, (i.e., without access to reward) in Section 3.
Under no covariate shift in the unobserved context, we characterize a sufﬁcient and necessary set of
optimal policies. In contrast, we prove that in the presence of a covariate shift, if the true reward
depends on the context, then the imitation-learning problem is non-identiﬁable and prone to catas-
trophic errors (see Section 3.2 and Theorem 2). We further analyze the RL setting (i.e., with access
to reward and confounded expert data) in Section 4. Figure 1 depicts a possible failure case of using
confounded expert data with unknown covariate shift in a dressing task. Unlike the imitation setting,
1
Published as a conference paper at ICLR 2022
Online Context Distribution
(observed)
55% female, 45% male
Average age 77
3% tremor impairment
Failure due to Covariate Shift
Success with Trajectory Sampling Correction
Illustration of Trembling Patient
Expert Context Distribution
(unobserved and unknown)
40% female, 60% male
Average age 85
10% tremor impairment
Distribution Mismatch
Failure: 
Arm misses
sleeve
Successful: 
Arm through
sleeve 
Figure 1: Failure of using confounded expert data under context distribution mismatch between online envi-
ronment and expert data. Caregiver does not learn to perform well in a dressing task when covariate shift of
hidden confounders is present but not accounted for.
we show that optimality can still be achieved in the RL setting while using confounded expert data
with arbitrary covariate shift. We use a corrective data sampling procedure and prove convergence
to an optimal policy.
Our contributions are as follows. (1) We introduce IL and RL with hidden confounding and prove
fundamental characteristics w.r.t. covariate shift and the feasibility of imitation. (2) In the RL
setting, under arbitrary covariate shift, we provide a novel algorithm with convergence guarantees
which uses a corrective sampling technique to account for the unknown context distribution in the
expert data. (3) Finally, we conduct extensive experiments on recommender system (Ie et al., 2019)
and assistive-healthcare (Erickson et al., 2020) environments, demonstrating our theoretical results,
and suggesting that confounded expert data can be used in a controlled manner to improve the
efﬁciency and performance of RL agents.
2
PRELIMINARIES
*
Figure 2: Causal Diagram Con-
textual MDP
Online Environment.
We consider a contextual MDP (Hallak
et al., 2015) deﬁned by the tuple M = (S, X, A, P, r, ρo, ν, γ),
where S is the state space, X is the context space, A is the ac-
tion space, P : S × S × A × X 7→[0, 1] is the context dependent
transition kernel, r : S × A × X 7→[0, 1] is the context dependent
reward function, and γ ∈(0, 1) is the discount factor. We assume
an initial distribution over contexts ρo : X 7→[0, 1] and an initial
state distribution ν : S × X 7→[0, 1].
The environment initializes at some context x ∼ρo(·), and state
s0 ∼ν(·|x). At time t, the environment is at state st ∈S and an
agent selects an action at ∈A. The agent receives a reward rt =
r(st, at, x) and the environment then transitions to state st+1 ∼
P(·|st, at, x).
We deﬁne a Markovian stationary policy π as a mapping
π : S × X × A 7→[0, 1], such that π(·|s, x) is the action sampling
probability.
We deﬁne the value of a policy π by vM(π) =
Eπ[ (1 −γ) P∞
t=0 γtr(st, at, x) | x ∼ρo, s0 ∼ν(· | x) ],
where
Eπ denotes the expectation induced by the policy π. We denote by Π the set of all Markovian
policies and Πdet the set of deterministic Markovian policies. We deﬁne the optimal value and policy
by v∗
M = maxπ∈Π vM(π), and π∗
M ∈arg maxπ∈Π vM(π), respectively. Whenever appropriate,
we simplify notation and write v∗, π∗. We use Π∗
M to denote the set of optimal policies in M, i.e.,
Π∗
M = arg maxπ∈Π vM(π). We also deﬁne the set of catastrophic policies Π†
M as the set
Π†
M = arg min
π∈Π vM(π).
(1)
2
Published as a conference paper at ICLR 2022
We later use this set to show impossibility of imitation under arbitrary covariate shift and a context-
independent transition function.
Expert Data with Unobserved Confounders.
We assume additional access to a con-
founded dataset consisting of expert trajectories D∗
=

(si
0, ai
0, si
1, ai
1, . . . , si
H, ai
H)
	n
i=1,
where
ai
j
∼
π∗
∈
Π∗
M.
The
trajectories
in
the
dataset
were
sampled
i.i.d.
from
the
marginalized
expert
distribution
(under
possible
context
covariate
shift)
P ∗(s0, a0, s1, a1, . . . , sH) = P
x ρe(x)ν(s0|x) QH−1
t=0 P(st+1|st, at, x)π∗(at|st, x),
where
ρe
is some distribution over contexts. Importantly, ρe does not necessarily equal ρo – the distribution
of contexts in the online environment. Notice that it is assumed that π∗that generated the data had
access to the context xi (i.e., π∗is context-dependent), though it is missing in the data.
In this work, we consider two settings:
1. Confounded Imitation Learning (Section 3): The agent has access to confounded ex-
pert data (with context distribution ρe) as well as real environment (S, X, A, P, ρo, ν, γ),
without access to reward.
2. Reinforcement Learning with Confounded Expert Data (Section 4): The agent has ac-
cess to confounded expert data (with context distribution ρe) as well as real environment
M = (S, X, A, P, r, ρo, ν, γ), with access to reward.
In both settings we aim to ﬁnd a context-dependent policy which maximizes the cumulative reward.
The confounding factor here is w.r.t. the unobserved context and distribution ρe in the ofﬂine data.
Marginalized State Action Distribution.
We denote the state-action frequency of a policy π ∈Π
given context x ∈X by dπ(s, a|x) = (1 −γ) P∞
t=0 γtP π(st = s, at = a|x, s0 ∼ν(·|x)), where
P π denotes the probability measure induced by π. Similarly, given a distribution over contexts,
we deﬁne the marginalized state-action frequency of a policy π under the corresponding context
distribution by
dπ
ρo(s, a) = Ex∼ρo[dπ(s, a | x)]
(online environment),
dπ∗
ρe (s, a) = Ex∼ρe
h
dπ∗(s, a | x)
i
(ofﬂine expert data).
A Causal Perspective.
Our work sits at an intersection between the ﬁelds of RL and Causal In-
ference (CI). We believe it is essential to bridge the gap between these two ﬁelds, and include an
interpretation of our model using CI terminology in Appendix D, where we equivalently deﬁne our
objective as an intervention over the unknown distribution ρe in a speciﬁed Structural Causal Model,
as depicted in Figure 2.
3
IMITATION LEARNING WITH UNOBSERVED CONFOUNDERS
In this section, we analyze the problem of confounded imitation learning, namely, learning from ex-
pert trajectories with hidden confounders and without reward. Similar to previous work, we consider
the task of imitation learning from expert data in the setting where the agent is allowed to interact
with the environment (Ho & Ermon, 2016; Fu et al., 2017; Kostrikov et al., 2019; Brantley et al.,
2019). In the ﬁrst part of this section we assume no covariate shift between the online environment
and the data is present, i.e., ρe = ρo. We lift this assumption in the second part, where we focus on
the covariate shift of the hidden confounders.
3.1
NO COVARIATE SHIFT: ρo = ρe
We ﬁrst consider the scenario in which no covariate shift is present between the ofﬂine data and the
online environment, i.e., ρo = ρe. We begin by deﬁning the marginalized ambiguity set, a central
component of our work.
Deﬁnition 1. For π ∈Π, we deﬁne the set of deterministic policies that match the marginalized
state-action frequency of π by Υπ =
n
π′ ∈Πdet : dπ′
ρo(s, a) = dπ
ρe(s, a)
∀s ∈S, a ∈A
o
.
3
Published as a conference paper at ICLR 2022
Context-Independent
Transition
(Theorem 2)
Bounded Confounding
(Appendix C)
Context-Independent
Reward
(Theorem 3)
Figure 3: A spectrum for the difﬁculty of confounded imitation with covariate shift. Context-Independent
transitions may result in non-identiﬁable and catastrophic candidates, whereas context-independent rewards
reduce the problem to a standard imitation learning.
Recall that, in general, π∗∈Π∗
M may depend on the context x ∈X. Therefore, the set Υπ∗
corresponds to all deterministic policies that cannot be distinguished from π∗based on the observed
expert data. The following theorem shows that for any policy π∗∈Π∗
M and any policy π0 ∈Υπ∗,
one could design a reward function r0, for which π0 is optimal, while the set Υπ∗is indiscernible
from Υπ0, i.e., Υπ∗= Υπ0 (see Appendix F for proof). In other words, Υπ∗is the smallest set of
candidate optimal policies.
Theorem 1. [Sufﬁciency of Υπ∗] Assume ρe ≡ρo. Let π∗∈Π∗
M and let π0 ∈Υπ∗. Then,
Υπ∗= Υπ0. Moreover, if π0 ̸= π∗, then there exists r0 such that π0 ∈Π∗
M0 but π∗/
∈Π∗
M0, where
M0 = (S, A, X, P, r0, ρo, ν, γ).
The above theorem shows that any policy in Υπ∗is a candidate optimal policy, yet without knowing
the context the expert used. Such ambiguity can result in selection of a suboptimal or even catas-
trophic policy. We provide a practical algorithm in Appendix B which calculates the ambiguity set
Υπ∗, and returns an average policy, with computational guarantees. In the next subsection we ana-
lyze a more challenging scenario, for which ρo ̸= ρe. In this case, Υπ∗may not be sufﬁcient for the
imitation problem.
3.2
WITH COVARIATE SHIFT: ρo ̸= ρe
Next, we assume covariate shift exists between the online environment and the expert data, i.e.,
ρo ̸= ρe. Particularly, without further assumptions on the extent of covariate shift, we show two
extremes of the problem. In Theorem 2 we prove that whenever the transitions are independent of
the context, the data cannot in general be used for imitation. In contrast, in Theorem 3 we prove that,
whenever the reward is independent of the context, the imitation problem can be efﬁciently solved.
Clearly, if Supp(ρo) ̸⊆Supp(ρe)1 then there exists x ∈Supp(ρo) for which π∗is not identiﬁable
from the expert data2. We therefore assume throughout that Supp(ρo) ⊆Supp(ρe). We begin by
deﬁning the set of non-identiﬁable policies as those that cannot be distinguished from their respective
state-action frequencies without information on ρe.
Deﬁnition 2. We say that {πi}k
i=1 are non-identiﬁable policies if there exist {ρi}k
i=1 such that
dπi
ρi (s, a) = dπj
ρj (s, a) for all i ̸= j.
Next, focusing on catastrophic policies (recall Equation (1)), we deﬁne catastrophic expert policies
as those which could be either optimal or catastrophic under ρo for different reward functions.
Deﬁnition 3. We say that {πi}k
i=1 are catastrophic expert policies if there exist {ri}k
i=1 such that for
all i, πi ∈Π∗
Mi, and ∃j ∈[k], j ̸= i such that πi ∈Π†
Mj, where Mj = (S, X, A, P, rj, ρo, ν, γ).
Using the fact that both ρe and r are unknown, the following theorem shows that whenever
P(s′|s, a, x) is independent of x, one could ﬁnd two policies which are non-identiﬁable, catastrophic
expert policies (see Appendix F for proof). In other words, in the case of context-independent tran-
sitions, without further information on ρe or r the expert data is useless for imitation. Furthermore,
attempting to imitate the policy using the expert data could result in a catastrophic policy.
1For a distribution P we denote by Supp(P) the support of P.
2We deﬁne non-identiﬁability in Deﬁnition 2. We use a similar notion of identiﬁability as in Pearl (2009b)
4
Published as a conference paper at ICLR 2022
Theorem 2. [Catastrophic Imitation] Assume |X| ≥|A|, and P(s′|s, a, x) = P(s′|s, a, x′) for all
x, x′ ∈X. Then ∃πe,1, πe,2 s.t. {πe,1, πe,2} are non-identiﬁable, catastrophic expert policies.
While Theorem 2 shows the impossibility of imitation for context-free transitions, whenever the
reward is independent of the context, the imitation problem becomes feasible. In fact, as we show
in the following theorem, for context-free rewards, any policy in Υπ∗is an optimal policy.
Theorem
3.
[Sufﬁciency
of
Context-Free
Reward]
Assume
Supp(ρo) ⊆Supp(ρe)
and
r(s, a, x) = r(s, a, x′) for all x, x′ ∈X. Then Υπ∗⊆Π∗
M.
Theorems 2 and 3 suggest that the hardness of the imitation problem under covariate shift lies on a
wide spectrum (as depicted in Figure 3). While dependence of the transition P(s′|s, a, x) on x pro-
vides us with information to identify x in the expert data, the dependence of the reward r(s, a, x) on
x increases the degree of confounding in the imitation problem. Both of these results are concerned
with arbitrary confounding.
Bounded Confounding: A Sensitivity Perspective.
A common approach in causal inference is
to bound the bias of unobserved confounding through sensitivity analysis (Hsu & Small, 2013;
Namkoong et al., 2020; Kallus & Zhou, 2021). In our setting, this confounding bias occurs due to a
covariate shift of the unobserved covariates. As we’ve shown in Theorem 2, though these covariates
are observed in the online environment, their shifted and unobserved distribution in the ofﬂine data
can render catastrophic results. Therefore, we consider the odds-ratio bounds of the sensitivity in
distribution between the online environment and the expert data, as stated formally below.
Assumption 1 (Bounded Sensitivity). We assume that Supp(ρe) ⊆Supp(ρo) and that there exists
some Γ ≥1 such that for all x ∈Supp(ρe), Γ−1 ≤ρo(x)(1−ρe(x))
ρe(x)(1−ρo(x)) ≤Γ.
Next, we deﬁne the notion of δ-ambiguity, a generalization of the ambiguity set in Deﬁnition 1.
Deﬁnition 4 (δ-Ambiguity Set). For a policy π ∈Π, we deﬁne the set of all deterministic policies
that are δ-close to π by Υδ
π =
n
π′ ∈Πdet :


dπ′
ρo(s, a) −dπ
ρe(s, a)


 < δ, s ∈S, a ∈A
o
.
Similar to Deﬁnition 1, the δ-ambiguity set considers all deterministic policies with a marginalized
state-action frequency of distance at most δ from π. The following results shows that ΥΓ−1
π∗
is a
sufﬁcient set of candidate optimal policies, as long as Assumption 2 holds for some Γ ≥1.
Theorem 4. [Sufﬁciency of ΥΓ−1
π∗] Let Assumption 2 hold for some Γ ≥1. Then π∗∈ΥΓ−1
π∗.
For the interested reader, we further analyze the case of bounded confounding in Appendix C. We
also demonstrate the effect of bounded confounding in Section 5. In the following section, we
show that, while arbitrary confounding may result in catastrophic results for the imitation learning
problem, when coupled with reward, one can still make use of the expert data.
4
USING EXPERT DATA WITH UNOBSERVED CONFOUNDERS FOR RL
In the previous section we showed sufﬁcient conditions under which imitation is possible, with and
without covariate shift. When covariate shift is present, but unknown, the imitation learning problem
may be hard, or even impossible (see Theorem 2, catastrophic imitation). We ask, had we had access
to the reward function, would the expert data be useful under arbitrary covariate shift? In this section
we show that expert data with unobserved confounders can be used to converge to an expert policy,
even when arbitary covariate shift is present. In our experiments (Section 5) we empirically show
that using our method can also improve overall performance.
We view the confounded expert data as side information to the RL problem. Speciﬁcally, we assume
access to the true reward signal in the online environment and wish to leverage the ofﬂine expert data
to aid the agent in converging to an optimal policy. To do this, we deﬁne an optimization problem
that maximizes the cumulative reward, while minimizing an f-divergence (e.g., KL-divergence, TV-
distance, χ2-divergence) of state-action frequencies in Υπ∗,
max
π∈Π Ex∼ρo,s,a∼dπ(s,a|x)[r(s, a, x)] −λDf(dπ
ρo(s, a)||dπ∗
ρe (s, a)).
(P1)
5
Published as a conference paper at ICLR 2022
Algorithm 1 RL using Expert Data with Unob-
served Confounders (Follow the Leader)
1: input:
Expert data with missing context D∗,
λ > 0, policy optimization algorithm ALG-RL
2: init: Policy π0
3: for k = 1, . . . do
4:
ρs ←arg minρ DKL(dπk−1
ρo
(s, a)||dπ∗
ρ (s, a))
5:
gk ←1
k

gk−1 + Es,a∼dπk−1
ρo

1
dπ∗
ρs (s,a)

6:
πk ←ALG-RL(r(s, a, x) −λgk(s, a))
7: end for
Algorithm 2 RL using Expert Data with Unob-
served Confounders (Online Gradient Descent)
1: input:
Expert
data
with
missing
context,
λ, B, N > 0, policy optimization alg. ALG-RL
2: init: Policy π0, bonus reward network gθ
3: for k = 1, . . . do
4:
ρs ←arg minρ Df(d
πk−1
ρo
(s, a)||dπ∗
ρ (s, a))
5:
for e = 1, . . . N do
6:
Sample batch {si, ai}B
i=1 ∼d
πk−1
ρo
(s, a)
7:
Sample batch {se
i , ae
i }B
i=1 ∼dπ∗
ρs (s, a)
8:
Update
gθ
according
to
∇θL(θ)
=
1
B
PB
i=1 ∇θ[f ∗(gθ(se
i , ae
i )) −gθ(si, ai)]
9:
end for
10:
πk ←ALG-RL(r(s, a, x) −λgθ(s, a))
11: end for
Here, λ > 0 and Df is the f-divergence, where f is a convex function f : (0, ∞) 7→R. The
solution to Problem (P1) is an optimal policy π∗∈Π∗
M as long as ρo ≡ρe. Rewriting Df using its
variational form (see Appendix A for background on the variational form of f-divergences), we get
the following equivalent optimization problem, motivated by Nachum et al. (2019):
max
π∈Π
min
g:S×A7→R Ex∼ρo,s,a∼dπ(s,a|x)[r(s, a, x) + λg(s, a)] −λEs,a∼dπ∗
ρe (s,a)[f ∗(g(s, a))],
(P1b)
where f ∗is the convex conjugate of f, i.e., f ∗(y) = supx xy −f(y).
Unfortunately, when covariate shift exists (i.e., ρo ̸= ρe), Problems (P1) and (P1b) are not ensured
to converge to an optimal policy (Theorem 2). Instead, we propose to reformulate Problem (P1b)
using a distribution ρs which minimizes the f-divergence, as follows,
max
π∈Π
min
g:S×A7→R
ρs∈B(X)
Ex∼ρo,s,a∼dπ(s,a|x)[r(s, a, x) + λg(s, a)] −λEs,a∼dπ∗
ρs (s,a)[f ∗(g(s, a))].
(P2)
Here,
B(X)
denotes
the
set
of
probability
measures
on
the
Borel
sets
of
X,
and
dπ∗
ρs (s, a) = Ex∼ρs

dπ∗(s, a | x)

.
Indeed,
whenever
Supp(ρo) ⊆Supp(ρe),
we
have
that
(π, ρs) = (π∗, ρo) is a solution to Problem (P2). That is, unlike Problems (P1) and (P1b), Prob-
lem (P2) can achieve an optimal solution to the RL problem which still uses the expert data.
Corrective Trajectory Sampling (CTS).
Solving Problem (P2) involves an expectation over an
unknown distribution, dπ∗
ρs (s, a). Fortunately, dπ∗
ρs (s, a) can be equivalently written as an expectation
over trajectories in D∗, rather than expectation over unobserved contexts, as shown by the following
proposition (see Appendix F for proof):
Proposition 1. [Trajectory Sampling Equivalence] Let ρ∗
s which minimizes Problem (P2) for some
π ∈Π, g : S × A 7→R, and assume Supp(ρo) ⊆Supp(ρe). Then, there exists pn ∈∆n such that
dπ∗
ρ∗
s (s, a) = lim
n→∞Ei∼pn
(1 −γ) P∞
t=0 γt1

(si
t, ai
t) = (s, a)
	
.
Proposition 1 allows us to estimate the inner minimization problem over ρs in Problem (P2) using
trajectory samples. Particularly, we uniformly sample k distributions pn
1, . . . pn
k, where pn
j ∈∆n,
and then estimate
min
ρs Df(dπ
ρo||dπ∗
ρs ) ≈
min
j∈{1,...,k}
(
Df
 
dπ
ρo(s, a)





 Ei∼pn
j
" ∞
X
t=0
γt1

(si
t, ai
t) = (s, a)
	
#!)
,
(2)
which can be estimated by using the variational form of Df (see Appendix A). We call this procedure
Corrective Trajectory Sampling (CTS), as it uses complete trajectory samples to account for the
unknown context distribution ρe.
Solving Problem (P2).
Algorithm 1 provides an iterative procedure for solving the optimization
problem in Problem (P2). It uses alternative updates of a cost player (line 5) and policy player (line
6). In line 5 the gradient of DKL w.r.t. dπ is taken using a Follow the Leader (FTL) cost player
6
Published as a conference paper at ICLR 2022
Figure 4: Plots compare training curves of using CTS vs. normal sampling of expert data for small (β = 0.3)
and large (β = 0.8) covariate shift bias in four assistive-healthcare tasks. Dashed black lines show expert and
RL (without data) scores. Runs were averaged over 5 seeds. Legend is shared across all plots.
to estimate the next bonus iterate. Finally, in line 6, an efﬁcient, approximate policy optimization
algorithm ALG-RL is executed using an augmented reward. The following theorem, provides con-
vergence guarantees for Algorithm 1 (see Appendix F for proof based on Zahavy et al. (2021)).
Theorem 5. Let ALG-RL be an approximate best response player that solves the RL problem in
iteration k to accuracy ϵk =
1
√
k. Then, Algorithm 1 will converge to an ϵ-optimal solution to
Problem (P2) in O
  1
ϵ4

samples.
Notice that, while Theorem 5 shows Algorithm 1 converges to an optimal policy, it does not deter-
mine whether the expert data improves overall learning efﬁciency. We leave this theoretical question
for future work. Nevertheless, in the following section we conduct extensive experiments to show
that such data can indeed improve overall performance on various tasks.
A drawback of Algorithm 1 is that it needs to estimate the state-action frequencies. A practical
implementation of Algorithm 1 using online gradient descent (OGD) is provided in Algorithm 2 –
the algorithm does not require approximate estimates of the state-action frequencies, but rather, only
the ability to sample from them. Similar to Algorithm 1, we use CTS (see Equation (2)) to estimate
ρs in line 4 according to some f-divergence. Here, samples are drawn from the current policy as
well as samples from D∗(with CTS). We write Df in its variational form, and use a neural network
representation for gθ. We then use the aforementioned samples to minimize the f-divergence using
OGD. Finally, the policy is updated using ALG-RL and an augmented reward.
5
EXPERIMENTS
We tested our proposed approach for using expert data with hidden confounding in recommender-
system and assistive-healthcare environments. For all our experiments we used χ2-divergence as our
choice of f-divergence, as we found it to work best. Comparison to other divergences is provided
in Figure 5 (left). We used PPO (Schulman et al., 2017) implemented in RLlib (Liang et al., 2018)
for both the imitation as well as RL settings. We include speciﬁc choice of hyperparameters and an
exhaustive overview of further implementation details in Appendix E.
7
Published as a conference paper at ICLR 2022
Figure 5: Left plot shows comparison of different choices of f-divergences for pure imitation (without reward
and without covariate shift) on the BedBathingPR2 environment. Middle plot depicts execution of imitation
with hidden confounding (without reward) for different levels of covariate shift. Right plot compares our CTS
correction on the RecSim environment with strong covariate shift bias. All runs were averaged over 5 seeds.
Assistive Healthcare. Consider the challenge of providing physical assistance to disabled persons.
A recently proposed set of tasks for assistive-healthcare, simulating autonomous robots as versatile
caregivers (Erickson et al., 2020). Each task has a unique goal, affected by both the physical world
as well as the patient speciﬁc preferences and disabilities.
We tested our algorithm on four tasks: feeding, dressing, bathing, and drinking. In these, we used
the following features to deﬁne user context: gender, mass, radius, height, patient impairment, and
patient preferences. The patient’s mass, radius, and height distributions were dependent on gender.
The patient’s impairment was given by either limited movement, weakness, or tremor (with sporadic
movement). Finally, the patient’s preferences were affected by the velocity and pressure of touch
forces applied by the robot. For the context distribution ρo we used the default values as provided
by the original environment. To enforce a distribution shift in the expert data, we shifted each distri-
bution randomly with an additive factor β · ˜
dx, where β ∈[0, 1], and ˜
dx was a random distribution
chosen from a set of shifting distributions (see Appendix E). Here β corresponds to the covariate
shift strength. The expert data was generated by a ﬁxed policy trained using a dense reward function.
A sparse reward signal was used for executing our experiments with the confounded expert data. For
further details, we refer the reader to Appendix E.
Figure 4 depicts results for executing Algorithm 2 on four assistive-gym environments with various
covariate shift strengths. As evident in most of the enviornments, covariate shift strongly affected
overall performance. Particularly in the feeding, drinking, and dressing environments, the success of
reaching the goal (i.e., spoon to mouth, cup to mouth, and sleeve to hand) was highly affected by the
degree of covariate shift. This is due to the changing distribution of size, movement, and preferences
of the patient, and thus of the goal. Nevertheless, in all environments, using the expert data (with
and without CTS) was found to help induce better policies than executing the same RL algorithm
without expert data. This suggests that expert data can assist in improving overall RL performance,
yet correcting for covariate shift may signiﬁcantly improve it in these domains.
Recommender Systems.
In practical recommender systems, sequential interaction with users
presents a great challenge for optimizing user long-term engagement and overall satisfaction (Ie
et al., 2019). Leveraging expert data collected using, e.g., surveys to users, may greatly beneﬁt fu-
ture solutions. Because features are repeatedly added to these systems, full information in the data
is rarely available. Here, we use the recently proposed RecSim Interest Evolution environment (Ie
et al., 2019), simulating sequential user interaction with a slate-based recommender system. The
environment consists of a document model for sampling documents, a user model for deﬁning a
distribution over user context features, and a user choice model, which deﬁnes the intent of the user
based on observable document features and the user’s sampled context (e.g., personality, satisfaction,
interests, demographics, and other behavioral features such as session length or visit frequency).
We used a slate of 10 documents and a user context of dimension 20. To test the severity of the
implications of Theorem 2 in the confounded imitation setting, we used a user-model sampled from
a Beta-distribution. Speciﬁcally, for the expert data the user context features x = (x0, . . . , x19)
were sampled from a Beta-distribution, where xi ∼Beta(αi, 4), and αi = 1.5 + 8.5
19 i.
In
contrast, the online environment features were sampled from a shifted Beta-distribution with
αi = (1 −β)
 1.5 + 8.5
19 i

+ β
 10 −8.5
19 i

, where β ∈[0, 1] deﬁned the shift strength. While the
8
Published as a conference paper at ICLR 2022
original environment used a uniform distribution to generate user contexts, the Beta-distribution let
us analyze severe forms of covariate shifts, testing the limits of our results in Sections 3 and 4.
Figure 5(a) depicts the effect of increased covariate shift on imitation in the RecSim environment
with a dataset of 100 expert trajectories (generated by an optimal policy that had access to the full
context). Without covariate shift (β = 0) an optimal score is achieved, and as β increases, perfor-
mance decreases. Particularly, as the mirrored distribution is reached (β = 1), a catastrophic policy
is reached. While the imitator “believes” to have reached an optimal policy, it has in fact reached a
catastrophic one, as shown by the orange plot. Conversely, Figure 5(b) depicts the beneﬁt of using
confounded expert data in the RL setting (with an online reward signal). Though strong confounding
is present, the agent is capable of leveraging the data to improve overall learning performance.
6
RELATED WORK
Imitation Learning. The imitation learning problem has been extensively studied in both the fully
ofﬂine (Pomerleau, 1989; Bratko et al., 1995) as well as online setting (Ho & Ermon, 2016; Fu
et al., 2018; Kim & Park, 2018; Brantley et al., 2019). Speciﬁc to our work are GAIL (Ho & Ermon,
2016), AIRL (Fu et al., 2017), and DICE (Kostrikov et al., 2019), which use distribution matching
methods. Our work generalizes these settings to imitation with hidden confounders.
Reinforcement Learning with Expert Data. Much work has revolved on leveraging ofﬂine data
for RL. Recently, ofﬂine RL (Levine et al., 2020) has shown great improvement over regular ofﬂine
imitation techniques (Kumar et al., 2020; Kostrikov et al., 2021; Tennenholtz et al., 2021a; Fujimoto
& Gu, 2021). In the online RL setting, the combination of ofﬂine data to improve RL efﬁciency has
shown great success (Nair et al., 2020). KL-regularized techniques (Peng et al., 2019; Siegel et al.,
2019) as well as DICE-based algorithms (Nachum et al., 2019) have also shown efﬁcient utilization
of ofﬂine data. Our work generalizes the latter to the confounded setting.
Intersection of Causal Inference and Imitation Learning. Closely related to our work is that of
Zhang et al. (2020). There, the authors suggest a notion of imitability, showing when observational
data can help identify a policy under some partially observed structural causal model. Our work
provides an alternative perspective on the problem. In contrast to their work, we rely on concurrent
imitation approaches (i.e. state-action frequency matching techniques) and importantly, allow access
to the online environment. Furthermore, we provide guarantees and practical algorithms for both the
imitation and RL settings. We refer the reader to Appendix D for an interpretation of our framework
in CI terminology, from a perspective of stochastic interventions in a Structural Causal Model.
Another intersection with causal inference discusses the problem of causal confusion in imitation
(de Haan et al., 2019). Causal confusion is concerned with the problem of nuisances in observed
confounded data due to an unknown causal structure. These “causal misidentiﬁcations” can lead to
spurious correlations and catastrophic failures in generalization. In contrast, our work discusses the
orthogonal problem of hidden confounders with possible covariate shift.
Intersection of Causal Inference and Reinforcement Learning. Previous work has analyzed
the problem of optimal control from logged data with unobserved confounders (Lattimore et al.,
2016), as well as utilizing (non-expert) confounded data for online interactions (Tennenholtz et al.,
2021b). Much work has revolved around the reinforcement learning setup with access to (non-
expert) confounded data (Zhang & Bareinboim, 2019; Wang et al., 2020). Other work has considered
the problem of off-policy evaluation from confounded data (Tennenholtz et al., 2020; Oberst &
Sontag, 2019; Kallus & Zhou, 2020). Our work is focused on leveraging expert data with hidden
confounders and possible covariate shift in both the imitation and the RL settings.
7
CONCLUSION
This work presented and analyzed the problem of using expert data with hidden confounders for
both the imitation and RL settings. We showed that covariate shift of hidden confounders between
the expert data and the online environment can result in learning catastrophic policies, rendering
imitation learning hard, or even impossible (Theorem 2). In addition, we showed that when a reward
is provided, using the expert data is still possible under arbitrary hidden covariate shift (Theorem 5),
and proposed new algorithms for tackling this problem using corrective trajectory sampling (CTS).
9
Published as a conference paper at ICLR 2022
REFERENCES
Kiant´
e Brantley, Wen Sun, and Mikael Henaff. Disagreement-regularized imitation learning. In
International Conference on Learning Representations, 2019.
Ivan Bratko, Tanja Urbanˇ
ciˇ
c, and Claude Sammut. Behavioural cloning: phenomena, results and
problems. IFAC Proceedings Volumes, 28(21):143–149, 1995.
Juan Correa and Elias Bareinboim. A calculus for stochastic interventions: Causal effect identiﬁca-
tion and surrogate experiments. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 34, pp. 10093–10100, 2020.
Imre Csisz´
ar and Paul C Shields. Information theory and statistics: A tutorial. 2004.
Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. Ad-
vances in Neural Information Processing Systems, 32:11698–11709, 2019.
Zackory Erickson, Vamsee Gangaram, Ariel Kapusta, C Karen Liu, and Charles C Kemp. Assis-
tive gym: A physics simulation framework for assistive robotics. In 2020 IEEE International
Conference on Robotics and Automation (ICRA), pp. 10169–10176. IEEE, 2020.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse rein-
forcement learning. arXiv preprint arXiv:1710.11248, 2017.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse rein-
forcement learning. In International Conference on Learning Representations, 2018.
Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to ofﬂine reinforcement learning.
arXiv preprint arXiv:2106.06860, 2021.
Omer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, David Sontag, Finale
Doshi-Velez, and Leo Anthony Celi. Guidelines for reinforcement learning in healthcare. Nature
medicine, 25(1):16–18, 2019.
Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes. arXiv
preprint arXiv:1502.02259, 2015.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural
information processing systems, 29:4565–4573, 2016.
Jesse Y Hsu and Dylan S Small. Calibrating sensitivity analyses to observed covariates in observa-
tional studies. Biometrics, 69(4):803–811, 2013.
Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning: A
survey of learning methods. ACM Computing Surveys (CSUR), 50(2):1–35, 2017.
Eugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and
Craig Boutilier. Recsim: A conﬁgurable simulation platform for recommender systems. arXiv
preprint arXiv:1909.04847, 2019.
Nathan Kallus and Angela Zhou. Confounding-robust policy evaluation in inﬁnite-horizon rein-
forcement learning. arXiv preprint arXiv:2002.04518, 2020.
Nathan Kallus and Angela Zhou. Minimax-optimal policy learning under unobserved confounding.
Management Science, 67(5):2870–2890, 2021.
Liyiming Ke, Sanjiban Choudhury, Matt Barnes, Wen Sun, Gilwoo Lee, and Siddhartha Srinivasa.
Imitation learning as f-divergence minimization. In International Workshop on the Algorithmic
Foundations of Robotics, pp. 313–329. Springer, 2020.
Kee-Eung Kim and Hyun Soo Park. Imitation learning via kernel mean embedding. In Thirty-Second
AAAI Conference on Artiﬁcial Intelligence, 2018.
Ilya Kostrikov, Oﬁr Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution
matching. arXiv preprint arXiv:1912.05032, 2019.
10
Published as a conference paper at ICLR 2022
Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Oﬁr Nachum. Ofﬂine reinforcement learning
with ﬁsher divergence critic regularization. In International Conference on Machine Learning,
pp. 5774–5783. PMLR, 2021.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Finnian Lattimore, Tor Lattimore, and Mark D Reid. Causal bandits: learning good interventions
via causal inference. In Proceedings of the 30th International Conference on Neural Information
Processing Systems, pp. 1189–1197, 2016.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gon-
zalez, Michael Jordan, and Ion Stoica. Rllib: Abstractions for distributed reinforcement learning.
In International Conference on Machine Learning, pp. 3053–3062. PMLR, 2018.
Friedrich Liese and Igor Vajda. On divergences and informations in statistics and information theory.
IEEE Transactions on Information Theory, 52(10):4394–4412, 2006.
Oﬁr Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with ofﬂine datasets. arXiv preprint arXiv:2006.09359, 2020.
Hongseok Namkoong, Ramtin Keramati, Steve Yadlowsky, and Emma Brunskill.
Off-policy
policy evaluation for sequential decisions under unobserved confounding.
arXiv preprint
arXiv:2003.05623, 2020.
Michael Oberst and David Sontag. Counterfactual off-policy evaluation with gumbel-max structural
causal models. In International Conference on Machine Learning, pp. 4881–4890. PMLR, 2019.
Judea Pearl. Causality: models, reasoning and inference, volume 29. Cambridge University Press,
2000.
Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, New York,
NY, USA, 2nd edition, 2009a. ISBN 052189560X, 9780521895606.
Judea Pearl. Causal inference in statistics: An overview. Statistics surveys, 3:96–146, 2009b.
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Technical report,
CARNEGIE-MELLON UNIV PITTSBURGH PA ARTIFICIAL INTELLIGENCE AND PSY-
CHOLOGY ..., 1989.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
J. Rapin and O. Teytaud. Nevergrad - A gradient-free optimization platform. https://GitHub.
com/FacebookResearch/Nevergrad, 2018.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert,
Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what worked:
Behavior modelling priors for ofﬂine reinforcement learning. In International Conference on
Learning Representations, 2019.
11
Published as a conference paper at ICLR 2022
Guy Tennenholtz, Uri Shalit, and Shie Mannor. Off-policy evaluation in partially observable en-
vironments. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp.
10276–10283, 2020.
Guy Tennenholtz, Nir Baram, and Shie Mannor. Gelato: Geometrically enriched latent model for
ofﬂine reinforcement learning. arXiv preprint arXiv:2102.11327, 2021a.
Guy Tennenholtz, Uri Shalit, Shie Mannor, and Yonathan Efroni. Bandits with partially observable
confounded data. In Conference on Uncertainty in Artiﬁcial Intelligence. PMLR, 2021b.
Lingxiao Wang, Zhuoran Yang, and Zhaoran Wang. Provably efﬁcient causal reinforcement learning
with confounded observational data. arXiv preprint arXiv:2006.12311, 2020.
Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough
for convex mdps. arXiv preprint arXiv:2106.00661, 2021.
Junzhe Zhang and Elias Bareinboim. Near-optimal reinforcement learning in dynamic treatment
regimes. Advances in Neural Information Processing Systems, 32:13401–13411, 2019.
Junzhe Zhang, Daniel Kumor, and Elias Bareinboim. Causal imitation learning with unobserved
confounders. Advances in neural information processing systems, 33, 2020.
12
Published as a conference paper at ICLR 2022
Distribution Matching
Equivalent Representation
Comments
Distribution Ratio
supg:S×A7→(0,1) Es,a∼dπ′
ρe [log(g(s, a))] + Es,a∼dπ
ρo [log(1 −g(s, a))]
GAIL
(Ho & Ermon, 2016)
KL-divergence
supg:S×A7→R Es,a∼dπ
ρo [g(s, a)] −log Es,a∼dπ′
ρe
h
eg(s,a)i
Donsker-Varadhan Representation
(Kostrikov et al., 2019)
χ2-divergence
supg:S×A7→R 2Es,a∼dπ
ρo [g(s, a)] −Es,a∼dπ′
ρe

g2(s, a)

Variational Representation
of f-Divergence
TV Distance
sup|g|≤1
2 Es,a∼dπ
ρo [g(s, a)] −Es,a∼dπ′
ρe [g(s, a)]
Variational Representation
of f-Divergence
Table 1:
Different distribution matching techniques and their equivalent representations.
APPENDIX
A
BACKGROUND: DISTRIBUTION MATCHING FOR IMITATION LEARNING
A common approach used in (non-confounded) imitation learning is matching the policy’s
state-action frequency dπ
ρo to the ofﬂine target distribution dπ∗
ρe .
Consider a source distribu-
tion p ∈∆N and target distribution q ∈∆N.
GAIL (Ho & Ermon, 2016) uses the distribu-
tion ratio objective log(p/q), which can estimated using a GAN-like objective DR(p||q) =
supg:Z7→(0,1) Ep[log(g(z))] + Eq[log(1 −g(z))], to match the distribution p to q.
This technique can be generalized to f-divergences (Csisz´
ar & Shields, 2004; Liese & Vajda, 2006;
Kostrikov et al., 2019; Ke et al., 2020). Speciﬁcally, we wish to minimize a discrepancy measure
from p to q, namely minp∈K D(p||q). For a convex function f : [0 : ∞) 7→R, the f-divergence of
p from q is deﬁned by Df(p||q) = Eq
h
f

p
q
i
. DICE (Kostrikov et al., 2019) uses the variational
representation of the f-divergence,
Df(p||q) =
sup
g:Z7→R
Ep[g(z)] −Eq[f ∗(g(z))],
where f ∗is the Fenchel conjugate of f deﬁned by f ∗(y) = supx xy −f(y). The convex conjugate
has closed form solutions for the total variation distance, KL-divergence, χ2-divergence, Squared
Hellinger distance, Le Cam distance, and Jensen-Shannon divergence. Using the variational repre-
sentation of the f-divergence we can estimate Df using samples from p and q. Table 1 presents
examples of various f-divergences and their respective dual formulation. We also add the distribu-
tion ratio for comparison to the table, though it is not an f-divergence.
B
CONFOUNDED IMITATION - ALGORITHM AND CONVERGENCE
GUARANTEES
B.1
A TOY EXAMPLE
To gain intuition, we start with a simple toy example. Consider the three-state example depicted
in Figure 6. Here, the environment initiates at state A w.p. 1, after which the agent can choose to
(deterministicaly) transition to state B or C. The agent then receives a reward depending on the
context. The optimal policy is given by π∗(a|s, x) = 1{a = aB, x = x1} + 1{a = aC, x = x2}
for s = A,
and any action is optimal for s ̸= A.
Without loss of generality we as-
sume π∗(aB|B, x)
=
π∗(aC|C, x)
=
1.
We turn to analyze the marginalized state-
action frequency, which uniquely deﬁnes the set of optimal policies (Puterman, 2014).
De-
noting ρe(x1)
=
ρ, we have that dπ∗
ρe (s, a) = ρdπ∗(s, a|x1) + (1 −ρ)dπ∗(s, a|x2). Then,
dπ∗
ρe (s, a) = (1 −γ)1{s = A} + ργ1{s = B, a = aB} + (1 −ρ)γ1{s = C, a = aC}.
13
Published as a conference paper at ICLR 2022
Figure 6: A contextual MDP with state space S = {A, B, C}, action space A = {aB, aC} and
context space X = {x1, x2}. We assume ν(A|x) = 1 for all x ∈X. The actions aB, aC transition
the agent to states B, C, respectively, after which the agent receives a reward r ∈{0, 1} depending
on the context. We assume B, C are sink states.
Algorithm 3 Confounded Imitation
1: input: Expert data with missing context D∗∼dπ∗
ρe , λ > 0, sensitivity bound δ ≥0.
2: init: Υ = ∅
3: for n = 1, . . . do
4:
Sample u(s, a) ∼U[0, δ], ∀s, a
5:
L∗(π; g0) := Es,a∼dπ
ρo(s,a)[g0(s, a)] −Es,a∼dπ∗
ρe (s,a)+u(s,a)[g0(s, a)]
6:
Li(π; gi) := Ex∼ρo,s,a∼dπ(s,a|x)[gi(s, a, x)] −Ex∼ρo,s,a∼dπi(s,a|x)[gi(s, a, x)]
, i ≥1
7:
Compute πn by solving
min
π∈Πdet
max
|g0|≤1
2 ,|gi|≤1
2
n
L∗(π; g0(s, a)) −λ min
i
Li(π; gi(s, a, x))
o
(4)
8:
if πn ∈Υ then
9:
Terminate and return ¯
π(a|s, x) =
Pn−1
i=1 dπi(s,a,x)
Pn−1
i=1
P
a′ dπi(s,a′,x)
10:
else
11:
Υ = Υ ∪{πn}
12:
end if
13: end for
No Covariate Shift.
Suppose ρo = ρe, and ρ = 1
2. Trivially dπ∗
ρe (s, a) = dπ∗
ρo (s, a). We deﬁne the
(suboptimal) policy
π0(a|A, x) = 1 −π∗(a|A, x)
, a ∈A, x ∈X.
(3)
It can be veriﬁed that dπ∗
ρe (s, a) = dπ0
ρo(s, a) still holds, yet π0 is catastrophic (Equation (1)) with
value zero. A question arises: can we show that π0 is a suboptimal policy given access to the expert
data (i.e., access to dπ∗
ρe (s, a)) and a forward model P(s′|s, a, x)?
Unfortunately, one cannot prove that π0 is suboptimal. Informally, notice that π0 is an optimal
policy for an alternative reward function, r0(s, a, x) = 1 −r(s, a, x), yet is catastrophic w.r.t. the
true reward r. Indeed, since r is unknown and dπ0
ρo(s, a) = dπ∗
ρo (s, a), we cannot reject r0 (i.e.,
we cannot conclude that r0 is not the true reward). In other words, one cannot use the data to
differentiate which of {π0, π∗} is the optimal policy.
With Covariate Shift.
Next, assume ρo ̸= ρe, and deﬁne π0 as in Equation (3). Let e
ρe = 1 −ρe
and recall that ρe(x1) = ρ. Then, we have that
dπ0
f
ρe (s, a) = (1 −ρ)dπ0(s, a|x1) + ρdπ0(s, a|x2) = (1 −ρ)dπ∗(s, a|x2) + ρdπ∗(s, a|x1) = dπ∗
ρe (s, a).
Indeed, the expert data is incapable of distinguishing π0 and π∗, since dπ0
f
ρe = dπ∗
ρe , and ρe is un-
known. Unfortunately, as we’ve shown previously, π0 achieves value zero. Notice that, unlike the
previous section, one cannot distinguish π∗from the catastrophic policy π0 for any choice of ρo.
14
Published as a conference paper at ICLR 2022
B.2
A PRACTICAL ALGORITHM
The ambiguity set of Theorem 1 may contain suboptimal policies. Instead of randomly selecting a
policy from this set, we can choose the average policy. The following proposition shows that such a
selection is favorable.
Proposition 2.
Deﬁne the mean policy ¯
π(a|s, x)
=
P
π∈Υπ∗dπ(s,a,x)
P
π∈Υπ∗
P
a′ dπ(s,a′,x), and denote
α∗= |Π∗
M
T Υπ∗|
|Υπ∗|
∈[0, 1]. Then, vM(¯
π) ≥α∗v∗+ (1 −α∗) minπ∈Υπ∗vM(π).
Remark 1. Note that ¯
π is generally not the average policy
1
|Υπ∗|
P
π∈Υπ∗π(a|s, x).
Remark 2. In an episodic setting, ¯
π can be estimated by uniformly sampling a policy π ∈Υπ∗at
the beginning of the episode, and playing it until the environment terminates.
Algorithm 3 describes our method for calculating the ambiguity set of Theorem 1, and returns ¯
π of
Proposition 2. At every iteration of the algorithm, we ﬁnd a new policy in the set by minimizing the
total variation distance (written in variational form) between dπ∗
ρo (s, a) and dπ
ρo(s, a), while regular-
izing it with the distance between π and all previously collected πi ∈Υ. Algorithm 3 also uses a
sensitivity parameter δ ≥0 (deﬁned formally in Appendix C) whenever bounded covariate shift is
present. For this section we assume δ = 0.
In practice, the functions L∗and Li in lines 4 and 5 are estimated using samples from trajectories
of π, πi, and D∗. We then solve the min-max problem of Equation (4) using a parametric represen-
tations of gi and online gradient decent. The following proposition states that Algorithm 3 indeed
retrieves the set Υπ∗.
Proposition 3. Assume ρe ≡ρo and |Υπ∗| < ∞. Then there exists λ∗> 0 such that for any
λ ∈(0, λ∗), Algorithm 3 (with δ = 0 sensitivity) will return ¯
π of Proposition 2 after exactly |Υπ∗|
iterations.
B.3
IMITATION WITH CONTEXT-FREE REWARD
We tested Algorithm 3 on both the RecSim environment as well as a four-rooms environment with
random instantiations of walls. Experiments for the RecSim environment are readily provided in
Section 5. Here we describe our simple four-rooms environment and show experiments w.r.t. Theo-
rem 3.
The four-rooms environment, as depicted in Figure 7, is a 15 × 15 grid-world in which an agent can
take one of four actions: LEFT, RIGHT, UP, or DOWN. Each action moves the agent in the speciﬁed
direction whenever no obstacle is present. The agent (shown in blue) must reach the (green) goal
while avoiding the (red) mine. When the goal is reached the agent receives a reward of +1 and the
episode terminates. In contrast, if the agent reaches the mine, she receives a reward of −1 and the
episode terminates. The state space of the environment consists of the agent’s (row, col) position in
the world. The rest of the information in the environment is deﬁned by the context x. Particularly,
the context is deﬁned by the position of the green goal, the position of the red mine, and the speciﬁc
instantiation of walls (two instantiations are depicted in Figure 7).
We trained an agent with full information (i.e., observed context, including goal location, mine
location, and walls). We generated expert data w.r.t. the trained agent. To demonstrate the result
of Theorem 3 we executed Algorithm 3 with both a shifted distribution and the default distribution
of walls. We did not change the distribution of goal and mine. Note that since the distribution of
walls only affects the transition function and not the reward, we expect, by Theorem 3, the optimal
solution to remain the same. Indeed, as shown in Figure 7 after training an agent with no access
to the contextual information of the walls in the expert data, the agent achieved comparable results
both with and without covariate shift on the distribution of walls.
This result seem surprising at ﬁrst, as the walls are essential for solving the task at hand. Never-
theless, since the distribution of wall is observed in the online environment, the partially observed
expert data sufﬁces to obtain an optimal policy. This settles with Theorem 3 which indeed states that
this information is not needed in the expert data in order to obtain an optimal policy.
15
Published as a conference paper at ICLR 2022
Figure 7: Results for the rooms environment with covariate shift affecting only the distribution of
walls. It is evident that whenever the reward is context-free comparable performance is obtained.
Runs averaged over 5 seeds.
C
BOUNDED HIDDEN CONFOUNDING
In this section we discuss the imitation learning problem under bounded hidden confounders. There
are several ways to deﬁne boundness of unobserved confounders. In Section 3 we showed that,
under arbitrary covariate shift and context-free transitions, the imitation learning problem is impos-
sible, i.e., one cannot rule out a catastrophic policy. We begin by considering the effect of bounded
covariate shift, i.e., ρo
ρe ≤C. We then consider almost-context-free rewards, showing a tradeoff w.r.t.
the hardness of the imitation problem.
A Sensitivity Perspective.
A common approach in causal inference is to bound the bias of unob-
served confounding through sensitivity analysis (Hsu & Small, 2013; Namkoong et al., 2020; Kallus
& Zhou, 2021). In our setting, this confounding bias occurs due to a covariate shift of the unobserved
covariates. As we’ve shown in Theorem 2, though these covariates are observed in the online envi-
ronment, their shifted and unobserved distribution in the ofﬂine data can render catastrophic results.
Therefore, we consider the odds-ratio bounds of the sensitivity in distribution between the online
environment and the expert data, as stated formally below.
16
Published as a conference paper at ICLR 2022
Assumption 2 (Bounded Sensitivity). We assume that Supp(ρe) ⊆Supp(ρo) and that there exists
some Γ ≥1 such that for all x ∈Supp(ρe)
Γ−1 ≤ρo(x)(1 −ρe(x))
ρe(x)(1 −ρo(x)) ≤Γ.
Next, we deﬁne the notion of δ-ambiguity, a generalization of the ambiguity set in Deﬁnition 1.
Deﬁnition 5 (δ-Ambiguity Set). For a policy π ∈Π, we deﬁne the set of all deterministic policies
that are δ-close to π by
Υδ
π =
n
π′ ∈Πdet :


dπ′
ρo(s, a) −dπ
ρe(s, a)


 < δ, s ∈S, a ∈A
o
.
Similar to Deﬁnition 1, the δ-ambiguity set considers all deterministic policies with a marginalized
state-action frequency of distance at most δ from π. The following results shows that ΥΓ−1
π∗
is a
sufﬁcient set of candidate optimal policies, as long as Assumption 2 holds for some Γ ≥1.
Theorem 6. [Sufﬁciency of ΥΓ−1
π∗] Let Assumption 2 hold for some Γ ≥1. Then π∗∈ΥΓ−1
π∗.
The above result suggests that Algorithm 3 can be executed over ΥΓ−1
π∗
by adding δ = Γ −1
additive uniform noise to dπ∗
ρe (s, a) (see Line 4 of Algorithm 3), and executing the algorithm for a
ﬁnite number of iterations, ﬁnally selecting an average policy from the approximate set.
Context Reconstruction.
When bounded covariate shift is present, one might attempt to learn an
inverse mapping of contexts from observed trajectories in the data.
We
denote
by
P π
ρ
the
probability
measure
over
contexts
x
∈
X
and
trajectories
τ = (s0, a0, s1, a1, . . . sH) as induced by the policy π and context distribution ρ. That is,
P π
ρ (x, τ) = ρ(x)ν(s0|x)
H−1
Y
t=0
P(st+1|st, at, x)π(at|st, x).
As the true context is observed in the online environment, we can calculate for any π the quan-
tity P π
ρo(x, τ).
As the expert data distribution was generated by the marginalized distribution
P π∗
ρo (τ) = P
x∈X P π∗
ρo (x, τ), it is unclear if knowledge of P π
ρo(x, τ) is beneﬁcial.
Fortunately, whenever Assumption 2 holds, a high probability of reconstructing a context in the
online environment induces a high probability of reconstructing it in the expert data. To see this,
assume that there exists δ ∈[0, 1] such that for all π ∈ΥΓ−1
π∗, τ ∈Supp(P π
ρe(τ)), there exists x ∈X
such that
P π
ρo(x|τ) ≥min{(1 −δ)(ρo(x) + Γ(1 −ρo(x))), 1}.
(5)
That is, we assume that for any policy that δ-ambiguous to π∗, and any induced trajectory of x ∈X,
one can with high probability identify x in the online environment. Importantly, this property can
be veriﬁed in the online environment. When Assumption 2 and 5 hold, we get that
P π
ρe(x|τ) = P π
ρe(τ|x)ρe(x)
P π
ρe(τ)
≥P π
ρe(τ|x)
P π
ρe(τ)
ρo(x)
ρo(x) + Γ(1 −ρo(x)) =
P π
ρo(x|τ)
ρo(x) + Γ(1 −ρo(x)) ≥1 −δ.
In other words, we can reconstruct x with probability 1 −δ for any trajectory τ which satisﬁes the
above. This allows us to deconfound essential parts of the expert data, rendering it useful for the
imitation problem, even when reward is not provided. We leave further analysis of this direction for
future work.
Context-Dependent Reward.
In Theorem 3 we showed that whenever the reward is independent
of the context then the imitation problem is easy, in the sense that any policy π0 ∈Υπ∗is also an
optimal policy. Here, we relax the assumption on the reward, and instead assume bounded depen-
dence of the reward on the context. The following deﬁnition upper bounds the confounding effect
of the reward w.r.t. the context.
17
Published as a conference paper at ICLR 2022
*
Figure 8: Contextual MDP Causal Diagram.
Deﬁnition 6. Let ϵ : X 7→R such that
min
r0:S×A7→R |r(s, a, x) −r0(s, a)| ≤ϵ(x)
, s ∈S, a ∈A, x ∈X
Using the above deﬁnition, we can now show that any policy in Υπ∗is still approximately optimal,
as shown by the following result.
Theorem 7. [Context Dependent Reward]
Let ϵ
:
X
7→
R of Deﬁnition 6.
Denote
ϵoe = Ex∼ρo(x)[ϵ(x)] + Ex∼ρe(x)[ϵ(x)]. Then for any π∗∈Π∗
M, π0 ∈Υπ∗
v(π0) ≥v(π∗) −ϵoe.
A direct corollary for the above result states that for ϵ : X 7→R of Deﬁnition 6, if ϵ(x) = ϵ, for
all x ∈X, then for π0, π∗of Theorem 7, it holds that v(π0) ≥v(π∗) −2ϵ. That is, π0 is an
approximately optimal policy.
D
RELATION TO CAUSAL INFERENCE
Our work is focused on the problem of hidden confounders in expert data for imitation and reinforce-
ment learning. We have chosen to write the paper in terminology famililar to the RL community. In
this section we address and formalize the problem in Causal Inference (CI) terminology. We begin
by deﬁning Structural Causal Models (SCM, Pearl (2009a)) – a basic building block of our frame-
work. We then show how the confounded imitation problem can be formalized as an intervention
over a speciﬁc SCM. Generally speaking, the causal view casts the environment, namely the expert
environment generating the ofﬂine data, and the online environment, as confounders.
Deﬁnition 7 (Structural Causal Models).
A Structural Causal Model (SCM) is a tuple
M = (U, V, F, P(U)) where U is a set of exogenous variables and V is a set of endogenous vari-
ables. F is a set of functions such that fi ∈F are functions mapping a set of endogenous variables
Pai ⊆V \{Vi} and a set of exogenous variables Ui ⊆U to the domain of Vi, i.e., Vi = fi(Pai, Ui).
Finally, P(u) is a probability distribution over the set of exogenous variables U. We assume that
the SCM is recursive, i.e., that the causal diagram associated with it is acyclic.
Every SCM M is associated with a causal diagram G, as depicted in Figure 8. Our framework relies
largely on the formulation of stochastic interventions, as proposed in Correa & Bareinboim (2020).
18
Published as a conference paper at ICLR 2022
We consider stochastic, conditional (non-atomic) interventions, deﬁned by regime indicators σZ
(Pearl, 2000; Correa & Bareinboim, 2020), deﬁned formally below.
Deﬁnition 8 (Non-Atomic Interventions). Given a SCM M = (U, V, F, P(U)) and a subset
Z ⊆V , an intervention σZ = {σZ1, . . . , σZn} deﬁnes a new SCM MσZ = (U, V, F∗, P(U)) in
which the set of functions F is changed to F∗= {f ∗
i }i:Vi∈{Zj}n
j=1
S{fi}i:Vi∈V \{Zj}n
j=1.
Non-atomic interventions are a generalization of the classic atomic do(X = x) interventions, de-
ﬁned by the SCM Mz and causal diagram GZ in which all edges incoming into Z are removed. We
have that
P(y|do(Z = z)) = P(y|z; σZ = do(Z = z)).
Atomic interventions replace function in F by constant functions, whereas non-atomic interventions
use general functions. For notational simplicity, when a single intervention is applied to some fi ∈
F, we denote it by σZ = d(fi ←f ∗
i ), indicating that in the interventional distribution f ∗
i is used
instead of fi. Next we deﬁne the identiﬁability of a causal effect under an intervention, as follows.
Deﬁnition 9 (Identiﬁability). Let X, Y, Z ⊆V with Y ∩Z = ∅in some SCM with causal diagram G.
Given an intervention σZ = {σZ1, . . . , σZn}, the causal effect P(y|x, σZ) is said to be identiﬁable
from V ′ ⊆V if it can be uniquely computed from P(V ′) for every assignment (y, x) in every model
that induces G and P(V ′).
Following the model deﬁnitions of Section 2, we deﬁne the contextual MDP SCM as follows.
Deﬁnition 10. A contextual MDP SCM is deﬁned by the causal diagram of Figure 8.
For some horizon H
>
0,
the SCM is deﬁned by the set of endogenous variables
V = {si}H
i=0 ∪{ai}H
i=0 ∪{x} ∪{ri}H
i=0 (denoting the states, actions, context and rewards, respec-
tively), a set of exogenous variables U, and functions F = {fsi, fai, fri, fρe, fν0}, where fsi corre-
spond to the transition function, fai the expert policy, fri the reward function, fρe the context expert
distribution, and fν0 the initial context-dependent state distribution.
Relating to our formal deﬁnition of our model in Section 2, with slight abuse of notations, the
functions {fs, fg, fa, fρe, fν0} adhere to the following relations
P(si+1 = s′|si = s, ai = a, x) = P(fsi(s, a, x, U))
P(ri = r|si = s, ai = a, x) = δ(fri(s, a, x) = r)
π∗(ai = a|si = s, x) = P(fai(a, s, x, U)
ρe(x) = P(fρe(x, U))
ν0(s0|x) = P(fν0(s0, x, U)),
where δ(·) indicates the Dirac delta distribution.
We are now ready to deﬁne the confounded imitation problem. We deﬁne the (non-atomic) interven-
tion σx = do(fρe ←fρo) which replaces fρe with fρo in the contextual MDP SCM deﬁned above.
The goal of imitation learning is then to identify the quantities
P(ai|si, x, σx = do(fρe ←fρo))
, 0 ≤i ≤H −1,
(6)
where, importantly, we assume we only have access to P(si, ai), P(si+1|si, ai, x), P(s0|x),
and P(x|σx = do(fρe ←fρo)).
Notice that in our setting, P(si+1|si, ai, x), P(s0|x), and
P(x|σx = do(fρe ←fρo)) correspond to known quantities of the online environment, whereas
P(si, ai) corresponds to the (partially observed) ofﬂine expert data.
We also emphasize that
P(si+1|si, ai, x) is not dependent on the intervention σZ. That is,
P(si+1|si, ai, x, σx = do(fρe ←fρo)) = P(si+1|si, ai, x).
Remark. Our work studies a slightly different version of the identiﬁability problem in Equation (6),
as we only wish to identify an optimal policy from the set Π∗
M, as opposed to the single speciﬁc
policy π∗. This requirement can be formalized by deﬁning an extended SCM which includes all
optimal policies in Π∗
M, with the assumption that only one is observed (corresponding to the expert
data).
19
Published as a conference paper at ICLR 2022
Algorithm 4 RL using Expert Data with Unobserved Confounders (Complete Algorithm)
1: input: Expert data with missing context D∗, λ, α, B, N, M > 0, policy optimization algorithm
ALG-RL
2: init: Policy π0, global bonus reward network g∗
θ
3: for k = 1, . . . do
4:
Generate dataset of rollouts Rk ∼dπk−1
ρo
(s, a)
5:
Initialize local networks gm
θm ←gθ, m ∈[M]
6:
for m = 1, . . . M do
7:
Sample weight vector wm uniformly from ∆n
8:
for e = 1 . . . N do
9:
Sample batch uniformly from Rk, i.e., {si, ai}B
i=1
U
∼Rk
10:
Sample batch according to weights wm from D∗, i.e., {se
i, ae
i}B
i=1
wm
∼D∗
11:
Update gm
θm according to
∇θmLm(θm) = 1
B
B
X
i=1
∇θm

(1 −α)f ∗(gm
θm(se
i, ae
i)) + αf ∗(gm
θm(si, ai)) −gm
θm(si, ai)

12:
end for
13:
m∗∈arg minm∈[M] Lm(θm)
14:
Update global parameters from the selected local network g∗
θ ←gm∗
θm∗
15:
πk ←ALG-RL(r(s, a, x) −λg∗
θ(s, a))
16:
end for
17: end for
Name
Value
Comments
Batch size
128
Learning rate
5e−5
Rollout size
19, 200
Total timesteps
5e6
Num epochs
50
How many training epochs to do after each rollout
γ
0.95
Discount factor
kl coef
0.2
Initial coefﬁcient for KL divergence
kl target
0.01
Target value for KL divergence
GAE λ
1
The GAE (lambda) parameter
Num workers
40
Table 2: Hyper-parameters used to train the PPO agent.
E
IMPLEMENTATION DETAILS
Our experiments were based off of the recently proposed assistive-gym (Erickson et al., 2020) and
recsim (Ie et al., 2019) environments. In this section we discuss further implementation details,
hyperparameters, context distributions, and generation of the expert data.
Algorithm Details.
A complete description of Algorithm 2 is presented in Algorithm 4. Speciﬁc
hyperparameters used are shown in Tables 2 and 3. We implemented the algorithm using the RLlib
framework (Liang et al., 2018). We used PPO (Schulman et al., 2017) as our policy-optimization
algorithm. All neural networks consisted of two-layer fully connected MLPs with 100 parameters
in each layer. We used the same rollout buffer (of size 19200 samples) for both our PPO agent as
well as our imitation module, which estimated the augmented reward.
20
Published as a conference paper at ICLR 2022
Name
Value
Comments
Batch size
128
Learning rate
1e−4
Imitation Method
χ-divergence
Num epochs
50
How many training epochs to do after each rollout
α
0.9
Df regularization coefﬁcient
M
10000
Budget for CTS optimizer
Table 3: Hyper-parameters used for imitation and CTS.
Motivated by Kostrikov et al. (2019), we regularized the expert demonstrations with samples from
dπ. Particularly, we let α ∈(0, 1], such that 1 −α corresponds to the probabiilty of sampling an
expert example and α corresponds to the probability of sampling from the replay. This leads to
minimizing an augmented version of the f-divergence which can be written as
min
g:S×A7→R Es,a∼dπ
ρo(s,a|x)[g(s, a) −αf ∗(g(s, a))] −(1 −α)Es,a∼dπ∗
ρe (s,a)[f ∗(g(s, a))].
Our imitation module consisted of two networks gθ and hθ as proposed in Fu et al. (2018). The
“done” signal was also added to the state for training the imitation module. For training CTS we
used the Nevergrad optimization platform (Rapin & Teytaud, 2018) with a budget of 10000 and one
worker. Here a copied version of the networks gθ and hθ were used to for initialization and then
approximate the minimum Df.
For choosing λ we used an adaptive strategy which ensured λ balanced the RL objective with the
imitation objective. Speciﬁcally, we used the following tradeoff between reward r and bonus g
(1 −λadap)r(s, a) + λadapg(s, a),
where λadap =
rmean
rmean+gmean . Here rmean corresponds to the average reward in the replay buffer and
gmean to the average bonus in the replay buffer. By averaging the two, we maintained a similar scale
to effectively use the expert data in all the evaluated environments without optimizing for λ.
Context Distribution.
For each environment we used a varying context distribution in the expert
data, with increasing distance to that of the online environment. The context distribution for the
RecSim environment is formally described in Section 5. For the assistive-gym environment the
context was deﬁned by the following features: gender, mass, radius, height, patient impairment, and
patient preferences. The patient’s mass, radius, and height distributions were dependent on gender.
The patient’s impairment was given by either limited movement, weakness, or tremor (with sporadic
movement). Finally, the patient’s preferences were affected by the velocity and pressure of touch
forces applied by the robot. We used default average values that were provided with the simulator.
Particularly, we used the following distributions for each feature
gender ∼Bern(pmale)
mass(gender) ∼N(µmass(gender), σ2
mass)
radius(gender) ∼N(µradius(gender), σ2
radius)
height(gender) ∼N(µheight(gender), σ2
height)
velocity weight ∼Unif([ℓvel, uvel])
force nontarget weight ∼Unif([ℓtarget, utarget])
high forces ∼Unif([ℓhigh forces, uhigh forces])
food hit weight ∼Unif([ℓhit, uhit])
food velocity weight ∼Unif([ℓfood vel, ufood vel])
high pressures weight ∼Unif([ℓhigh pressure, uhigh pressure])
impairment ∼Multinomial(pnone, plimits, pweakness, ptremor).
21
Published as a conference paper at ICLR 2022
Name
Value
Name
Value
Name
Value
pmale
0.3
ℓvel
0.225
ℓhigh pressure
0.009
µmass(male)
78.4
uvel
0.275
uhigh pressure
0.011
µmass(female)
62.5
ℓtarget
0.009
pnone
0.1
σ2
mass
10
utarget
0.011
plimits
0.4
µradius(male)
1
ℓhigh forces
0.045
pweakness
0.3
µradius(female)
1
uhigh forces
0.055
ptremor
0.2
σ2
radius
0.1
ℓhit
0.9
µheight(male)
1
uhit
1.1
µheight(female)
1
ℓfood vel
0.9
σ2
height
0.1
ℓfood vel
1.1
Table 4: Parameters for context distribution used in assistive-gym
Name
Value
Name
Value
Name
Value
pmale
0.8
ℓvel
0.225
ℓhigh pressure
0.009
µmass(male)
88.4
uvel
0.275
uhigh pressure
0.111
µmass(female)
72.5
ℓtarget
0.007
pnone
0.1
σ2
mass
20
utarget
0.016
plimits
0.1
µradius(male)
0.9
ℓhigh forces
0.035
pweakness
0.1
µradius(female)
0.9
uhigh forces
0.06
ptremor
0.7
σ2
radius
0.2
ℓhit
0.4
µheight(male)
1.1
uhit
2.1
µheight(female)
1.1
ℓfood vel
0.4
σ2
height
0.2
ℓfood vel
2.1
Table 5: Parameters for one of the shifted context distribution used in assistive-gym
The values for each distribution are provided in Table 4. For setting covariate shift, we used a
a set of distributions that were shifted w.r.t. the default context distribution. We then sampled a
shifted distribution w.p. β and the default distribution w.p. 1 −β. That is, when β = 1, the user
sampled a context only from the shifted distribution. Table 5 shows an example of one of the shifted
distribution that were used.
Expert Data Generation.
For the assistive-gym experiments we a dense reward function for gen-
erating the expert data and a sparse one for our experiments using the expert data. Speciﬁcally, the
dense reward function used the environment’s default reward function, deﬁned by
w1 · distance to goal + w2 · action + w3 · task speciﬁc reward + w4 · preference score,
where the preferences were weighted according to the context features. Speciﬁc weights are pro-
vided in the implementation of assistive-gym (Erickson et al., 2020). The sparse reward function
did not use the distance to goal (i.e., w1 = 0).
22
Published as a conference paper at ICLR 2022
F
MISSING PROOFS
F.1
PROOFS FOR SECTION 3
We begin by proving two auxilary lemmas.
Lemma 1. Let π2 ∈Υπ1. Then, Υπ1 = Υπ2.
Proof. We show that Υπ1 ⊆Υπ2 and Υπ2 ⊆Υπ1.
Let π ∈Υπ1, then dπ(s, a) = dπ1(s, a). By our assumption, π2 ∈Υπ1, then dπ2(s, a) = dπ1(s, a).
Hence, dπ(s, a) = dπ2(s, a). That is, π ∈Υπ2. This proves Υπ1 ⊆Υπ2.
Similarly, let π ∈Υπ2, then dπ(s, a) = dπ2(s, a). By our assumption, π2 ∈Υπ1, then dπ2(s, a) =
dπ1(s, a). Hence, dπ(s, a) = dπ1(s, a). That is, π ∈Υπ1. This proves Υπ2 ⊆Υπ1, completing the
proof.
Lemma 2. Let π0 be a deterministic policy and let M0
=
(S, A, X, P, r0, γ) such that
r0(s, a, x) = 1{a = π0(s, x)}. Then π0 is the unique, optimal policy in M0.
Proof. By deﬁnition of π0 and r0,
r0(s, π0(s, x), x) = 1, ∀s ∈S, x ∈X.
In particular, Eπ0[r0(st, at, x)] = 1. Then
V ∗
M0 ≤(1 −γ)
∞
X
t=0
γt = Eπ0
"
(1 −γ)
∞
X
t=0
γtr0(st, at, x)
#
= V π0
M0.
This proves π0 is an optimal policy. To prove uniqueness, assume by contradiction there exists an
optimal policy π1 ̸= π0. Then,
V π1 = Es,a,x∼dπ1(s,a,x)[1{a = π0(s, x)}] = Es,x∼dπ1(s,x)

Ea∼π1(·|s,x)[1{a = π0(s, x)}]

< 1 = V π0
M0.
In contradiction to π1 is optimal. Then, π0 is a unique optimal policy.
We are now ready to prove Theorem 1.
Theorem 1. [Sufﬁciency of Υπ∗] Assume ρe ≡ρo. Let π∗∈Π∗
M and let π0 ∈Υπ∗. Then,
Υπ∗= Υπ0. Moreover, if π0 ̸= π∗, then there exists r0 such that π0 ∈Π∗
M0 but π∗/
∈Π∗
M0, where
M0 = (S, A, X, P, r0, ρo, ν, γ).
Proof. Let π∗∈Π∗
M and let π0 ∈Υπ∗. By Lemma 1, as π0 ∈Υπ∗, it holds that Υπ∗= Υπ0.
Next, choosing r0(s, a, x) = 1{a = π0(s, x)}, by Lemma 2 we get that π0 is an optimal policy in
M0. This proves π0 ∈Π∗
M0. Finally, by Lemma 2, Π∗
M0 = {π0}, proving π∗/
∈Π∗
M0 if and only
if π∗̸= π0.
Proposition 2.
Deﬁne the mean policy ¯
π(a|s, x)
=
P
π∈Υπ∗dπ(s,a,x)
P
π∈Υπ∗
P
a′ dπ(s,a′,x), and denote
α∗= |Π∗
M
T Υπ∗|
|Υπ∗|
∈[0, 1]. Then, vM(¯
π) ≥α∗v∗+ (1 −α∗) minπ∈Υπ∗vM(π).
Proof. Let ˜
π as deﬁned. Then by linearity of expectation
V ˜
π
M = Es,a,x∼d˜
π[r(s, a, x)] =
1
|Υπ∗|
X
π∈Υπ∗
Es,a,x∼dπ[r(s, a, x)] =
1
|Υπ∗|
X
π∈Υπ∗
V π
M.
23
Published as a conference paper at ICLR 2022
Denote B∗= Π∗
M ∩Υπ∗, then
V ˜
π
M =
1
|Υπ∗|
X
π∈B∗
V π
M +
1
|Υπ∗|
X
Υπ∗\B∗
V π
M
= |B∗|
|Υπ∗|V ∗
M +
1
|Υπ∗|
X
Υπ∗\B∗
V π
M
≥|B∗|
|Υπ∗|V ∗
M + |Υπ∗\B∗|
|Υπ∗|
min
π∈Υπ∗\B∗V π
M
≥|B∗|
|Υπ∗|V ∗
M + |Υπ∗\B∗|
|Υπ∗|
min
π∈Υπ∗V π
M,
completing the proof.
Theorem 2. [Catastrophic Imitation] Assume |X| ≥|A|, and P(s′|s, a, x) = P(s′|s, a, x′) for all
x, x′ ∈X. Then ∃πe,1, πe,2 s.t. {πe,1, πe,2} are non-identiﬁable, catastrophic expert policies.
Proof. We ﬁrst sketch the proof for the special case X = {x0, x1}, A = {a0, a1} and a singleton
state space S = {s0}. The general proof follows similarly and is given below.
By letting π1, π2 be the determinisic policies which choose opposite actions at opposite contexts,
i.e., π1(xi) = ai, π2(xi) = a1−i, we can choose ρe(x) = d∗(π1(x)) and e
ρe(x) = d∗(π2(x)) which
yield
dπ1
ρe (a) =
1
X
i=0
ρe(xi)1{a = π1(xi)}
=
2
X
i=1
d∗(π1(xi))1{a = π1(xi)}
=
k
X
i=1
d∗(ai)1{ai = a} := d∗(a).
Similarly, dπ2
e
ρe (a) = d∗(a).
For the second part of the proof choose r1(a, x)
=
1{x = xi, a = ai} and r2(a, x)
=
1{x = xi, a = a1−i}. Notice that πi is optimal for ri under any distribution of contexts, yet πi
achieves zero reward for r1−i.
We now provide a complete proof for the general case.
Let ρo, d∗(a). Without loss of generality, let X = {x0, . . . , xm}, A = {a0, . . . , ak} with m ≥k,
and denote Xk = {x1, . . . , xk} ⊆X. By deﬁnition there exists an injective function from A into X.
Deﬁne
f(x) =
ai
, x = xi, i = 0, . . . , k
a0
, o.w.
g(x) =
ai+1 (mod k)
, x = xi, i = 0, . . . , k
a0
, o.w.
Then we can select π1, π2, ρe, e
ρe as follows
π1(a|x) = 1{a = f(x), x ∈Xk} +
1
k + 11{x /
∈Xk}
π2(a|x) = 1{a = g(x), x ∈Xk} +
1
k + 11{x /
∈Xk},
and
ρe(x) = d∗(f(x))1{x ∈Xk},
e
ρe(x) = d∗(g(x))1{x ∈Xk}.
24
Published as a conference paper at ICLR 2022
We get that
dπ1
ρe (a) =
m
X
i=1
ρe(xi)π1(a|xi)
=
k
X
i=1
d∗(f(xi))1{a = f(xi)}
=
k
X
i=1
d∗(ai)1{ai = a} = d∗(a).
Similarly,
dπ2
e
ρe (a) =
k
X
i=1
d∗(g(xi))1{a = g(xi)}
=
k
X
i=1
d∗(ai+1 (mod k))1

ai+1 (mod k) = a
	
=
k
X
i=1
d∗(ai)1{ai = a} = d∗(a).
This proves the ﬁrst part of the theorem. For the other parts, choose r1, r2 as follows
r1(a, x) = 1{x = xi, a = ai, 0 ≤i ≤k}
r2(a, x) = 1

x = xi, a = ai+1 (mod k), 0 ≤i ≤k
	
.
Then, by deﬁnition, for any P(x) such that Supp(P) ∩Xk ̸= ∅,
Ex∼P (x),a∼π1(·|x)[r1(a, x)] = 1 = max
π∈Π Ex∼P (x),a∼π(·|x)[r1(a, x)],
Ex∼P (x),a∼π1(·|x)[r2(a, x)] = 0 = min
π∈Π Ex∼P (x),a∼π(·|x)[r2(a, x)].
And similarly,
Ex∼P (x),a∼π2(·|x)[r1(a, x)] = 0 = min
π∈Π Ex∼P (x),a∼π(·|x)[r1(a, x)],
Ex∼P (x),a∼π2(·|x)[r2(a, x)] = 1 = max
π∈Π Ex∼P (x),a∼π(·|x)[r2(a, x)].
The condition on the support holds for ρe, e
ρe by deﬁnition. If, Supp(ρo) ∩Xk = ∅, then the result
holds trivially as Ex∼ρo(x),a∼π(·|x)[r1(a, x)] = Ex∼ρo(x),a∼π(·|x)[r2(a, x)] = 0 for all π ∈Π. This
completes the proof.
Lemma 3. Assume Supp(ρo) ⊆Supp(ρe). Then
arg max
π
Ex∼ρe(x),s,a∼dπ(s,a|x)[r(s, a, x)] ⊆arg max
π
Ex∼ρo(x),s,a∼dπ(s,a|x)[r(s, a, x)]
Proof. For clarity we denote
Π∗
ρe = arg max
π
Ex∼ρe(x),s,a∼dπ(s,a|x)[r(s, a, x)]
Π∗
ρo = arg max
π
Ex∼ρo(x),s,a∼dπ(s,a|x)[r(s, a, x)]
Π∗
Supp(ρe) = ×
x∈Supp(ρe)
arg max
π
Es,a∼dπ(s,a|x)[r(s, a, x)].
To prove the lemma, we will show Π∗
ρe = Π∗
Supp(ρe) ⊆Π∗
ρo.
We begin by proving Π∗
ρe = Π∗
Supp(ρe). Indeed, let π∗∈Π∗
Supp(ρe). Then, for any x ∈Supp(ρe)
Es,a∼dπ∗(s,a|x)[r(s, a, x)] = max
π
Es,a∼dπ(s,a|x)[r(s, a, x)].
25
Published as a conference paper at ICLR 2022
In particular,
Ex∼ρe(x),s,a∼dπ∗(s,a|x)[r(s, a, x)] = Ex∼ρe(x)
h
max
π
Es,a∼dπ(s,a|x)[r(s, a, x)]
i
≥max
π
Ex∼ρe(x),s,a∼dπ∗(s,a|x)[r(s, a, x)],
where we used Jensen’s inequality. This proves Π∗
Supp(ρe) ⊆Π∗
ρe.
To see the other direction, let πe ∈Π∗
ρe and assume by contradiction that πe /
∈Π∗
Supp(ρe). Then,
there exists ˜
x ∈Supp(ρe) such that
Es,a∼dπe(s,a|˜
x)[r(s, a, ˜
x)] < max
π
Es,a∼dπ(s,a|˜
x)[r(s, a, ˜
x)].
Deﬁne
˜
π(·|s, x) = 1{x = ˜
x}π˜
x(·|s, ˜
x) + 1{x ̸= ˜
x}πe(·|s, x),
where π˜
x ∈arg maxπ Es,a∼dπ(s,a|˜
x)[r(s, a, ˜
x)]. Then,
v(πe) = P(x = ˜
x)Es,a∼dπe(s,a|˜
x)[r(s, a, ˜
x)] +
X
x∈Supp(ρe)\{˜
x}
P(x)Es,a∼dπe(s,a|x)[r(s, a, x)]
< P(x = ˜
x)Es,a∼d˜
π(s,a|˜
x)[r(s, a, ˜
x)] +
X
x∈Supp(ρe)\{˜
x}
P(x)Es,a∼dπe(s,a|x)[r(s, a, x)] = v(˜
π),
in contradiction to πe ∈Π∗
ρe. This proves Π∗
ρe ⊆Π∗
Supp(ρe). We have thus shown that Π∗
ρe =
Π∗
Supp(ρe).
Finally, it is left to show that Π∗
Supp(ρe) ⊆Π∗
ρo. Similar to before, let π∗∈Π∗
Supp(ρe). Then, for any
x ∈Supp(ρe), by Jensen’s inequality
Ex∼ρo(x),s,a∼dπ∗(s,a|x)[r(s, a, x)] = Ex∼ρo(x)
h
max
π
Es,a∼dπ(s,a|x)[r(s, a, x)]
i
≥max
π
Ex∼ρo(x),s,a∼dπ∗(s,a|x)[r(s, a, x)].
This completes the proof.
Theorem
3.
[Sufﬁciency
of
Context-Free
Reward]
Assume
Supp(ρo) ⊆Supp(ρe)
and
r(s, a, x) = r(s, a, x′) for all x, x′ ∈X. Then Υπ∗⊆Π∗
M.
Proof. Let π0 ∈Υπ∗, we will show π0 ∈Π∗
M. Since r(s, a, x) = r(s, a, x′) for all x ∈X we
denote r(s, a) = r(s, a, x). By deﬁnition of Υπ∗we have that.
dπ0
ρo(s, a) = dπ∗
ρe (s, a)
Then,
v(π0) = Ex∼ρo(x),s,a∼dπ0(s,a|x)[r(s, a)]
= Ex∼ρo(x)


X
s∈S,a∈A
dπ0(s, a | x)r(s, a)


=
X
s∈S,a∈A
r(s, a)Ex∼ρo(x)[dπ0(s, a | x)]
= Es,a∼dπ0
ρo (s,a)[r(s, a)]
= Es,a∼dπ∗
ρe (s,a)[r(s, a)]
= Ex∼ρe(x),s,a∼dπ∗(s,a|x)[r(s, a)]
= max
π
Ex∼ρe(x),s,a∼dπ(s,a|x)[r(s, a)]
Then, π0 ∈arg maxπ Ex∼ρe(x),s,a∼dπ(s,a|x)[r(s, a)]. Applying Lemma 3
π0 ∈arg max
π
Ex∼ρo(x),s,a∼dπ(s,a|x)[r(s, a)] = Π∗
M,
completing the proof.
26
Published as a conference paper at ICLR 2022
F.2
PROOFS FOR SECTION 4
Proposition 1. [Trajectory Sampling Equivalence] Let ρ∗
s which minimizes Problem (P2) for some
π ∈Π, g : S × A 7→R, and assume Supp(ρo) ⊆Supp(ρe). Then, there exists pn ∈∆n such that
dπ∗
ρ∗
s (s, a) = lim
n→∞Ei∼pn
(1 −γ) P∞
t=0 γt1

(si
t, ai
t) = (s, a)
	
.
Proof. We can write
dπ(s, a | x) = (1 −γ)
∞
X
t=0
γtP(st = s, at = a|x)
= (1 −γ)
X
τ
∞
X
t=0
γtP(st = s, at = a|x, τ)P(τ|x)
= (1 −γ)
X
τ
∞
X
t=0
γt1{τt = (s, a)}P(τ|x).
Then, denoting P π
ρ∗
s(τ) = Ex∼ρ∗
s[ P(τ | x) ], we get that
dπ
ρ∗
s(s, a) = (1 −γ)
X
τ
∞
X
t=0
γt1{τt = (s, a)}P π
ρ∗
s(τ)
= Eτ∼P π
ρ∗
s
"
(1 −γ)
∞
X
t=0
γt1{τt = (s, a)}
#
.
Since,
Supp(ρo)
⊆
Supp(ρe),
there
exists
pn
∈
∆n
such
that
Ei∼pn
(1 −γ) P∞
t=0 γt1

si
t, ai
t = (s, a)
	
is an unbiased estimator of dπ
ρ∗
s(s, a).
The result
follows by the law of large numbers.
Theorem 5. Let ALG-RL be an approximate best response player that solves the RL problem in
iteration k to accuracy ϵk =
1
√
k. Then, Algorithm 1 will converge to an ϵ-optimal solution to
Problem (P2) in O
  1
ϵ4

samples.
Proof. We begin by showing that h(P) = minx∈∆n Df(P||Ex[Qx]) is convex in P. We can write
Df in its variational form, rewriting h(P) as
h(P) = min
x∈∆n max
g:Z7→R Ez∼P [g(z)] −Ex,z∼Qx[f ∗(g(z))],
where
f ∗(w) = sup
y {yw −f(y)}.
We have that Ez∼P [g(z)] −Ex,z∼Qx[f ∗(g(z))] is afﬁne in g and x. Therefore, strong duality holds,
yielding
h(P) = max
g:Z7→R min
x∈∆n Ez∼P [g(z)] −Ex,z∼Qx[f ∗(g(z))]
= max
g:Z7→R

Ez∼P [g(z)] +

max
x∈∆n Ex,z∼Qx[f ∗(g(z))]

We have that maxx∈∆n Ex,z∼Qx[f ∗(g(z))] is convex in g as a maximum over convex (afﬁne) func-
tions in a compact set. Therefore h(P) is also convex as a maximum over convex functions.
Then, the objective in Problem (P2) is convex in dπ
ρo. Following the meta algorithm framework
for convex RL in Zahavy et al. (2021), we write the gradient of Df(dπ
ρo(s, a)||dπ∗
ρe (s, a)). Notice
that for any general f-divergence Df(xi||yi) = Eyi
h
f

xi
yi
i
it holds that
∇xjDf(xi||yi) = 0, j ̸= i,
27
Published as a conference paper at ICLR 2022
and
∇xiDf(xi||yi) = ∇xiEyi

f
xi
yi

= Eyi
 1
yi
∇zf(z) |z= xi
yi

.
Speciﬁcally, for the KL-divergence, DKL(pi||qi) = −Eqi
h
log

pi
qi
i
. Then,
∇piDKL(pi||qi) = Eqi
 1
pi

.
Applying Lemma 2 of Zahavy et al. (2021) with a Follow the Leader (FTL) cost player completes
the proof.
F.3
PROOFS OF ADDITIONAL RESULTS IN APPENDIX
Proposition 3. Assume ρe ≡ρo and |Υπ∗| < ∞. Then there exists λ∗> 0 such that for any
λ ∈(0, λ∗), Algorithm 3 (with δ = 0 sensitivity) will return ¯
π of Proposition 2 after exactly |Υπ∗|
iterations.
Proof. Denote
λ∗
1 =
max
π∈Πdet,π̸∈Υπ∗,π′∈Υπ∗dT V

dπ
ρo(s, a, x), dπ′
ρo(s, a, x)

,
λ∗
2 =
min
π∈Πdet,π̸∈Υπ∗dT V (dπ
ρo(s, a), dπ∗
ρe (s, a)),
where dT V is the total variation distance. Let λ∗= λ∗
2
λ∗
1 and λ ∈(0, λ∗) and notice that λ∗> 0.
To prove the result., we will show that at iteration n of the algorithm πn ∈Υπ∗and that either
πn /
∈Υn−1 := {πj}n−1
j=1 or Υn−1 = Υπ∗.
Base case (n = 1). By the variational representation of the f-divergence,
max
g0:S×A7→R Es,a∼dπ
ρo(s,a)[g0(s, a)] −Es,a∼dπ∗
ρe (s,a)[f ∗(g0(s, a))] = dT V (dπ
ρo(s, a), dπ∗
ρe (s, a)).
By deﬁnition Υπ∗= arg minπ∈Πdet dT V (dπ
ρo(s, a)||dπ∗
ρe (s, a)). Then, π1 ∈Υπ∗. Finally since
Υ0 = ∅, we have that π1 /
∈Υ0.
Induction step. Suppose the claim holds for some n = k. We will show it holds for n = k + 1.
We begin by showing that πk+1 ∈Υπ∗. Assume by contradiction that πk+1 ∈Πdet, πk+1 /
∈Υπ∗.
Using the variational form of the f-divergence,
max
gi:S×A×X Li(πk+1; gi) = dT V (dπk+1
ρo
(s, a, x), dπi
ρo(s, a, x)) ≤λ∗
1,
max
g0:S×A L∗(πk+1; g0) = dT V (dπk+1
ρo
(s, a), dπ∗
ρe (s, a)) ≥λ∗
2.
We have that
max
g0:S×A7→R,
gi:S×A×X7→R
L∗(πk+1; g0) −λ min
i
Li(πk+1; gi) ≥λ∗
2 −λλ∗
1 > λ∗
2 −λ∗λ∗
1 = 0.
Next, let ˜
πk+1 ∈Υπ∗, then
max
g0:S×A7→R,
gi:S×A×X7→R
L∗(˜
πk+1; g0) −λ min
i
Li(˜
πk+1; gi) ≤0,
where we used the fact that L∗(˜
πk+1; g0) = 0 by deﬁnition of Υπ∗, and Li ≥0. We have reached a
contradiction to πk+1 being a solution to Equation (4). This proves that πk+1 ∈Υπ∗.
28
Published as a conference paper at ICLR 2022
Finally, we show that πk+1 /
∈Υk if and only if Υk ̸= Υπ∗. First, notice that if Υk = Υπ∗
then Equation (4) will return πk+1 ∈Υk by deﬁnition of the total variation distance. Next, as-
sume Υk ̸= Υπ∗and assume by contradiction πk+1 ∈Υk. Then, ∃i : maxgi Li(πk+1; gi) = 0, and
maxg0:S×A7→R L∗(πk+1; g0) = 0, by deﬁnition of Υπ∗. Hence,
max
g0:S×A7→R,
gi:S×A×X7→R
L∗(πk+1; g0) −λ min
i
Li(πk+1; gi) = 0.
In contrast, since Υk ̸= Υπ∗, there exists ˜
π ∈Υπ∗such that ˜
π /
∈Υk, and
max
g0:S×A7→R,
gi:S×A×X7→R
L∗(˜
π; g0) −λ min
i
Li(˜
π; gi) ≤λ∗
1 < 0,
in contradiction to πk+1 being a solution Equation (4). This completes the proof.
Theorem 6. [Sufﬁciency of ΥΓ−1
π∗] Let Assumption 2 hold for some Γ ≥1. Then π∗∈ΥΓ−1
π∗.
Proof. Let π ∈Π. We will show that π ∈ΥΓ−1
π
. By elementary algebra, we have that, under
Assumption 2,
ρo(x)(1 −Γ−1) + Γ−1 ≤ρo(x)
ρe(x) ≤ρo(x)(1 −Γ) + Γ.
Since Supp(ρe) ⊆Supp(ρo),
dπ
ρo(s, a) = Ex∼ρo(x)[dπ(s, a | x)]
= Ex∼ρe(x)
ρo(x)
ρe(x)dπ(s, a | x)

≤Ex∼ρe(x)[(ρo(x)(1 −Γ) + Γ)dπ(s, a | x)].
Subtracting dπ
ρe from both sides we get that
dπ
ρo(s, a) −dπ
ρe(s, a) ≤Ex∼ρe(x)[(ρo(x)(1 −Γ) + Γ −1)dπ(s, a | x)]
= (Γ −1)Ex∼ρe(x)[(1 −ρo(x))dπ(s, a | x)]
≤Γ −1.
Similarly,
dπ
ρo ≥Ex∼ρe(x)
 ρo(x)(1 −Γ−1) + Γ−1
dπ(s, a | x)

.
Hence,
dπ
ρo(s, a) −dπ
ρe(s, a) ≥Ex∼ρe(x)
 ρo(x)(1 −Γ−1) + Γ−1 −1

dπ(s, a | x)

=
 Γ−1 −1

Ex∼ρe(x)[(1 −ρo(x))dπ(s, a | x)]
≥−(1 −Γ−1)
≥−(Γ −1)
where the last two transitions hold since Γ ≥1. Then, we have that

dπ
ρo(s, a) −dπ
ρe(s, a)

 ≤Γ −1.
This completes the proof.
Theorem 7. [Context Dependent Reward]
Let ϵ
:
X
7→
R of Deﬁnition 6.
Denote
ϵoe = Ex∼ρo(x)[ϵ(x)] + Ex∼ρe(x)[ϵ(x)]. Then for any π∗∈Π∗
M, π0 ∈Υπ∗
v(π0) ≥v(π∗) −ϵoe.
29
Published as a conference paper at ICLR 2022
Proof. Let π∗∈Π∗
M, π0 ∈Υπ∗. The proof follows similar steps to that of Theorem 3.
v(π0) = Ex∼ρo(x),s,a∼dπ0(s,a|x)[r(s, a, x)]
= Ex∼ρo(x),s,a∼dπ0(s,a|x)[r(s, a, x) −r0(s, a)] + Ex∼ρo(x),s,a∼dπ0(s,a|x)[r0(s, a)]
= Ex∼ρo(x),s,a∼dπ0(s,a|x)[r(s, a, x) −r0(s, a)] + Ex∼ρo(x)


X
s∈S,a∈A
dπ0(s, a | x)r0(s, a)


= Ex∼ρo(x),s,a∼dπ0(s,a|x)[r(s, a, x) −r0(s, a)] +
X
s∈S,a∈A
r0(s, a)Ex∼ρo(x)[dπ0(s, a | x)]
= Ex∼ρo(x),s,a∼dπ0(s,a|x)[r(s, a, x) −r0(s, a)] + Es,a∼dπ0
ρo (s,a)[r0(s, a)]
= Ex∼ρo(x),s,a∼dπ0(s,a|x)[r(s, a, x) −r0(s, a)] + Es,a∼dπ∗
ρe (s,a)[r0(s, a)]
= Ex∼ρo(x),s,a∼dπ0(s,a|x)[r(s, a, x) −r0(s, a)] + Ex∼ρe(x),s,a∼dπ∗(s,a|x)[r0(s, a)].
Then,
v(π∗) −v(π0) = Ex∼ρe(x),s,a∼dπ∗(s,a|x)[r(s, a, x) −r0(s, a)] −Ex∼ρo(x),s,a∼dπ0(s,a|x)[r(s, a, x) −r0(s, a)]
≤Ex∼ρe(x)[ϵ(x)] + Ex∼ρo(x)[ϵ(x)],
completing the proof.
30
