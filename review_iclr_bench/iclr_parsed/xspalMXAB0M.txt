# A BOOSTING APPROACH TO REINFORCEMENT LEARNING

**Anonymous authors**
Paper under double-blind review

ABSTRACT

We study efficient algorithms for reinforcement learning in Markov decision processes, whose complexity is independent of the number of states. This formulation
succinctly captures large scale problems, but is also known to be computationally
hard in its general form. Previous approaches attempt to circumvent the computational hardness by assuming structure in either transition function or the value
function, or by relaxing the solution guarantee to a local optimality condition.
We consider the methodology of boosting, borrowed from supervised learning, for
converting weak learners into an effective policy. The notion of weak learning we
study is that of sampled-based approximate optimization of linear functions over
policies. Under this assumption of weak learnability, we give an efficient algorithm that is capable of improving the accuracy of such weak learning methods
iteratively. We prove sample complexity and running time bounds on our method,
that are polynomial in the natural parameters of the problem: approximation guarantee, discount factor, distribution mismatch and number of actions. In particular,
our bound does not explicitly depend on the number of states.
A technical difficulty in applying previous boosting results, is that the value function over policy space is not convex. We show how to use a non-convex variant of
the Frank-Wolfe method, coupled with recent advances in gradient boosting that
allow incorporating a weak learner with multiplicative approximation guarantee,
to overcome the non-convexity and attain global optimality guarantees.

1 INTRODUCTION

The field of reinforcement learning, formally modelled as learning in Markov decision processes
(MDP), models the mechanism of learning from rewards, as opposed to examples. Although the
case of tabular MDPs is well understood, the main difficulty in applying RL to practice is the size
of the state space.

Various techniques have been suggested and applied to cope with very large MDPs. The most
common of which is function approximation of either the value or the transition function of the
underlying MDP, many times using deep neural networks. Training deep neural networks in the
supervised learning model is known to be computationally hard. Therefore reinforcement learning
with neural function approximation is also computationally hard in general, and for this reason lacks
provable guarantees.

This challenge of finding efficient and provable algorithms for MDPs with large state space is the
focus of our study. Previous approaches can be categorized in terms of the structural assumptions
made on the MDP to circumvent the computational hardness. Some studies focus on structured
dynamics, whereas others on structured value function or policy classes w.r.t. to the dynamics.

In this paper we study another methodology to derive provable algorithms for reinforcement learning: ensemble methods for aggregating weak or approximate algorithms into substantially more
accurate solutions. Our method can be thought of as extending the methodology of boosting from
supervised learning (Schapire & Freund, 2012) to reinforcement learning. Interestingly, however,
our resulting aggregation of weak learners is not linear.

In order to circumvent the computational hardness of solving general MDPs with function approximation, we assumes access to a weak learner: an efficient sample-based procedure that is capable


-----

of generating an approximate solution to any linear optimization objective over the space of policies. We describe an algorithm that iteratively calls this procedure on carefully constructed new
objectives, and aggregates the solution into a single policy. We prove that after sufficiently many
iterations, our resulting policy is provably near-optimal.

1.1 CHALLENGES AND TECHNIQUES

Reinforcement learning is quite different from supervised learning and several difficulties have to be
circumvented for boosting to work. Amongst the challenges that the reinforcement learning setting
presents, consider the following,

(a) The value function is not a convex or concave function of the policy. This is true even in
the tabular case, and even more so if we use a parameterized policy class.

(b) The transition matrix is unknown, or prohibitively large to manipulate for large state spaces.
This means that even evaluation of a policy cannot be exact, and can only be computed
approximately.

(c) It is unrealistic to expect a weak learner that attains near-optimal value for a given linear
objective over the policy class. At most one can hope for a multiplicative and/or additive
approximation of the overall value.

Our approach overcomes these challenges by applied several new as well as recently developed
techniques. To overcome the nonconvexity of the value function, we use a novel variant of the
Frank-Wolfe optimization algorithm that simultaneously delivers on two guarantees. First, it finds a
first order stationary point with near-optimal rate. Secondly, if the objective happens to admit a certain gradient domination property, an important generalization of convexity, it also guarantees near
optimal value. The application of the nonconvex Frank-Wolfe method is justified due to previous
recent investigation of the policy gradient algorithm (Agarwal et al., 2019; 2020a), which identified
conditions under which the value function is gradient dominated.

The second information-theoretic challenge of the unknown transition function is overcome by careful algorithmic design: our boosting algorithm requires only samples of the transitions and rewards.
These are obtained by rollouts on the MDP.

The third challenge is perhaps the most difficult to overcome. Thus far, the use of the Frank-Wolfe
method in reinforcement learning did not include a multiplicative approximation, which is critical
for our application. Luckily, recent work in the area of online convex optimization (Hazan & Singh,
2021) studies boosting with a multiplicative weak learner. We make critical use of this new technique
which includes a non-linear aggregation (using a 2-layer neural network) of the weak learners. This
aspect is perhaps of general interest to boosting algorithm design, which is mostly based on linear
aggregation.

1.2 OUR CONTRIBUTIONS

Our main contribution is a novel efficient boosting algorithm for reinforcement learning. The input
to this algorithm is a weak learning method capable of approximately optimizing a linear function
over a certain policy class.

The output of the algorithm is a policy which does not belong to the original class considered. It is
rather a non-linear aggregation of policies from the original class, according to a two-layer neural
network. This is a result of the two-tier structure of our algorithm: an outer loop of non-convex
Frank-Wolfe method, and an inner loop of online convex optimization boosting. The final policy
comes with provable guarantees against the class of all possible policies.

Our algorithm and guarantees come in four flavors, depending on the mode of accessing the MDP
(two options), and the boosting methodology for the inner online convex optimization problem (two
options).

It is important to point out that we study the question from an optimization perspective, and hence,
assume the availability of an efficient exploration scheme â€“ either via access to a reset distribution
that has some overlap with the state distribution of the optimal policy, or constraining the policy


-----

_âˆž_

|Col1|Supervised weak learner|Online weak learner|Col4|
|---|---|---|---|
|Episodic model|C6 (Î )/Î±4Îµ5 âˆž|C4 (Î )/Î±2Îµ3 âˆž|C = max d dÏ€ Ï€âˆ— âˆž Ï€âˆˆÎ |
|Rollouts w. Î½-resets|6 /Î±4Îµ6 D âˆž|4 /Î±2Îµ4 D âˆž|âˆž dÏ€ Î½âˆ— D = âˆž|



Table 1: Sample complexity of the proposed algorithms for different Î±-weak learning models (supervised & online) and modes of accessing the MDP (rollouts & rollouts with reset distribution Î½),
suppressing polynomial factors in |A|, 1/(1 âˆ’ _Î³). See Theorem 11 for details._

class to policies that explore sufficiently. Such considerations also arise when reducing reinforcement learning to a sequence of supervised learning problems, e.g. Conservative Policy Iteration
(Kakade & Langford, 2002) assumes the former. One contribution we make here is to quantitatively differentiate between these two modes of exploration in terms of the rates of convergence they
enable for the boosting setting.

1.3 RELATED WORK

To cope with prohibitively large MDPs, the method of choice to approximate the policy and transition space are deep neural networks, dubbed â€œdeep reinforcement learning". Deep RL gave rise to
beyond human performance in games such as Go, protein folding, as well as near-human level autonomous driving. In terms of provable methods for deep RL, there are two main lines of work. The
first is a robust analysis of the policy gradient algorithm (Agarwal et al., 2019; 2020a). Importantly,
the gradient domination property of the value function established in this work is needed in order to
achieve global convergence guarantees of our boosting method.

The other line of work for provable approaches is policy iteration, which uses a restricted policy
class, making incremental updates, such as Conservative Policy Iteration (CPI) (Kakade & Langford,
2002; Scherrer & Geist, 2014), and Policy Search by Dynamic Programming (PSDP)(Bagnell et al.,
2003).

Our boosting approach for provable deep RL builds on the vast literature of boosting for supervised
learning (Schapire & Freund, 2012), and recently online learning (Leistner et al., 2009; Chen et al.,
2012; 2014; Beygelzimer et al., 2015; Jung et al., 2017; Jung & Tewari, 2018). One of the crucial
techniques important for our application is the extension of boosting to the online convex optimization setting, with bandit information (Brukhim & Hazan, 2021), and critically with a multiplicative
weak learner (Hazan & Singh, 2021). This latter technique implies a non-linear aggregation of the
weak learners. Non-linear boosting was only recently investigated in the context of classification
(Alon et al., 2020), where it was shown to potentially enable significantly more efficient boosting.

Perhaps the closest work to ours is boosting in the context of control of dynamical systems (Agarwal et al., 2020b). However, this work critically requires knowledge of the underlying dynamics
(transitions), which we do not, and cannot cope with a multiplicative approximate weak learner.

The Frank-Wolfe algorithm is extensively used in machine learning, see e.g. (Jaggi, 2013), references therein, and recent progress in stochastic Frank-Wolfe methods (Hassani et al., 2017; Mokhtari
et al., 2018; Chen et al., 2018; Xie et al., 2019). Recent literature has applied a variant of this algorithm to reinforcement learning in the context of state space exploration (Hazan et al., 2019).

2 PRELIMINARIES

**Optimization.** We say that a differentiable function f : K 7â†’ R over some domain K is L-smooth
with respect to some norm if for every x, y we have
_âˆ¥Â· âˆ¥âˆ—_ _âˆˆK_

_f_ (y) _f_ (x) _f_ (x)[âŠ¤](y _x)_
_âˆ’_ _âˆ’âˆ‡_ _âˆ’_ _â‰¤_ _[L]2_ _âˆ—[.]_

_[âˆ¥][x][ âˆ’]_ _[y][âˆ¥][2]_


-----

For constrained optimization (such as over âˆ†A), the projection Î“ : R[|][A][|] _â†’_ âˆ†A of a point x to onto
a domain âˆ†A is
Î“[x] = arg minyâˆˆâˆ†A _âˆ¥x âˆ’_ _yâˆ¥._

An important generalization of convex function we use henceforth is that of gradient domination,
**Definition 1 (Gradient Domination). A function f : K â†’** R is said to be (Îº, Ï„, K1, K2)-locally
gradient dominated (around K1 by K2) if for all x âˆˆK1, it holds that

max _f_ (x)[âŠ¤](y _x)_ + Ï„.
_y_ _[f]_ [(][y][)][ âˆ’] _[f]_ [(][x][)][ â‰¤] _[Îº][ Ã—][ max]y_ 2 _âˆ‡_ _âˆ’_
_âˆˆK_ _âˆˆK_



**Markov decision process.** An infinite-horizon discounted Markov Decision Process (MDP) M =
(S, A, P, r, Î³, d0) is specified by: a state space S, an action space A, a transition model P where
_P_ (s[â€²]|s, a) denotes the probability of immediately transitioning to state s[â€²] upon taking action a at
state s, a reward function r : S Ã— A â†’ [0, 1] where r(s, a) is the immediate reward associated with
taking action a at state s, a discount factor Î³ âˆˆ [0, 1); a starting state distribution d0 over S. For any
infinite-length state-action sequence (hereafter, called a trajectory), we assign the following value


_Î³[t]r(st, at)._
_t=0_

X


_V (Ï„ = (s0, a0, s1, a1, . . . )) =_


The agent interacts with the MDP through the choice of stochastic policy Ï€ : S â†’ âˆ†A it executes, where âˆ†A denotes the probability simplex over A. The execution of such a policy induces a
distribution over trajectories Ï„ = (s0, a0, . . . ) as


(P (st+1 _st, at)Ï€(at_ _st))._ (1)
_|_ _|_
_t=0_

Y


_P_ (Ï„ _Ï€) = d0(s0)_
_|_


Using this description we can associate a state V _[Ï€](s) and state-action Q[Ï€](s, a) value function with_
any policy Ï€. For an arbitrary distrbution d over S, define:

_âˆž_

_Q[Ï€](s) = E_ _Î³[t]r(st, at)_ _Ï€, s0 = s, a0 = a_ _,_

" _t=0_ #
X

_V_ _[Ï€](s) = Eaâˆ¼Ï€(Â·|s) [Q[Ï€](s, a)|Ï€, s],_ _Vd[Ï€]_ [=][ E][s]0[âˆ¼][d] [[][V][ Ï€][(][s][)][|][Ï€][]][ .]
Here the expectation is with respect to the randomness of the trajectory induced by Ï€ in M . When
convenient, we shall use V _[Ï€]_ to denote Vd[Ï€]0 [, and][ V][ âˆ—] [to denote][ max][Ï€][ V][ Ï€][.]

Similarly, to any policy Ï€, one may ascribe a (discounted) state-visitation distribution d[Ï€] = d[Ï€]d0 [.]

_âˆž_

_d[Ï€]d_ [(][s][) = (1][ âˆ’] _[Î³][)]_ _t=0_ _Î³[t][ X]Ï„_ :st=s _P_ (Ï„ _|Ï€, s0 âˆ¼_ _d)_

X

**Modes of Accessing the MDP.** We henceforth consider two modes of accessing the MDP, that are
standard in the reinforcement learning literature, and provide different results for each.

The first natural access model is called the episodic rollout setting. This mode of interaction allows
us to execute a policy, stop and restart at any point, and do this multiple times.

Another interaction model we consider is called rollout with Î½-restarts. This is similar to the
episodic setting, but here the agent may draw from the MDP a trajectory seeded with an initial state
distribution Î½ = d0. This interaction model was considered in prior work on policy optimization
_Ì¸_
Kakade & Langford (2002); Agarwal et al. (2019). The motivation for this model is two-fold:
first, Î½ can be used to incorporate priors (or domain knowledge) about the state coverage of the
optimal policy; second, Î½ provides a mechanism to incorporate exploration into policy optimization
procedures.

3 SETTING: POLICY AGGREGATION AND WEAK LEARNING

Our boosting algorithms henceforth call upon weak learners to generate weak policies, and aggregate these policies in a way that guarantees eventual convergence to optimality. In this section we
formalize both components.


-----

**A Policy Tree**
_Ï€Â¯ âˆˆ_ Î (Î , N, T )

_Ï€Â¯_

_w1[â€²]_ _w2[â€²]_

Î“[Î»2] Î“[Î»2]

_Î»1_ _Î»2_

**A Shrub**

_w1_ _Î»t âˆˆ_ Î›(Î , N ) _w6_

_Ï€1_ _Ï€2_ _Ï€3_ _Ï€4_ _Ï€5_ _Ï€6_


Figure 1: The figure illustrates a Policy Tree hierarchy (see Definition 5), obtained by setting N = 3
on the inner loop, andthe lower level. The middle level holds T = 2 on the outer loop, to overall get all base policies T = 2 Policy Shurbs (see Definition 4), where each Shrub Ï€1, ..., Ï€6 âˆˆ Î W on
_projectedÎ»t âˆˆ_ Î›(Î  shrubs, N ) is an aggregation of base policies. The top level is an weighted aggregation of the Î“[Î»t], which forms the overall Policy Tree Â¯Ï€ âˆˆ Î (Î , N, T ).

3.1 POLICY AGGREGATION

For a base class of policies Î W, our algorithm incrementally builds a more expressive policy class
by aggregating base policies via both linear combinations and non-linear transformations. In effect,
the algorithm produces a finite-width depth-2 circuit over some subset of the base policy class. We
start with the simpler linear aggregation.

**Definition 2 (Function Aggregation). Given some N0 âˆˆ** Z+, w âˆˆ R[N][0], (f1, . . . fN0 ) âˆˆ (S â†’
R[|][A][|])[âŠ—][N][0], we define f = _n=1_ _[w][n][f][n][ to be the unique function][ f][ :][ S][ â†’]_ [R][A][ for which simultane-]
ously for all s âˆˆ _S, it holds_ _N0_

[P][N][0]

_f_ (s) = _wnf_ (s).

_n=1_

X

Next, the projection operation below may be viewed as a non-linear activation, such as ReLU, in
deep learning terms. Note that the projection of any function from S to R[|][A][|] produces a policy, i.e.
a mapping from states to distributions over actions.

**Definition 3 (Policy Projection). Given a function f : S â†’** R[|][A][|], define a projected policy Ï€ = Î“[f ]
to be a policy such that simultaneously for all s âˆˆ _S, it holds that Ï€(Â·|s) = Î“ [f_ (s)] .

The next definition defines the class of functions represented by circuits of depth 1 over a base
policy class. Note that these function do not necessarily represent policies since they take an affine
(vs. convex) combination of policies.

**Definition 4 (Shrub). For an arbitrary base policy class Î  âŠ†** _S â†’_ âˆ†A, define Î›(Î , N ) to be a set
such that Î» âˆˆ Î›(Î , N ) if and only if there exists N0 â‰¤ _N, w âˆˆ_ R[N][0] _, (Ï€1, . . . Ï€N0_ ) âˆˆ Î [âŠ—][N][0] such
that Î» = _n=1_ _[w][n][Ï€][n][.]_

The final definition describes the set of possible outputs of the boosting procedure.

[P][N][0]

**Definition 5 (Policy Tree). For an arbitrary base policy class Î  âŠ†** _S â†’_ âˆ†A, define Î (Î , N, T ) to be
a policy class such that Ï€ Î (Î , N, T ) if and only if there exists T0 _T, w_ âˆ†T0 _, (Î»1, . . . Î»T0_ )
_âˆˆ_ _â‰¤_ _âˆˆ_ _âˆˆ_
Î›(Î , N )[âŠ—][T][0] such that Ï€ = _t=1_ _[w][t][Î“[][Î»][t][]][.]_

It is important that the policy that the boosting algorithm outputs can be evaluated efficiently. In the

[P][T][0]

appendix we show it is indeed the case (see Lemma 15).


-----

3.2 MODELS OF WEAK LEARNING

We consider two types of weak learners, and give different end results based on the different assumptions: weak supervised and weak online learners. In the discussion below, let Ï€r be a uniformly
random policy, i.e. (s, a) _S_ _A, Ï€r(a_ _s) = 1/_ _A_ .
_âˆ€_ _âˆˆ_ _Ã—_ _|_ _|_ _|_

**Supervised Learning.** The natural way to define weak learning is an algorithm whose performance is always slight better than that of random policy, one that chooses an action uniformly at
random at any given state. However, in general no learner can outperform a random learner over
all label distributions (this is called the â€œno free lunch" theorem). This motivates the literature on
agnostic boosting (Kanade & Kalai, 2009; Brukhim et al., 2020; Hazan & Singh, 2021) that defines
a weak learner as one that can approximate the best policy in a given policy class.
**Definition 6 (Weak Supervised Learner). Let Î± âˆˆ** (0, 1). Consider a class L of linear loss functions
_â„“_ : R[A] _â†’_ R, and D a family of distributions that are supported over S Ã— L, policy classes Î W, Î .
A weak supervised learning algorithm, for every Îµ, Î´ > 0, given m(Îµ, Î´) samples Dm from any
distribution D âˆˆ D outputs a policy W(Dm) âˆˆ Î W such that with probability 1 âˆ’ _Î´,_

E(s,â„“) _â„“(_ (Dm)) _Î± max_ _â„“(Ï€[âˆ—](s))_ + (1 _Î±)E(s,â„“)_ _â„“(Ï€r(s))_ _Îµ._
_âˆ¼D_ _W_ _â‰¥_ _Ï€[âˆ—]âˆˆÎ _ [E][(][s,â„“][)][âˆ¼D] _âˆ’_ _âˆ¼D_ _âˆ’_
     

Note that the weak learner outputs a policy in Î W which is approximately competitive against
the class Î . As an additional relaxation, instead of requiring that the weak learning guarantee
holds for all distributions, in our setup, it will be sufficient that the weak learning assumption holds
over natural distributions. We define these below. Hereafter, we refer to Î (Î W, N, T ) as Î  for
_N, T = O(poly(|A|, (1 âˆ’_ _Î³)[âˆ’][1], Îµ[âˆ’][1], Î±[âˆ’][1], log Î´[âˆ’][1])) specified later._
**Assumption 1 (Weak Supervised Learning). The booster has access to a weak supervised learning**
_oracle (Definition 6) over the policy class Î , for some Î± âˆˆ_ (0, 1). Furthermore, the weak learning
_condition holds only for a class of natural distributions D â€“ D âˆˆ_ D if and only if there exists some
_Ï€ âˆˆ_ Î  such that

_S(s) =_ (s, â„“)dÂµ(â„“) = d[Ï€](s).
_D_ _â„“_ _D_
Z

In particular, while a natural distribution may have arbitrary distribution over labels, its marginal
distribution over states must be realizable as the state distribution of some policy in Î  over the MDP
_M_ . Therefore, the complexity of weak learning adapts to the complexity of the MDP itself. As an
extreme example, in stochastic contextual bandits where policies do not affect the distribution of
states (say d0), it is sufficient that the weak learning condition holds with respect to all couplings of
a single distribution d0.

**Online Learning.** The second model of weak learning we consider requires a stronger assumption,
but will give us better sample and oracle complexity bounds henceforth.
**Definition 7 (Weak Online Learner). Let Î± âˆˆ** (0, 1). Consider a class L of linear loss functions
_â„“_ : R[A] _â†’_ R. A weak online learning algorithm, for every M > 0, incrementally for each timestep
computes a policy Wm âˆˆ Î W and then observes the state-loss pair (s, â„“t) âˆˆ _S Ã— L such that_


_â„“m(Ï€[âˆ—](sm)) + (1_ _Î±)_
_âˆ’_
_m=1_

X


_â„“m(_ _m(sm))_ _Î± max_
_W_ _â‰¥_ _Ï€[âˆ—]_ Î 
_m=1_ _âˆˆ_

X


_â„“m(Ï€r(sm)) âˆ’_ _RW_ (M ).
_m=1_

X


**Assumption 2 (Weak Online Learning). The booster has access to a weak online learning oracle**
_(Definition 7) over the policy class Î , for some Î± âˆˆ_ (0, 1).
**Remark 8. A similar remark about natural distributions applies to the online weak learner. In par-**
ticular, it is sufficient the guarantee in 7 holds for arbitrary sequence of loss functions with high
probability over the sampling of the state from d[Ï€] for some Ï€ âˆˆ Î . Although stronger than supervised weak learning, this oracle can be interpreted as a relaxation of the online weak learning
oracle considered in (Brukhim et al., 2020; Brukhim & Hazan, 2021; Hazan & Singh, 2021). A similar model of hybrid adversarial-stochastic online learning was considered in (Rakhlin et al., 2011;
Lazaric & Munos, 2009; Beygelzimer et al., 2011). In particular, it is known (Lazaric & Munos,
2009) that unlike online learning, the capacity of a hypothesis class for this model is governed by its
VC dimension (vs. Littlestone dimension).


-----

4 ALGORITHM & MAIN RESULTS

In this section we describe our RL boosting algorithm. Here we focus on the case where a supervised
weak learning is provided. The online weak learners variant of our result is detailed in the appendix.
We next define several definitions and algorithmic subroutines required for our method.

**The Extension Operator.** The extension operator (Hazan & Singh, 2021) operate overs functions
and modifies their value outside and near the boundary of the convex set âˆ†A to aid the boosting
algorithm.

_FG,Î²[f_ ](x) = min _f_ (y) + G min
_yâˆˆâˆ†A_  _zâˆˆâˆ†A_ _[âˆ¥][y][ âˆ’]_ _[z][âˆ¥]_ [+ 1]2Î² _[âˆ¥][x][ âˆ’]_ _[y][âˆ¥][2]_

To state the results, we need the following definitions. The first generalizes the policy completeness
notion from (Scherrer & Geist, 2014). It may be seen as the policy-equivalent analogue of inherent
bellman error (Munos & SzepesvÃ¡ri, 2008). Intuitively, it measures the degree to which a policy in Î 
can best approximate the bellman operator in an average sense with respect to the state distribution
induced by a policy from Î .
**Definition 9 (Policy Completeness). For any initial state distribution Âµ, define**


_EÂµ(Î , Î ) = maxÏ€âˆˆÎ _ _Ï€[min][âˆ—]âˆˆÎ _ [E][s][âˆ¼][d]Âµ[Ï€]


max
_a_ _A_ _[Q][Ï€][(][s, a][)][ âˆ’]_ _[Q][Ï€][(][s,][ Â·][)][âŠ¤][Ï€][âˆ—][(][Â·|][s][)]_
_âˆˆ_


The following notion of the distribution mismatch coefficient is often useful to characterize the
exploration problem faced by policy optimization algorithms.
**Definition 10 (Distribution Mismatch). Let Ï€[âˆ—]** = arg maxÏ€ V _[Ï€], and Î½ a fixed initial state distribu-_
tion (see section 2). Define the following distribution mismatch coefficients:[1]


_d[Ï€][âˆ—]_

_d[Ï€]_


_d[Ï€][âˆ—]_
_D_ =
_âˆž_ _Î½_


_C_ (Î ) = max
_âˆž_ _Ï€âˆˆÎ _


4.1 RL BOOSTING VIA WEAK SUPERVISED LEARNING

We give the main RL boosting algorithm, assuming supervised weak learners. We use a simple
sub-routine for choosing a step size, provided in the appendix.

**Algorithm 1 RL Boosting via Weak Supervised Learning**

2:1: for Input parameters t = 1 to T do T, N, M, P, Âµ. Initialize a policy Ï€0 âˆˆ Î W arbitrarily.
3: Set Ït,0 to be an arbitrary policy in Î W .

4: **for n = 1 to N do**

5: Execute Ï€t 1 for M episodes with initial state distribution Âµ via Algorithm 2, to get
_âˆ’_


6: Modify Dt,n to produce a new datasetDt,n D =t,n[â€²] {(s[=]i,[ {]Q[c][(][s]i)[i][m]i[, f]=1[i][)][}][}][.] _i[m]=1[, such that for all][ i][ âˆˆ]_ [[][m][]][:]


_fi = âˆ’âˆ‡FG,Î²[âˆ’Q[b]i](Ït,n(Â·|si))_

.

7: Let At,n be the policy chosen by the weak learning oracle when given data set 1 _Dt,n[â€²]_ [.]

8:9: **end forUpdate Ït,n = (1 âˆ’** _Î·2,n)Ït,nâˆ’1 +_ _[Î·][2]Î±[,n]_ _[A][t,n][ âˆ’]_ _[Î·][2][,n]_ _Î±_ _[âˆ’]_ [1] _Ï€r._

10: Declare Ï€t[â€²] [= Î“ [][Ï][t,N] []][.]   

11: Choose Î·1,t = min{1, [2][C]t[âˆž] _} if Âµ = d0 else Î·1,t = StepChooser(Ï€tâˆ’1, Ï€t[â€²][, Âµ, P]_ [)][.]

12: Update Ï€t = (1 _Î·1,t)Ï€t_ 1 + Î·1,tÏ€t[â€²][.]
_âˆ’_ _âˆ’_

13: end for
14: Output Â¯Ï€=Ï€T if Âµ = d0 else output Ï€t 1 with the smallest Î·t.
_âˆ’_

1For brevity, We use the shorthand C where clear from context.
_âˆž_


-----

**Theorem 11. Algorithm 1 samples T** (MN + P ) episodes of length _O[Ëœ](_ 1 1 _Î³_ [)][ with probability][ 1][ âˆ’] _[Î´][.]_

2 _âˆ’_

_In the episodic model, for T = O_ (1 _CÎ³âˆž[2])[3]Îµ_ _, N =_ (116|AÎ³|)C[2]Î±Ïµâˆž _, M = m_ (1Câˆ’Î³)A[2]Î±Îµ _[,]_ _NTÎ´_ _,Âµ = d0,_

_âˆ’_ _âˆ’_ _âˆž|_ _|_

_P = 0, with probability 1_ _Î´,_      
_âˆ’_

(Î , Î )
_V_ _V_ _[Ï€]_ _C_ _E_ + Îµ.

_[âˆ—]_ _âˆ’_ _â‰¤_ _âˆž_ 1 _Î³_

_âˆ’_

2

_In the Î½-reset model, for T_ = (18DÎ³âˆž)[2][6]Îµ[2][,][ N] = (116|AÎ³|)D[3]Î±Ïµâˆž _, P_ = _OËœ(_ [200](1 _[|][A]Î³[|])[2][6][D]Îµ[2]âˆž[2][ )][,][ M]_ =

_âˆ’_ _âˆ’_ _âˆ’_

_m_ (18|âˆ’AÎ³|D)[3]âˆžÎ±Îµ _[,]_ 2NTÎ´ _,Âµ = Î½, with probability 1 âˆ’_ _Î´,_  
 

_Î½(Î , Î )_
_V_ _V_ _[Ï€]_ _D_ _E_

_[âˆ—]_ _âˆ’_ _â‰¤_ _âˆž_ (1 _Î³)[2][ +][ Îµ.]_

_âˆ’_


4.2 TRAJECTORY SAMPLER

In Algorithm 2 we describe an episodic sampling procedure, that is used in our sample-based RL
boosting algorithms described above. For a fixed initial state distribution Âµ, and any given policy
_Ï€, we apply the following sampling procedure: start at an initial state s0_ _Âµ, and continue to_
act thereafter in the MDP according to any policy Ï€, until termination. With this process, it is âˆ¼
straightforward to both sample from the state visitation distribution s âˆ¼ _d[Ï€], and to obtain unbiased_
samples of Q[Ï€](s, Â·); see Algorithm 2 for the detailed process.

**Algorithm 2 Trajectory Sampler: s** _d[Ï€], unbiased estimate of Q[Ï€]s_
_âˆ¼_

1: Sample state s0 _Âµ, and action a[â€²]_ (A) uniformly.
2: Sample s âˆ¼ _d[Ï€] âˆ¼as follows: at every timestepâˆ¼U_ _h, with probability Î³, act according to Ï€; else,_
accept sh as the sample and proceed to Step 3.

3: Take action a[â€²] at state sh, then continue to execute Ï€, and use a termination probability of 1 _Î³._
_âˆ’_
Upon termination, set R(sh, a[â€²]) as the undiscounted sum of rewards from time h onwards.

4: Define the vector _Q[Ï€]sh_ [, such that for all][ a][ âˆˆ] _[A][,][ d]Q[Ï€]sh_ [(][a][) =][ |][A][| Â·][ R][(][s][h][, a][â€²][)][ Â·][ I][a][=][a][â€²] [.]

5: return (sh, _Q[Ï€]sh_ [)][.]

[d]

[d]

5 ANALYSIS â€“ PROOF SKETCH

We sketch the high-level ideas of the proof of our main result, stated in Theorem 11, and refer the
reader to the appendix for the formal proof. Throughout the analysis, we use the notation _Ï€[V][ Ï€][ to]_
_âˆ‡_
denote the gradient of the value function with respect to the |S| Ã— |A|-sized representation of the
policy Ï€, namely the functional gradient of V _[Ï€]._

We establish an equivalence between the outlined algorithm and an abstraction of the Frank-Wolfe
algorithm (Algorithm D) from optimization theory. This variant of the Frank-Wolfe (FW) algorithm operates over non-convex and gradient dominated functions to obtain the following novel
convergence guarantees. We establish the necessary gradient domination results from the policy
completeness results.
**Theorem 12. Let f :** R be L-smooth in some norm _, H-bounded, and the diameter of_
_K â†’_ _âˆ¥Â· âˆ¥âˆ—_ _K_
_in âˆ¥Â· âˆ¥âˆ—_ _be D. Then, for a (Ïµ0, K2)-linear optimization oracle, the output Â¯x of Algorithm D satisfies_

2HLD[2]

max _x)[âŠ¤](u_ _xÂ¯)_ + 3Ïµ + Ïµ0.
_u_ 2 _âˆ’_ _â‰¤_ _T_
_âˆˆK_ r

_[âˆ‡][f]_ [(Â¯]

_Furthermore, if f is (Îº, Ï„,_ 1, 2)-locally gradient-dominated and x0, . . . xT 1, then it holds
_K_ _K_ _âˆˆK_

max _x)_ + Ï„ + ÎºÏµ0.
_x[âˆ—]_ _[f]_ [(][x][âˆ—][)][ âˆ’] _[f]_ [(Â¯] _â‰¤_ [2][Îº][2][ max][{]T[LD][2][, H][}]
_âˆˆK_

The Frank-Wolfe algorithm utilizes an inner gradient optimization oracle as a subroutine. To implement this oracle using approximate optimizers, we utilize yet another variant of the FW method as
â€œinternal-boostingâ€ for the weak learners (by employing an adapted analysis of Theorem 13).


-----

5.1 INTERNAL-BOOSTING WEAK LEARNERS

We utilize a variant of the Frank-Wolfe method as a form â€œinternal-boostingâ€ for the weak learners,
by employing an adapted analysis of previous work that is stated below.

Note that _Q[Ï€](s, Â·) produced by Algorithm 2 satisfies âˆ¥Q[b][Ï€](s, Â·)âˆ¥_ = 1|âˆ’AÎ³| [. We can now borrow the]

following result on boosting for statistical learning from (Hazan & Singh, 2021), specializing the
decision set to be[b] âˆ†A. Let _t be the distribution induced by the trajectory sampler in round t._
_D_

**Theorem 13 ((Hazan & Singh, 2021)). Let Î² =** _Î±N1_ _[, and][ Î·][2][,n][ = min][{][ 2]n_ _[,][ 1][}][. Then, for any][ t][,][ Ï€]t[â€²]_

_produced by Algorithm 1 satisfies with probabilityq 1_ _Î´ that_
_âˆ’_

2 _A_ 2
max _Q[âŠ¤]Ï€(s)_ E(s,Q) _t_ _Q[âŠ¤]Ï€t[â€²][(][s][)]_ _|_ _|_ + Îµ
_Ï€âˆˆÎ _ [E][(][s,Q][)][âˆ¼D][t] _âˆ’_ _âˆ¼D_ _â‰¤_ (1 âˆ’ _Î³)Î±_  _âˆšN_ 
   

5.2 FROM WEAK LEARNING TO LINEAR OPTIMIZATION

In the following Lemma, we give an important observation which allows us to re-state the guarantee
in the previous subsection in terms of linear optimization over functional gradients.
**Lemma 14. Applying Algorithm 2 for any given policy Ï€, yields an unbiased estimate of the gradi-**
_ent, such that for any Ï€[â€²],_

1
(âˆ‡Ï€[V][ Ï€]Âµ [)][âŠ¤][Ï€][â€²][ =] 1 âˆ’ _Î³_ [E][(][s,][ c]Q[Ï€](s,Â·))âˆ¼DhQ[Ï€](s, Â·)[âŠ¤]Ï€[â€²](Â·|s)i, (2)

_where Ï€[â€²](_ _s)_ âˆ†A, and _is the distribution induced on the outputs of Algorithm 2, for a givenc_

_Â·|_ _âˆˆ_ _D_
_policy Ï€ and initial state distribution Âµ._

_Proof. Recall âˆ‡Ï€[V][ Ï€][ denotes the gradient with respect to the][ |][S][| Ã— |][A][|][-sized representation of the]_
policy Ï€ â€“ the functional gradient. Then, using the policy gradient theorem (Williams, 1992; Sutton
et al., 2000), it is given by,
_âˆ‚VÂµ[Ï€]_ 1 _Âµ[(][s][)][Q][Ï€][(][s, a][)][.]_ (3)

_âˆ‚Ï€(a_ _s) [=]_ 1 _Î³ [d][Ï€]_
_|_ _âˆ’_

The following sources of randomness are at play in the sampling algorithm (Algorithm 2): the
distribution d[Ï€] (which encompasses the discount-factor-based random termination, the transition
probability, and the stochasticity of Ï€), and the uniform sampling over A. For a fixed s, Ï€, denote by
_Qs[Ï€]_ [as the distribution over][ c]Q[Ï€](s, Â·) âˆˆ R[A], induced by all the aforementioned randomness sources.
To conclude the claim, observe that by construction

E _Ï€(s,_ )[Q[Ï€](s, ) _Ï€, s] = Q[Ï€](s,_ ). (4)
_Q_ _Â·_ [c] _Â·_ _|_ _Â·_

6 CONCLUSIONS

Building on recent advances in boosting for online convex optimization and bandits, we have described a boosting algorithm for reinforcement learning over large state spaces with provable guarantees. We see this as a first attempt at using a tried-and-tested methodology from supervised learning
in RL, and many challenges remain.

First and foremost, our notion of weak learner optimizes a linear function over policy space. A more
natural weak learner would be an RL agent with multiplicative optimality guarantee, and it would
be interesting to extend our methodology to this notion of weak learnability.

Another important aspect that is not discussed in our paper is that of state-space exploration. Potentially boosting can be combined with state-space exploration techniques to give stronger guarantees
independent of distribution mismatch C _, D_ factors.
_âˆž_ _âˆž_

Finally, a feature of our method is that it produces nonlinear aggregations of weak learners as per a
two layer neural network. Are simpler aggregations with provable guarantees possible?


-----

REFERENCES

Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. arXiv preprint arXiv:1908.00261,
2019.

Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. Pc-pg: Policy cover directed exploration for provable policy gradient learning. arXiv preprint arXiv:2007.08459, 2020a.

Naman Agarwal, Nataly Brukhim, Elad Hazan, and Zhou Lu. Boosting for control of dynamical
systems. In International Conference on Machine Learning, pp. 96â€“103. PMLR, 2020b.

Noga Alon, Alon Gonen, Elad Hazan, and Shay Moran. Boosting simple learners. arXiv preprint
_arXiv:2001.11704, 2020._

J Andrew Bagnell, Sham Kakade, Andrew Y Ng, and Jeff G Schneider. Policy search by dynamic
programming. In Advances in Neural Information Processing Systems, 2003.

Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit
algorithms with supervised learning guarantees. In Proceedings of the Fourteenth International
_Conference on Artificial Intelligence and Statistics, pp. 19â€“26. JMLR Workshop and Conference_
Proceedings, 2011.

Alina Beygelzimer, Satyen Kale, and Haipeng Luo. Optimal and adaptive algorithms for online
boosting. In International Conference on Machine Learning, pp. 2323â€“2331, 2015.

Nataly Brukhim and Elad Hazan. Online boosting with bandit feedback. In Algorithmic Learning
_Theory, pp. 397â€“420. PMLR, 2021._

Nataly Brukhim, Xinyi Chen, Elad Hazan, and Shay Moran. Online agnostic boosting via regret
minimization. In Advances in Neural Information Processing Systems, 2020.

Lin Chen, Christopher Harshaw, Hamed Hassani, and Amin Karbasi. Projection-free online optimization with stochastic gradient: From convexity to submodularity. In International Conference
_on Machine Learning, pp. 814â€“823, 2018._

Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu. An online boosting algorithm with theoretical
justifications. In Proceedings of the 29th International Coference on International Conference on
_Machine Learning, pp. 1873â€“1880, 2012._

Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu. Boosting with online binary learners for the
multiclass bandit problem. In International Conference on Machine Learning, pp. 342â€“350, 2014.

John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto
the l 1-ball for learning in high dimensions. In Proceedings of the 25th international conference
_on Machine learning, pp. 272â€“279, 2008._

Hamed Hassani, Mahdi Soltanolkotabi, and Amin Karbasi. Gradient methods for submodular maximization. In Advances in Neural Information Processing Systems, pp. 5841â€“5851, 2017.

Elad Hazan. Introduction to online convex optimization. arXiv preprint arXiv:1909.05207, 2019.

Elad Hazan and Karan Singh. Boosting for online convex optimization. _arXiv preprint_
_arXiv:2102.09305, 2021._

Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum entropy
exploration. In International Conference on Machine Learning, pp. 2681â€“2691. PMLR, 2019.

Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In International
_Conference on Machine Learning, pp. 427â€“435. PMLR, 2013._

Young Hun Jung and Ambuj Tewari. Online boosting algorithms for multi-label ranking. In Inter_national Conference on Artificial Intelligence and Statistics, pp. 279â€“287, 2018._


-----

Young Hun Jung, Jack Goetz, and Ambuj Tewari. Online multiclass boosting. In Advances in neural
_information processing systems, pp. 919â€“928, 2017._

Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
_In Proc. 19th International Conference on Machine Learning. Citeseer, 2002._

Varun Kanade and Adam Kalai. Potential-based agnostic boosting. In Advances in neural informa_tion processing systems, pp. 880â€“888, 2009._

Alessandro Lazaric and RÃ©mi Munos. Hybrid stochastic-adversarial on-line learning. In Conference
_on Learning Theory, 2009._

Christian Leistner, Amir Saffari, Peter M Roth, and Horst Bischof. On robustness of on-line
boosting-a competitive study. In IEEE 12th International Conference on Computer Vision Work_shops, ICCV Workshops, pp. 1362â€“1369. IEEE, 2009._

Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Stochastic conditional gradient methods:
From convex minimization to submodular maximization. arXiv preprint arXiv:1804.09554, 2018.

RÃ©mi Munos and Csaba SzepesvÃ¡ri. Finite-time bounds for fitted value iteration. Journal of Machine
_Learning Research, 9(5), 2008._

Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Stochastic and constrained adversaries. arXiv preprint arXiv:1104.5070, 2011.

Robert E Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012.

Bruno Scherrer and Matthieu Geist. Local policy search in a convex space and conservative policy iteration as boosted policy search. In Joint European Conference on Machine Learning and
_Knowledge Discovery in Databases, pp. 35â€“50. Springer, 2014._

Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen,
and K. MÃ¼ller (eds.), Advances in Neural Information Processing Systems, volume 12.
MIT Press, 2000. [URL https://proceedings.neurips.cc/paper/1999/file/](https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)
[464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf.](https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229â€“256, 1992.

Jiahao Xie, Zebang Shen, Chao Zhang, Hui Qian, and Boyu Wang. Stochastic recursive gradientbased methods for projection-free online learning. arXiv preprint arXiv:1910.09396, 2019.


-----

A APPENDIX

It is important that the policy that the boosting algorithm outputs can be evaluated efficiently. Towards that end, we give the following claim.

**Claim 15. For any Ï€ âˆˆ** Î (Î , N, T ), Ï€(Â·|s) for any s âˆˆ _S can be evaluated using TN base policy_
_evaluations and O(T Ã— (NA + A log A)) arithmetic and logical operations._

_Proof. Since Ï€ âˆˆ_ Î (Î , N, T ), it is composed of TN base policies. Producing each aggregated
function takes NA additions and multiplications; there are T of these. Each projection takes time
equivalent to sorting |A| numbers, due to a water-filling algorithm (Duchi et al., 2008); these are
also T in number. The final linear transformation takes an additional TA operations.

B STEP-SIZE SUBROUTINE

Below we give an algorithm for choosing step sizes used in both of the RL boosting methods (for
online, and supervised, weak learners).

**Algorithm 3 StepChooser(Ï€t** 1, Ï€t[â€²][, Âµ, P] [)]
_âˆ’_

1: Execute Ï€t 1 for P episodes with initial state distribution Âµ via Algorithm 2, to get
_âˆ’_


_D = {(si,_ _Qi)[P]i=1[}][.]_

2: For any policy Ï€, let _G[Ï€]_ = _P[1]_ _Pp=1_ _QiâŠ¤Ï€(_ _si).[c]_

_Â·|_

3: Return

2

P (1 _Î³)_

[c]Î·1,t = clip[0[c],1] _âˆ’_ _G[Ï€]t[â€²]_ _G[Ï€][t][âˆ’][1]_

2 _âˆ’_ [\]

 d []

C RL BOOSTING VIA WEAK ONLINE LEARNING

**Algorithm 4 RL Boosting via Weak Online Learning**


1:2: Initialize a policy for t = 1 to T do Ï€0 âˆˆ Î W arbitrarily.
3: Initialize online weak learners 1, . . . .
_W_ _W_ _[N]_

4: **for m = 1 to M do**

5: Execute Ï€t 1 once with initial state distribution Âµ via Algorithm 2, to get (st,m, _Qt,m)._
_âˆ’_

7:6: Choosefor n = 1 Ït,m, to N0 âˆˆ doÎ W arbitrarily.

1 [b]

9:8: **end forSet Ït,m,n = (1 âˆ’** _Î·2,n)Ït,m,nâˆ’1 +_ _[Î·][2]Î±[,n]_ _[W]_ _[n][ âˆ’]_ _[Î·][2][,n]_ _Î±_ _[âˆ’]_ [1] _Ï€r._

10: Pass to each the following loss linear ft,m,n:   
_W_ _[n]_

_ft,m,n = âˆ’âˆ‡FG,Î²[âˆ’Q[b]t,m](Ït,m,n(Â·|si))_

11: **end for**

12: Declare Ï€t[â€²] [=] _M1_ _Mm=1_ [Î“ [][Ï][t,m,N] []][.]

13: Choose Î·1,t = minP{1, [2][C][âˆž]t [(][Î ][)] _} if Âµ = d0 else set Î·1,t = StepChooser(Ï€tâˆ’1, Ï€t[â€²][, Âµ, P]_ [)][.]

14: Update Ï€t = (1 _Î·1,t)Ï€t_ 1 + Î·1,tÏ€t[â€²][.]
_âˆ’_ _âˆ’_

15: end for
16: Output Â¯Ï€= Ï€T if Âµ = d0 else output Ï€t 1 with the smallest Î·t.
_âˆ’_


**Theorem 16. Algorithm 4 samples T** (M + _P_ ) episodes of length 1 1 _Î³_ [log][ T][ (][M]Î´[+][P][ )] _with probability_

_âˆ’_ 2

1âˆ’Î´. In the episodic model, Algorithm 4 guarantees as long as T = [16](1[C]âˆ’âˆž[2]Î³)[(][3][Î ]Îµ[)] _[,][ N][ =]_ 16(1|Aâˆ’|Î³C)âˆž[2]Î±Ïµ(Î ) _,_

 


-----

_M = max_ 1000(1 _|AÎ³|)[2][4]CÎµ[2]âˆž[2]Î±[(][2][Î ][)]_ log[2] _TÎ´,_ [8][|][A][|][C](1[âˆž][(]Î³[Î ])[)][2][R]Î±Îµ[W] [(][M] [)] _,Âµ = d0, we have with probability 1_ _Î´_

_âˆ’_ _âˆ’_ _âˆ’_

n o

_V_ _[âˆ—]_ _âˆ’_ _V_ _[Ï€]_ _â‰¤_ _Câˆž(Î )_ _[E]1[(][Î ][,][ Î )]Î³_ + Îµ

_âˆ’_ 2

_In the Î½-reset model, Algorithm 1 guarantees as long as T =_ (1100âˆ’Î³D)[6]âˆž[2]Îµ[2][,][ N][ =] (120âˆ’|AÎ³|)D[3]Î±Ïµâˆž _,_

2

_P =_ [250](1 _[D]Î³âˆž[2])[6][|][A]Îµ[2][|][ log][2]_ [2][ T]Î´ _[,][ M][ = max]_ (140|AÎ³|)D[3]Î±Îµâˆž [log][ T]Î´ _,_ [10][|][A](1[|][D][âˆž]Î³)[R][3]Î±Îµ[W] [(][M] [)] _,Âµ = Î½, we have with_ 

_âˆ’_ _âˆ’_ _âˆ’_

_probability 1 âˆ’_ _Î´_   

_Î½(Î , Î )_
_V_ _V_ _[Ï€]_ _D_ _E_

_[âˆ—]_ _âˆ’_ _â‰¤_ _âˆž_ (1 _Î³)[2][ +][ Îµ]_

_âˆ’_

_If R_ (M ) = _M log_ _for some measure of weak learning complexity_ _, the algorithm_
_W_ _C[4]_ _|W|_ _D[4]_ _|W|_

_samples_ _O[Ëœ]_ _âˆžp(1[(][Î ][)][|]Î³[A])[|][7][2]Î±[ log][2]Îµ[3][ |W|]_ _episodes in the episodic model, and_ _O[Ëœ]_ (1âˆž[|][A]Î³[|])[2][12][ log]Î±[2][ |W|]Îµ[4] _in the Î½-_

_âˆ’_ _âˆ’_

_reset model._   

D NON-CONVEX FRANK-WOLFE

In this section, we give an abstract high-level procedural template that the previously introduced RL
boosters operate in. This is based on a variant of the Frank-Wolfe optimization technique, adapted
to non-convex and gradient dominated function classes (see Definition 1).

The Frank-Wolfe (FW) method assumes oracle access to a black-box linear optimizer, denoted O,
and utilizes it by iteratively making oracle calls with modified objectives, in order to solve the
harder task of convex optimization. Analogously, boosting algorithms often assume oracle access
to a â€weakâ€ learner, which are utilized by iteratively making oracle calls with modified objective,
in order to obtain a â€strongâ€ learner, with boosted performance. In the RL setting, the objective is
in fact non-convex, but exhibits gradient domination. By adapting Frank-Wolfe technique to this
setting, we will in subsequent section obtain guarantees for the algorithms given in Section 4.

**Setting.** Denote by O a black-box oracle to an (Ïµ0, K2)-approximate linear optimizer over a convex set K âŠ† R[d] such that for any given v âˆˆ R[d], we have

_v[âŠ¤]_ (v) max
_O_ _â‰¥_ _u_ 2 _[v][âŠ¤][u][ âˆ’]_ _[Ïµ][0][.]_
_âˆˆK_

**Algorithm 5 Non-convex Frank-Wolfe**

1: Input: T > 0, objective f, linear optimization oracle O
2: Choose x0 arbitrarily.
3: for t = 1, . . ., T do
4: Call zt = ( _t_ 1[)][, where][ âˆ‡]t 1 [=][ âˆ‡][f] [(][x]t 1[)][.]
_O_ _âˆ‡_ _âˆ’_ _âˆ’_ _âˆ’_

5: Choose Î·t = min 1, [2]t[Îº]
_{_ _[}][ in the gradient-dominated case, else choose][ Î·][t][ so that]_

_|LD[2]Î·t âˆ’âˆ‡âŠ¤tâˆ’1[(][z][t]_ _[âˆ’]_ _[x][t][âˆ’][1][)][| â‰¤]_ _[Ïµ.]_


6: Set xt = (1 _Î·t)xt_ 1 + Î·tzt.
_âˆ’_ _âˆ’_

7: end for
8: return Â¯x = xT in the gradient-dominated case, else xt 1 with the smallest Î·t.
_âˆ’_

**Theorem 17. Let f :** R be L-smooth in some norm _, H-bounded, and the diameter of_
_K â†’_ _âˆ¥Â· âˆ¥âˆ—_ _K_
_in âˆ¥Â· âˆ¥âˆ—_ _be D. Then, for a (Ïµ0, K2)-linear optimization oracle, the output Â¯x of Algorithm D satisfies_

2HLD[2]

max _x)[âŠ¤](u_ _xÂ¯)_ + 3Ïµ + Ïµ0.
_u_ 2 _âˆ’_ _â‰¤_ _T_
_âˆˆK_ r

_[âˆ‡][f]_ [(Â¯]

_Furthermore, if f is (Îº, Ï„,_ 1, 2)-locally gradient-dominated and x0, . . . xT 1, then it holds
_K_ _K_ _âˆˆK_

max _x)_ + Ï„ + ÎºÏµ0.
_x[âˆ—]_ _[f]_ [(][x][âˆ—][)][ âˆ’] _[f]_ [(Â¯] _â‰¤_ [2][Îº][2][ max][{]T[LD][2][, H][}]
_âˆˆK_


-----

E ANALYSIS FOR BOOSTING WITH SUPERVISED LEARNING (PROOF OF
THEOREM 11)

**Theorem (Formal version of Theorem 11). Algorithm 1 samples T** (MN + P ) episodes of length

1âˆ’1 _Î³_ [log][ T][ (][MN]Î´ [+][P][ )] _with probability 12_ _âˆ’_ _Î´. In the episodic model, Algorithm 1 guarantees as long as_

_T =_ [16](1[C]âˆž[2]Î³)[(][3][Î ]Îµ[)] _[,][ N][ =]_ 16(1|A|Î³C)âˆž[2]Î±Ïµ(Î ) _, M = m_ 8(1Câˆ’Î³(Î )[2])Î±ÎµA _NTÎ´_ _,Âµ = d0, we have with probability_

_âˆ’_ _âˆ’_ _âˆž_ _|_ _|_ _[,]_

1 _Î´_    
_âˆ’_

_V_ _[âˆ—]_ _âˆ’_ _V_ _[Ï€]_ _â‰¤_ _Câˆž(Î )_ _[E]1[(][Î ][,][ Î )]Î³_ + Îµ

_âˆ’_ 2

_In the Î½-reset model, Algorithm 1 guarantees as long as T =_ (1âˆ’8DÎ³âˆž)[2][6]Îµ[2][,][ N][ =] (116âˆ’|AÎ³|)D[3]Î±Ïµâˆž _, P =_

200(1 _|AÎ³|)[2][6]DÎµ[2]âˆž[2][ log][ 2][T N]Î´_ _[,][ M][ =][ m]_ (18âˆ’AÎ³D)[3]Î±Îµ 2NTÎ´ _,Âµ = Î½, we have with probability 1_ _Î´_ 

_âˆ’_ _|_ _|_ _âˆž_ _[,]_ _âˆ’_

 

_Î½(Î , Î )_
_V_ _V_ _[Ï€]_ _D_ _E_

_[âˆ—]_ _âˆ’_ _â‰¤_ _âˆž_ (1 _Î³)[2][ +][ Îµ]_

_âˆ’_

_If m(Îµ, Î´) =_ [log]Îµ[ |W|][2] log [1]Î´ _[for some measure of weak learning complexity][ |W|][, the algorithm samples]_

_OËœ_ _Câˆž[6](1[(][Î ]âˆ’[)]Î³[|][A])[11][|][4]Î±[ log][4]Îµ[ |W|][5]_ _episodes in the episodic model, and_ _O[Ëœ]_ _D(1âˆž[6]_ _âˆ’[|][A]Î³[|])[4][18][ log]Î±[4][ |W|]Îµ[6]_ _in the Î½-reset model._
   

_Proof of Theorem 11. The broad scheme here is to utilize an equivalence between Algorithm 1 and_
Algorithm D on the function V _[Ï€]_ (or VÎ½[Ï€] [in the][ Î½][-reset model), to which Theorem 17 applies.]

To this end, firstly, note V _[Ï€]_ is 1 1 _Î³_ [-bounded. Define a norm][ âˆ¥Â· âˆ¥][âˆž][,][1][ :][ R][|][S][|Ã—|][A][|][ â†’] [R][ as][ âˆ¥][x][âˆ¥][1][,][âˆž] [=]

_âˆ’_

following lemma specifies the smoothness ofmaxsâˆˆS _aâˆˆA_ _[|][x][s,a][|][. Further, observe that for any policy] V_ _[Ï€]_ in this norm.[ Ï€][ :][ S][ â†’] [âˆ†][A][,][ âˆ¥][Ï€][âˆ¥][âˆž][,][1][ = 1][. The]

P

**Lemma 18. V** _[Ï€]_ _is_ (1 2Î³Î³)[3][ -smooth in the][ âˆ¥Â· âˆ¥][âˆž][,][1][ norm.]

_âˆ’_


To be able to interpret Algorithm 1 as an instantiation of the algorithmic template Algorithm D
presents, we advance two claims: one, the step-size choices of the two algorithms conincide; two,
_Ï€t[â€²]_ [(Line 3-10) serves as an approximate linear optimizers for][ âˆ‡][V][ Ï€][t][âˆ’][1] [. Together, these imply that]
the iterates produced by the two algorithms conincide. The first of these, which provides a value of
_Ïµ to use in the statement of Theorem 17, is established below._

**Claim 19. Upon every invocation of StepChooser, the output Î·1,t satisfies with probability 1** _âˆ’_ _Î´_
2Î·1,t 16 _A_

(1 _Î³)[3][ âˆ’]_ [(][âˆ‡][V][ Ï€]Âµ _[t][âˆ’][1]_ )[âŠ¤](Ï€t[â€²] _[âˆ’]_ _[Ï€][t][âˆ’][1][)]_ _[â‰¤]_ (1 _Î³|)[2]|[âˆš]P_ log [1]Î´
_âˆ’_ _âˆ’_

Next, we move onto the linear optimization equivalence. Indeed, Claim 20 demonstrates that Ï€t[â€²]
serves a linear optimizer over gradients of the function V _[Ï€]; the suboptimality specifies Ïµ0._

**Claim 20. Let Î² =** _Î±N1_ _[, and][ Î·][2][,n][ = min][{][ 2]n_ _[,][ 1][}][. Then, for any][ t][,][ Ï€]t[â€²]_ _[produced by Algorithm 1]_

_satisfies with probabilityq_ 1 _Î´_
_âˆ’_

2 _A_ 2
max _Âµ_ )[âŠ¤](Ï€ _Ï€t[â€²][)][ â‰¤]_ _|_ _|_ + ÎµW
_Ï€âˆˆÎ _ [(][âˆ‡][V][ Ï€][t][âˆ’][1] _âˆ’_ (1 âˆ’ _Î³)[2]Î±_  _âˆšN_ 

is the class of all policies,Finally, observe that it is by construction that K1 = Î , K2 = Î . _Ï€t âˆˆ_ Î . Therefore, in terms of the previous section, K

In the episodic model, we wish to invoke the second part of Theorem 17. The next lemma establishes
gradient-domination properties of V _[Ï€]_ to support this.


**Lemma 21. V** _[Ï€]_ _is_ _C_ (Î ),
_âˆž_



1âˆ’1 _Î³_ _[C][âˆž][(][Î ][)][E][(Î ][,][ Î ][)][,][ Î ][,][ Î ]_ _-gradient dominated, i.e. for any Ï€ âˆˆ_ Î :



_V_ _[âˆ—]_ _âˆ’_ _V_ _[Ï€]_ _â‰¤_ _Câˆž(Î )_


1

1 âˆ’ _Î³_ _[E][(Î ][,][ Î ][) + max]Ï€[â€²]âˆˆÎ [(][âˆ‡][V][ Ï€][)][âŠ¤][(][Ï€][â€²][ âˆ’]_ _[Ï€][)]_


-----

Deriving Îº, Ï„ from the above lemma along with Ïµ0 from Claim 20 and Ïµ from Claim 19, as a consequence of the second part of Theorem 17, we have with probability 1 âˆ’ _NTÎ´_


_V_ _[âˆ—]_ _âˆ’_ _V_ _Ï€[Â¯] â‰¤_ _Câˆž(Î )_ _[E]1[(] âˆ’[Î ][,][ Î )]Î³_ + (1[4][C] âˆ’âˆž[2]Î³[(])[Î ][3][)]T [+] (14 âˆ’|A|Î³C)âˆž[2]Î±(âˆšÎ )N


+ [2][|][A][|][C][âˆž][(][Î ][)]

(1 _Î³)[2]Î± [Îµ][W][ .]_
_âˆ’_


Similarly, in the Î½-reset model, the first part of Theorem 17 provides a local-optimality guarantee for
_VÎ½[Ï€][. Lemma 22 provides a bound on the function-value gap (on][ V][ Ï€][) provided such local-optimality]_
conditions.

**Lemma 22. For any Ï€ âˆˆ** Î , we have


_V_ _[âˆ—]_ _âˆ’_ _V_ _[Ï€]_ _â‰¤_


1

_Î½_ [)][âŠ¤][(][Ï€][â€²][ âˆ’] _[Ï€][)]_
1 âˆ’ _Î³_ _[E][Î½][(Î ][,][ Î ][) + max]Ï€[â€²]âˆˆÎ [(][âˆ‡][V][ Ï€]_


1 _Î³ [D][âˆž]_
_âˆ’_


Again, using the bound on maxÏ€â€² Î ( _VÎ½Ï€[Â¯][)][âŠ¤][(][Ï€][â€²][ âˆ’]_ _Ï€[Â¯]) Theorem 17 provides, we have that with_
_âˆˆ_ _âˆ‡_
probability 1 âˆ’ 2NTÎ´

_V_ _V_ _Ï€[Â¯]_ + 2Dâˆž + [2][|][A][|][D][âˆž] 2 + ÎµW + 48|A|Dâˆž log [1]

_[âˆ—]_ _âˆ’_ _â‰¤_ _[D][âˆž](1[E][Î½][(]Î³[Î ])[,][2][ Î )]_ (1 _Î³)[3][âˆš]T_ (1 _Î³)[3]Î±_ _âˆšN_ (1 _Î³)[3][âˆš]P_ _Î´_

_âˆ’_ _âˆ’_ _âˆ’_   _âˆ’_

F ANALYSIS FOR BOOSTING WITH ONLINE LEARNING (PROOF OF
THEOREM 16)

_Proof of Theorem 16. Similar to the proof of Theorem 11, we establish an equivalence between_
Algorithm 1 and Algorithm D on the function V _[Ï€]_ (or VÎ½[Ï€] [in the][ Î½][-reset model), to which Theorem 17]
applies provided smoothness (see Lemma 18).

Indeed, Claim 23 demonstrates Ï€t[â€²] [serves a linear optimizer over gradients of the function][ V][ Ï€][, and]
provides a bound on Ïµ0. Claim 19 ensures that that the step size choices (and hence iterates) of the
two algorithms coincide. As before, observe that it is by construction that Ï€t âˆˆ Î .

**Claim 23. Let Î² =** _Î±N1_ _[, and][ Î·][2][,n][ = min][{][ 2]n_ _[,][ 1][}][. Then, for any][ t][,][ Ï€]t[â€²]_ _[produced by Algorithm 4]_

_satisfies with probabilityq_ 1 _Î´_
_âˆ’_


16 log Î´[âˆ’][1]


2 _A_
max _Âµ_ )[âŠ¤](Ï€ _Ï€t[â€²][)][ â‰¤]_ _|_ _|_
_Ï€_ Î  [(][âˆ‡][V][ Ï€][t][âˆ’][1] _âˆ’_ (1 _Î³)[2]Î±_
_âˆˆ_ _âˆ’_


+ _[R][W]_ [(][M] [)]


In the episodic model, one may combine the second part of Theorem 17, which provides a bound
on function-value gap for gradient dominated functions, which Lemma 21 guarantees, to conclude
with probability 1 âˆ’ _TÎ´_


_V_ _V_ _Ï€[Â¯]_ + [4][C]âˆž[2] [(][Î ][)] 4|A|Câˆž(Î )

_[âˆ—]_ _âˆ’_ _â‰¤_ _[C][âˆž][(][Î ]1 âˆ’[)][E][(]Î³[Î ][,][ Î )]_ (1 âˆ’ _Î³)[3]T_ [+] (1 âˆ’ _Î³)[2]Î±âˆš_


+ [8][|][A][|][C][âˆž][(][Î ][) log][ Î´][âˆ’][1]

(1 âˆ’ _Î³)[2]Î±âˆšM_


_R_ (M )

+ [2][|][A][|][C][âˆž][(][Î ][)] _W_

(1 âˆ’ _Î³)[2]Î±_ _M_


Similarly, in the Î½-reset model, Lemma 22 provides a bound on the function-value gap provided
local-optimality conditions, which the first part of Theorem 17 provides for. Again, with probability
1 âˆ’ _TÎ´_

_V_ _V_ _Ï€[Â¯]_ + 2Dâˆž 1 + 2 + _[R][W]_ [(][M] [)] + [4 log][ Î´][âˆ’][1] + [24][|][A][|] log [1]

_[âˆ—]_ _âˆ’_ _â‰¤_ _[D][âˆž](1[E][Î½][(]Î³[Î ])[,][2][ Î )]_ (1 _Î³)[3]_ _âˆšT_ _[|][A]Î±[|]_ _âˆšN_ _M_ _âˆšM_ _âˆšP_ _Î´_

_âˆ’_ _âˆ’_   


-----

G PROOFS OF SUPPORTING CLAIMS

G.1 NON-CONVEX FRANK-WOLFE METHOD (THEOREM 17)

_Proof of Theorem 17. Non-convex case. Note that for any timestep t, it holds due to smoothness_
that
_f_ (xt) = f (xt 1 + Î·t(zt _xt_ 1))
_âˆ’_ _âˆ’_ _âˆ’_

_L_
_â‰¥_ _f_ (xtâˆ’1) + Î·tâˆ‡âŠ¤tâˆ’1[(][z][t] _[âˆ’]_ _[x][t][âˆ’][1][)][ âˆ’]_ _[Î·]t[2]_ 2 _[D][2]_

= f (xtâˆ’1) âˆ’ 2LD1 [2] _LD[2]Î·t âˆ’âˆ‡âŠ¤tâˆ’1[(][z][t]_ _[âˆ’]_ _[x][t][âˆ’][1][)]_ 2 + (âˆ‡âŠ¤tâˆ’1[(]2[z]LD[t] _[âˆ’]_ [2][x][t][âˆ’][1][))][2]

Using the step-size definition to bound on the middle term, and telescoping this inequality over  
function-value differences across successive iterates, we have


_T_

_t=1(âˆ‡âŠ¤tâˆ’1[(][z][t]_ _[âˆ’]_ _[x][t][âˆ’][1][))][2][ â‰¤]_ [2][LD]T [2][H]

X


min _âŠ¤t_ 1[(][z][t]
_t_ [(][âˆ‡] _âˆ’_ _[âˆ’]_ _[x][t][âˆ’][1][))][2][ â‰¤]_ _T[1]_


+ Ïµ[2]


Let t[â€²] = arg mint Î·t and t[âˆ—] = arg mint( _âŠ¤t_ 1[(][z][t]
_âˆ‡_ _âˆ’_ _[âˆ’]_ _[x][t][âˆ’][1][))][2][. Then]_

_âŠ¤t[â€²]_ 1[(][z][t][â€²][ âˆ’] _[x][t][â€²][âˆ’][1][)][ â‰¤]_ _[LD][2][Î·][t][â€²][ +][ Ïµ][ â‰¤]_ _[LD][2][Î·][t][âˆ—]_ [+][ Ïµ]
_âˆ‡_ _âˆ’_

_âŠ¤t[âˆ—]_ 1[(][z][t][âˆ—] _[âˆ’]_ _[x][t][âˆ—][âˆ’][1][) + 2][Ïµ][ â‰¤]_
_â‰¤âˆ‡_ _âˆ’_


2LD[2]H


+ Ïµ[2] + 2Ïµ


To conclude the claim for the non-convex part, observe _âˆša + b â‰¤_ _[âˆš]a +_ _âˆšb for a, b > 0, and that_

since ztâ€² = O(âˆ‡t[â€²]âˆ’1[)][, it follows by oracle definition that]

max _âŠ¤t[â€²]_ 1[u][ â‰¤âˆ‡]âŠ¤t[â€²] 1[z][t][â€²][ +][ Ïµ][0][.]
_u_ 2 _âˆ’_ _âˆ’_
_âˆˆK_

_[âˆ‡]_

**Gradient-Dominated Case.** Define x[âˆ—] = arg maxx _f_ (x) and ht = f (x[âˆ—]) _f_ (xt).
_âˆˆK_ _âˆ’_


_ht â‰¤_ _htâˆ’1 âˆ’_ _Î·tâˆ‡âŠ¤tâˆ’1[(][z][t]_ _[âˆ’]_ _[x][t][âˆ’][1][) +][ Î·]t[2]_


_L_

smoothness
2 _[D][2]_


_L_
_ht_ 1 _Î·t max_ _âŠ¤t_ 1[(][y][ âˆ’] _[x][t][âˆ’][1][) +][ Î·]t[2]_ oracle
_â‰¤_ _âˆ’_ _âˆ’_ _yâˆˆK2_ _[Î·][t][âˆ‡]_ _âˆ’_ 2 _[D][2][ +][ Î·][t][Ïµ][0]_

_L_

_â‰¤_ _htâˆ’1 âˆ’_ _[Î·]Îº[t]_ [(][f] [(][x][âˆ—][)][ âˆ’] _[f]_ [(][x][t][âˆ’][1][)) +][ Î·]t[2] 2 _[D][2][ +][ Î·][t]_ _Ïµ0 +_ _Îº[Ï„]_ gradient domination

_L_  

= 1 âˆ’ _[Î·]Îº[t]_ _htâˆ’1 + Î·t[2]_ 2 _[D][2][ +][ Î·][t]_ _Ïµ0 +_ _Îº[Ï„]_

The theorem now follows from the following claim.   

**Claim 24. Let C â‰¥** 1. Let gt be a H-bounded positive sequence such that

_gt_ 1 _gt_ 1 + Ïƒt[2][D][ +][ Ïƒ][t][E.]
_â‰¤_ _âˆ’_ _[Ïƒ]C[t]_ _âˆ’_

_Then choosing Ïƒt = min_ 1, [2]t[C]   2C[2] maxt{2D,H} + CE.
_{_ _[}][ implies][ g][t][ â‰¤]_


G.2 SMOOTHNESS OF VALUE FUNCTION (LEMMA 18)

_Proof of Lemma 18. Consider any two policies Ï€, Ï€[â€²]. Using the Performance Difference Lemma_
(Lemma 3.2 in (Agarwal et al., 2019), e.g.) and Equation 2, we have

_|V_ _[Ï€][â€²]_ _âˆ’_ _V_ _[Ï€]_ _âˆ’âˆ‡V_ _[Ï€](Ï€[â€²]_ _âˆ’_ _Ï€)|_

1
= 1 _Î³_ Esâˆ¼dÏ€â€² _Q[Ï€](Â·|s)[âŠ¤](Ï€[â€²](Â·|s) âˆ’_ _Ï€(Â·|s)_ _âˆ’_ Esâˆ¼dÏ€ _Q[Ï€](Â·|s)[âŠ¤](Ï€[â€²](Â·|s) âˆ’_ _Ï€(Â·|s)_

_âˆ’_

1  

[] []

_â‰¤_ (1 _Î³)[2][ âˆ¥][d][Ï€][â€²][ âˆ’]_ _[d][Ï€][âˆ¥][1][âˆ¥][Ï€][â€²][ âˆ’]_ _[Ï€][âˆ¥][âˆž][,][1]_

_âˆ’_


-----

_dThe last inequality uses the fact that[Ï€]_ 1 1 _Î³_ _Î³_ To establish this, consider the Markov operator maxs,a Q[Ï€](s, a) _â‰¤_ 1âˆ’1 _Î³_ [.] It suffices to show P _[Ï€]( âˆ¥s[â€²]_ _ds[Ï€]) =[â€²]_ _âˆ’_
_âˆ¥_ _â‰¤_ _âˆ’_ _[âˆ¥][Ï€][â€²][ âˆ’]_ _[Ï€][âˆ¥][âˆž][,][1][.]_ _|_

_a_ _A_ _[P]_ [(][s][â€²][|][s, a][)][Ï€][(][a][|][s][)][ induced by a policy][ Ï€][ on MDP][ M] [. For any distribution][ d][ supported on]
_âˆˆ_
_S, we have_

P


_âˆ¥(P_ _[Ï€][â€²]_ _âˆ’_ _P_ _[Ï€])dâˆ¥1 =_


_P_ (s[â€²]|s, a)d(s)(Ï€[â€²](a|s) âˆ’ _Ï€(a|s)_
_s,a_

X

_s[â€²]_ _P_ (s[â€²]|s, a)âˆ¥dâˆ¥1âˆ¥Ï€[â€²] _âˆ’_ _Ï€âˆ¥âˆž,1 â‰¤âˆ¥Ï€[â€²]_ _âˆ’_ _Ï€âˆ¥âˆž,1_

X


_s[â€²]_


Using sub-additivity of the l1 norm and applying the above observation t times, we have for any t


_âˆ¥((P_ _[Ï€][â€²]_ )[t] _âˆ’_ (P _[Ï€])[t])dâˆ¥1 â‰¤_ _tâˆ¥Ï€[â€²]_ _âˆ’_ _Ï€âˆ¥âˆž,1._


Finally, observe that


_d[Ï€][â€²]_ _d[Ï€]_ 1 (1 _Î³)_
_âˆ¥_ _âˆ’_ _âˆ¥_ _â‰¤_ _âˆ’_


_âˆž_

_Î³[t]_ ((P _[Ï€][â€²]_ )[t] (P _[Ï€])[t])d0_ 1
_âˆ¥_ _âˆ’_ _âˆ¥_
_t=0_

X


_â‰¤âˆ¥Ï€[â€²]_ _âˆ’_ _Ï€âˆ¥âˆž,1(1 âˆ’_ _Î³)_

G.3 STEP-SIZE GUARANTEE (CLAIM 19)


_âˆž_

_tÎ³[t]_ =

_t=0_

X


_Î³_

1 âˆ’ _Î³_ _[âˆ¥][Ï€][â€²][ âˆ’]_ _[Ï€][âˆ¥][âˆž][,][1]_


_Proof of Claim 19. Let D be the distribution induced by Algorithm 2 upon being given Ï€tâˆ’1. Due_
to Lemma 14, it suffices to demonstrate that for any Ï€ âˆˆ{Ï€t[â€²][, Ï€][t][âˆ’][1][}][ the following claim holds with]
probability 1 2 [. The claim in turn follows from Hoeffdingâ€™s inequality, while noting][ \]Q[Ï€][t][âˆ’][1] (s, )

is 1|AÎ³| [-bounded in the] âˆ’ _[Î´]_ _[ l][âˆž]_ [norm.] _Â·_

_âˆ’_
_G[Ï€]_ _âˆ’_ E(s, \Q[Ï€t][âˆ’][1] (s,Â·))âˆ¼DhQ\[Ï€][t][âˆ’][1] (s, Â·)[âŠ¤]Ï€(Â·|s)i â‰¤ (1 âˆ’8|Î³A)|âˆšP log 2[1]Î´

[c]

G.4 GRADIENT DOMINATION (LEMMA 21 AND LEMMA 22)

_Proof of Lemma 21. Invoking Lemma 4.1 from (Agarwal et al., 2019) with Âµ = d0, we have_


_d[Ï€][âˆ—]_
_V_ _[âˆ—]_ _âˆ’_ _V_ _[Ï€]_ _â‰¤_ _d[Ï€]_ maxÏ€0 [(][âˆ‡][V][ Ï€][)][âŠ¤][(][Ï€][0][ âˆ’] _[Ï€][)]_

_âˆž_

_C_ (Î )(max
_â‰¤_ _âˆž_ _Ï€0_ [(][âˆ‡][V][ Ï€][)][âŠ¤][Ï€][0][ âˆ’] _Ï€[max][â€²]_ Î [(][âˆ‡][V][ Ï€][)][âŠ¤][Ï€][â€²][ + max]Ï€[â€²] Î [(][âˆ‡][V][ Ï€][)][âŠ¤][(][Ï€][â€²][ âˆ’] _[Ï€][))]_
_âˆˆ_ _âˆˆ_

Finally, with the aid of Equation 2, observe that


1
max max _Q[Ï€](s, a)_ _Q[Ï€](_ _s)[âŠ¤]Ï€[â€²][i]_
_Ï€0_ [(][âˆ‡][V][ Ï€][)][âŠ¤][Ï€][0][ âˆ’] _Ï€[max][â€²]âˆˆÎ [(][âˆ‡][V][ Ï€][)][âŠ¤][Ï€][â€²][ = min]Ï€[â€²]âˆˆÎ _ 1 âˆ’ _Î³_ [E][s][âˆ¼][d][Ï€] h _a_ _âˆ’_ _Â·|_

1
_â‰¤_ 1 _Î³_

_âˆ’_ _[E][(Î ][,][ Î ][)]_

_Proof of Lemma 22. Invoking Lemma 4.1 from (Agarwal et al., 2019) with Âµ = Î½, we have_


1 _d[Ï€][âˆ—]_

1 _Î³_ _Î½_ maxÏ€0 [(][âˆ‡][V][ Ï€]Î½ [)][âŠ¤][(][Ï€][0] _[âˆ’]_ _[Ï€][)]_
_âˆ’_ _âˆž_

1

1 âˆ’ _Î³ [D][âˆž][(max]Ï€0_ [(][âˆ‡][V][ Ï€]Î½ [)][âŠ¤][Ï€][0] _[âˆ’]_ _Ï€[max][â€²]âˆˆÎ [(][âˆ‡][V][ Ï€]Î½_ [)][âŠ¤][Ï€][â€²][ + max]Ï€[â€²]âˆˆÎ [(][âˆ‡][V][ Ï€]Î½ [)][âŠ¤][(][Ï€][â€²][ âˆ’] _[Ï€][))]_


_V_ _[âˆ—]_ _âˆ’_ _V_ _[Ï€]_ _â‰¤_


-----

Again, with the aid of Equation 2, observe that

1
maxÏ€0 [(][âˆ‡][V][ Ï€]Î½ [)][âŠ¤][Ï€][0] _[âˆ’]_ _Ï€[max][â€²]âˆˆÎ [(][âˆ‡][V][ Ï€]Î½_ [)][âŠ¤][Ï€][â€²][ = min]Ï€[â€²]âˆˆÎ  1 âˆ’ _Î³_ [E][s][âˆ¼][d]Î½[Ï€] hmaxa _Q[Ï€](s, a) âˆ’_ _Q[Ï€](Â·|s)[âŠ¤]Ï€[â€²][i]_

1
_â‰¤_ 1 _Î³_

_âˆ’_ _[E][Î½][(Î ][,][ Î ][)]_

G.5 SUPERVISED LINEAR OPTIMIZATION GUARANTEES (CLAIM 20)

_Proof of Claim 20. The subroutine presented in lines 3-10 (which culminate in Ï€t[â€²][) is an instantiation]_
of Algorithm 3 from (Hazan & Singh, 2021), specializing the decision set to be âˆ†A. To note the
equivalence, note that in (Hazan & Singh, 2021) the algorithm is stated assuming that the center-ofmass of the decision set is at the origin (after a coordinate transform); correspondingly, the update
rule in Algorithm 1 can be written as


(Ït,n âˆ’ _Ï€r) = (1 âˆ’_ _Î·2,n)(Ït,nâˆ’1 âˆ’_ _Ï€r) +_ _[Î·][2]Î±[,n]_ [(][A][t,n][ âˆ’] _[Ï€][r][)][.]_

For any state s, Ï€r(Â·|s) = _A1_ **[1][|][A][|][ corresponds to the center-of-masss of][ âˆ†][A][. Finally, note that]**

maximizing f _[âŠ¤]x over x âˆˆK is equivalent to minimizing (âˆ’f_ )[âŠ¤]x over the same domain. Therefore,
we can borrow the following result on boosting for statistical learning from (Hazan & Singh, 2021)
(Theorem 13). Note that _Q[Ï€](s, Â·) produced by Algorithm 2 satisfies âˆ¥Q[c][Ï€](s, Â·)âˆ¥_ = 1|âˆ’AÎ³| [. Let][ D][t][ be]

the distribution induced by the trajectory sampler in round t.

[c]

**Theorem 25 ((Hazan & Singh, 2021)). Let Î² =** _Î±N1_ _[, and][ Î·][2][,n][ = min][{][ 2]n_ _[,][ 1][}][. Then, for any][ t][,][ Ï€]t[â€²]_

_produced by Algorithm 1 satisfies with probabilityq 1_ _Î´ that_
_âˆ’_

2 _A_ 2
max _Q[âŠ¤]Ï€(s)_ E(s,Q) _t_ _Q[âŠ¤]Ï€t[â€²][(][s][)]_ _|_ _|_ + Îµ
_Ï€âˆˆÎ _ [E][(][s,Q][)][âˆ¼D][t] _âˆ’_ _âˆ¼D_ _â‰¤_ (1 âˆ’ _Î³)Î±_  _âˆšN_ 
   

Lemma 14 allows us to restate the guarantees in the previous subsection in terms of linear optimization over functional gradients. The conclusion thus follows immediately by combining Lemma 14
and Theorem 25.

G.6 ONLINE LINEAR OPTIMIZATION GUARANTEES (CLAIM 23)

_Proof of Claim 23. In a similar vein to the proof of Claim 20, here we state the a result on boosting_
for online convex optimization (OCO) from (Hazan & Singh, 2021) (Theorem 6), the counterpart of
Theorem 13 for the online weak learning case.


**Theorem 26 ((Hazan & Singh, 2021)). Let Î² =** _Î±N1_ _[, and][ Î·][2][,n][ = min][{][ 2]n_ _[,][ 1][}][. Then, for any][ t][,]_

Î“[Ït,m,N ] produced by Algorithm 4 satisfies q


2M
_âˆšN_



2 _A_
_QË†[âŠ¤]t,m[Î“[][Ï][t,m,N]_ [](][s][t,m][)] _|_ _|_
_â‰¤_ (1 _Î³)Î±_
i _âˆ’_


_QË†[âŠ¤]t,m[Ï€][(][s][t,m][)]_


max
_Ï€âˆˆÎ _


+ R (M )
_W_


_m=1_


_m=1_


Next we invoke online-to-batch conversions. Note that in Algorithm 4, (st,m, _Q[Ë†]t,m) for any fixed_
_t is sampled i.i.d. from the same distribution. Therefore, we can apply online-to-batch results, i.e._
Theorem 9.5 in (Hazan, 2019), on Theorem 26 to get


2 _A_
max _Q[âŠ¤]Ï€(s)_ E(s,Q) _t_ _Q[âŠ¤]Ï€t[â€²][(][s][)]_ _|_ _|_
_Ï€âˆˆÎ _ [E][(][s,Q][)][âˆ¼D][t] _âˆ’_ _âˆ¼D_ _â‰¤_ (1 âˆ’ _Î³)Î±_
   

We finally invoke Lemma 14.


16 log Î´[âˆ’][1]


+ _[R][W]_ [(][M] [)]


-----

G.7 REMAINING PROOFS (CLAIM 24)

_gProof of Claim 24.previous display. Now, assumet â‰¤_ _H â‰¤_ 2Ct[2]H Let. For T t[âˆ—] â‰¥= arg max gTt[âˆ—]âˆ’, we proceed by induction. The base case (1 â‰¤ _t{2tC :[2] tmax â‰¤tâˆ’{12D,H2C}}. For any+ CE for some t â‰¤_ _t > TT_ _[âˆ—], we havet =[âˆ—]. T_ _[âˆ—]) is true by the Ïƒt = 1 and_

2C 2 max 2D, H

_gt_ 1 _{_ _}_ + CE + [4][C] [2][D] + [2][CE]
_â‰¤_ _âˆ’_ [2]t _t_ 1 _t[2]_ _t_
   _âˆ’_ 

1
_CE + 2C_ [2] max 2D, H 1 + [1]
_â‰¤_ _{_ _}_ _t_ 1 _âˆ’_ [2]t _t[2]_
 _âˆ’_   

= CE + 2C [2] max 2D, H
_{_ _}_ _[t][2][ âˆ’]t[2][2](t[t][ +][ t]1)[ âˆ’]_ [1]

_âˆ’_

_CE + 2C_ [2] max 2D, H _[t][(][t][ âˆ’]_ [1)]
_â‰¤_ _{_ _}_ _t[2](t_ 1)

_âˆ’_


-----

