# SELF-SUPERVISE, REFINE, REPEAT: IMPROVING UN## SUPERVISED ANOMALY DETECTION

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Anomaly detection (AD) – separating anomalies from normal data – has many applications across domains, from manufacturing to healthcare. While most previous
works have been shown to be effective for cases with fully or partially labeled data,
that setting is in practice less common due to labeling being particularly tedious
for this task. In this paper, we focus on fully unsupervised AD, in which the entire
training dataset, containing both normal and anomalous samples, is unlabeled. To
tackle this problem effectively, we propose to improve the robustness of one-class
classification trained on self-supervised representations using a data refinement
process. Our proposed data refinement approach is based on an ensemble of oneclass classifiers (OCCs), each of which is trained on a disjoint subset of training
data. Representations learned by self-supervised learning on the refined data are
iteratively updated as the refinement improves. We demonstrate our method on
various unsupervised AD tasks with image and tabular data. With a 10% anomaly
ratio on CIFAR-10 image data / 2.5% anomaly ratio on Thyroid tabular data, the
proposed method outperforms the state-of-the-art one-class classification method
by 6.3 AUC and 12.5 average precision / 22.9 F1-score.

1 INTRODUCTION

Anomaly detection (AD), the task of distinguishing anomalies from normal data, plays a crucial role in
many real-world applications such as detecting faulty products using visual sensors in manufacturing,
fraudulent behaviors at credit card transactions, or adversarial outcomes at intensive care units.

AD has been considered under various settings based on the availability of negative (normal) and
positive (anomalous) data and their labels at training, as overviewed in Sec. 2. Each application
scenario is dominated by different challenges. When entire positive and negative data are available
along with their labels (Fig. 1a), the problem can be treated as supervised classification and the
dominant challenge becomes the imbalance in label distributions [4, 16, 20, 27, 32, 35]. When only
negative labeled data are available (Fig. 1b), the problem is ‘one-class classification’ [23, 26, 33,
45, 47, 51, 53]. Various works have also extended approaches designed for these to settings with
additional unlabeled data (Fig. 1c,d,e) [10, 18, 24, 46, 58] in a semi-supervised setting. While there
exist many prior works in these settings, they all depend on some labeled data, which is not desirable
in all application scenarios.

Unsupervised AD, on the other hand, poses unique challenges in the absence of any labeled data
information, and a straightforward adaption of methods developed with the assumption of labeled
data would be suboptimal. For example, some recent studies [7, 60] have applied one-class classifiers
(OCCs) that are known to yield impressive performance when trained on negative samples [7, 23, 26,
33, 51] to unsupervised AD, but their performance for unsupervised AD has been quite sub-optimal.
Fig. 2 illustrates this, showing the unsupervised AD performance of state-of-the-art Deep OCCs [51]
with different anomaly ratios in unlabeled training data – the average precision significantly drops
even when a small portion (2%) of training data is contaminated with anomalies.

Our framework SRR (Self-supervise, Refine, Repeat), overviewed in Fig. 3, brings a novel approach
to unsupervised AD with the principles of self-supervised learning without labels and iterative data
refinement based on the agreement of OCC outputs. We propose to improve the state-of-the-art
performance of OCCs, e.g. [33, 51], by refining the unlabeled training data so as to address the
fundamental challenges elaborated above. SRR iteratively trains deep representations using refined


-----

Negative (N) sample,    Positive (P) sample,   Unlabeled (U) sample

**(a) P+N** **(b) N** **(c) P+N+U** **(d) P+U** **(e) N+U** **(f) U**


Figure 1: AD problem settings. Blue and red dots are for labeled negative (normal) and positive
(anomalous) samples, respectively. Grey dots denote unlabeled samples. While previous works
mostly focus on supervised (a, b) or semi-supervised (c, d, e) settings, we tackle an AD problem
using only unlabeled data (f) that may contain both negative and positive samples.

data while improving the refinement of unlabeled data by excluding potentially-positive (anomalous)
samples. For the data refinement process, we employ an ensemble of OCCs, each of which is trained
on a disjoint subset of unlabeled training data. The samples are declared as normal if there is a
consensus between all the OCCs. The refined training data are used to train the final OCC to generate
the anomaly scores in the unsupervised setting. Most prior unsupervised AD works [23, 26, 33, 51]
assume that the data contains entirely negative samples, which makes them not truly unsupervised
as they require having humans to do the data filtering. Similar to ours, some prior unsupervised
AD works [7, 45, 60] considered evaluating on an unsupervised setting where there exist a small
percentage of anomalous samples in the training data, i.e. operating in ‘truly’ unsupervised setting
without having the need for humans to do any filtering in the training data. However, these methods
often suffered from significant performance degradation as the ratio of anomalous sample ratio has
increased (see Sec. 4.2). We would like to highlight that our method distinguishes from the literature
by bringing a data-centric approach (refining the unlabeled data) to unsupervised anomaly detection
beyond the model-centric approaches (improving the model itself). Our framework SRR aims to
provide robustness in performance as the anomalous sample ratio increases, as shown in Fig. 2.


We conduct extensive experiments across various
datasets from different domains, including semantic
AD (CIFAR-10 [29], Dog-vs-Cat [19]), real-world
manufacturing visual AD use case (MVTec [8]), and
tabular AD benchmarks. We consider methods with
both shallow [36, 47] and deep [7, 33, 51] models.
We evaluate models at different anomaly ratios of
unlabeled training data and show that SRR significantly boosts performance. For example, in Fig. 2,
SRR improves more than 15.0 average precision (AP)
with a 10% anomaly ratio compared to a state-of-theart one-class contrastive representation model [51]
on CIFAR-10. Similarly, on MVTec SRR retains a
strong performance, dropping less than 1.0 AUC with
10% anomaly ratio, while the best existing OCC [33]
drops more than 6.0 AUC. We further investigate the
efficacy of our design choices, such as the number
of ensemble classifiers, thresholds, and refinement of
deep representations via ablation studies.

2 RELATED WORK


70

65


60

55


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
|||||ve)|||
||Ba|seline (C|ontrasti|ve)|||
||Pr|oposed|model (S|RR)|||
||||||||


10


Anomaly ratio (%) in training data


Figure 2: Performance of our proposed model
and a baseline OCC using contrastive representation [51] on CIFAR-10 with different
anomaly ratios in the training data.


There are various existing works under the different settings described in Fig. 1:

**The positive + negative setting is often considered as a supervised binary classification problem.**
The challenge arises due to the imbalance in label distributions as positive (anomalous) samples are
rare. As summarized in [12], to address this, over-/under-sampling [16, 20], weighted optimization

[4, 27], synthesizing data of minority classes [32, 35], and hybrid methods [2, 22] have been studied.

**The negative setting is often converted to a one-class classification problem, with the goal of**
finding a decision boundary that includes as many one-class samples as possible. Shallow models
for this setting include one-class support vector machines [47] (OC-SVM), support vector data


-----

description [53] (SVDD), kernel density estimation (KDE) [31], and Gaussian density estimation
(GDE) [44]. There are also auto-encoder based models [59] that treat the reconstruction error as the
anomaly score. Deep learning based OCCs have been developed, such as Deep OCC [45], geometric
transformation [23], or outlier exposure [26]. Noting the degeneracy or inconsistency of learning
objectives of existing end-to-end trainable Deep OCCs, [51] proposed a deep representation OCC, a
two-stage framework that learns self-supervised representations [17, 28] followed by shallow OCCs.
That work was extended for texture anomaly localization with CutPaste [33]. The robustness to very
low anomaly ratios of these methods under the unsupervised setting was explored in [7, 60].

**The semi-supervised setting is defined as utilizing a small set of labeled samples and large set of**
unlabeled samples to distinguish anomalies from normal data. Depending on which labeled samples
are given, this setting can be split into three sub-categories. When only some positive/negative labeled
samples are provided, we denote that as a PU/NU setting. Most previous works in semi-supervised
AD settings focus on the NU setting where only some of the normal labeled samples are given

[1, 41, 52]. The PNU setting is a more general semi-supervised setting where subsets of both positive
and negative labeled samples are given. Deep SAD [46] and SU-IDS [39] are included in this category.
We show the significant outperformance of our proposed SRR framework compared to Deep SAD,
for multiple benchmark datasets when not using any labeled data (see Sec. 4.2).

**The unlabeled setting has received relatively less attention despite its significance in automating**
machine learning. The popular methods for this setting include isolation forest [36] and local
outlier factor [13]. However, they are difficult to scale, and less compatible with recent advances in
representation learning. While OCCs, such as OC-SVM, SVDD, or their deep counterparts, apply
to unlabeled settings by assuming the data is all negative, and the robustness of those methods has
also been demonstrated in part [7, 60], in practice we observe a significant performance drop with a
high anomaly ratio, shown in Fig. 2. In contrast, our proposed framework is able to maintain high
performance across anomaly ratios.

**Data refinement has been applied to AD in some prior works. [5, 42] generate pseudo-labels using**
binary classification and OC-SVM for data refinement to boost the consequent AD performances in
unsupervised settings. [6, 30, 40, 55, 59] used the reconstruction errors of the auto-encoder as an
indicator for removing possible anomalies. [21] and [38] used data refinement for AD in supervised
and semi-supervised settings. Additional discussions can be found in Appendix A.9.

**Self-training [37, 48] is an iterative training mechanism using predicted pseudo labels as targets**
for model training. It has regained popularity recently with its successful results in semi-supervised
image classification [9, 50, 57]. To improve the quality of pseudo labels, employment of an ensemble
of classifiers has also been studied. [14] trains an ensemble of classifiers with different classification
methods to make a consensus for noisy label verification. Co-training [11] trains multiple classifiers,
each of which is trained on the distinct views, to supervise other classifiers. Co-teaching [25] and
DivideMix [34] share a similar idea in that they both train multiple deep networks on separate data
batches to learn different decision boundaries, thus becoming useful for noisy label verification. While
sharing a similarity, the proposed framework has clear differences from the previous works. SRR
performs iterative training with data refinement (with robust ensemble methods) and self-supervised
learning for unsupervised learning of an anomaly detector.

3 PROPOSED FRAMEWORK

**Self-supervise, Refine, and Repeat (SRR) is an iterative training framework, where we refine the data**
(Sec. 3.1) and update the representation with the refined data (Sec. 3.2), followed by OCCs on refined
representations. Fig. 3 overviews the framework and Algorithm 1 provides the pseudo code.

**Notation. We denote the training data as D = {xi}i[N]=1** [where][ x][i][ ∈X][ and][ N][ is the number of training]
samples.anomaly (positive). Note that labels are not provided in the unsupervised setting. yi ∈{0, 1} is the corresponding label to xi, where 0 denotes normal (negative) and 1 denotes

Let us denote a feature extractor as g : X →Z. g may include any data preprocessing functions, an
identity function (if raw data is directly used for one-class classification), and learned or learnable
representation extractors such as deep neural networks. Let us define an OCC as f : Z → [−∞, ∞]
that outputs anomaly scores given the input features g(x). The higher the score f (g(x)), the more
anomalous the sample x is. The binary anomaly prediction is made after thresholding: 1 _f_ (g(x)) ≥
 


-----

**Data refinement (Ensemble one-class classifier)**

Train
Inference

One-class

Subset 1

classifier 1 Intersect

Train

**Representation** Unlabeled data One-class

**learner** (D) Subset 2 One-class **∩** classifier

classifier 2

**...** **...**

One-class

Subset K

classifier K


Figure 3: Block diagram of SRR composed of representation learner (Sec. 3.2), data refinement
(Sec. 3.1), and final OCC blocks. The representation learner updates the deep models using refined
data from the data refinement block. Data refinement is done by an ensemble of OCCs, each of
which is trained on K disjoint subsets of unlabeled training data. Samples predicted as normal by all
classifiers are retained in the refined data, and are used to update the representation learner and final
OCC. The process is repeated iteratively until convergence. Convergence graphs can be found in the
Appendix A.8.

3.1 DATA REFINEMENT

A naive way to generate pseudo labels of unlabeled data is to construct an OCC on raw data or learned
representations as in [51] and threshold the anomaly score to obtain a binary label for normal vs.
anomalous. As we update the model with refined data that excludes samples that are predicted to be
anomalous, it is important to generate pseudo labels of training data as accurately as possible.

To this end, instead of training a single classifier (unlike most previous works on data refinement
for AD [5, 42, 59]), we train an ensemble of K OCCs and aggregate their predictions to generate
pseudo labels. We illustrate the data refinement block in Fig. 3 and as REFINEDATA in Algorithm 1.
Specifically, we randomly divide the unlabeled training data into K disjoint subsets 1, ..., _K,_
_D_ _D_ _D_
and train K different OCCs (f1, ..., fK) on corresponding subsets ( 1, ..., _K). Then, we estimate_
_D_ _D_
a binary pseudo-label of the data xi as follows:
_∈D_

_K_

_yˆi = 1 −_ 1 − 1 _fk(g(xi)) ≥_ _ηk_ (1)

_k=1_

Y h   [i]


_ηk = max η s.t._


1 _fk(g(xi)) ≥_ _η_ _≥_ _γ_ (2)
_i=1_

X   


where 1( ) is the indicator function that outputs 1/0 if the input is True/False. fk(g(xi)) represents

_·_
an anomaly score of xi for an OCC fk. ηk in equation 2 is a threshold determined as a γ percentile
of the anomaly score distribution {fk(g(xi))}i[N]=1[.]

To interpret equation 1, xi is predicted as normal, i.e. ˆyi = 0, if all K OCCs predict it as normal.
While this may be too strict and potentially reject many true normal samples in the training set, we
find that empirically, it is critical to be able to exclude true anomalous samples from the training set.
The effectiveness of the employment of an ensemble of classifiers is empirically shown in Sec. 4.3.

3.2 REPRESENTATION UPDATE

SRR follows the idea of deep representation OCCs [51], where in the first stage a deep neural
network is trained with self-supervised learning (such as rotation prediction [23], contrastive [51], or
CutPaste [33]) to obtain meaningful representations of the data, and in the second stage OCCs are
trained on these learned representations. Such a two-stage framework is shown to be beneficial as it
prevents the ‘hypersphere collapse’ of the deep OCCs by the favorable inductive bias it brings with
the architectural constraints [45].

Here, we propose to conduct self-supervised representation learning jointly with data refinement.
More precisely, we train a feature extractor g using _D[ˆ] = {xi | ˆyi = 0}, a subset of unlabeled data_


-----

**Algorithm 1 SRR: Self-supervise, Refine, Repeat.**

**Input: Training data D = {xi}i[N]=1[, Ensemble count (][K][), threshold (][γ][)]**
**Output: Refined data (D[ˆ]), trained OCC (f** ), feature extractor (g)

1: function REFINEDATA(D, g, K, γ)
2: Train OCC models {fk}k[K]=1 [on][ {D][k][}]k[K]=1[,][ K][ disjoint subsets of the training data][ D][.]

3: Compute thresholds ηk’s for γ percentile of anomaly distributions (equation 2).

4: Predict binary labels ˆyi (equation 1).

6:5: end functionReturn _D[ˆ] = {xi : ˆyi = 0, xi ∈D}._
7: function SRR(D, K, γ)
8: Initialize the feature extractor g.

9: **while g not converged do**

10: _Dˆ = REFINEDATA(D, g, K, γ)._

11: Update g using _D[ˆ] with self-supervised learning objectives._

12: **end while**

13: _Dˆ = REFINEDATA(D, g, K, γ)._

14: Train an OCC model (f ) on refined data ( D[ˆ]).

15: end function


_D that only includes samples whose predicted labels with an ensemble OCC from Sec. 3.1 are_
negative. We also update _D[ˆ] as we proceed with representation learning. The proposed method is_
illustrated in Algorithm 1 as SRR. In contrast to previous works [33, 51] that use the entire training
data for learning self-supervised representation, we find it necessary to refine the training data even
for learning deep representations. Without representation refinement, the performance improvements
of SRR are limited, as shown in Sec. 4.3.2. Last, for test-time prediction, we train an OCC on refined
data _D[ˆ] using updated representations by g as in line 13-14 in Algorithm 1._

3.3 UNSUPERVISED MODEL SELECTION

As SRR is designed for unsupervised AD, labeled validation data for hyperparameter tuning is
typically not available and the framework should enable robust model selection without any reliance
_on labeled data. Here, we provide insights on how to select important hyperparameters, and later in_
Sec. 4.3.1 perform sensitivity analyses for these hyperparameters.

Data refinement of SRR introduces two hyperparameters: the number of OCCs (K) and the percentile
threshold (γ). There is a trade-off between the number of classifiers for the ensemble and the size
of disjoint subsets for training each classifier. With large K, we aggregate prediction from many
classifiers, each of which may contain randomness from training. This comes at a cost of reduced
performance per classifier as we use smaller subsets to train them. In practice, we find K = 5 works
well across different datasets and anomaly ratios. γ controls the purity and coverage of refined
data. If γ is large, and thus classifiers reject too many samples, the refined data could be more pure
and contain mostly the normal samples; however, the coverage of the normal samples would be
limited. On the other hand, with a small γ, the refined data may still contain many anomalies and the
performance improvement with SRR would be limited. We empirically observe that SRR is robust
to the selection of γ when it is chosen from a reasonable range. In our empirical experiments, we
find ∼ 1 − 2× of the true anomaly ratio to be a reasonable choice. In other words, it is safer to use
_γ higher than the expected true anomaly ratio. In some cases, the true anomaly ratio may not be_
available at all; for such scenarios, we propose Otsu’s method [49] to estimate the anomaly ratio of
the training data for determining the threshold γ (experimental results are in the Appendix A.5).

4 EXPERIMENTS

We evaluate the efficacy of our proposed framework for unsupervised AD tasks on tabular (Sec. 4.1)
and image (Sec. 4.2) data types. We experiment varying ratios of anomaly samples in unlabeled
training data and with different combinations of representation learning and OCCs. In Sec. 4.3, we


-----

provide performance analyses to better explain major constituents of the performance, as well as
sensitivity to hyperparameter values.

**Implementation details: To reduce the computational complexity of the data refinement block, we**
utilize a simple OCC such as GDE in the data refinement block. In a two-stage model, we only
update the data refinement block at 1st, 2nd, 5th, 10th, 20th, 50th, 100th, 500th epochs instead of
every epoch. After 500 epochs, we update the data refinement block per each 500th epoch. Each run
of experiments requires a single V100 GPU. Additional discussions can be found in Appendix A.10.

4.1 EXPERIMENTS ON TABULAR DATA


**Datasets. Following [7, 60], we test the efficacy of SRR on a variety of tabular datasets, including**
KDDCup, Thyroid, or Arrhythmia from the UCI repository [3]. We also use KDDCup-Rev, where
the labels of KDDCup are reversed so that an attack represents anomaly [60]. To construct data
splits, we use 50% of normal samples for training. In addition, we hold out some anomaly samples
(amounting to 10% of the normal samples) from the data. This allows us to simulate unsupervised
settings with an anomaly ratio of up to 10% of entire training set. The rest of the data is used for
testing.[1] We conduct experiments using 5 random splits and 5 random seeds, and report the average
and standard deviation of 25 F1-scores (with scale 0-100) for the performance metric.

**Models. We mainly compare with GOAD [7] (the state-of-the-art AD model in the tabular domain)**
and implement SRR on top of it. GOAD utilizes random transformation classification as the pretext
task of self-supervised learning, and the normality score is determined by whether transformations
are accurately included in the transformed space of the normal samples. We re-implement GOAD [7]
with a few modifications. First, instead of using embeddings to compute the loss, we use a parametric
classifier, similarly to augmentation prediction [51]. Second, we follow the two-stage framework [51]
to construct deep OCCs. For the clean training data setting, our implementation achieves 98.0 for
KDD, 95.0 for KDD-Rev, 75.1 for Thyroid, and 54.8 for Arrhythmia F1-scores, which are comparable
to those reported in [7]. Please see Appendix A.3 for formulation and implementation details.


Thyroid

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
|S|RR+G|OAD|||



0.0 0.5 1.0 1.5 2.0 2.5

SRR+GOAD
GOAD
OC-SVM (rbf kernel)

Anomaly ratio (%) in training data


Arrhythmia

|Col1|Col2|SRR+|GOAD|Col5|
|---|---|---|---|---|
|||GOAD OC-SV|M (rbf|kernel)|
||||||
||||||
||||||



0 2 4 6 8 10

SRR+GOAD
GOAD
OC-SVM (rbf kernel)

Anomaly ratio (%) in training data


KDD-Rev

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
|S|RR+G|OAD|||



0.0 0.5 1.0 1.5 2.0 2.5

SRR+GOAD
GOAD
OC-SVM (rbf kernel)

Anomaly ratio (%) in training data


|Col1|Col2|KDD|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|S|RR+G|OAD||||


0 2 4 6 8 10

SRR+GOAD
GOAD
OC-SVM (rbf kernel)

Anomaly ratio (%) in training data


80

70

60

50

40

30

20


100

90

80

70

60

50

40

30

20


97.5

95.0

92.5

90.0

87.5

85.0

82.5

80.0


55

50

45

40

35


Figure 4: Unsupervised AD performance (F1-score) using OC-SVM (with rbf kernel), GOAD [7],
and GOAD with the proposed method SRR on various tabular datasets. Shaded areas represent the
standard deviation.

**Results. We show results of GOAD (the baseline) and GOAD with SRR in Fig. 4. The ranges of the**
noise ratio are set to 0% for the anomaly ratios in the original dataset (if anomaly ratios are larger than
10%, we set the maximum anomaly ratio as 10%). For KDD-Rev, we set the maximum anomaly ratio
to 2.5% because the performance of GOAD (without SRR) drops significantly even with a small ratio
of anomalies in the training data. Fig. 4 shows that integrating SRR significantly improves GOAD
(the state-of-the-art methods for tabular OCC). The improvements are more significant especially
at higher anomaly ratios. This underlines how SRR can achieve significant improvements for both
small (Thyroid & Arrhythmia) and large-scale datasets (KDD & KDD-Rev).

4.2 EXPERIMENTS ON IMAGE DATA


**Datasets. We evaluate SRR on visual AD benchmarks, including CIFAR-10 [29], f-MNIST [56],**
Dog-vs-Cat [19], and MVTec [8]. For CIFAR-10, f-MNIST, and Dog-vs-Cat datasets, samples from

1Note that the experimental settings with contaminated training data in GOAD [7] and DAGMM [60] are
slightly different from ours. Our contamination ratio is defined as the anomaly ratio over the entire training data,
while their contamination ratio is the anomaly ratio over all the anomalies in the dataset.


-----

one class are set to be normal and the rest from other classes are set to be an anomaly. Similar to
the experiments on tabular data in Section. 4.1, we swap a certain amount of the normal training
data with anomalies given the target anomaly ratio. For MVTec, since there are no anomalous data
available for training, we borrow 10% of the anomalies from the test set and swap them with normal
samples in the training set. Note that 10% of samples borrowed from the test set are excluded from
evaluation. For all datasets, we experiment with varying anomaly ratios from 0% to 10%.

We use area under ROC curve (AUC) and average precision (AP) metrics to quantify the performance
for visual AD (with scale 0-100). When computing AP, we set the minority class of the test set as
label 1 and majority as label 0 (e.g., normal samples are set as label 1 for CIFAR-10 experiments as
there are more anomaly samples that are from 9 classes in the test set). We run all experiments with 5
random seeds and report the average performance for each dataset across all classes. Per-class AUC
and AP are reported in Appendix A.4.

**Models. For semantic AD benchmarks, CIFAR-10, f-MNIST, and Dog-vs-Cat, we compare the**
SRR with two-stage OCCs [51] using various representation learning methods, such as distributionaugmented contrastive learning [51], rotation prediction [23, 28] and its improved version [51], and
denoising autoencoder. For MVTec benchmarks, we use CutPaste [33] as a baseline and compare to
its version with SRR integration. For both experiments, we use the ResNet-18 architecture, trained
from random initialization, using the hyperparameters from [51] and [33]. The same model and
hyperparameter configurations are used for SRR with K = 5 classifiers in the ensemble. We set γ as
twice the anomaly ratio of training data. For 0% anomaly ratio, we set γ as 0.5. Finally, a Gaussian
Density Estimator (GDE) on learned representations is used as OCC.


70

60

50

40

30


90

85


80

75


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||SR Co Ro|R+Cont ntrastiv tation|rastive e||||
||Ro DA|tNet E|||||
||||||||


10


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||SR Co Ro|R+Cont ntrastiv tation|rastive e||||
||Ro|tNet|||||
||DA|E|||||
||||||||


10


Anomaly ratio (%) in training data


Anomaly ratio (%) in training data


Figure 5: Unsupervised AD performance with various OCCs on CIFAR-10. For SRR we adapt
distribution-augmented contrastive representation learning [51]. (Left) AUC, (Right) Average Precision (AP).

**Results. Fig. 5 shows a significant performance drop with increased anomaly ratio, regardless of**
representation learning. For example, the AUC of distribution-augmented contrastive representation [51] drops from 92.1 to 83.6 when anomaly ratio becomes 10%. Similarly, the improved rotation
prediction representation [28] drops from 90.0 to 84.1. On the other hand, SRR effectively handles
the contamination in the training data and achieves 89.9 AUC with 10% anomaly ratio, reducing the
performance drop by 74.1%. An ‘oracle’ upper bound would be the removal of all anomalies from
the training data (which is the same as the performance at 0% anomaly ratio for the same size of the
data). As Fig. 5 shows, the performance of SRR is similar to this oracle upper bound performance
(less than 2.5 AUC difference) even with high anomaly ratios (10%). The results are also similar in
other metrics, such as AP in Fig. 5 or Recall at Precision of 70 and 90 in Fig. 9 in the Appendix.

We repeat experiments on 3 additional visual AD datasets and report results in Fig. 6. We observe
consistent and significant improvements over the baseline across different datasets and different
one-class classification methods. Note that the improvement is more significant at higher anomaly
ratios. For instance, on MVTec dataset, SRR improves AUC by 4.9 and AP by 7.1 compared to the
state-of-the-art CutPaste OCC with an anomaly ratio of 10%. In the Appendix (Sec. A.4), we also
illustrate per-class performance of SRR compared with the state-of-the-art.


-----

95

90

85

80

75

80

70

60

50

40

30


90.0

87.5

85.0

82.5

80.0

77.5

75.0

72.5

70.0

90.0

87.5

85.0

82.5

80.0

77.5

75.0

72.5


96

94

92

90

88

86

SRR+CutPaste
CutPaste


97.5

95.0

92.5

90.0

87.5

85.0

82.5

80.0

77.5

|Col1|Col2|Col3|Col4|SRR+ CutPa|CutPaste ste|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|SR|R+Cont|rastive||||
|Co|ntrastiv|e||||

|Col1|Col2|Col3|Col4|SRR+Co Contrast|ntrastive ive|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|||||||
|||||||


SRR+Contrastive
Contrastive


10

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|SR|R+Cont|rastive||||
|Co|ntrastiv|e||||



10


10

|Col1|Col2|Col3|Col4|SRR+|CutPaste|
|---|---|---|---|---|---|
|||||CutPa|ste|
|||||||
|||||||
|||||||
|||||||
|||||||



10


10

|Col1|Col2|Col3|Col4|SRR+Co Contrast|ntrastive ive|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|||||||
|||||||


SRR+Contrastive
Contrastive


10


Anomaly ratio (%) in training data

2 4 6 8

SRR+Contrastive
Contrastive

Anomaly ratio (%) in training data


Anomaly ratio (%) in training data

2 4 6 8

SRR+CutPaste
CutPaste

Anomaly ratio (%) in training data


Anomaly ratio (%) in training data

2 4 6 8

SRR+Contrastive
Contrastive

Anomaly ratio (%) in training data


(a) MVTec


(b) f-MNIST


(c) Dog-vs-Cat


Figure 6: Unsupervised AD performance on (a) MVTec (b) f-MNIST, and (c) Dog-vs-Cat datasets
with varying anomaly ratios. We use state-of-the-art one-class classification models for baselines,
such as distribution-augmented contrastive representations [51] for f-MNIST and Dog-vs-Cat, or
CutPaste [33] for MVTec, and build SRR on top of them.

4.3 PERFORMANCE ANALYSES


In this section, we conduct sensitivity analyses on two hyperparameters of SRR, namely, the number
_K for ensemble OCCs and the percentile threshold γ that determines normal and anomalies in the_
data refinement module. In addition, we show the importance of updating the representation with
refined data. Ablation studies are conducted on two visual AD benchmarks, CIFAR-10 and MVTec.
More experimental results and discussions can be found in the Appendix.

4.3.1 SENSITIVITY TO HYPERPARAMETERS


SRR is designed for unsupervised AD and it is important to ensure robust performance against
changes in the hyperparameters as model selection without labeled data would be very challenging.
Fig. 7 presents the sensitivity analyses of SRR with respect to various hyperparameters. In Fig. 7a,
we observe the performance improvement as we increase the number of classifiers for ensemble. This
is particularly effective on CIFAR (top in Fig. 7a), where the number of samples in the training data
is large enough that even with large K, the number of samples to train each OCC (N/K) would be
sufficient. In Fig. 7b, we observe that SRR performs robustly when γ is set to be larger than the actual
anomaly ratio (10%). When γ is less than 10%, however, we see a significant drop in performance,
all the way to the baseline (γ = 0). Our results show that SRR improves upon baseline regardless of
the threshold. In addition, it suggests that γ could be set to be anywhere from the true anomaly ratio
and and 2x the anomaly ratio to maximize its effectiveness. When the true anomaly ratio is unknown,
as discussed in Appendix A.5, Otsu’s method could be used.

4.3.2 ITERATIVELY UPDATING REPRESENTATIONS WITH REFINED DATA


It is possible to decouple representation learning and data refinement of SRR, which would result in a
three-stage framework, where we learn representations until convergence without data refinement,
followed by the data refinement and learning OCC. Fig. 7c shows that SRR using a pre-trained
then fixed representation (i.e. when data refinement is used for OCC only) already improves the
performance upon the baseline with no data refinement at any stage of learning representation or
classifier. Improvements on CIFAR-10 are 4.9 in AUC and 8.0 in AP; and on MVTec are 1.3 in AD
and in 1.3 in AP. This underlines the effectiveness of our data refinement module. More results on
applying SRR on raw tabular features or learned (and fixed) visual representations are provided in
Appendix A.2. Moreover, we find that learning representation with refined data plays a crucial role,
resulting in another improvement in AUC by 1.4, AP by 4.5 on CIFAR-10, and AUC by 3.2, AP by
4.8 on MVTec, upon a SRR using a fixed representation.


-----

70

68

66

64

62

60

58




80

60

40

20


50


|95|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col21|Col22|Col23|Col24|Col25|Col26|Col27|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|95 90 Precision 90 70.0 90 85 67.5 Precision 89 65.0 88 78 50 Average 88 62.5 70 60.0 86 AUC Average AUC 87 57.5 65 86 55.0 84 60 AUC / 55 52.5 3 6 9 12 15 18 0 5 10 15 20 25 30 50 Ensemble count (K) Threshold ( ) 92 95.0 95 90 94 92.5 Precision Precision 94 88 90.0 93 86 92 87.5 AUC AUC Average 92 84 90 85.0 Average 91 82 82.5 90 88 80.0 80 AUC / 89 78 86 77.5 3 6 9 12 15 18 0 5 10 15 20 25 30 75.0 Ensemble count (K) Threshold ( ) (a) Ensemble count K (b) Percentile threshold γ ure 7: Ablation studies on (top) CIFAR-10 and (bottom) MVTec und h respect to (a) ensemble count K, (b) percentile threshold γ, and hout representation update. QUANTIFYING THE REFINEMENT EFFICACY evaluate how many normal and anomalies are excluded by the pro in Fig. 8(a, b), with data refinement, we can exclude more than 80% without removing too many normal samples. For instance, among a, SRR is able to exclude 80% anomalies while removing less than h recall of anomalies of SRR is not only useful for unsupervised improving the annotation efficiency when a budget for active lea demonstrates the removed normal and abnormal samples by the d ning epochs. It shows that better representation learning (as training roves the efficacy of the data refinement. samples Anomalous samples Anomalous samples (%) Normal samples 80 Normal samples 90 samples excluded 60 80 anomalous 40 70 of centage 20 60 oved||||||||||||||||No data refinement Data refinement for only OCC|||||||||||
||||||||||||||||||||||||||||
||||||||||||||||||||||||||||
|||||||||||||||||No dat Data r|a refin efinem|ement ent for|only OCC||||||||
|||||||||||||||||Data r A|efinem UC|ent for|re|present Avera||ation & ge Pre||OCC cison|||
|||||||||||||||||N D D|o data ata re ata re|refine fineme fineme|me nt nt|nt for only for repre||OCC sentatio||n & OC||C|
||||||||||||||||||||||||||||
|||||||||||||||||A (c) Rep er 10% (c) da posed of ano 4% an 20% no AD, bu rning ata refi epochs (%) (%) 34 90 samples samples 33 85 32 80 normal anomalous 31 75 30 70 moved 29 65 oved|UC res an ta dat m om rm t al is a ne in|enta om refi a re alie alie al s so vai me crea|ti a n fi s s a c la nt s|Aver on u ly ra eme nem in th in mpl ould ble. mo e) co||age Pre pdat tio s nt w ent e tr CIFA es. S be Fi dul nsis||cison e etti ith bloc aini R-1 uch usef g. 8( e ov tent (%) 36 samples 34 32 30 normal 28 26 moved 24||n o k n u c e l|
||||No|rm|al s|ampl|es||||||||||||||||||||
||||||||||||||||||||||||||||
||||||||||||||||||||||||||||
||||||||||||||||||||||||||||


22

|Col1|Col2|Col3|Col4|Col5|Normal|samples|Col8|
|---|---|---|---|---|---|---|---|


28


60


2% 4% 6% 8% 10%

Anomaly ratio (%) in training data

(a) CIFAR-10


2% 4% 6% 8% 10%

Anomaly ratio (%) in training data

(b) MVTec


250 500 750 Epochs1000 1250 1500 1750 2000

(c) CIFAR-10


50 100Epochs150 200 250

(d) MVTec


Figure 8: Percentage of excluded anomalous and normal samples by data refinement (a, b) with
different anomaly ratios in training data and (c, d) over training epochs for 10% anomaly ratio.

5 CONCLUSION


AD has wide range of practical use cases. A challenging and costly aspect of building an AD system
is that anomalies are rare and not easy to detect by humans, making them difficult to label. To
this end, we propose a novel AD framework to enable high performance AD without any labels
called SRR. SRR can be flexibly integrated with any OCC, and applied on raw data or on a trainable
representations. SRR employs an ensemble of multiple OCCs to propose candidate anomaly samples
that are refined from training, which allows more robust fitting of the anomaly decision boundaries as
well as better learning of data representations. We demonstrate the state-of-the-art AD performance
of SRR on multiple tabular and image data.


-----

ETHICS STATEMENT

SRR has the potential to make significant positive impact in real-world AD applications where detecting anomalies is crucial, such as for financial crime elimination, cybersecurity advances, or improving
manufacturing quality. We note a potential risk associated with using SRR: when representations are
not sufficiently good, there will be a negative cycle of refinement and representation updates/OCC.
While we rely on the existence of good representations, in some applications they may be difficult to
obtain (e.g., cybersecurity). This paper focuses on the unsupervised setting and demonstrates strong
AD performance, opening new horizons for human-in-the-loop AD systems that are low cost and
robust. We leave these explorations to future work.

REPRODUCIBILITY STATEMENT

The source code of SRR will be published upon acceptance. For reproducibility, we only use public
tabular and image datasets to evaluate the performances of SRR. Complete descriptions of those
datasets can be found in Datasets subsections in Sec. 4.1 and 4.2. Detailed experimental settings and
hyperparameters can be found in Models subsections in Sec. 4.1 and 4.2. The implementation details
(including hyperparameters that we used) on GOAD [7] are described in Sec. A.3.

REFERENCES

[1] S. Akcay, A. Atapour-Abarghouei, and T. P. Breckon. Ganomaly: Semi-supervised anomaly
detection via adversarial training. In ACCV, 2018. 3

[2] R. Alejo, R. M. Valdovinos, V. García, and J. H. Pacheco-Sanchez. A hybrid method to face class
overlap and class imbalance on neural networks and multi-class scenarios. PRL, 34(4):380–388,
2013. 2

[3] A. Asuncion and D. Newman. UCI machine learning repository, 2007. 6

[4] S. Barua, M. M. Islam, X. Yao, and K. Murase. Mwmote–majority weighted minority oversampling technique for imbalanced data set learning. IEEE Trans on knowledge and data
_engineering, 26(2):405–425, 2012. 1, 2_

[5] L. Beggel, M. Pfeiffer, and B. Bischl. Robust anomaly detection in images using adversarial
autoencoders. Machine Learning and Knowledge Discovery in Databases, 2019. 3, 4, 18, 21

[6] A. Berg, J. Ahlberg, and M. Felsberg. Unsupervised learning of anomaly detection from
contaminated image data using simultaneous encoder training. arXiv preprint arXiv:1905.11034,
2019. 3

[7] L. Bergman and Y. Hoshen. Classification-based anomaly detection for general data. In
_International Conference on Learning Representations, 2019. 1, 2, 3, 6, 10, 14, 15, 19_

[8] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. MVTec AD–a comprehensive real-world
dataset for unsupervised anomaly detection. In CVPR, 2019. 2, 6

[9] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel. Mixmatch:
A holistic approach to semi-supervised learning. Advances in Neural Information Processing
_Systems, 32, 2019. 3_

[10] G. Blanchard, G. Lee, and C. Scott. Semi-supervised novelty detection. JMLR, 11:2973–3009,
2010. 1

[11] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In CLT, 1998.

3

[12] P. Branco, L. Torgo, and R. Ribeiro. A survey of predictive modelling under imbalanced
distributions. arXiv preprint arXiv:1505.01658, 2015. 2

[13] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander. Lof: identifying density-based local
outliers. In International conference on management of data, 2000. 3, 20

[14] C. E. Brodley, M. A. Friedl, et al. Identifying and eliminating mislabeled training instances. In
_Proc National Conference on Artificial Intelligence, 1996. 3_


-----

[15] E. J. Candès, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? Journal of the
_ACM (JACM), 58(3):1–37, 2011. 20_

[16] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: synthetic minority
over-sampling technique. Journal of artificial intelligence research, 16:321–357, 2002. 1, 2

[17] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning
of visual representations. In ICML, 2020. 3

[18] F. Denis. PAC learning from positive statistical queries. In ICALT, 1998. 1

[19] J. Elson, J. R. Douceur, J. Howell, and J. Saul. Asirra: a captcha that exploits interest-aligned
manual image categorization. In Proc Computer and Communications Security, 2007. 2, 6

[20] A. Estabrooks, T. Jo, and N. Japkowicz. A multiple resampling method for learning from
imbalanced data sets. Computational intelligence, 20(1):18–36, 2004. 1, 2

[21] J.-C. Feng, F.-T. Hong, and W.-S. Zheng. Mist: Multiple instance self-training framework for
video anomaly detection. In CVPR, 2021. 3

[22] M. Galar, A. Fernandez, E. Barrenechea, H. Bustince, and F. Herrera. A review on ensembles
for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches. IEEE
_Trans on Systems, Man, and Cybernetics, 42(4):463–484, 2011. 2_

[23] I. Golan and R. El-Yaniv. Deep anomaly detection using geometric transformations. In
_Proceedings of the 32nd International Conference on Neural Information Processing Systems,_
pages 9781–9791, 2018. 1, 2, 3, 4, 7, 13, 14

[24] N. Görnitz, M. Kloft, K. Rieck, and U. Brefeld. Toward supervised anomaly detection. Journal
_of Artificial Intelligence Research, 46:235–262, 2013. 1_

[25] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. W. Tsang, and M. Sugiyama. Co-teaching:
robust training of deep neural networks with extremely noisy labels. In NIPS, 2018. 3

[26] D. Hendrycks, M. Mazeika, and T. Dietterich. Deep anomaly detection with outlier exposure.
In International Conference on Learning Representations, 2018. 1, 2, 3, 14

[27] J. P. Hwang, S. Park, and E. Kim. A new weighted approach to imbalanced data classification problem via support vector machine with quadratic cost function. Expert Systems with
_Applications, 38(7):8580–8585, 2011. 1, 2_

[28] N. Komodakis and S. Gidaris. Unsupervised representation learning by predicting image
rotations. In International Conference on Learning Representations (ICLR), 2018. 3, 7, 13, 14

[29] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. 2009. 2, 6

[30] C.-H. Lai, D. Zou, and G. Lerman. Robust subspace recovery layer for unsupervised anomaly
detection. In International Conference on Learning Representations, 2019. 3

[31] L. J. Latecki, A. Lazarevic, and D. Pokrajac. Outlier detection with kernel density functions. In
_International Workshop on Machine Learning and Data Mining in Pattern Recognition, 2007. 3_

[32] S. S. Lee. Noisy replication in skewed binary classification. Computational statistics & data
_analysis, 34(2):165–191, 2000. 1, 2_

[33] C.-L. Li, K. Sohn, J. Yoon, and T. Pfister. Cutpaste: Self-supervised learning for anomaly
detection and localization. In CVPR, 2021. 1, 2, 3, 4, 5, 7, 8, 17, 19, 20

[34] J. Li, R. Socher, and S. C. Hoi. Dividemix: Learning with noisy labels as semi-supervised
learning. In ICLR, 2019. 3

[35] A. Liu, J. Ghosh, and C. E. Martin. Generative oversampling for mining imbalanced datasets.
In DMIN, pages 66–72, 2007. 1, 2

[36] F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation forest. In ICDM, 2008. 2, 3

[37] G. J. McLachlan. Iterative reclassification procedure for constructing an asymptotically optimal
rule of allocation in discriminant analysis. Journal of the American Statistical Association,
70(350):365–369, 1975. 3

[38] X. Meng, S. Wang, Z. Liang, D. Yao, J. Zhou, and Y. Zhang. Semi-supervised anomaly detection
in dynamic communication networks. Information Sciences, 571:527–542, 2021. 3


-----

[39] E. Min, J. Long, Q. Liu, J. Cui, Z. Cai, and J. Ma. Su-ids: A semi-supervised and unsupervised
framework for network intrusion detection. In International Conference on Cloud Computing
_and Security, 2018. 3_

[40] M. Mohseni, J. Yap, W. Yolland, A. Koochek, and S. Atkins. Can self-training identify
suspicious ugly duckling lesions? In CVPR, 2021. 3

[41] J. Munoz-Marí, F. Bovolo, L. Gómez-Chova, L. Bruzzone, and G. Camp-Valls. Semisupervised˜
one-class support vector machines for classification of remote sensing data. IEEE transactions
_on geoscience and remote sensing, 48(8):3188–3197, 2010. 3_

[42] G. Pang, C. Yan, C. Shen, A. v. d. Hengel, and X. Bai. Self-trained deep ordinal regression for
end-to-end video anomaly detection. In CVPR, 2020. 3, 4, 18, 21

[43] J. Ren, P. J. Liu, E. Fertig, J. Snoek, R. Poplin, M. A. DePristo, J. V. Dillon, and B. Lakshminarayanan. Likelihood ratios for out-of-distribution detection. arXiv preprint arXiv:1906.02845,
2019. 21

[44] D. A. Reynolds. Gaussian mixture models. Encyclopedia of biometrics, 741:659–663, 2009. 3

[45] L. Ruff, R. Vandermeulen, N. Goernitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Müller, and
M. Kloft. Deep one-class classification. In ICML, 2018. 1, 2, 3, 4

[46] L. Ruff, R. A. Vandermeulen, N. Görnitz, A. Binder, E. Müller, K.-R. Müller, and M. Kloft.
Deep semi-supervised anomaly detection. In ICLR, 2020. 1, 3

[47] B. Schölkopf, R. C. Williamson, A. J. Smola, J. Shawe-Taylor, J. C. Platt, et al. Support vector
method for novelty detection. In NIPS, 1999. 1, 2

[48] H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transac_tions on Information Theory, 11(3):363–371, 1965. 3_

[49] M. Sezgin and B. Sankur. Survey over image thresholding techniques and quantitative performance evaluation. Journal of Electronic imaging, 13(1):146–165, 2004. 5, 18

[50] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin,
and C.-L. Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence.
_NeurIPS, 2020. 3_

[51] K. Sohn, C.-L. Li, J. Yoon, M. Jin, and T. Pfister. Learning and evaluating representations for
deep one-class classification. In ICLR, 2021. 1, 2, 3, 4, 5, 6, 7, 8, 13, 14, 16, 18, 19, 20, 21

[52] H. Song, Z. Jiang, A. Men, and B. Yang. A hybrid semi-supervised anomaly detection model
for high-dimensional data. Computational intelligence and neuroscience, 2017. 3

[53] D. M. Tax and R. P. Duin. Support vector data description. Machine learning, 54(1):45–66,
2004. 1, 3

[54] S. Wold, K. Esbensen, and P. Geladi. Principal component analysis. Chemometrics and
_intelligent laboratory systems, 2(1-3):37–52, 1987. 20_

[55] Y. Xia, X. Cao, F. Wen, G. Hua, and J. Sun. Learning discriminative reconstructions for
unsupervised outlier removal. In ICCV, 2015. 3, 18, 21

[56] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. 6

[57] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le. Self-training with noisy student improves imagenet
classification. In CVPR, 2020. 3

[58] B. Zhang and W. Zuo. Learning from positive and unlabeled examples: A survey. In 2008
_International Symposiums on Information Processing, 2008. 1_

[59] C. Zhou and R. C. Paffenroth. Anomaly detection with robust deep autoencoders. In KDD,
2017. 3, 4, 20

[60] B. Zong, Q. Song, M. R. Min, W. Cheng, C. Lumezanu, D. Cho, and H. Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In ICLR, 2018. 1, 2, 3,


-----

A APPENDIX

A.1 ADDITIONAL RESULTS WITH DIFFERENT METRICS


There are various performance metrics of OCCs. In the main manuscript, we mainly use AUC,
Average Precision (AP) and F1-score as the evaluation metrics. In this subsection, we report the
performances of the proposed model (SRR) and baselines in terms on Recall at Precision 70 and 90
as the additional metrics on CIFAR-10 dataset.


60

50

40

30

20

10


40

30


20

10

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|SR|R+Cont|rastive||||
|Co Ro Ro|ntrastiv tation tNet|e||||

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||astive||||
|SR|R+Contr|||||
|Co Ro|ntrastiv tation|e||||
|Ro D|tNet AE|||||


10


10


SRR+Contrastive
Contrastive
Rotation
RotNet
DAE


SRR+Contrastive
Contrastive
Rotation
RotNet
DAE


Anomaly ratio (%) in training data


Anomaly ratio (%) in training data


Figure 9: Performance of various OCCs on CIFAR-10 dataset. SRR is applied on top of Contrastive

[51]. (Left) Recall at Precision 70, (Right) Recall at Precision 90.

As in Fig. 9, we observe a similar trends with these two additional metrics as well. For example,
the performances of SRR are robust across various anomaly ratios. On the other hand, all the other
OCCs show consistent and significant performance degradation as the anomaly ratio increases. SRR
performs 15.8 and 10.9 better than the state-of-the-art OCC [51] in terms of recall at precision 70 and
90, respectively.


A.2 SRR ON RAW TABULAR FEATURES / LEARNED IMAGE REPRESENTATIONS


Thyroid


Arrhythmia


KDD-Rev


85.0

82.5

80.0

77.5

75.0

72.5

70.0

67.5

65.0

80

70

60

50

40

30

20


44

42

40

38

36

34

32

30

Baseline
SRR on raw features


110

100

90

80

70

60

50


70

60

50

40

30

20

90

85

80

75

70

65

60

55

50


Baseline
SRR on raw features


Baseline
SRR on raw features


(b) Thyroid

|Ba SR|seline R on ra|w features|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
|Average|Preciso|n|F1-|score|


CIFAR-10 / Rotation representation



|Col1|Base SRR|line on learned represenations|
|---|---|---|
||||


|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
|erage CIFA|Precis (a) K R-10 /|on DD-Re RotNet repr|F1-s v esenta|core tion|
||Base SRR|line on learned r|eprese|nation|
||||||
|AU (|C d) R||erage 3]|Precisi|


the performance improvements are significant.

|Baseline SRR on raw features|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|Average ( CIFAR||Precison c) Ar -10 / Co|rhythm ntrastive r|F1-s ia eprese|core ntation|
|||Baseli SRR o|ne n learned|repres|enations|
|||||||
|||||||
|A (f)||UC Contr|A astive imag Cont|verage [51]|Precisio|
|er) le hyroi||arned d, and||e rep rasti|resen ve [5|


Baseline
SRR on learned represenations

AUC Average Precision

(e) Rotation [28]

Figure 10: Performance of SRR on (top) raw tabular features and (lower) learned image representa-
tions. SRR consistently outperforms baseline and in some cases (e.g., Thyroid, and Contrastive [


-----

SRR is also applicable on raw tabular features or learned image representation without representation
update using data refinement. In this section, we demonstrate the performance improvements by SRR
without representation update to verify the effectiveness of data refinement block of SRR for shallow
OCCs.

Fig. 10 (upper) demonstrates consistent and significant performance improvements when we apply
SRR on top of raw tabular features. Specifically, the Average Precision (AP) improvements are 10.2,
29.0, and 4.1 with KDD-Rev, Thyroid, and Arrhythmia tabular datasets, respectively.

We also apply SRR on top of various learned image representations. As can be seen in Fig. 10
(lower), the performance improvements of SRR are consistent across various different learned image
representations (without representation update). For instance, the AP improvements are 9.2, 1.0,
and 12.1 with learned image representations using RotNet [23], Rotation [28], and Contrastive [51],
respectively.

A.3 IMPLEMENTATION DETAILS ON GOAD [7] FOR TABULAR DATA EXPERIMENTS

A classification-based AD method, GOAD [7], has demonstrated strong AD performance on tabular
datasets. Unlike previous works [23, 26] that formulate a parametric classifier for multiple transformation classification, GOAD employs distance-based classification of multiple transformations. For
the set of transformations Tm : X →D, m = 1, ..., M, the loss function of GOAD is written as in
equation 3 with the probability defined in equation 4.

_L = −Em,x_ log P (m|Tm(x)) _,_ (3)

exp( _f_ (Tm(x)) c ˆm[∥][2][)]
_P_ ( ˆm _Tm(x)) =_ _−∥_ _−_ (4)
_|_ _n_ [exp(][−∥][f] [(][T][m][(][x][))][ −] _[c][n][∥][2][)]_ _[,]_

where the centers cm’s are updated by the average feature over the training set. While it is shown toP
perform well [7], we find that the distance-based formulation is not necessary, and we achieve the
similar performance, if not worse, to [7] using a parametric classifier when computing the probability:

exp _wm[⊤]ˆ_ _[f]_ [(][T][m][(][x][)) +][ b][ ˆ]m
_P_ ( ˆm _Tm(x)) =_ (5)
_|_ _n_ [exp (]  _[w]n[⊤][f]_ [(][T][m][(][x][)) +][ b][n][)]

P


The formulation in equation 5 is easier to optimize than its original form in equation 4 as it can be
fully optimized with backpropagation without alternating updates of feature extractor f and centers
_cm. Once we learn a representation by optimizing the loss in equation 3 using equation 5, we follow a_
two-stage one-class classification framework of [51] to construct a set of Gaussian density estimation
OCCs for each transformation. Finally, we aggregate a maximum normality scores from a set of
classifiers as the normality score.

In Table 1, we summarize the implementation details, such as network architecture or hyperparameters,
and AD performance under clean training data setting that reproduces the results in [7].


-----

|Datasets|KDD|KDD-Rev|Thyroid|Arrhythmia|
|---|---|---|---|---|


|F-score [7] F-score (ours)|98.4 ±0.2 98.0 ±0.2|98.9 ±0.3 95.0 ±0.2|74.5 ±1.1 75.1 ±2.4|52.0 ±2.3 54.8 ±3.2|
|---|---|---|---|---|


|f (feature) Optimizer Learning rate L2 weight regularization Batch size Random projection dimension|Linear(8), LeakyReLU(0.2) 5 × Momentum SGD (momentum= 0.9) 0.001 0.00003 64 M × 32|
|---|---|


|M (number of transformations) Train steps|32 210|256 216|
|---|---|---|


Table 1: The AD performance under clean only data setting of GOAD in [7] and our implementation.
Our implementation demonstrates comparable, if not worse, performance to those reported in [7].
Our implementation also shares most hyperparameters across datasets except the M, the number of
transformations, and the train steps, which are closely related to the size of training data.

A.4 PER-CLASS AUC AND AP

In the main manuscript, we report the mean and standard deviation of AUC and AP across all classes
in each dataset. In this section, we report the mean and standard deviation of AUC and AP for each
class in each dataset (including CIFAR-10 (Table 2), MVTec (Table 3), fMNIST (Table 4), and
Dog-vs-Cat (Table 5) datasets).

A.4.1 PER-CLASS AUC AND AP ON CIFAR-10 DATASET


-----

|anomaly ratios|0%|10%|
|---|---|---|


|Classes / Methods|Baseline [51]|SRR + [51]|Baseline [51]|SRR + [51]|
|---|---|---|---|---|


AUC

|Airplane Automobile Bird Cat Deer Dog Frog Horse Ship Truck|90.5 ±0.6 98.8 ±0.1 87.5 ±0.6 81.5 ±0.5 90.3 ±0.4 90.8 ±0.7 92.0 ±0.6 97.8 ±0.1 96.5 ±0.2 95.1 ±0.3|90.4 ±0.4 98.7 ±0.1 87.1 ±0.8 80.3 ±0.7 90.2 ±0.5 90.8 ±0.6 91.4 ±0.9 97.9 ±0.1 96.4 ±0.1 95.8 ±0.1|85.3 ±0.8 95.2 ±0.5 75.6 ±1.1 68.9 ±1.0 79.9 ±0.8 79.2 ±2.5 78.5 ±2.1 93.3 ±0.5 91.5 ±0.9 88.9 ±0.4|87.6 ±0.7 97.9 ±0.3 84.7 ±1.2 78.5 ±1.5 87.4 ±0.5 89.6 ±0.6 87.5 ±1.3 96.0 ±0.6 95.2 ±0.5 94.7 ±0.4|
|---|---|---|---|---|



Average Precision

|Airplane Automobile Bird Cat Deer Dog Frog Horse Ship Truck|68.3 ±1.7 93.9 ±0.2 62.2 ±2.1 44.0 ±2.4 59.5 ±0.8 59.1 ±6.5 72.7 ±2.0 89.0 ±0.7 83.9 ±0.8 79.0 ±1.6|67.9 ±1.1 94.1 ±0.1 61.7 ±2.4 42.9 ±1.9 58.8 ±1.2 59.8 ±4.7 70.4 ±6.9 89.2 ±0.6 83.7 ±0.6 81.3 ±0.4|61.3 ±1.9 85.9 ±3.4 42.4 ±3.0 30.3 ±3.8 42.9 ±3.5 39.7 ±9.1 38.5 ±11.0 80.6 ±1.9 74.9 ±2.4 67.0 ±1.9|61.8 ±1.1 91.5 ±0.7 57.5 ±2.3 44.9 ±0.9 53.6 ±1.5 64.5 ±2.0 73.6 ±3.6 86.2 ±1.1 79.8 ±0.7 75.8 ±1.2|
|---|---|---|---|---|



Table 2: The AD performance per class on CIFAR-10 dataset in terms of AUC and AP. Class
represents the label of normal samples.

A.4.2 PER-CLASS AUC AND AP ON MVTEC DATASET


-----

|anomaly ratios|0%|10%|
|---|---|---|


|Classes / Methods|Baseline [33]|SRR + [33]|Baseline [33]|SRR + [33]|
|---|---|---|---|---|


AUC

|Bottle Cable Capsule Hazelnut Metal Nut Pill Screw Toothbrush Transistor Zipper Carpet Grid Leather Tile Wood|98.0 ±1.0 77.7 ±0.7 96.1 ±1.1 97.2 ±0.7 98.5 ±1.0 92.1 ±3.1 87.4 ±1.6 98.5 ±1.5 96.4 ±2.0 99.4 ±0.4 90.0 ±3.9 99.2 ±0.5 99.8 ±0.3 93.1 ±1.5 98.8 ±0.9|98.8 ±0.7 77.9 ±2.6 97.2 ±1.3 97.5 ±0.4 98.6 ±0.4 91.1 ±2.5 86.8 ±2.5 98.3 ±1.1 96.7 ±1.6 98.0 ±0.6 90.8 ±4.6 99.7 ±0.3 99.7 ±0.4 93.3 ±2.7 98.5 ±1.4|94.3 ±0.8 72.9 ±3.3 92.4 ±1.7 83.5 ±3.6 91.2 ±2.8 85.1 ±1.5 79.9 ±1.0 98.6 ±1.3 93.9 ±2.8 91.1 ±2.9 80.4 ±6.1 92.9 ±7.6 98.0 ±1.1 86.4 ±3.4 92.1 ±2.2|98.7 ±0.7 80.4 ±0.9 96.8 ±0.7 96.7 ±0.6 98.7 ±0.3 92.1 ±0.5 86.4 ±1.4 99.3 ±1.2 95.7 ±0.7 99.2 ±0.7 91.2 ±2.7 99.7 ±0.2 100.0 ±0.0 92.4 ±1.8 99.0 ±0.5|
|---|---|---|---|---|



Average Precision

|Bottle Cable Capsule Hazelnut Metal Nut Pill Screw Toothbrush Transistor Zipper Carpet Grid Leather Tile Wood|98.8 ±0.7 81.8 ±0.9 99.0 ±0.3 97.1 ±0.6 99.6 ±0.3 98.2 ±0.7 93.8 ±1.1 99.2 ±0.9 93.0 ±3.0 99.8 ±0.2 94.4 ±3.5 99.5 ±0.3 99.9 ±0.2 96.1 ±0.9 99.4 ±0.5|99.3 ±0.5 82.3 ±1.7 99.3 ±0.3 97.0 ±0.4 99.6 ±0.1 97.9 ±0.6 93.6 ±1.3 99.2 ±0.6 93.7 ±2.4 99.2 ±0.3 95.7 ±2.8 99.8 ±0.2 99.9 ±0.2 96.4 ±1.4 99.4 ±0.6|94.3 ±1.6 70.8 ±4.3 79.5 ±5.3 88.8 ±3.6 76.1 ±8.4 56.6 ±2.6 60.1 ±2.8 97.4 ±2.2 97.8 ±1.6 86.1 ±2.8 64.2 ±8.2 90.9 ±6.1 97.2 ±1.3 82.4 ±4.5 88.1 ±3.2|98.0 ±0.6 74.1 ±3.0 89.8 ±2.7 93.6 ±1.9 92.1 ±3.1 67.3 ±2.2 63.7 ±1.8 98.6 ±2.0 97.1 ±0.5 98.0 ±1.5 71.6 ±4.2 99.4 ±0.4 100.0 ±0.1 86.4 ±2.8 97.2 ±1.7|
|---|---|---|---|---|



Table 3: The AD performance per class on MVTec dataset in terms of AUC and AP. Class represents
the label of normal samples.

A.4.3 PER-CLASS AUC AND AP ON FMNIST DATASET

A.4.4 PER-CLASS AUC AND AP ON DOG-VS-CAT DATASET


-----

|anomaly ratios|0%|10%|
|---|---|---|


|Classes / Methods|Baseline [51]|SRR + [51]|Baseline [51]|SRR + [51]|
|---|---|---|---|---|


AUC

|T-shirt/top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Ankle boot|93.3 ±1.2 99.2 ±0.3 93.8 ±0.2 94.4 ±0.5 95.1 ±0.1 95.4 ±0.3 87.9 ±0.2 99.1 ±0.0 97.8 ±0.2 98.6 ±0.2|93.2 ±1.0 99.2 ±0.2 93.8 ±0.2 94.0 ±0.5 94.9 ±0.2 95.5 ±0.4 87.9 ±0.2 99.2 ±0.1 97.6 ±0.3 98.5 ±0.2|79.2 ±1.3 92.9 ±0.8 78.4 ±2.7 78.4 ±3.5 80.4 ±3.1 67.6 ±4.3 73.9 ±1.2 80.2 ±4.9 58.2 ±3.3 76.6 ±2.6|84.7 ±1.0 94.5 ±1.9 90.1 ±1.1 85.8 ±1.6 88.7 ±1.7 81.7 ±1.8 81.0 ±2.5 94.7 ±3.6 68.6 ±2.3 87.1 ±3.6|
|---|---|---|---|---|



Average Precision

|T-shirt/top Trouser Pullover Dress Coat Sandal Shirt Sneaker Bag Ankle boot|68.7 ±12.1 96.5 ±2.9 62.7 ±1.2 72.3 ±3.7 66.2 ±1.1 70.9 ±2.8 49.6 ±0.9 91.9 ±0.4 92.2 ±0.4 87.9 ±1.4|67.9 ±8.9 98.2 ±0.3 63.6 ±1.3 71.6 ±2.7 65.6 ±1.0 70.8 ±2.4 49.7 ±0.9 92.9 ±0.9 91.8 ±1.0 86.5 ±1.5|43.0 ±4.7 78.2 ±5.2 27.2 ±6.9 36.1 ±8.8 24.3 ±3.9 17.8 ±5.9 27.2 ±5.8 21.0 ±3.8 13.2 ±2.0 19.6 ±3.1|56.8 ±3.7 87.1 ±3.7 57.1 ±1.5 57.3 ±2.6 50.7 ±9.5 40.1 ±5.4 37.6 ±4.7 81.6 ±11.4 20.1 ±3.7 48.0 ±16.5|
|---|---|---|---|---|



Table 4: The AD performance per class on fMNIST dataset in terms of AUC and AP. Class represents
the label of normal samples.

|anomaly ratios|0%|10%|
|---|---|---|


|Classes / Methods|Baseline [51]|SRR + [51]|Baseline [51]|SRR + [51]|
|---|---|---|---|---|



AUC

|Dog Cat|89.7 ±0.6 88.2 ±0.4|89.2 ±0.4 88.0 ±0.7|74.7 ±2.1 70.1 ±2.4|79.1 ±1.3 70.7 ±1.8|
|---|---|---|---|---|



Average Precision

|Dog Cat|91.6 ±0.7 90.2 ±0.3|91.0 ±0.6 89.8 ±1.0|80.0 ±2.2 71.6 ±3.8|84.0 ±1.2 71.5 ±4.8|
|---|---|---|---|---|



Table 5: The AD performance per class on Dog-vs-Cat dataset in terms of AUC and AP. Class
represents the label of normal samples.

A.5 SRR WITH OTSU’S METHOD

In the main manuscript we show that the performances of the proposed unsupervised anomaly
detection framework (SRR) are significantly better than the state-of-the-art OCCs if the given data
is fully unlabeled. Although SRR does not need any labeled data, it needs a true anomaly ratio for
hyper-parameter optimization. In this subsection, we further extend SRR to the setting where there
is no access to the true anomaly ratio as well. We utilize the Otsu’s method [49] to identify the
threshold between normal and anomalous samples and use that as the hyper-parameter (γ) instead of
twice of the true anomaly ratio. Note that other previous works [5, 42, 55] also utilized intra-variance
minimization, in a similar vein to Otsu’s method, for determining the threshold for data refinement.


-----

The key idea of the Otsu’s method is to find the threshold that minimizes the intra-class variance. This
is defined as the weighted sum of variances of the two classes. Let us denote the normality scores
as {si}i[N]=1 [and threshold as][ η][. Then, we try to find the threshold (][η][) that minimizes the weighted]
sum of the variance (w0(η) × σ0(η) + w1(η) × σ1(η)) where w0(η) = _i=1_ [I][(][s][i][ < η][)][/N][ and]
_w1(η) =_ _i=1_ [I][(][s][i][ ≥] _[η][)][/N]_ [.][ σ][0][(][η][)][ and][ σ][1][(][η][)][ are the variances of each class. The optimal threshold]
(η[∗]) is determined as [P][N]
_η[∗]_ = min _w0(η)_ _σ0(η) + w1(η)_ _σ1(η)._

[P][N] _η_ _×_ _×_

We use the twice of η[∗] as the hyperparameter (γ) in SRR.


We evaluate the performances of Otsu’s method (on top of SRR) in comparison to the state-of-the-art
OCC [33] and original SRR with the knowledge of the true anomaly ratio using MVTec dataset.


97.5

95.0

92.5

90.0

87.5

85.0

82.5

80.0

77.5


96

94

92

90

88

86

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|SRR|+CutPast|e (with tr|ue anoma|ly ratio)||
|SRR CutP|+CutPast aste|e (with O|tsu metho|d)||

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|||||||
|SRR|+CutPast|e (with tr|ue anoma|ly ratio)||
|SRR|+CutPast|e (with O|tsu metho|d)||
|CutP|aste|||||


10


SRR+CutPaste (with true anomaly ratio)
SRR+CutPaste (with Otsu method)
CutPaste


10


SRR+CutPaste (with true anomaly ratio)
SRR+CutPaste (with Otsu method)
CutPaste


Anomaly ratio (%) in training data


Anomaly ratio (%) in training data


Figure 11: Unsupervised anomaly detection performances with Otsu’s method on top of SRR on
MVTec dataset. (Left) AUC, (Right) Average Precision (AP).

Fig. 11 demonstrates that even without true anomaly ratio, the performance of SRR can be significantly better than the state-of-the-art OCC [33] with Otsu’s method. Also, Fig. 11 shows that the
knowledge of true anomaly ratio is crucial information for maximizing the performance of SRR in
fully unsupervised settings.

We further extend the experimental results using Otsu’s method with other datasets such as CIFAR-10
and Thyroid datasets. Table 6 are the overall results - Otsu’s method yields only slight degradation
compared to SRR with true anomaly ratio; however, it still significantly outperforms SOTA OCC
baselines.

|Datasets / Method|SOTA OCC|SRR with Otsu’s method|SRR|
|---|---|---|---|


|CIFAR-10|0.855 / 0.585|0.906 / 0.703|0.910 / 0.709|
|---|---|---|---|


|Thyroid|0.506|0.623|0.639|
|---|---|---|---|


Table 6: Additional ablation studies. SOTA OCC methods are [51] for CIFAR-10 dataset and GOAD

[7] for Thyroid dataset. We introduce 6% noise on MVTec dataset and 1.5% noise on the Thyroid
dataset. Metrics are (AUC/AP) for CIFAR-10 dataset and F1 score for Thyroid dataset.


A.6 ADDITIONAL ABLATION STUDIES

To better understand the source of gains, we include comparisons between the final ensemble model
on the converged self-supervised extractor (SRR without final OCC) with the proposed SRR (using an
additional final OCC). Also, to further support the novelty of SRR, we report additional experimental
results only with ensemble model without data refinement (Ensemble Only).

As can be seen in Table 7, the proposed version of SRR (with an additional final OCC) outperforms
significantly, which is attributed to the fact that while fitting the individual OCC models in the
ensemble, we do not exclude the possible anomaly samples for diversity of the trained submodels,
so the anomaly decision boundaries can be fitted robustly. Therefore, the ensemble model can be
somewhat less accurate for classifying difficult samples (near the decision boundary between normal


-----

and abnormal) compared to the final OCC used (that is trained with only normal samples). Also,
employment of ensemble learning without data refinement (Ensemble only), yields much worse
performance than the proposed method (SRR), underlining the importance of the core data refinement
idea of SRR.

|Datasets / Method|SOTA OCC|Ensemble Only|SRR without final OCC|SRR|
|---|---|---|---|---|


|MVTec|0.905 / 0.845|0.911 / 0.849|0.922 / 0.870|0.937 / 0.887|
|---|---|---|---|---|


|CIFAR-10|0.855 / 0.585|0.862 / 0.599|0.890 / 0.677|0.910 / 0.709|
|---|---|---|---|---|


Table 7: Additional ablation studies. SOTA OCC methods are CutPaste [33] for MVTec dataset and

[51] for CIFAR-10 dataset. We introduce 6% noise on both datasets. Metrics are (AUC/AP).


A.7 ADDITIONAL BASELINES

We add extra baselines from robust anomaly detection literature: Standard PCA [54], Robust PCA

[15] and Local Outlier Factor (LOF) [13] for tabular data and Robust autoencoder (Robust AE) [59]
for image data. As can be seen in Table 8, the performance of PCA and LOF are highly degraded
even with a small amount of anomalies in the training data. For Robust PCA and Robust AE, the
performance degradation is less but still significant in comparison to SRR. Overall, SRR outperforms
other benchmarks in fully unsupervised settings, underlining the importance of data refinement in
improving the robustness to anomaly ratio in training data, as the core constituent of SRR framework.


|Datasets / Method|PCA|Robust PCA|LOF|Robust AE|SRR|
|---|---|---|---|---|---|

|CIFAR-10|-|-|-|0.636 / 0.174|0.910 / 0.709|
|---|---|---|---|---|---|


|Thyroid|0.299|0.377|0.338|-|0.506|
|---|---|---|---|---|---|


|KDD|0.836|0.893|0.873|-|0.942|
|---|---|---|---|---|---|


Table 8: Additional experiments with extra baselines from robust anomaly detection literature. We
introduce 6% noise on CIFAR-10 and KDD datasets. For Thyroid dataset, we introduce 1.5% noise.
Metrics are (AUC/AP) for image data and F1 score for tabular data.


A.8 CONVERGENCE GRAPHS

The proposed SRR framework is converged when the iterative training of the self-supervised learning
is converged. Depending on the data refinement model training, corresponding self-supervised
models are also trained with differently refined training data. Usually, the data refinement model and
self-supervised models converge after a similar number of epochs. Fig. 12 illustrate the convergence
graphs of SRR with MVTec and CIFAR-10 datasets.


83.0

82.5

82.0

81.5

81.0

80.5

80.0

79.5

79.0


84

82

80

78

76

74

72

70

68


1.4

1.2

1.0

0.8

0.6

0.4

0.2


50 100 150 200 250

Epochs


250 500 750 1000 1250 1500 1750 2000

Epochs


Figure 12: Convergence graphs of SRR with (left) MVTec dataset, (right) CIFAR-10 dataset.


-----

A.9 ADDITIONAL DISCUSSIONS: RELATED WORKS FOR DATA REFINEMENT

In this subsection, we further discuss the related works [5, 42, 55] for data refinement that we briefly
mentioned in Section 2. We highlight the differences between SRR and [5, 42, 55] below.

First, [5, 55] are based on auto-encoders and they directly utilize the reconstruction errors as informative signals for anomaly detection and iterative data refinement. However, prior work [43] discovered
that the reconstruction is not a good informative signal for outlier detection. Also as shown in Fig. 5,
the performance of DAE (reconstruction errors based anomaly detection model) works much worse
than alternatives.

Second, SRR utilizes ensemble learning for data refinement to improve the robustness which is
critical in unsupervised anomaly detection. On the other hand, [5, 42, 55] utilize a single model for
data refinement. As can be seen in Fig. 7a, the impact of the ensemble model for data refinement is
significant.

Third, [5, 42, 55] directly utilize the (pseudo) abnormal samples for model training in addition to
(pseudo) normal samples. For instance, the model in [42] is trained via two-class ordinal regression
where two classes come from (pseudo) normal and (pseudo) abnormal samples. Directly relying
on the (imperfectly) labeled abnormal samples can be harmful for the anomaly detection due to the
overfitting problems and high False Positive Rates (FPR) in (pseudo) abnormal samples. For instance,
with 80% of the recall (for the abnormal sample discovery), the precisions (for the abnormal sample
discovery) are 28.2% for CIFAR-10 and 31.7% with MVTec datasets (with 6% abnormal ratio) using
SRR framework. It means that the majority of the (pseudo) abnormal samples are actually the normal
samples (71.8% for CIFAR-10 and 68.3% for MVTec datasets). This would be strong evidence that
directly utilizing the (pseudo) abnormal samples for training can be harmful.

Lastly, [42, 55] are based on the end-to-end anomaly detection models which are empirically less
accurate than two-stage models [51].

A.10 ADDITIONAL DISCUSSIONS: COMPUTATIONAL COMPLEXITY

Note that when applying SRR on top of representation learning, the computational complexity of
the representation learning part is not changed. With SRR, the additional computations come from
training the ensemble models on top of learned representations. Note that we use shallow one-class
classifiers (such as GDE or one-class SVM) for submodels; thus, the additional computational
complexity is marginal. We would like to mention that all the experiments are done on a single V100
GPU and each experiment needs at most 12 hours for training (the additional training time caused
by the SRR framework is an average 13.1% of the total training time). The computations of the
ensemble parts can be further improved by the model parallelization.


-----

