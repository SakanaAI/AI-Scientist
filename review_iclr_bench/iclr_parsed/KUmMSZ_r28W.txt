# PARTICLE BASED STOCHASTIC POLICY OPTIMIZA## TION

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Stochastic policy has been widely applied for its good property in exploration
and uncertainty quantification. Modeling policy distribution by joint state-action
distribution within the exponential family has enabled flexibility in exploration and
learning multi-modal policies and also involved the probabilistic perspective of
deep reinforcement learning (RL). The connection between probabilistic inference
and RL makes it possible to leverage the advancements of probabilistic optimization tools. However, recent efforts are limited to the minimization of reverse KL
divergence which is confidence-seeking and may fade the merit of a stochastic
policy. To leverage the full potential of stochastic policy and provide more flexible
property, there is a strong motivation to consider different update rules during
policy optimization. In this paper, we propose a particle-based probabilistic policy optimization framework, ParPI, which enables the usage of a broad family
of divergence or distances, such as f -divergences and the Wasserstein distance
which could serve better probabilistic behavior of the learned stochastic policy.
Experiments in both online and offline settings demonstrate the effectiveness of the
proposed algorithm as well as the characteristics of different discrepancy measures
for policy optimization.

1 INTRODUCTION

Deep reinforcement learning (DRL) leverages the power of neural-network function approximators
and has shown great promise in a diverse field, such as control (Lillicrap et al., 2015; Andrychowicz et al., 2020), robotics (Haarnoja et al., 2017; Gu et al., 2017; Plappert et al., 2018). Classic
views on the notion of optimality in reinforcement learning state that an optimal policy can be
deterministic (Sutton & Barto, 2011). However, recent studies by Daniel et al. (2012); Ziebart (2010)
demonstrate that deterministic policy may suffer from several drawbacks, e.g., the multi-modal
behaviors have significant applications in real robotic control tasks (Daniel et al., 2012) and inflexibility on exploration (Ostrovski et al., 2017). Particularly in offline RL, the deterministic policy
lacks the structure to manage risk towards uncertainty dynamics. The branch of probabilistic RL
is then introduced where the optimal solution is stochastic. Probabilistic RL not only enables the
stochasticity of the learned policy but also provides a different perspective to reveal the connection
between reinforcement learning and probabilistic inference (Todorov, 2008), where the optimization
procedure of the policy could be considered a statistical divergence minimization procedure towards
distribution in the state-action or trajectory space, which is implied by the corresponding optimal RL
policy.

Besides the different definitions of optimality and above mentioned priorities, another appealing property of probabilistic reinforcement learning lies in that advanced techniques in approximate inference
can then be applied in the optimization for RL. There are two widely applied frameworks, i.e., the
pseudo-likelihood framework and maximum entropy (MERL) framework. The pseudo-likelihood defines the optimality by considering a specified positive and bounded function of reward as the density
function. By regressing to the reweighted samples, the pseudo-likelihood based methods not only is
not an explicit probabilistic model but also suffers from high variance and poor risk handling (Levine,
2018). MERL is a prominent choice for policy improvement for stochastic policy which tends to
maximize the expected reward and the expected conditional entropy. It avoids the drawbacks of
pseudo-likelihood that it suffers from high variance and poor risk handling ability (Levine, 2018).
In practice, the MERL is generally implemented with the reverse KL minimization which promotes


-----

the mode-seeking behavior and could fade the merits of a stochastic policy. For example, in offline
setting, the mode-seeking policy will result in an overly-optimistic estimation of Q-function (Kumar
et al., 2020) which could be problematic when generalizing in the test scenario.

In this work, we explore the possibility of using a broad class of distribution discrepancies for
stochastic policy improvement, and find it is possible by leveraging particle-based methods. By
providing better distribution matching methods, our proposed framework, ParPI, enjoys the power of
probabilistic RL. Moreover, ParPI is naturally compatible with the offline RL setting. And we show
that with specific discrepancy measure selected, ParPI could conjoin the benefit of different policy
constraint methods. We firstly demonstrate that using a broader class of distribution discrepancies
achieves significant improvements in stabilizing the policy optimization and achieving the state-ofthe-art expected returns on high-dimensional and complex continuous control tasks (Todorov et al.,
2012). Besides, we conduct extensive experiments in the offline setup (Yu et al., 2020) to show that
the ParPI could be more defensive for optimization thus results in more generalizable optimal policy.

2 PRELIMINARIES

2.1 PROBABILISTIC INFERENCE FRAMEWORK FOR RL

The reinforcement learning (RL) problem can be modeled as the following infinite-horizon Markov
Decision Process (MDP), which is implied by the tuple _,_ _, r, p, p0, Î³_ . We consider the common
_âŸ¨S_ _A_ _âŸ©_
cases where the state space and action space are assumed to be continuous. p(st+1 _st, at) refers_
_S_ _A_ _|_
to the transition probability, i.e. the probability density of next state st+1 given the current state
_st and action at. when an agent chooses an action a in state s, the environment emits a reward_
_r(s, a) :_ [rmin, rmax] during the transition. The policy distribution Ï€(a _s) is presented as_
_S Ã— A â†’_ _|_
a distribution over the action space conditioned on any state. The distribution of state-action pairs
when navigating the environment with policy Ï€ could be defined as ÏÏ€(s, a), which is also referred
to as occupancy measure (Ho & Ermon, 2016), and the marginal distribution on the state space is
_ÏÏ€(s). With a predefined MDP, the notation trajectory Ï„Ï€ refers to the following Markov sequence_
_Ï„Ï€ = {(s0, a0), Â· Â· Â· (sT, aT )} generated by interacting with environment under policy Ï€. The discount_
factor is denoted as Î³. Standard reinforcement learning seeks to find the optimal policy that could
maximize the expected return, and the optimal solution could be deterministic (Sutton & Barto, 2011).
However, considering the exploration (Schulman et al., 2015), robustness (Ziebart, 2010) and multimodal objectives (Daniel et al., 2012) in real-world application scenarios, a stochastic/multi-modal
policy is desired. To address this desideratum, optimal maximum entropy policy (Ziebart et al., 2008;
Haarnoja et al., 2017) is defined as:


_Ï€[âˆ—]_ := argmax


EÏÏ€(st,at)[r(st, at) + Î±H[Ï€(Â·|st)]].


and the soft Bellman optimality equation (Ziebart, 2010) is defined as:
_Q[âˆ—](st, at) = rt + Î³Est+1_ [V _[âˆ—](st+1)],_ (1)
which gives the soft Bellman operator T _[Ï€]Q[Ï€](Â·) = Q[Ï€](Â·) and T_ _[Ï€]Â· := r(h) + Î³Ehâ€²âˆ¼p(sâ€²|h)Ï€(aâ€²|sâ€²)[Â·]_
and also referred to as Maximum Entropy RL(MERL). Ziebart (2010); Haarnoja et al. (2017) and
Haarnoja et al. (2018) show that the target of the policy update is given by the Q-function in an energybased form, Ï€(a|s) âˆ exp(âˆ’Q(s, a)/Î±). Due to the intractability in estimating the normalizing
constant Z(s) = exp(âˆ’Q(s, a)/Î±)da for continuous or large discrete action space, the energybased form poses difficulties in utilizing the policy such as sampling or density estimation, so various
methods use a standalone parametric policy modelR _Ï€Ï†(a_ _s) to distill from the energy function, and_
_|_
the MERL task is turned into iteratively updating the value model QÎ¸ and the policy model Ï€Ï†(a|s).

3 PARTICLE-BASED POLICY IMPROVEMENT

3.1 DISTRIBUTION MATCHING FOR POLICY IMPROVEMENT

We aim to update a policy model to match a better target p(a|s), which is demonstrated by particles.
We now formalize it as a distribution matching task under a general distribution metric/discrepancy
D[ ]. The first problem is that the policy model parameterizes a conditional distribution Ï€Ï†(a _s), so_

_Â·âˆ¥Â·_ _|_
we need to match the target p(a _s) by minimizing D(Ï€Ï†(a_ _s), p(a_ _s)), however, it is intractable for_
_|_ _|_ _|_


-----

any given s. To make the minimization practical, we note that in almost all modern policy update
tasks, we also have an associated state trajectory, e.g., experiences in a replay buffer in off-policy
RL, or simulated trajectories in inverse RL (Finn et al., 2016) and demonstrative trajectories in
imitation learning (Ho & Ermon, 2016). This state trajectory demonstrates a marginal distribution
on the state, Ï(s), which is widely assumed to be stationary among different policies within the
same MDP (Tsitsiklis & Van Roy, 1996; Haarnoja et al., 2017; 2018). Hence we instead minimize
D(Ï(s)Ï€Ï†(a _s), Ï(s)p(a_ _s)). When the joint distributions match, the conditionals also match. This_
_|_ _|_
idea has been exploited in other fields, such as expectation propagation algorithms (Minka, 2001)
for Bayesian inference. And we further provide discussions in the Appendix F. Along with the state
trajectory, we also have samples/particles from the augmented target, Ï(s)p(a|s). Particularly for existing MERL methods, they minimize the reverse KL divergence DKL(Ï€Ï†( _s), p(_ _s)) averaged over_

_Â·|_ _Â·|_
a state distribution Ï(s) which can be implemented by conducting sampling from the replay buffer:
EÏ(s)[DKL(Ï€Ï†(Â·|s), p(Â·|s)]. This objective can be reformulated as DKL(Ï(s)Ï€Ï†(a|s), Ï(s)p(a|s)),
corresponding to our proposed method with D instantiated as the reverse KL.

As stressed, this framework allows matching the distribution by minimizing more powerful metrics/discrepancies that do not have a particular propensity, like the reverse KL. To balance utility and
implementability, we consider the f -divergences DF and the Wasserstein distance (WD) DW as two
instances of D[Â·âˆ¥Â·] of the proposed framework.

**Minimizing f-divergences for policy updates.** The f -divergence is defined as DF (q, p) :=
Ep[F (q/p)], where F is a real-valued convex function with F (1) = 0. Although the forward
and reverse KL are also f-divergence instances, we do not consider them here since they would
introduce the respective propensity of the learned model. Nowozin et al. (2016) show a useful lower
bound of an f-divergence that benefits practical optimization:


DF (q, p) â©¾ sup


_DËœ_ _F (q, p; f_ ) := Eq[f (Â·)] âˆ’ Ep[F _[âˆ—](f_ (Â·))] (2)



where F _[âˆ—](t) := supx{xt âˆ’_ _F_ (x)} is the Fenchel conjugate of F . If the parametric model is defined
via a reparameterization h _qÏ†(h) : h = gÏ†(Ïµ) with Ïµ following a fixed and easy-to-sample_
_âˆ¼_
distribution q(Ïµ) like the standard Gaussian, the first term can also be estimated as Eq(Ïµ)[f (gÏ†(Ïµ))].
By also parameterizing f in some function class, the supremum can be estimated after optimizing
over f, which also tightens the lower bound. Thus minimizing _D[Ëœ]_ _F well serves as minimizing DF ._

**Minimizing WD for policy updates. The WD is defined as the minimal cost of transferring from**
one distribution to the other by a probabilistic transferring plan. It is shown to have an optimization
utility even when the two distributions do not have overlapping support (Arjovsky et al., 2017). Its
formulation under Kantorovich-Rubinstein duality makes it convenient to optimize:
DW(q, p) = _âˆ¥fsupâˆ¥Lâ©½1_ Eq[f ] âˆ’ Ep[f ] (3)

For distributions q, p with a bounded expectation on a Polish metric space (Villani, 2008, Particular
Case 5.16) (e.g., the common Euclidean space), where âˆ¥Â·âˆ¥L denotes the Lipschitz constant of a
function. To enforce the Lipschitz constant constraint, various implementations are proposed, e.g.
parameter clipping (Arjovsky et al., 2017), gradient penalty (Gulrajani et al., 2017) and spectral norm
/ hinge loss regularizations (Miyato et al., 2018).

**Framework** **1.** _The_ _Particle-based_ _Policy_ _Improvement_ _(ParPI)_ _framework,_
minÏ† D(Ï(s)Ï€Ï†(a|s), Ï(s)p(a|s)) can be used for the sub-task of updating policy in RL, as
_long as the improved policy can be demonstrated by particles. The process is shown in Fig. 1._

**Remark 1. The ParPI framework can be applied to policy update in SQL (Ziebart, 2010; Haarnoja**
_et al., 2017) and SAC (Haarnoja et al., 2018). In SQL, once the optimal Q-function Q[âˆ—]_ _is achieved,_
_the optimal policy is shown to be Ï€[âˆ—](a|s) = exp(Q[âˆ—](s, a)/Î±)/Z_ _[âˆ—](s) where Z_ _[âˆ—](s) is a normalizing_
_constant. In SAC, it is shown that Ï€[â€²](a|s) := exp(Q[Ï€](s, a)/Î±)/Z(s) is a better policy than Ï€_
_where Q[Ï€]_ _is the Q-function under policy Ï€. In both cases, the policy update target is given in an_
_energy-based form, whose particles can be drawn using MCMC algorithms._


-----

3.2 PARPI FOR MERL

The particle-based policy improvement (ParPI) Q-Networks ParPI
framework can be applied to any RL task where ParPI
a state-action trajectory demonstrates a better pol-icy. We now show in detail how it helps to up- Policy ğœ‹ğœ™(ğ‘|ğ‘ ) ğ‘„(ğ‘, ğ‘ ) ğ‘€ğ¶ğ‘€ğ¶
date policy in the MERL framework. As intro- Minimize discrepancy: ğ’Ÿ
duced, MERL (Ziebart et al., 2008; Ziebart, 2010; ğ‘
Haarnoja et al., 2017; Liu et al., 2017; Haarnoja ğ‘ 
et al.timal policy which also considers the entropy to, 2018; Zhang et al., 2018) defines the op- Replay Buffer Environments

Q-Networks ParPI

TD-update

ParPI

Policy ğœ‹ğœ™(ğ‘|ğ‘ ) ğ‘„(ğ‘, ğ‘ ) ğ‘€ğ¶ğ‘€ğ¶

Minimize discrepancy: ğ’Ÿ

ğ‘

ğ‘ 

Replay Buffer Environments

cover multiple optimalities and encourage explo- Figure 1: The Particle-based Policy Improveration. Based on the definition, a policy update tar- ment (ParPI) framework. The current Qget is given in the form p(a|s) âˆ exp(Q(s, a)/Î±), function provides a better policy in an energywhere Q(s, a) is either the optimal Q-function or the based form, whose particles can be drawn by
Q-function of the current policy, which can be esti- an MCMC algorithm. The policy model is
mated from simulated trajectories. These methods updated to match the better policy by miniupdate the policy model by minimizing the reverse mizing a broader class of distribution metrics
KL DKL(Ï(s)Ï€Ï†(a|s), Ï(s)p(a|s)). The merit of it is using the particles.
that estimating the troublesome normalizing constant
is not required in optimization, which is also widely
exploited in variational inference. However, it has been pointed out (HuszÃ¡r, 2015; Theis et al., 2016)
that minimizing the reverse KL promotes the mode-seeking behavior in finite optimization steps,
making Ï(s)Ï€Ï†(a _s) concentrate to one mode of the target Ï(s)p(a_ _s). This is because a substantial_
_|_ _|_
penalty can be incurred when Ï(s)Ï€Ï†(a _s) puts more mass to where Ï(s)p(a_ _s) is dilute, but a regular_
_|_ _|_
loss in the other way. The behavior is also observed in practice, and there are attempts to ameliorate
it in other fields, like variational inference (HernÃ¡ndez-Lobato et al., 2016; Li & Gal, 2017) As the
goal of MERL is to capture the diversity and multi-modality of policy, such behavior would weaken
the spirit of MERL.

The ParPI framework allows MERL to benefit from more powerful metrics/discrepancies, as long
as the trajectory samples are available. To draw from the target policy, we employ the Langevin
dynamics, which only requires an unnormalized density function of the target distribution.

**Sampling with Langevin Dynamics.** To minimize the general metrics/discrepancies above, samples from the target distribution ÏÏ€(s)p(a _s) are required. Fortunately, this can be done by subse-_
_|_
quently sampling from ÏÏ€(s) and p(a _s), and both can be implemented. Samples from ÏÏ€(s) can_
_|_
be drawn by simulating the (s, a) trajectory following the current policy Ï€(a|s) and environment
transition. Sampling from p(a|s) can be done by running dynamics-based MCMC algorithms, which
only require the unnormalized density, i.e. exp(Q(s, a)/Î±). From various particle-based inference
algorithms, we employ the Langevin dynamics (Roberts et al., 1996; Roberts & Stramer, 2002;
Welling & Teh, 2011) due to the low computational cost, whose transition is given by:
_a[(][i][+1)]_ = a[(][i][)] + Îµ log p(a[(][i][)] _s) +_ (0, 2ÎµI) = a[(][i][)] + (Îµ/Î±) _aQ(a[(][i][)], s) +_ (0, 2ÎµI) (4)
_âˆ‡_ _|_ _N_ _âˆ‡_ _N_

It has been shown to achieve Î´ precision in total variance (Dalalyan, 2017), Wasserstein distance (Durmus & Moulines, 2016) and KL divergence (Cheng & Bartlett, 2017) in O(1/Î´[2]) steps for strongly
log-concave densities. Note that although we can use MCMC to draw samples from the policy target,
the sampling is slow in deploying the policy, where we need to run Markov chains for every state
along the trajectory, so explicitly modeling policy Ï€Ï†(a _s) is still necessary._
_|_


**Value Function Update.** The proposed policy improvement rule described above only requires a
model for the state-action value function QÎ¸(s, a), so it can be applied with any value function update
method. The soft-Q learning method (Haarnoja et al., 2017) aims to update the value function towards
optimal by a temporal difference implementation based on the soft Bellman optimality equation 1:

1 2

_JQ(Î¸) = E(st,at)_ _QÎ¸ (st, at)_ _r (st, at) + log Est+1_ _p(st+1_ **st,at) [exp (V (st+1))]** (5)
_âˆ¼D_ 2 _âˆ’_ _âˆ¼_ _|_

 

  


-----

The soft actor-critic (SAC) method (Haarnoja et al., 2018) instead updates the state-value function
model towards the state-action value of the current policy which gives a better policy target as
following:

1 2

_JV (Ïˆ) = Est_ _Ï(s)_ _VÏˆ (st)_ Eat _Ï€Ï† [QÎ¸ (st, at)_ log Ï€Ï† (at **st)]**
_âˆ¼_ 2 _âˆ’_ _âˆ¼_ _âˆ’_ _|_

 

 1  2

_JQ(Î¸) = E(st,at)_ _QÎ¸ (st, at)_ _r (st, at) + Î³Est+1_ _p(st+1_ **st,at)** _V Â¯Ïˆ_ [(][s][t][+1][)]
_âˆ¼D_ 2 _âˆ’_ _âˆ¼_ _|_

 

    (6)

And we provide convergence analysis of ParPI in Appendix E.

3.3 PARPI FOR OFFLINE RL

Another important advantage of ParPI lies in that it is naturally compatible with the offline RL setting.
In the offline RL setting, we only have access to some static dataset, i.e. = _s, a, r, s0_, which is
_D_ _{_ _}_
usually collected by the behavior policy Ï€B. And during the learning procedure, the algorithm could
not interact further with the environment. Thus the key challenge for offline RL algorithms is to
generalize beyond the state and action support of the offline data. There are two mainstream offline RL
algorithms: the value function regularization methods and the policy constrained methods. The value
function regularization methods generally refers to the methods which conservatively regularized the
estimation of value function in either model-free or model based fashion, e.g. MOPO (Yu et al., 2020)
and CQL (Kumar et al., 2020). Such methods do not have requirements on the policy improvement
procedure, thus we could just directly change the vanilla policy optimization algorithm, e.g. entropy
regularized update (Haarnoja et al., 2018), into ParPI as illustrated in Remark 1.

We mainly discuss the relationship between the policy constrained based offline RL algorithm and
ParPI . The policy constrained offline RL algorithm tends to stabilize the training by adding constraints
into the policy improvement procedure. Without loss of generality, the constrained policy objective
could be then formalized as:
_Ï€Ï† = arg max_ (7)
_Ï€Ï†_ [E][s][âˆ¼D][,a][âˆ¼][Ï€][Ï†][(][a][|][s][)][[][Q][(][s, a][)]][ s.t.][ D][ (][Ï€][Ï†][(][a][|][s][)][, Ï€][Î²][(][a][|][s][))][ â‰¤] _[Îµ]_

Here D(Â·, Â·) stands for some discrepancy measure between two distributions, and Ï€Î² is the behavior
policy. Two main types of policy constraints are distributional constraints and support constraint. The
distribution constraints (Nair et al., 2020; Wang et al., 2020; Peng et al., 2019; Wu et al., 2019) are
generally more strict, which could provide a stable supervision signal yet limit the search space for the
learned policy. And they are usually implemented by the minimization of density-based divergence,
_e.g. KL divergence and f_ -divergence. In comparison, the support constraints are relatively loose and
only require the support of learned distribution to be equal to the behavior distribution. The support
constraint enlarges the search space while it could bring unstable optimization when the support
distance is large. Correspondingly, the support constraint compares samples regardless of density
which is in line with the Integral Probability Metric (IPM), e.g. Maximum Mean Discrepancy and
Wasserstein distance.

We then show that within a specific choose of divergence in ParPI, we could conjoin the benefit of
both the distribution constraints and support constraint. With the D(Â·, Â·) as reverse KL divergence,
the Lagrangian duality of Eq. 7 could be formulated as: Ï€Ï† = arg maxÏ€Ï† Esâˆ¼D,aâˆ¼Ï€Ï†(a|s)[Q(s, a)] +
_Î±DKL (Ï€Ï†(a|s), Ï€Î²(a|s)) where Î± is the Lagrangian multiplier. The above formulation has the_
closed-form solution Ï€Ï†âˆ— (a _s)_ _Ï€Î²(a_ _s) exp(Q(s, a)/Î±) and we could then use ParPI to optimize_
_|_ _âˆ_ _|_
_Ï€Ï† towards Ï€Ï†[âˆ—]. Particularly, the âˆ‡_ log p(a[(][i][)]|s) in Eq. 4 is âˆ‡Ï€Î²(a|s) exp(Q(s, a)/Î±) in this case
and the initial state of langevin dynamics is set as the behavior policy Ï€Î². And the Wasserstein
distance is used in particle optimization. Then we have the following fact:

**Remark 2. With limited steps of langevin dynamics, ParPI is approximately solving the optimization**
_problem:_


_Ï†k+1 = arg min_
_Ï†_ [KL(][Ï€][Ï†][(][a][|][s][)][âˆ¥] _[Ï€][Î²][(][a][|][s][) exp(]Z[Q][(][s, a][)][/Î±][)]_


) + Î±W2[2] [(][Ï€][Ï†][, Ï€][Î²][) +][ Î³W]2[ 2] [(][Ï€][Ï†][, Ï€][Ï†]k [)][ (8)]


_Here Z stands for the normalizing constant and the W2[2][(][Â·][,][ Â·][)][ is the][ W][2]_ _[distance.]_


-----

We leave the formal discussion of Remark. 2 in the Appendix D. The first two terms in Eq. 8
correspond to the distribution constraint and the support constraint. And the last term is a specific
property introduced by ParPI which regularizes the update of policy to be close to the current
policy. The intuition ensembles the trust region methods such as TRPO (Schulman et al., 2015) and
PPO (Schulman et al., 2017) while the regularization of ParPI is conducted according to Wasserstein
distance instead of KL divergence.

3.4 PRACTICAL OPTIMIZATION WITH PARPI

**Estimating and Minimizing the Discrepancy Measure The occupancy measure from the current**
policy can be sampled by firstly sampling states from replay buffer and take action under the current
policy Ï€Ï†(a _s) on each state, we carefully overload the notation and refer to the corresponding_
_|_
(s, a) distribution as ÏÏ†. As introduced in Eq.4, the occupancy measure implied by the QÎ¸ can
approximately be retained by finite steps in Langevin Dynamics. In practice, we use the samples
from ÏÏ† as the initial state, and the corresponding distribution is referred to as ÏQ.

With the samples from ÏÏ† and ÏQ available, we parameterize the discriminator(critic) as fÏ‰ to
compute the objective in Eq.2 and Eq.3. More specifically, the Lipschitz constraint in minimizing
Wasserstein Distance(WD) cases is imposed by spectral normalization following (Miyato et al., 2018).
The weight matrix W in the discriminator network is regularized as _W[Â¯]_ SN := _Ïƒ(1W )_ _[W][ where][ Ïƒ][(][W]_ [)]

denotes the largest singular value of W . Empirically, the amortization optimization procedure of
finding the appropriate Ï† of policy model can lead to an intractable problem due to the complexity of
the space S Ã— A which leashes the very purpose of function approximation. To counter this obstacle,
following the recent success in apprenticeship learning and imitation learning (Ho & Ermon, 2016),
we add an entropy regularizer to the objective of the policy(actor) model. Without loss of generality,
we reparameterize the policy model (Ï€Ï†(at _st)) as a neural network transformation:_
_|_

so Ï€Ï†(a _s) = pÏµ_ _aâˆ’ÏƒÏ†Âµ(Ï†s()s)_ _/ÏƒÏ†a(ts =). Before each policy update, we first update the discrimina- ÂµÏ†(st) + ÏƒÏ†(st)Ïµt, Ïµt âˆ¼_ _p(Ïµ),_
_|_

tive(critic) function  _fÏ‰ for estimating_ _D[Ëœ]_ _F or DW using particles of the target policy_ (a[â€²]t[, s][t][)][}][t][, by]
_{_
maximizing

EÏ(s)Ï€Ï†(a _s)[log Sigm(fÏ‰(s, a))]_ EÏ(s)p(a _s)[log Sigm(1_ _fÏ‰(s, a))]_
_|_ _âˆ’_ _|_ _âˆ’_

ï£±

_Jf (Ï‰):=ï£´ï£´ï£´ï£´ï£´ï£´ï£²Eâ‰ˆÏ(slog Sigm)Ï€Ï†(a_ _s)[fÏ‰f(Ï‰s, a(st)], ÂµÏ†(EstÏ) +(s)p Ïƒ(aÏ†s()s[ft)Ï‰Ïµ(ts, a)_ _âˆ’)]log Sigm(1 âˆ’_ _fÏ‰(st, a[â€²]t[))][,]_ for JSD, (9)

_|_   _âˆ’_ _|_ 

take Jensen-Shannon Divergence(JSD) as an example of f-divergence andï£´ï£´ï£´ï£´ï£´ï£´ï£³ _â‰ˆ_ _fÏ‰(st, ÂµÏ†(st) + ÏƒÏ†(st)Ïµt) âˆ’_ _fÏ‰(st, a[â€²]t[)][,]_ Sigm is the sigmoid func-for WD,
tion. After updating Ï‰, we update the policy model by minimizing JÏ€(Ï†) := âˆ’EÏ(s)[H[Ï€Ï†(Â·|s)]] +
_D(Ï(s)Ï€Ï†(a_ _s), Ï(s)p(a_ _s)) as the final objective._
_|_ _|_

**Squashing Correction and Sampling in Latent Space Following the default setting in (Haarnoja**
et al., 2018), we model the action distribution with an unbounded Gaussian. As the experiment
settings always limit the action space in a finite interval, we apply tanh on the samples from Gaussian.
However, if we conduct the Langevin Dynamics(Eq. 4) in the bounded support, e.g. (âˆ’1, 1), we
empirically find the method is susceptible to the selection of noise level Ïµ and some numerical issues
could raise due to that the produced samples could be out of the boundary. To alleviate the above
issues, we do the MCMC steps in the raw action space before squashing. Here, we slightly overload
the notation by denoting the random variable of the raw output as u which has infinite support and
the variable of corrected action is a = tanh(u). With the stationary distribution in the action space as
exp(Q(s, a)/Î±), we could get the corresponding stationary distribution in u space by applying the
change of variable formula. The score âˆ‡u log p(u|s) used in Langevin Dynamics, i.e. Eq. 4, is:

_u log p(u_ _s)=_ _u(Q(s,tanh(u))/Î±+2_ _i=1_
_âˆ‡_ _|_ _âˆ‡_ _[âˆ‡][u][ log(1][âˆ’][(][tanh][(][u][i][))]_
where ui denotes the i-th element of u.

[P][D]

3.5 DISCUSSION AND RELATED WORKS

The final objective is related to the regularized variants of apprenticeship learning algorithms in
imitation learning (Ho et al., 2016; Syed et al., 2008). While in our setting, the corresponding target
occupancy measure is defined implicitly also changes along with the optimization of Q-function


-----

**Algorithm 1 ParPI:Particle-Based Policy Improvement**

1: Input: Replay buffer D. Policy model Ï€Ï†(a|s), parameterized Q-function QÎ¸, state function VÏˆ
and discriminator(critic) fÏ„ .

2: Set the step size Ïµ, the length of MCMC steps K and the total iterations T .
3: while not converged do
4: Sample a batch of triple {(s, a, r)t}t[m]=1 [from the replay buffer][ D][.]

5: Conducting update on the QÎ¸:

7:6:8: _ÏˆÎ¸Sample a batch of statei â† â†_ _ÏˆÎ¸i âˆ’ âˆ’Î»Î»VQ âˆ‡âˆ‡ÏˆÎ¸iJJVQ ( (ÏˆÎ¸ {)i)s for it}t[m]=1 âˆˆ{[from]1, 2[ D]}_ [.] Eq. 6 # ParPIEq. 5 or Eq. + SAC 6

9: Draw action under current policy and get the state-action pairs _st, aÏ€Ï†_ _t=1_
_{_ _}[m]_

10: Fixing the state, and conduct Langevin dynamics with QÏ† following equation 4 to acquire the
updated state-action pairs {st, Ë†aÏ€Ï† _}t[m]=1[.]_

11: _Ï‰ â†_ _Ï‰ âˆ’_ _Î»f_ _âˆ‡Ï‰Jf (Ï‰)_ Eq. 9.

12: _Ï† â†_ _Ï† âˆ’_ _Î»Ï€âˆ‡Ï†JÏ€ (Ï†)._

13: end while


which indicates a more difficult optimization problem. Liu et al. (2017) consider a particle-based
policy update, but rather than the target policy, their particles represent the posterior of the policy
model parameter, and the update rule is also based on minimizing the reverse KL. Zhang et al.
(2018) also propose a particle-based method for policy update by utilizing the minimal movement
discretization of the Wasserstein gradient flow to minimize, but still the reverse KL. Many recent
methods learn energy-based models especially with deep models (Du & Mordatch, 2019; Song
& Ermon, 2019; Li et al., 2019a), while we instead learn a parametric model to approximate a
distribution defined by an energy function for efficient prediction. We note that there are other
methods to do so like the amortized MCMC methods (Li et al., 2017; 2019b; Feng et al., 2017),
but the stationality-oriented objective may not be efficient for directly matching the target, and we
cover more scenarios where only the particles are available. Our method is also related to the policy
constraint methods in the offline RL scenario. Wu et al. (2019) and Kumar et al. (2019) proposed to
constrained the learned policy with f -divergence or IPM, while their methods differ from us in the
fact that the target occupancy could be directly acquired from the static dataset and the divergence
minimization term is only served as a regularizer.

Ant-V3 12000 HalfCheetah-v3 3500 Hopper-v3

5000 10000 3000

4000 8000 2500

3000 6000 2000

Average Return 2000 Average Return 4000 Average Return 15001000

1000 2000 500

0 0 0

0 0.2 0.4 Humanoid-v3 0.6 0.8 1.0 0 0.2 0.4 Walker2d-v3 0.6 0.8 1.0 0 0.2 0.4# frames (million)0.6 0.8 1.0

5000 5000 SQL

4000 4000 SACParPIJSD+SQL

30002000 30002000 ParPIParPIJSDWD+SQL+SAC

Average Return 10000 Average Return 10000 ParPIParPIParPIWDRKLRKL+SAC+SQL+SAC

0 0.2 0.4# frames (million)0.6 0.8 1.0 0 0.2 0.4# frames (million)0.6 0.8 1.0


Figure 2: Training curves on continuous control benchmarks (Todorov et al., 2012). Our ParPI
framework improves the performance of base RL algorithms SQL and SAC, and achieves the best
results consistently across all tasks. We average the return over the past 100 episodes, where the solid
lines indicate the mean and shaded areas indicate the standard deviation.
4 EXPERIMENTS

We conduct extensive experiments on both online scenarios and offline scenarios. Besides, we delve
deeply into how the component would affect the performance of ParPI on task HalfCheetah-v3.

4.1 ONLINE SCENARIOS

We choose five well-known representative benchmark tasks with continuous control (Ant, Hopper,
Humanoid, HalfCheetah and Walker2d) from Mujoco (Todorov et al., 2012). The method is compared


-----

to SQL (Haarnoja et al., 2017) and SAC (Haarnoja et al., 2018), which represents the SoTA modelfree off-policy methods for continuous control tasks. We choose Reverse KL divergence(RKL) and
Jensen-Shannon Divergence(JSD) out of the f -divergence family. For the Wasserstein distance, we
adopt the hinge loss (Miyato et al., 2018). And the Lipschitz constraint is enforced through gradient
penalty (Gulrajani et al., 2017) and spectral normalization (Miyato et al., 2018). The proposed
method is robust, We keep all of ParPI â€™s hyper-parameter constant across all tasks in all domains for
simplicity. For hyper-parameters and additional implementation details, please refer to Appendix G.
All our experiments are conducted on machines equipped with Nvidia P100 GPUs using five different
random seeds.

As shown in Figure 2,we see that ParPI yields significant improvements in both performance and
sample efficiency across a wide range of environments. Comparing learning curves of different
methods, ParPI improves both SQL and SAC by a significant boost with almost no hyper-parameter
tuning effort. One observation is that the ParPI with SQL shows more improvement then with SAC.
The reason lies in the fact that the policy implied by the Q function in SAC is also shifted along with
the training, which could be the unstable element for the optimization of ParPI . And such situation
could be alleviated by optimizing the Q network for more steps than the policy network in each
update. Here we only involve the same training configurations for fair comparison. Interestingly,
we found that the Reverse KL divergence with ParPI has still consistently outperformed the original
SQL and SAC which optimized the reverse KL divergence in the primal form. This could be due to
that the intermediate target distribution constructed by MCMC smooths the density ratio estimation
procedure (Rhodes et al., 2020) and favors the dual form optimization. The fact further justifies the
effectiveness of proposed framework.

4.2 OFFLINE SCENARIOS

To further demonstrate the efficacy of ParPI, we then experiment with the D4RL offline reinforcement
learning benchmark (Fu et al., 2020), including three environments(hopper, walker2d, and halfcheetah), which consist of five different logged data-sets types (random, medium, medium-replay, expert,
and medium-expert), in a total of 15 sub-tasks. These datasets are generated by an agent using SAC
in rlkit (Pong, 2020), with each dataset containing 1 million time-steps of environment interaction.
As we discussed in Sec. 3.3, we study the performance of ParPI from the combination with both
value function regularized methods and policy constraint methods. Particularly, we implement ParPI
on top of MOPO (Yu et al., 2020) and BRAC (Wu et al., 2019)(see Appendix H), and the discrepancy
measure in ParPI is set as wasserstein distance. We follow the same schema as the D4RL paper (Fu
et al., 2020) to calculate the normalized return. And the result of 15 sub-tasks is presented in Tabel 1.
Compared to the baseline, our approach achieves better performance in most environments, showing
the power of ParPI .

4.3 COMPONENT ANALYSIS AND COMPUTATIONAL EFFICIENCY

We conduct ablation study of different discrepancy measures and MCMC parameters on the task
_HalfCheetah-v3 with the same settings as ParPI + SQL (see 1)._

**Discrepancy Measure We demonstrate the characteristics of different distribution metrics for policy**
optimization including: JSD, original WD and the hinge loss. Here we retain all the hyper-parameters
of our method and the choice of the metric is the only difference. Interesting results can be observed
from Figure 3: while the entropy of learned policy remains almost the same for different metrics, the
standard deviation varies. The large standard deviation indicates that the learned policy tends to be
more certain on frequently observed state regions and does not mislead in other regions. According
to the empirical performance, we provide the following guidelines for the future usage of ParPI
: in general the Wasserstein distance could be a more desirable discrepancy measure and it also
achieves the best performance in most tasks; Jensen-Shannon divergence, though it is reported to
show some tendency of mode seeking (Theis et al., 2015), it also effectively ameliorates the problem
as it consistently outperforms the exclusive KL. Hence, we suggest that ParPI with W-distance could
be the generally recommended setting. When the dimension of action is not large, JSD could also
show good performance.

**Langevin Dynamics We examine the choice of the step length K and step size Ïµ of MCMC steps.**
We observe that the sample that have higher Q-value could be fetched along with the MCMC steps,


-----

Table 1: Results for D4RL benchmarks. Each number is the normalized score computed as (score random policy score) / (expert policy score - random policy score) of the policy at the last iteration of
training, averaged over 5 random seeds, Â± standard deviation. Results of MOPO (Yu et al., 2020),
BEAR (Wu et al., 2019) and CQL (Kumar et al., 2020) are reported from their respective papers.
Remaining results are taken from the D4RL white-paper (Fu et al., 2020).

|Offline Types|Tasks|ParPIWD+BRAC ParPIWD+MOPO|MOPO|CQL|BEAR BRACv|BC|
|---|---|---|---|---|---|---|
|random|Walker2d 9.9Â±3.2 15.2Â±8.2 13.6Â±2.6 7 7.3 1.9 1.6 HalfCheetah 31.5Â±1.3 36.2Â±1.5 35.4Â±2.5 35.4 25.1 31.2 2.1 Hopper 10.2Â±4.7 11.6Â±0.8 11.7Â±0.4 10.8 11.4 12.2 9.8||||||
|medium|Walker2d 79.1Â±7.2 39.4Â±13.2 17.8Â±19.3 79.2 59.1 81.1 6.6 HalfCheetah 49.6Â±2.3 45.8Â±6.2 42.3Â±1.6 44.4 41.7 46.3 36.1 Hopper 89.2Â±9.5 64.1Â±9.3 28.0Â±12.4 58 52.1 31.1 29.0||||||
|medium-replay|Walker2d 41.8Â±7.9 38.8Â±12.6 39.0Â±9.6 26.7 19.2 0.9 11.3 HalfCheetah 39.9Â±1.7 54.8Â±3.1 53.1Â±2.0 46.2 38.6 47.7 38.4 Hopper 42.1Â±8.1 64.5Â±27.1 67.5Â±24.7 48.6 33.7 0.6 11.8||||||
|medium-expert|Walker2d 105.6Â±3.4 91.2Â±11.2 44.6Â±12.9 98.7 40.1 81.6 6.4 HalfCheetah 93.9Â±12.2 104.1Â±8.9 63.3Â±38.0 62.4 53.4 41.9 35.8 Hopper 135.2Â±8.1 78.4Â±18.1 23.7Â±6.0 111 96.3 0.8 111.9||||||
|expert|Walker2d 108.0Â±3.1 99.4Â±10.1 - 153.9 106.1 0 125.7 HalfCheetah 151.4Â±6.7 109.4Â±7.1 - 104.8 108.2 -1.1 107 Hopper 128.8Â±2.3 84.1Â±14.4 - 109.9 110.3 3.7 109||||||



(a) Entropy of learnt policy (b) The hyper-parameter of Langevin Dynamic analysis.

Figure 3: Study of component of ParPI on HalfCheetah-v3. Every experiment in ablation study was
conducted using three different seeds.

and a longer MCMC run is or larger step size is, the sample has higher Q value as illustrated by
Figure 3.

Table 2: Wall-clock Time Comparison on Ant

**Training Time Efficiency To better understand**
the trade-off of ParPI on performance and training efficiency, we conduct a case study on walk
clock time in Ant as shown in Table. 2. Here
seconds to return indicates how much time is

|Seconds to Return|1000|2000|3000|
|---|---|---|---|
|ParPI +SAC WD SAC|9463 5953|11548 10006|14544 12323|


Seconds to Return 1000 2000 3000

ParPIWD+SAC 9463 11548 14544

SAC 5953 10006 12323

needed to gain the corresponding level of returns. ParPI would take slight more time as it takes additional time for MCMC steps. Moreover, the
wall-clock time is usually not the main obstacle comparing to the environment time, i.e., time spent
during interaction with the environment, in online settings and sample efficiency for offline settings.

5 CONCLUSIONS AND DISCUSSION

We devise a novel algorithms framework for model-free reinforcement learning, ParPI, a particlebased discrepancy/metric minimization framework for policy improvement, which can leverage the
full potential of stochastic policy by enable the broad family of divergence and discrepancy, such
as f -divergences and IPM-based metric. Our experiments on both online and offline settings show
that the baseline algorithms benefit from the ParPI in that they achieve state-of-the-art performance
with less hyper-parameter tuning efforts. ParPI accumulates many desirable properties: robustness,
stochasticity, and sample efficiency. Furthermore, ParPI exhibits a stabilized training and has been
shown to be time and sample efficient as compared to state-of-the-art approaches. Moreover, the
ParPI is model invariant, allowing it adapt to different RL task settings.


-----

REFERENCES

MartÃ­n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for
large-scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and
_Implementation ({OSDI} 16), pp. 265â€“283, 2016._

OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew,
Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning
dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3â€“20,
2020.

Martin Arjovsky, Soumith Chintala, and LÃ©on Bottou. Wasserstein GAN. _arXiv preprint_
_arXiv:1701.07875, 2017._

Xiang Cheng and Peter Bartlett. Convergence of Langevin MCMC in KL-divergence. arXiv preprint
_arXiv:1705.09048, 2017._

Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.

Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave
densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3):
651â€“676, 2017.

Christian Daniel, Gerhard Neumann, and Jan Peters. Hierarchical relative entropy policy search. In
_Artificial Intelligence and Statistics, pp. 273â€“281, 2012._

Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In
_Advances in Neural Information Processing Systems 32, pp. 3603â€“3613, 2019._

Alain Durmus and Eric Moulines. High-dimensional Bayesian inference via the unadjusted Langevin
algorithm. arXiv preprint arXiv:1605.01559, 2016.

Yihao Feng, Dilin Wang, and Qiang Liu. Learning to draw samples with amortized Stein variational
gradient descent. arXiv preprint arXiv:1707.06626, 2017.

Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control
via policy optimization. In International conference on machine learning, pp. 49â€“58, 2016.

Justin Fu. rail-berkeley/d4rlevaluations, 2020. URL [https://github.com/](https://github.com/rail-berkeley/d4rl_evaluations)
[rail-berkeley/d4rl_evaluations.](https://github.com/rail-berkeley/d4rl_evaluations)

Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.

Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.

Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for
robotic manipulation with asynchronous off-policy updates. In Robotics and Automation (ICRA),
_2017 IEEE International Conference on, pp. 3389â€“3396. IEEE, 2017._

Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of Wasserstein gans. In Advances in neural information processing systems, pp.
5767â€“5777, 2017.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In Proceedings of the 34th International Conference on Machine
_Learning-Volume 70, pp. 1352â€“1361. JMLR. org, 2017._

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290,
2018.


-----

JM HernÃ¡ndez-Lobato, Y Li, M Rowland, D HernÃ¡ndez-Lobato, TD Bui, and RE Turner. Black-box
_Î±-divergence minimization. In Proceedings of the 33rd International Conference on Machine_
_Learning, volume 48, pp. 1511â€“1520. International Machine Learning Society, 2016._

Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in neural
_information processing systems, pp. 4565â€“4573, 2016._

Jonathan Ho, Jayesh Gupta, and Stefano Ermon. Model-free imitation learning with policy optimization. In International Conference on Machine Learning, pp. 2760â€“2769, 2016.

Ferenc HuszÃ¡r. How (not) to train your generative model: Scheduled sampling, likelihood, adversary?
_arXiv preprint arXiv:1511.05101, 2015._

Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokkerâ€“planck
equation. SIAM journal on mathematical analysis, 29(1):1â€“17, 1998.

Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Neural Information Processing Systems (NeurIPS),
2019.

Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.

Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
_arXiv preprint arXiv:1805.00909, 2018._

Chongxuan Li, Chao Du, Kun Xu, Max Welling, Jun Zhu, and Bo Zhang. To relieve your headache
of training an mrf, take advil. In International Conference on Learning Representations, 2019a.

Chunyuan Li, Ke Bai, Jianqiao Li, Guoyin Wang, Changyou Chen, and Lawrence Carin. Adversarial
learning of a sampler based on an unnormalized distribution. In The 22nd International Conference
_on Artificial Intelligence and Statistics, pp. 3302â€“3311. PMLR, 2019b._

Yingzhen Li and Yarin Gal. Dropout inference in bayesian neural networks with alpha-divergences. In
_Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2052â€“2061._
JMLR. org, 2017.

Yingzhen Li, Richard E Turner, and Qiang Liu. Approximate inference with amortised MCMC.
_arXiv preprint arXiv:1702.08343, 2017._

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
_preprint arXiv:1509.02971, 2015._

Chang Liu, Jingwei Zhuo, and Jun Zhu. Understanding mcmc dynamics as flows on the wasserstein
space. In International Conference on Machine Learning, pp. 4093â€“4103. PMLR, 2019.

Yang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng. Stein variational policy gradient. In 33rd
_Conference on Uncertainty in Artificial Intelligence, UAI 2017, 2017._

Thomas Peter Minka. A family of algorithms for approximate Bayesian inference. PhD thesis,
Massachusetts Institute of Technology, Cambridge, 2001.

Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.

Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang,
Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan, et al. Ray: A distributed framework
for emerging {AI} applications. In 13th {USENIX} Symposium on Operating Systems Design and
_Implementation ({OSDI} 18), pp. 561â€“577, 2018._

Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.


-----

Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Advances in neural information processing systems,
pp. 271â€“279, 2016.

Georg Ostrovski, Marc G Bellemare, AÃ¤ron Oord, and RÃ©mi Munos. Count-based exploration with
neural density models. In International conference on machine learning, pp. 2721â€“2730. PMLR,
2017.

Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.

Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell,
Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464,
2018.

[Vitchyr Pong. vitchyr/rlkit, 2020. URL https://github.com/vitchyr/rlkit.](https://github.com/vitchyr/rlkit)

Benjamin Rhodes, Kai Xu, and Michael U Gutmann. Telescoping density-ratio estimation. arXiv
_preprint arXiv:2006.12204, 2020._

Gareth O Roberts and Osnat Stramer. Langevin diffusions and Metropolis-Hastings algorithms.
_Methodology and computing in applied probability, 4(4):337â€“357, 2002._

Gareth O Roberts, Richard L Tweedie, et al. Exponential convergence of Langevin distributions and
their discrete approximations. Bernoulli, 2(4):341â€“363, 1996.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889â€“1897, 2015.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In Advances in Neural Information Processing Systems, pp. 11895â€“11907, 2019.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2011.

Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear programming. In Proceedings of the 25th international conference on Machine learning, pp. 1032â€“1039,
2008.

L Theis, A van den Oord, and M Bethge. A note on the evaluation of generative models. In
_International Conference on Learning Representations (ICLR 2016), pp. 1â€“10, 2016._

Lucas Theis, AÃ¤ron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. arXiv preprint arXiv:1511.01844, 2015.

Emanuel Todorov. General duality between optimal control and estimation. In 2008 47th IEEE
_Conference on Decision and Control, pp. 4286â€“4292. IEEE, 2008._

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026â€“5033.
IEEE, 2012.

JN Tsitsiklis and B Van Roy. An analysis of temporal-difference learning with function approximationtechnical. Report LIDS-P-2322). Laboratory for Information and Decision Systems, Massachusetts
_Institute of Technology, Tech. Rep., 1996._

CÃ©dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.

Ziyu Wang, Alexander Novikov, Konrad ZoÅ‚na, Jost Tobias Springenberg, Scott Reed, Bobak[Ë™]
Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. arXiv preprint arXiv:2006.15134, 2020.


-----

Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
_Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681â€“688,_
2011.

Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
_arXiv preprint arXiv:1911.11361, 2019._

Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and
Tengyu Ma. Mopo: Model-based offline policy optimization. arXiv preprint arXiv:2005.13239,
2020.

Ruiyi Zhang, Changyou Chen, Chunyuan Li, and Lawrence Carin. Policy optimization as wasserstein
gradient flows. In International Conference on Machine Learning, pp. 5737â€“5746. PMLR, 2018.

Brian D Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal
entropy. 2010.

Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In Aaai, volume 8, pp. 1433â€“1438. Chicago, IL, USA, 2008.


-----

A PARTICLE IMPROVEMENT WITH LANGEVIN DYNAMICS

As in the practical implementation, only the finite-step Langevin dynamics could be conducted.
Fortunately, the property of MCMC guarantees the improvement of the particle distribution. With the
_p denotes the unique stationary distribution, e.g., the distribution implied by the Q-function in SQL_
and SAC. qt and qt 1 refer to the distribution which is implicitly implied by an initial distribution
_âˆ’_
and t or t âˆ’ 1 steps Langevin dynamics, the following monotonic property is satisfied:
DKL(qt||p) â‰¤ DKL(qtâˆ’1||p). (10)
And qt converges to the stationary distribution p as t â†’âˆ. The above proposition is the direct result
of the following lemma, we also provide the proof here for the completeness.

**Lemma 1. Cover & Thomas (2012) Let q and r be two distributions for z0. Let qt and rt be**
_the corresponded distributions of state zt at time t, induced by the transition kernel K. Then_
DKL[qt||rt] â‰¥ DKL[qt+1||rt+1] for all t â‰¥ 0.

_Proof._


log _[q][t][(][z][t][)]_

_rt(zt)_


DKL[qt||rt] = Eqt


= Eqt(zt) (zt+1 **_zt)_** log _[q][t][(][z][t][)][K][(][z][t][+1][|][z][t][)]_
_K_ _|_ _rt(zt)_ (zt+1 **_zt)_**

 _K_ _|_ 

= Eqt+1(zt+1)qt+1(zt **_zt+1)_** log _[q][t][+1][(][z][t][+1][)][q][(][z][t][|][z][t][+1][)]_
_|_ _rt+1(zt+1)r(zt_ **_zt+1)_**

 _|_ 

= DKL[qt+1||rt+1] + Eqt+1 DKL[qt+1(zt|zt+1)||rt+1(zt|zt+1)].


B POLICY IMPROVEMENT THEOREM FOR PARPI

Given the objective function as the expected discounted sum of rewards, the policy improvement
theorem corresponds to how policies can be improved monotonically. Similar theorems have been
derived under the maximum entropy framework, i.e., SAC Haarnoja et al. (2018) and SQL Haarnoja
et al. (2017). In this section, we shows that optimization of the policy with ParPI does not hurt the
monotonic property.

B.1 PARPISQL

In the SQL-version of ParPI, considering the original policy improvement theorem of SQL:

**Theorem 1. Haarnoja et al. (2017) Given a policy Ï€, define a new policy ËœÏ€ as**
_Ï€Ëœ(Â·|s) âˆ_ exp _Q[Ï€]soft[(][s][,][ Â·][)]_ _,_ _âˆ€s_
_Assume that throughout our computation, Q is bounded and_ exp(Q(s, a))da is bounded for any s
  
_(for both Ï€ and ËœÏ€). Then we have QÏ€soft[Ëœ]_ [(][s][,][ a][)][ â‰¥] _[Q]soft[Ï€]_ [(][s][,][ a][)][âˆ€][s][,][ a][.]
R

It should be noticed that there is no constraint on the policy optimization in Theorem. 1, hence we
could directly replace the original reverse KL minimization with ParPI without hurting the policy
improvement theorem.

B.2 PARPISAC


In the situation of ParPISAC, we need to generalize the previous objective:



[))]






_Ï€Ï† (_ **st)**

_Â·|_ _âˆ¥_ [exp (]Z[Q]Î¸ ([Î¸][ (]st[s])[t][,][ Â·][))]


_JÏ€(Ï†) = Est_
_âˆ¼D_



_JÏ€(Ï†) = Est_
_âˆ¼D_


DKL


to


_JÏ€(Ï†) = Est_ D _Ï€Ï† (_ **st)**
_âˆ¼D_ _Â·|_ _âˆ¥_ [exp (]Z[Q]Î¸ ([Î¸][ (]st[s])[t][,][ Â·][))]

 

To make it clear, we follow the proof provided in Haarnoja et al. (2018):


-----

**Lemma 2.JÏ€(Ï†) := D Let(Ï(s Ï€)Ï€oldÏ†( âˆˆa** _sÎ ), Ï and let(s)p(a Ï€s))new. Then be the optimizer of the minimization problem defined in Q[Ï€][new]_ (s, a) _Q[Ï€][old](s, a) for all (s, a)_ _with_
_|_ _|_ _â‰¥_ _âˆˆS Ã— A_
_|A| < âˆ_

_Proof. Let Ï€old_ Î  and let Q[Ï€][old] and V _[Ï€][old]_ be the corresponding soft state-action value and soft
state value, and let âˆˆ _Ï€new be defined as_
_Ï€new (_ **st) = arg min**

_Â·|_ _Ï€[â€²]_ Î  [D][KL][ (][Ï€][â€²][ (][Â·|][s][t][)][ âˆ¥] [exp (][Q][Ï€][old][ (][s][t][,][ Â·][)][ âˆ’] [log][ Z] _[Ï€][old][ (][s][t][)))]_
_âˆˆ_

= arg min
_Ï€[â€²]_ Î  _[J][Ï€][old][ (][Ï€][â€²][ (][Â·|][s][t][))]_
_âˆˆ_

Note thatHence _JÏ€old (Ï€new (Â·|st)) â‰¤_ _JÏ€old (Ï€old (Â·|st)), since we can always choose Ï€new = Ï€old âˆˆ_ Î .
Eat _Ï€new [log Ï€new (at_ **st)** _Q[Ï€][old]_ (st, at) + log Z _[Ï€][old]_ (st)]
_âˆ¼_ _|_ _âˆ’_ _â‰¤_
Eat _Ï€old [log Ï€old (at_ **st)** _Q[Ï€][old]_ (st, at) + log Z _[Ï€][old]_ (st)] (11)
_âˆ¼_ _|_ _âˆ’_
Applying soft Bellman Equation, we could get the policy improvement for SAC.

Note in ParPISAC, we could only need to guarantee the condition in Eq. 11 is also satisfied. This
directly follows the monotonic property of Langevin dynamics as shown in Eq.10.

C DERIVATION OF SQUASH CORRECTION

To constrain the action space in a finite interval, the tanh is applied on the samples from the raw
output u. And we also conduct the Langevin dynamics on u space. The change of variable formula
indicates the following equation is satisfied:

da _âˆ’1_ _D_
log p(a **s) = log p(u** **s)** = log p(u **s)** log 1 tanh[2] (ui)
_|_ _|_ du _|_ _âˆ’_ _âˆ’_
  _i=1_

X   

The stationary distribution on the a[det] space is exp(Q(s, a)/Î±), the corresponding density function on
the u space satisfying that:


log p(u|s) = exp(Q(s, tanh(u)Î±) + 2

We take derivative and get the score function as:

_âˆ‡u log p(u|s) = âˆ‡u(Q(s, tanh(u))/Î± + 2_

D DISCUSSION ON REMARK 2


log(1 (tanh(ui))
_âˆ’_
_i=1_

X

_D_

_âˆ‡u log(1 âˆ’_ (tanh(ui))
_i=1_

X


The langevin dynamic is a special case of wasserstein gradient flow (Liu et al., 2019). At step k + 1,
the particle simulation is to solve the following problem:

_W2[2]_ _Âµ, Âµ[(]k[h][)]_
_Âµ[(]k[h]+1[)]_ [= arg min]Âµ [KL(][Âµ][âˆ¥][p][(][x][)) +] 2h 

Here Âµk denotes the sampled distribution after k-th step and p(x) is the target distribution, i.e.
_Ï€Î²_ (a|s) exp(ZQ(s,a)/Î±)) in our case. With limited steps of langevin dynamics, according to the triangle

inequality of wasserstein distance we have:


_W2[2]_ _Âµ[h]k+1[, Âµ][(]k[h][)]_ _W2[2]_ _Âµ[(]k[h]+1[)]_ _[, Âµ]1[(][h][)]_

(12)

 2h  _â‰¥_  2h 


_Ci_
_â‰¥_


_i_ _i=1_

The above inequality is to say that if each step the wasserstein distance could be bounded (by Ci),
then the wasserstein distance between the final distribution and the original distribution is also
bounded. And we replace Âµ[(]1[h][)] with Ï€Î² and Âµ[(]k[h]+1[)] [with][ Ï€][Ï†]k [, we get the][ W][ 2]2 [(][Ï€][Î²][, Ï€][Ï†]k [)][ is constrained.]
Note that we optimize the wasserstein distance between Ï€Ï† and Ï€Ï†k explicitly, and the wasserstein
distance W2[2] [(][Ï€][Ï†][, Ï€][Ï†]k [)][ is also constrained. Applying the triangle inequality agian, we get the]


-----

bounded condition on W2[2] [(][Ï€][Ï†][, Ï€][Î²][)][ â‰¤] _[W][ 2]2_ [(][Ï€][Ï†][, Ï€][Ï†]k [) +][ W][ 2]2 [(][Ï€][Î²][, Ï€][Ï†]k [)][. Integrating the][ W][ 2]2 [(][Ï€][Ï†][, Ï€][Î²][)]
and W2[2] [(][Ï€][Ï†][, Ï€][Ï†]k [)][ into Eq.][ 12][ with Lagrangian multiplier][ Î±][ and][ Î³][, we then get the Remark][ 2][.]

E CONVERGENCE ANALYSIS OF PARPI

The convergence property of ParPI is highly correlated with the particle updates in Eq. 4. Note that
Jordan et al. (1998) indicates that with unlimited samples and infinitely small step size, the sample
result could approach some stationary distribution e[U] [(][x][)]. Such property provide a good theoretical
intuition for convergence analysis on ParPI :
**Proposition 1. With unbiased gradient estimation on the âˆ‡Ï†W2[2][(][Ï€][Ï†][, Ï€][sampled][)][, where][ Ï€][sampled]** _[indi-]_
_cates the empirical distribution acquired from the langevin dynamics. If the sample size M â†’âˆ_
_and step size Î±_ 0, Ï€Ï† in ParPI would converge to the global minimum _[Ï€][Î²]_ [(][a][|][s][) exp(]Z[Q][(][s,a][)][/Î±][))] _._
_â†’_

Note the Proposition 1 is based on the fact the KL(Â·âˆ¥Â·) is convex and we leave the detailed derivation
in the Appendix.

To start with, we introduce the following lemma:
**Lemma 3.** _(Jordan et al., 1998) Assume that log p(x)_ _â‰¤_ _C1 is infinitely differentiable,_
_and âˆ¥âˆ‡_ log p(x)âˆ¥ _â‰¤_ _C2 (1 +K_ _C1 âˆ’_ log p(x)) (âˆ€x) for some constants {C1, C2} . Let T =

_hK, Âµ0 â‰œ_ _q0(x), and_ _Âµ[(]k[h][)]_ _k=1_ _[be the solution of the functional optimization problem:][Âµ]k[(][h]+1[)]_ [=]

arg minÂµ KL(Âµ _p(x)) +n_ _W2[2]oÂµ,Âµ2h_ _k[(][h][)]_ _, which are restricted to the space with finite second-order_
_âˆ¥_

_moments. Then i) the problem is convex;and ii) Âµ[(]K[h][)]_ _converges to ÂµT in the limit of h_ 0,
_â†’_
_i.e., limhâ†’0 Âµ[(]K[h][)]_ = ÂµT, where ÂµT is the solution of Fokker-Planck (FP) equation: âˆ‚Ï„ _ÂµÏ„ =_
_ÂµÏ„_ _U +_ _ÂµÏ„_ _ÏƒÏƒ[âŠ¤][]_ _at T_ _._
_âˆ‡Â·_ _âˆ’_ _âˆ‡_ _âˆ‡Â·_
   

Note the stationary distribution of the FP Equation is proportional to e[U] [(][x][))], Lemma 3 shows that
limkâ†’âˆ,hâ†’0 Âµ[(]k[h][)] = _Z1_ _[e][U]_ [. In our case the stationary distribution refers to][ Ï€][Î²] [(][a][|][s][) exp(]Z[Q][(][s,a][)][/Î±][))] .

Thus Lemma 3 suggests that with sample size M â†’âˆ and step size Î± â†’ 0, Ï€Ï† would converge to
the global minimum _[Ï€][Î²]_ [(][a][|][s][) exp(]Z[Q][(][s,a][)][/Î±][))] .

F DISCUSSION ON MATCHING THE JOINT STATE-ACTION DISTRIBUTION

Note that ultimate goal of learning stochastic policy could be formulated as matching the conditional
distribution, i.e., for every value of the state: minÎ¸ D (qÎ¸(Â· | s)âˆ¥p(Â· | s)), âˆ€s. While in this situation,
the optimization problems need to be solved independently in different states. For example, if we
want to optimize JSD or WD, then we will need many different discriminators or critics for different
states. The corresponding computational complexity is unacceptable. To make the training tractable,
we "amortized" the optimization problem on different states into a joint matching problem. Put it
in another way, using the joint distribution is obvious when considering the analogy to supervised
learning: the task is to match fÎ¸(x) to the corresponding target y for every x, but a joint distribution
is used in the objective: Ep(x,y) [D (fÎ¸(x), y)]. The analogy is achieved by replacing the model fÎ¸(x)
with qÎ¸( _s) and the target y with p(_ _s)._

_Â· |_ _Â· |_

G EXPERIMENT DETAILS

All algorithms are implemented in Tensorflow (Abadi et al., 2016) and use a distributed implementation powered by Ray (Moritz et al., 2018). Following the implementation of TD3 (Fujimoto et al.,
2018), we use two Q-value functions parameterized by a two-layer feed-forward network to fend off
overestimation. A discriminator model sharing the same architecture is introduced for the divergence
minimization purpose. Besides, we do not introduce any reward shaping for all tasks. When collecting
rollouts for evaluations, we simply take the action selected by the policy at every state for every 1000
updates.

Policy, Q-value function & Discriminator architectures for both baselines and our algorithms:


-----

_s âˆˆ_ R[s], a âˆˆ R[a],z âˆˆ R[a] _âˆ¼N_ (0, I)

Affine Transformation

Dense layer 256, ReLU

Dense layer 256, ReLU

Dense layer 256, Tanh

Table 3: Policy


_s âˆˆ_ R[s], a âˆˆ R[a]

Dense layer 256, ReLU

Dense layer 256, ReLU

Dense layer 256, ReLU

dense â†’ 1

Table 4: (Double)Q Function

_s âˆˆ_ R[s], a âˆˆ R[a]

Affine Transformation

Dense layer 256, ReLU

Dense layer 256, ReLU

Dense layer 256, Linear

Table 5: Discriminator

Table 6: Hyper-parameter Settings of ParPI

|Parameter|Policy|Q-function|Discriminator|
|---|---|---|---|
|optimizer Learning rate discount(Î³) replay buffer size entropy target MCMC steps MCMC step size gradient steps target update interval|Adam 3 10âˆ’4 Â· 0.99 106 dim( ) âˆ’ A n/a n/a 1 1|Adam 3 10âˆ’4 Â· n/a n/a n/a n/a n/a 1 1|Adam 3 10âˆ’4 Â· n/a n/a n/a 5 3 10âˆ’4 Â· 1 1|



H OFFLINE ALGORITHM DETAILS

For completeness, we list the detailed algorithms for ParPI +MOPO in Algorithm 2. We use rollout
length 5 for all tasks, and the same penalty coefficients reported by Yu et al. (2020).


-----

**Algorithm 2 ParPI on top of MOPO**

1: Input: reward penalty coefficient Î» rollout horizon h, rollout batch size b.
2: Train on batch data Denv an ensemble of N probabilistic dynamics {T[Ë†][i](s[â€²], r|s, a)
_N_ (Âµ[i](s, a), Ïƒ[i](s, a))}i[N]=1[.]

3:4: Initialize policy for epoch 1, 2, . . . Ï€ and empty replay buffer do _Dmodel â†_ âˆ….
5: **for 1, 2, . . ., b (in parallel) do**

6: Sample state s1 from Denv for the initialization of the rollout.

7: **for j = 1, 2, . . ., h do**

8: Sample an action aj _Ï€(sj)._
_âˆ¼_

9: Randomly pick dynamics _T[Ë†] from {T[Ë†][i]}i[N]=1_ [and sample][ s][j][+1][, r][j][ âˆ¼] _T[Ë†](sj, aj)._

10: Compute Ëœrj = rj _Î» max[N]i=1_

11: Add sample (sj, aâˆ’j, Ëœrj, sj+1) to[âˆ¥][Î£] D[i][(]model[s][j][, a].[j][)][âˆ¥][F][.]

12: **end for**

13: **end for**

14:15: end forDrawing samples from Denv âˆªDmodel, use ParPI to update Ï€.


For the ParPI +BRAC, following the original implementation (Wu et al., 2019; Fu, 2020), we using
dual form for the behavior policy regulation(with KL) and value penalty with fixed Î±. Then we use
ParPi for both Value-function and Q-function update. We using (256,256) fully connected network as
the critic in the minimax objective. We also adapt the gradient penalty in KL dual training. we using
Adam for all optimizers.


-----

