# THE CONVEX GEOMETRY OF BACKPROPAGATION: NEURAL NETWORK GRADIENT FLOWS CONVERGE TO EXTREME POINTS OF THE DUAL CONVEX PROGRAM


**Yifei Wang**
Department of Electrical Engineering
Stanford University
Stanford, CA 94305, USA
wangyf18@stanford.edu


**Mert Pilanci**
Department of Electrical Engineering
Stanford University
Stanford, CA 94305, USA
pilanci@stanford.edu

ABSTRACT


We study non-convex subgradient flows for training two-layer ReLU neural networks from a convex geometry and duality perspective. We characterize the implicit
bias of unregularized non-convex gradient flow as convex regularization of an equivalent convex model. We then show that the limit points of non-convex subgradient
flows can be identified via primal-dual correspondence in this convex optimization
problem. Moreover, we derive a sufficient condition on the dual variables which
ensures that the stationary points of the non-convex objective are the KKT points
of the convex objective, thus proving convergence of non-convex gradient flows
to the global optimum. For a class of regular training data distributions such as
orthogonal separable data, we show that this sufficient condition holds. Therefore,
non-convex gradient flows converge to optimal solutions of a convex optimization
problem. We present numerical results verifying the predictions of our theory for
non-convex subgradient descent.

1 INTRODUCTION

Neural networks (NNs) exhibit remarkable empirical performance in various machine learning tasks.
However, a full characterization of the optimization and generalization properties of NNs is far from
complete. Non-linear operations inherent to the structure of NNs, over-parameterization and the
associated highly nonconvex training problem makes their theoretical analysis quite challenging.

In over-parameterized models such as NNs, one natural question arises: Which particular solution does
gradient descent/gradient flow find in unregularized NN training problems? Suppose that X âˆˆ R[N] _[Ã—][d]_
is the training data matrix and y âˆˆ{1, âˆ’1}[N] is the label vector. For linear classification problems
such as logistic regression, it is known that gradient descent (GD) exhibits implicit regularization
properties, see, e.g., (Soudry et al., 2018; Gunasekar et al., 2018). To be precise, under certain
assumptions, GD converges to the following solution which maximizes the margin:

1
arg min 2[,][ s.t.][ y][n][w][T][ x][n] (1)
**wâˆˆR[d]** 2 _[âˆ¥][w][âˆ¥][2]_ _[â‰¥]_ [1][, n][ âˆˆ] [[][N] []][.]

Here we denote [N ] = {1, . . ., N _}. Recently, there are several results on the implicit regularization_
of the (stochastic) gradient descent method for NNs. In (Lyu & Li, 2019), for the multi-layer
homogeneous network with exponential or cross-entropy loss, with separable training data, it is
shown that the gradient flow (GF) and GD finds a stationary point of the following non-convex
max-margin problem:


1
2[,][ s.t.][ y][n][f] [(][Î¸][;][ x][n][)][ â‰¥] [1][, n][ âˆˆ] [[][N] []][,] (2)
2 _[âˆ¥][Î¸][âˆ¥][2]_


arg min


where f (Î¸; x) represents the output of the neural network with parameter Î¸ given input x. In (Phuong
& Lampert, 2021), by further assuming the orthogonal separability of the training data, it is shown
that all neurons converge to one of the two max-margin classifiers. One corresponds to the data with
positive labels, while the other corresponds to the data with negative labels.


-----

However, as the max-margin problem of the neural network (2) is a non-convex optimization problem,
the existing results only guarantee that it is a stationary point of (2), which can be a local minimizer
or even a saddle point. In other words, the global optimality is not guaranteed.

In a different line of work (Pilanci & Ergen, 2020; Ergen & Pilanci, 2020; 2021b), exact convex
optimization formulations of two and three-layer ReLU NNs are developed, which have global
optimality guarantees in polynomial-time when the data has a polynomial number of hyperplane
arrangements, e.g., in any fixed dimension or with convolutional networks of fixed filter size. The
convex optimization framework was extended to vector output networks (Sahiner et al., 2021b),
quantized networks (Bartan & Pilanci, 2021b), autoencoders (Sahiner et al., 2021c; Gupta et al.,
2021), networks with polynomial activation functions (Bartan & Pilanci, 2021a), networks with batch
normalization (Ergen et al., 2021), univariate deep ReLU networks, deep linear networks (Ergen &
Pilanci, 2021c) and Generative Adversarial Networks (Sahiner et al., 2021a).

In this work, we first derive an equivalent convex program corresponding to the maximal margin
problem (2). We then consider non-convex subgradient flow for unregularized logistic loss. We show
that the limit points of non-convex subgradient flow can be identified via primal-dual correspondence
in the convex optimization problem. We then present a sufficient condition on the dual variable to
ensure that all stationary points of the non-convex max-margin problem are KKT points of the convex
max-margin problem. For certain regular datasets including orthogonal separable data, we show that
this sufficient condition on the dual variable holds, thus implies the convergence of gradient flow
on the unregularized problem to the global optimum of the non-convex maximalo margin problem
(2). Consequently, this enables us to fully characterize the implicit regularization of unregularized
gradient flow or gradient descent as convex regularization applied to a convex model.

1.1 RELATED WORK

There are several works studying the property of two-layer ReLU networks trained by gradient
descent/gradient flow dynamics. The following papers study the gradient descent like dynamics in
training two-layer ReLU networks for regression problems. Ma et al. (2020) show that for two-layer
ReLU networks, only a group of a few activated neurons dominate the dynamics of gradient descent.
In (Mei et al., 2018), the limiting dynamics of stochastic gradient descent (SGD) is captured by
the distributional dynamics from a mean-field perspective and they utlize this to prove a general
convergence result for noisy SGD. Li et al. (2020) focus on the case where the weights of the second
layer are non-negative and they show that the over-parameterized neural network can learn the
ground-truth network in polynomial time with polynomial samples. In (Zhou et al., 2021), it is shown
that mildly over-parameterized student network can learn the teacher network and all student neurons
converge to one of the teacher neurons.

Beyond (Lyu & Li, 2019) and (Phuong & Lampert, 2021), the following papers study the classification
problems. In (Chizat & Bach, 2018), under certain assumptions on the training problem, with overparameterized model, the gradient flow can converge to the global optimum of the training problem.
For linear separable data, utilizing the hinge loss for classification, Wang et al. (2019) introduce a
perturbed stochastic gradient method and show that it can attain the global optimum of the training
problem. Similarly, for linear separable data, Yang et al. (2021) introduce a modified loss based on
the hinge loss to enable (stochastic) gradient descent find the global minimum of the training problem,
which is also globally optimal for the training problem with the hinge loss.

1.2 PROBLEM SETTING

We focus on two-layer neural networks with ReLU activation, i.e., f (Î¸, X) = (XW1)+w2, where
**Wthis neural network is homogeneous, i.e., for any scalar1 âˆˆ** R[d][Ã—][m], w2 âˆˆ R[m] and Î¸ = (W1, w2) represents the parameter. Due to the ReLU activation, c > 0, we have f (cÎ¸; X) = c[2]f (Î¸; X). The
training problem is given by


_â„“(ynf_ (Î¸; xn)), (3)
_n=1_

X


min


where â„“(q) : R â†’ R+ is the loss function. We focus on the logistic, i.e, cross-entropy loss, i.e.,
_â„“(q) = log(1 + exp(âˆ’q))._


-----

Then, we briefly review gradient descent and gradient flow. The gradient descent takes the update rule

**_Î¸(t + 1) = Î¸(t) âˆ’_** _Î·(t)g(t),_

where g(t) âˆˆ _âˆ‚[â—¦]L(Î¸(t)) and âˆ‚[â—¦]_ represents the Clarkeâ€™s subdifferential.

The gradient flow can be viewed as the gradient descent with infinitesimal step size. The trajectory
of the parameter Î¸ during training is an arc Î¸ : [0, + ) Î˜, where Î˜ = **_Î¸ = (W1, w2)_** **W1**
R[d][Ã—][m], W2 âˆˆ R[m]}. More precisely, the gradient flow is given by the differential inclusionâˆž _â†’_ _{_ _|_ _âˆˆ_
_d_

for t 0, almost everywhere.
_dt_ **_[Î¸][(][t][)][ âˆˆâˆ’][âˆ‚][â—¦][L][(][Î¸][(][t][))][,]_** _â‰¥_

2 MAIN RESULTS


In this section, we present our main results and defer the detailed analysis to the following sections.
Consider the more general multi-class version of the problem with K classes. Suppose that Â¯y âˆˆ [K][N]
is the label vector. Let Y = (yn,k)n [N ],k [K] R[N] _[Ã—][K]_ be the encoded label matrix such that
_âˆˆ_ _âˆˆ_ _âˆˆ_

1, if Â¯yn = k,
_yn,k =_
1, otherwise.
 _âˆ’_

Similarly, we consider the following two-layer vector-output neural networks with ReLU activation:

_f1(Î¸1, X)_ (XW1[(1)][)][+][w]2[(1)]
. .

_F_ (Î˜, X) = ï£® .. ï£¹ = ï£® .. ï£¹ _,_

ï£¯fK(Î¸K, X)ï£º ï£¯(XW1[(][K][)])+w2[(][K][)]ï£º
ï£° ï£» ï£° ï£»

where we write Î˜ = (Î¸1, . . ., Î¸K). For k = 1, . . ., K, we have Î¸k = (W1[(][k][)][,][ w]2[(][k][)][)][ where]
**W1[(][k][)]** R[N] _[Ã—][m]_ and w2[(][k][)] R[m]. One can view each of the K outputs of F (Î˜, X) as the output of a
_âˆˆ_ _âˆˆ_
two-layer scalar-output neural network. Consider the following training problem:


_â„“(yn,kfk(Î¸k, xn))._ (4)
_n=1_

X


min


_k=1_


According to (Lyu & Li, 2019), the gradient flow and the gradient descent finds a stationary point of
the following non-convex max-margin problem:


1
2[,][ s.t.][ y][n,k][f] [(][Î¸][k][;][ x][n][)][ â‰¥] [1][, n][ âˆˆ] [[][N] []][, k][ âˆˆ] [[][K][]][.] (5)
2 _[âˆ¥][Î¸][k][âˆ¥][2]_


arg min
**Î˜**

_k=1_

X


Denote the set of all possible hyperplane arrangement as

_P = {diag(I(Xw â‰¥_ 0))|w âˆˆ R[d]}, (6)

and let p = . We can also write = **D1, . . ., Dp** . From (Cover, 1965), we have an upper
_|P|_ _r_ _P_ _{_ _}_

bound p 2r _e(Nrâˆ’1)_ where r = rank(X). We first reformulate (5) as convex optimization.
_â‰¤_

**Proposition 1 The non-convex problem**  (5) is equivalent to the following convex program


(âˆ¥uj,kâˆ¥2 + âˆ¥u[â€²]j,k[âˆ¥][2][)][,]

_k=1_ _j=1_

X X


min


(7)


_j=1_ **DjX(uj,k âˆ’** **u[â€²]j,k[)][ â‰¥]** **[1][,]**

X


_s.t. diag(yk)_


(2Dj _I)Xuj,k_ 0, (2Dj _I)Xu[â€²]j,k_
_âˆ’_ _â‰¥_ _âˆ’_ _[â‰¥]_ [0][, j][ âˆˆ] [[][p][]][, k][ âˆˆ] [[][K][]][.]

_where yk is the k-th column of Y. The dual problem of (7) is given by_

max tr(Î›[T] **Y),**

(8)
_s.t. diag(yk)Î»k_ 0, max _k_ [(][X][T][ w][)][+][| â‰¤] [1][, k][ âˆˆ] [[][K][]][.]
_âª°_ _âˆ¥wâˆ¥2â‰¤1_ _[|][Î»][T]_

_where Î»k is the k-th column of Î›._


-----

We present the detailed derivation of the convex formulation (7) and its dual problem (8) in the
appendix. Givendefine the cosine angle between u âˆˆ R[d], we define u and D v( byu) = cos diag âˆ (u,( vI() =Xu >âˆ¥uâˆ¥u 0))2[T]âˆ¥vvâˆ¥. For two vectors2 [.] **u, v âˆˆ** R[d], we

2.1 OUR CONTRIBUTIONS

The following theorem illustrate that for neurons satisfying sign(yk[T] [(][Xw]1[(][k],i[)][)][+][) =][ sign][(][w]2[(][k],i[)][)][ at]

initialization, w1[(][k],i[)] [align to the direction of][ Â±][X][T][ D][(][w]1[(][k],i[)][)][y][k][ at a certain time][ T] [, depending on]

**sign(w2[(][k],i[)]k,+** [)][ at initialization. In Section 2.3, we show that these are dual extreme points of (7).]

**Theorem 1 Consider the K-class classification training problem (4) for any dataset. Suppose that**
_the neural network is scaled at initialization such that_ **w1[(][k],i[)]** 2,i
_âˆ¥_ _[âˆ¥][2][ =][ |][w][(][k][)][|][ for][ i][ âˆˆ]_ [[][m][]][ and][ k][ âˆˆ] [[][K][]][.]

_Assume that at initialization, for k âˆˆ_ [K], there exists neurons (w1[(][k],i[)]k _[,][ w]2[(][k],i[)]k_ [)][such that]

**sign(yk[T]** [(][Xw]1[(][k],i[)]k [)][+][) =][ sign][(][w]2[(][k],i[)]k [) =][ s,] (9)

_where s âˆˆ{1, âˆ’1}. Consider the subgradient flow applied to the non-convex problem (4). Let_
_Î´ âˆˆ_ (0, 1). Suppose that the initialization is sufficiently close to the origin. Then, for k âˆˆ [K], there
_exist T = T_ (Î´, k) such that

cos âˆ  **w1[(][k],i[)]k** [(][T] [)][, s][X][T][ D][(][w]1[(][k],i[)]k [(][T] [))][y][k] _â‰¥_ 1 âˆ’ _Î´._
 

Next, we impose conditions on the dataset to prove a stronger global convergence results on the flow.
We say that the dataset (X, Â¯y) is orthogonal separable among multiple classes if for all n, n[â€²] _âˆˆ_ [N ],

**x[T]n** **[x][n][â€²][ >][ 0][,][ if][ Â¯]yn = Â¯ynâ€²** _,_

**x[T]n** **[x][n][â€²][ â‰¤]** [0][,][ if][ Â¯]yn Ì¸= Â¯ynâ€² _._

For orthogonal separable dataset among multiple classes, the subgradient flow for the non-convex
problem (4) can find the global optimum of (5) up to a scaling constant.

**Theorem 2 Suppose that (X, Â¯y) âˆˆ** R[N] _[Ã—][d]_ _Ã— [K][N]_ _is orthogonal separable among multiple classes._
_Consider the non-convex subgradient flow applied to the non-convex problem (4). Suppose that the_
_initialization is sufficiently close to the origin and scaled as in Theorem 1. Then, the non-convex_
_subgradient flow converges to the global optimum of the convex program (7) and hence the non-convex_
_objective (5) up to scaling._

Therefore, the above result characterizes the implicit regularization of unregularized gradient flow as
_convex regularization, i.e., group â„“1 norm, in the convex formulation (7). It is remarkable that group_
sparsity is enforced by small initialization magnitude with no explicit form of regularization.

2.2 CONVEX GEOMETRY OF NEURAL GRADIENT FLOW

Suppose that Î» âˆˆ R[N] . Here we provide an interesting geometric interpretation behind the formula

cos âˆ (u, X[T] **D(u)Î») > 1 âˆ’** _Î´._

which describes a dual extreme point to which hidden neurons approach to as predicted by
Theorem 1. We now explain the geometric intuition behind this result. Consider an ellipsoid
**Xu :** **u** 2 1 . A positive extreme point of this ellipsoid along the direction Î» is defined by
_{arg max âˆ¥u : âˆ¥âˆ¥u â‰¤âˆ¥2â‰¤1 Î»}_ _[T]_ **Xu, which is given by the formula** _âˆ¥XX[T][T]Î»Î»âˆ¥2_ [. Next, we consider the rectified]

ellipsoid set Q := {(Xu)+ : âˆ¥uâˆ¥2 â‰¤ 1} introduced in (Ergen & Pilanci, 2021a) and shown in
Figure 1. The constraintthe absolute polar set of Q max, which appears as a constraint in the convex programu:âˆ¥uâˆ¥2â‰¤1 |Î»[T] (Xu)+| â‰¤ 1 on Î» is equivalent to Î» (8) âˆˆQ and is defined as[âˆ—]. Here Q[âˆ—] is
the following convex set
= **_Î» : max_** (10)
_Q[âˆ—]_ _{_ **z**
_âˆˆQ_ _[|][Î»][T][ z][| â‰¤]_ [1][}][.]


-----

An extreme point of this non-convex body along the direction Î» is given by the solution of the
problem

max max (11)
**u :** **u** 2 1 **_[Î»][T][ (][Xu][)][+][ = max]Dj_** **u :** **u** 2 1,(2Dj _I)Xu_ 0 **_[Î»][T][ D][j][Xu][.]_**
_âˆ¥_ _âˆ¥_ _â‰¤_ _âˆˆP_ _âˆ¥_ _âˆ¥_ _â‰¤_ _âˆ’_ _â‰¥_

Here, (Î», u) are primal-dual pairs as they appear in the convex dual program (8). First, note that a
stationary point of gradient flow on the objective in (11) is given by the identity cu _âˆ‚u[â—¦][Î»][T][ (][Xu][)][+]_
_âˆˆ_
where c is a constant. In particular, by picking the zero as the subgradient of (x[T]n **[u][)][+]** [when][ x]n[T] **[u][ = 0][,]**

**X[T]** **D(u)Î»** _Nn=1_ _[Î»][n][x][n][I][(][u][T][ x][n][ >][ 0)]_
**u =** = _._ (12)

**X[T]** **D(u)Î»** 2
_âˆ¥_ _âˆ¥_ _âˆ¥_ [P]Pn[N]=1 _[Î»][n][x][n][I][(][u][T][ x][n][ >][ 0)][âˆ¥][2]_

Note that the formula cos âˆ (u, X[T] **D(u)Î») > 1** _âˆ’_ _Î´ appearing in Theorem 1 shows that gradient flow_
reaches the extreme points of projected ellipsoids **DjXu :** **u** 2 1 in the direction of Î» = yk,
where Dj corresponds to a valid hyperplane arrangement. This interesting phenomenon is { _âˆ¥_ _âˆ¥_ _â‰¤_ _}_
depicted in Figures 3 and 4. The one-dimensional spikes in Figures 1 and 3 are projected ellipsoids. âˆˆP
Detailed setup for Figure 1 to 4 and additional experiments can be found in Appendix F.


2.0

optimal

y[T]u = 0

maximal

1.5 minimal

optimal (Xw1,[*] i[)][ +]

1.0

0.5

0.0

0.5

1.0

1.0 0.5 0.0 0.5 1.0 1.5 2.0

Figure 1: Rectified Ellipsoid Q := {(Xu)+ :
**u** 2 1 and its extreme points (spikes).
_âˆ¥_ _âˆ¥_ _â‰¤_ _}_

2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00


Figure 3: Trajectories of (X Ë†w1,i)+ along the
training dynamics of gradient descent.


2.0

y

1.5 { : u : |maxu|2 1[|] T(Xu) + | 1}

{ : diag(y) 0}

optimal

1.0

0.5

0.0

0.5

1.0

1.5

2.0

2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0

Figure 2: Convex absolute polar set Q[âˆ—] of
the Rectified Ellipsoid (purple) and other dual
constraints (grey).


1.0

cone boundary

0.5 1-th neuron

2-th neuron
3-th neuron
4-th neuron
5-th neuron

0.0

6-th neuron
7-th neuron
8-th neuron
9-th neuron

0.5 10-th neuron

optimal neuron

1.0

1.0 0.5 0.0 0.5 1.0


Figure 4: Trajectories of Ë†w1,i = _âˆ¥ww11,i,iâˆ¥2_

along the training dynamics of gradient descent.


Figure 5: Two-layer ReLU network gradient descent dynamics on an orthogonal separable dataset.
**wË†** 1,i = _âˆ¥ww11,i,iâˆ¥2_ [is the normalized vector of the][ i][-th hidden neuron in the first layer.]


-----

3 CONVEX MAX-MARGIN PROBLEM

In this section, we consider the equivalent convex model of the max-margin problem and its optimality
conditions. We primarily focus on the binary classification problem for simplicity, which are later
extended to the multi-class case. We can reformulate the nonconvex max-margin problem (2) as

min [1] _F_ [+][ âˆ¥][w][2][âˆ¥]2[2][)][,][ s.t.][ Y][(][XW][1][)][+][w][2] (13)

2 [(][âˆ¥][W][1][âˆ¥][2] _[â‰¥]_ **[1][,]**

where Y = diag(y). This is a nonconvex optimization problem due to the ReLU activation and
the two-layer structure of neural network. Analogous to the convex formulation introduced in
(Pilanci & Ergen, 2020) for regularized training problem of neural network, we can provide a convex
optimization formulation of (13) and derive the dual problem.
**Proposition 2 The problem (13) is equivalent to**


_Pcvx[âˆ—]_ [= min]


(âˆ¥ujâˆ¥2 + âˆ¥u[â€²]j[âˆ¥][2][)][,]
_j=1_

X


(14)


_s.t. Y_


**DjX(u[â€²]j**
_j=1_ _[âˆ’]_ **[u][j][)][ â‰¥]** **[1][,]**

X


(2Dj _I)Xuj_ 0, (2Dj _I)Xu[â€²]j_
_âˆ’_ _â‰¥_ _âˆ’_ _[â‰¥]_ [0][,][ âˆ€][j][ âˆˆ] [[][p][]][.]

_The dual problem of (14) is given by_

_D[âˆ—]_ = max **y[T]** **_Î» s.t. YÎ»_** 0, max (15)
**_Î»_** _âª°_ **u:** **u** 2 1
_âˆ¥_ _âˆ¥_ _â‰¤_ _[|][Î»][T][ (][X][T][ u][)][+][| â‰¤]_ [1][.]

The following proposition gives a characterization of the KKT point of the non-convex max-margin
problem (2). The definition of B-subdifferential can be found in Appendix A.
**Proposition 3 Let (W1, w2, Î») be a KKT point of the non-convex max-margin problem (2) (in terms**
_of B-subdifferential). Suppose that w2,i_ = 0 for certain i [m]. Then, there exists a diagonal matrix
**DË†** _i âˆˆ_ R[N] _[Ã—][N]_ _satisfying_ _Ì¸_ _âˆˆ_
( D[Ë†] _i)n = 1, for x[T]n_ **[w][1][,i]** _[>][ 0][,]_


( D[Ë†] _i)n_ 0, 1 _, for x[T]n_ **[w][1][,i]** [= 0][,]
_âˆˆ{_ _}_

( D[Ë†] _i)n = 0, for x[T]n_ **[w][1][,i]** _[<][ 0][.]_
_such that_
**w1,i**

= X[T][ Ë†]DiÎ», **X[T][ Ë†]DiÎ»** 2 = 1.
_w2,i_ _âˆ¥_ _âˆ¥_

Based on the characterization of the KKT point of the non-convex max-margin problem (2), we
provide an equivalent condition to ensure that it is also the KKT point of the convex max-margin
problem (14).
**Theorem 3 The KKT point of the non-convex max-margin problem (13) (in terms of B-subdifferential)**
_corresponds to a KKT point of the convex max-margin problem (14) if Î» is dual feasible, i.e.,_

max (16)
**u:âˆ¥uâˆ¥2â‰¤1** _[|][Î»][T][ (][Xu][)][+][| â‰¤]_ [1][.]

_This condition is equivalent to for all Dj_ _, the dual variable Î» satisfies that_
_âˆˆP_

max (17)
_âˆ¥uâˆ¥2â‰¤1,(2Dj_ _âˆ’I)Xuâ‰¥0_ _[|][Î»][T][ D][j][Xu][| â‰¤]_ [1][.]


3.1 DUAL FEASIBILITY OF THE DUAL VARIABLE

A natural question arises: is it possible to examine whether Î» is feasible in the dual problem? We say
the dataset (X, y) is orthogonal separable if for all n, n[â€²] _âˆˆ_ [N ],

**x[T]n** **[x][n][â€²][ >][ 0][,][ if][ y][n]** [=][ y][n][â€²] _[,]_

**x[T]n** **[x][n][â€²][ â‰¤]** [0][,][ if][ y][n]

_[Ì¸][=][ y][n][â€²]_ _[.]_


-----

For orthogonal separable data, as long as the induced diagonal matrices in Proposition 3 cover the
positive part and the negative part of the labels, the KKT point of the non-convex max-margin problem
(2) is the KKT point of the convex max-margin problem (14).
**Proposition 4 Suppose that (X, y) is orthogonal separable. Suppose that the KKT point of the non-**
_convex problem include two neurons (w1,i+_ _, w2,i+_ ) and (w1,iâˆ’ _, w2,iâˆ’_ ) such that the corresponding
_diagonal matrices_ **D[Ë†]** _i+ and_ **D[Ë†]** _iâˆ’_ _defined in Proposition 3 satisfy that_

**DË†** _i+_ **diag(I(y = 1)),** **DË†** _i_ **diag(I(y =** 1)).
_â‰¥_ _âˆ’_ _â‰¥_ _âˆ’_

_Then, the dual variable Î» is dual feasible, i.e., satisfying (16)._

The spike-free matrices discussed in (Ergen & Pilanci, 2021a) also makes examining the dual
feasibility of Î» easier. The definition of spike-free matrices can be found in Appendix A
**Proposition 5 Suppose that X is spike-free. Suppose that the KKT point of the non-convex problem**
_include two neurons (w1,i+_ _, w2,i+_ ) and (w1,iâˆ’ _, w2,iâˆ’_ ) such that the corresponding diagonal matrices
**DË†** _i+ and_ **D[Ë†]** _iâˆ’_ _defined in Proposition 3 satisfy that_

**DË†** _i+_ **diag(I(y = 1)),** **DË†** _i_ **diag(I(y =** 1)).
_â‰¥_ _âˆ’_ _â‰¥_ _âˆ’_

_Then, the dual variable Î» is dual feasible, i.e., satisfying (16)._

**Remark 1 For the spike-free data, the constraint on the dual problem is equivalent to**


max or equivalently
**Xuâ‰¥0,âˆ¥uâˆ¥2â‰¤1** _[|][Î»][T][ Xu][| â‰¤]_ [1][,]

max min
**Xu** 0, **u** 2 1 **_[Î»][T][ Y][+][Xu][ â‰¤]_** [1][,] **Xu** 0 **_[Î»][T][ Y][âˆ’][Xu][ â‰¥âˆ’][1][.]_**
_â‰¥_ _âˆ¥_ _âˆ¥_ _â‰¤_ _â‰¥_

4 SUB-GRADIENT FLOW DYNAMICS OF LOGISTIC LOSS

In this section, we consider the following sub-gradient flow of the logistic loss (3)

_âˆ‚_

_Î»Ëœn(t)xn(t)_ _,_

_âˆ‚t_ **[w][1][,i][(][t][) =][w][2][,i][(][t][)]** ï£« ï£¶

_n:(w1,i(t))[T]_ **xn>0**

X

_N_ ï£­ ï£¸

_âˆ‚_

_Î»Ëœn(t)((w1,i(t))[T]_ **xn(t))+.**

_âˆ‚t_ _[w][2][,i][(][t][) =]_

_n=1_

X


(18)


where the n-th entry of **_Î»(t) âˆˆ_** R[N] is defined

[e] _Î»Ëœn = âˆ’ynâ„“[â€²](qn),_ _qn = yn(x[T]n_ **[W][1][)][+][w][2][.]** (19)

For simplicity, we omit the term (t). For instance, we write w1,i = w1,i(t). To be specific,
when w1[T],i[x][n][ = 0][, we select][ 0][ as the subgradient of][ w][2][,i][(][w]1[T],i[x][n][)][+][ with respect to][ w][1][,i][. Denote]
**_Ïƒi = sign(Xui). For Ïƒ âˆˆ{1, âˆ’1, 0}[N]_**, we define

**g(Ïƒ,** **_Î») =_** _Î»Ëœnxn._ (20)

_n:Ïƒn>0_

X

For simplicity, we also write [e]

**g(u,** **_Î») := g(sign(Xu), Î») =_** _Î»Ëœnxn._ (21)

_n:w1[T],i[x][n][>][0]_

X

[e]

Then, we can rewrite sub-gradient flow of the logistic loss (3) as follows:

_âˆ‚_ _âˆ‚_

**_Î»),_** _i,1[g][(][u][,][ e]Î»)._ (22)
_âˆ‚t_ **[w][i,][1][ =][ w][2][,i][g][(][u][,][ e]** _âˆ‚t_ _[w][i,][2][ =][ w][T]_

Assume that the neural network is scaled at initialization, i.e., âˆ¥w1,i(0)âˆ¥2[2] [=][ w]2[2],i[(0)][ for][ i][ âˆˆ] [[][m][]][.]
Then, the neural network is scaled for t â‰¥ 0.


-----

**Lemma 1 Suppose that âˆ¥w1,i(0)âˆ¥2 = |w2,i(0)| > 0 for i âˆˆ** [m]. Then, for any t > 0, we have
_âˆ¥w1,i(t)âˆ¥2 = |w2,i(t)| > 0._

According to Lemma 1, for all t 0, sign(w2,i(t)) = sign(w2,i(0)). Therefore, we can simply
_â‰¥_
write si = si(t) = sign(w2,i(t)). As the neural network is scaled for t â‰¥ 0, it is interesting to study
the dynamics of w1,i in the polar coordinate. We write w1,i(t) = e[r][i][(][t][)]ui(t), where âˆ¥ui(t)âˆ¥2 = 1.
The gradient flow in terms of polar coordinate writes

_âˆ‚_ _âˆ‚_

_i_ **[g][(][u][i][,][ e]Î»),** **g(ui,** **_Î»)_** **u[T]i** **[g][(][u][i][,][ e]Î»)** **ui** _._ (23)
_âˆ‚t_ _[r][i][ =][ s][i][u][T]_ _âˆ‚t_ **[u][i][ =][ s][i]** _âˆ’_

   

Let xmax = maxi [n] **xi** 2. Define gmin to be [e]
_âˆˆ_ _âˆ¥_ _âˆ¥_

_gmin = minÏƒâˆˆQ_ _[âˆ¥][g][(][Ïƒ][,][ y][/][4)][âˆ¥][2][,][ s.t.][ g][(][Ïƒ][,][ y][/][4)][ Ì¸][= 0][,]_ where we denote (24)

_Q = {Ïƒ âˆˆ{1, 0, âˆ’1}[N]_ _|Ïƒ = sign(Xw), w âˆˆ_ R[d]}. (25)

As the set 1, 1, 0 is finite, we note that gmin > 0. We note that when maxn [N ] _qn_ 0,
we have **_Î» Q âŠ†{ â‰ˆ_** **y4** [. The following lemma shows that with initializations sufficiently close to] âˆ’ _}[N]_ _âˆˆ_ _|_ _| â‰ˆ[ 0][,]_

**g(u(t),** **_Î»(t))_** **g(u(t), y/4)** 2 and _dt[d]_ **[g][(][u][(][t][)][,][ e]Î»(t))**
_âˆ¥_ _âˆ’_ _âˆ¥_ 2 [can be very small.]

[e]

**Lemma 2 Suppose that T > 0 and Î´ > 0. Suppose that (u(t), r(t)) follows the gradient flow (23)**

[e]

_with s = 1 and the initialization u(0) = u0 and r(0) = r0. Suppose that r0 is sufficiently small._
_Then, the following two statements hold._

-  For all t _T_ _, we have_ **g(u(t),** **_Î»(t))_** **g(u(t), y/4)** 2 8 _._
_â‰¤_ _âˆ¥_ _âˆ’_ _âˆ¥_ _â‰¤_ _[g][min][Î´]_

-  Fordt[d] _t[g][(] â‰¤[u][(][t][)][,]T[ e]Î» such that(t))_ 2 **signmin16** _[Î´]_ _[.]_ (Xu[e] (s)) is constant in a small neighbor of t, we have

_[â‰¤]_ _[g][2]_

Based on the above lemma on the property of g(u(t), **_Î»(t)), we introduce the following lemma_**
to upper-bound the time such that cos âˆ (u(t), g(u(t), **_Î»(t))) approaches 1 âˆ’_** _Î´ or sign(Xu(t))_
changes. [e]

**Lemma 3 Let Î´ âˆˆ** (0, 1).Suppose that u0 satisfies that âˆ¥u[e]0âˆ¥2 = 1 and **_Î»(0)[T]_** (Xu0)+ > 0. Suppose
_that (u(t), r(t)) follows the gradient flow (23) with s = 1 and the initialization u(0) = u0 and_

_r(0) = r0. Let v(t) =_ **g(u(t),Î»[e](t))** [e]

_âˆ¥g(u(t),Î»[e](t))âˆ¥2_ _[. We write][ v][0][ =][ v][(0)][,][ Ïƒ][0][ =][ Ïƒ][(0)][ and][ g][0][ =][ âˆ¥][g][(][Ïƒ][0][,][ y][/][4)][âˆ¥][2][.]_
_Denote_

1 1 _Î´/8 + 1_ _Î´_ 1 _Î´/8 + v0[T]_ **[u][0]**
_T_ _[âˆ—]_ = 2g0 1 _Î´/8_ log p1 âˆ’ _Î´/8_ 1 + âˆ’ Î´ _âˆ’_ log p1 âˆ’ _Î´/8_ **v0[T]** **[u][0]** ! _._ (26)

_âˆ’_ _âˆ’_ _âˆ’_ _âˆ’_ _âˆ’_

_For c âˆˆ_ (0, 1 âˆ’ _Î´], definep_ p p

_T_ [shift](c) = 2g0 11 _Î´/8_ log p11 âˆ’ _Î´/Î´/8 +8_ _cc_ _âˆ’_ log p11 âˆ’ _Î´/Î´/8 +8_ **vv00[T][T]** **[u][u][0][0]** ! (27)

_âˆ’_ _âˆ’_ _âˆ’_ _âˆ’_ _âˆ’_

_Suppose that r0 is sufficiently small such that the statements in Lemma 2 holds forp_ p p _T = T_ _[âˆ—]. Then, at_
_least one of the following event happens_

-  There exists a time T such that we have sign(Xu(t)) = sign(Xu0) for t [0, T ) and
_âˆˆ_
1sign âˆ’ _Î´(, then the timeXu(t)) Ì¸= sign T satisfies that(Xu0). Let u T1 â‰¤ =T u[shift](T_ )( andv1[T] **[u] v[1][)][.]1[ Otherwise, there exists a time] = limtâ†’T âˆ’0 v(t). If u[T]1** **[v][1][ T][â‰¤][ â€²]**
_satisfying T_ _T_ _, such that we have sign(Xu(t)) = sign(Xu0) for t_ [0, T ] and

_[â€²]_ _â‰¤_ _[âˆ—]_ _âˆˆ_ _[â€²]_
**u(T** _[â€²])[T]_ **v(T** _[â€²]) â‰¥_ 1 âˆ’ _Î´._

-  There exists a time T _T_ _, such that we have sign(Xu(t)) = sign(Xu0) for t_ [0, T ]
_â‰¤_ _[âˆ—]_ _âˆˆ_
_and u(T_ )[T] **v(T** ) â‰¥ 1 âˆ’ _Î´._


-----

**Corollary 1 Suppose that there exists a time T** _such that we have sign(Xu(t))_ =
**sign(Xu0) for t** _âˆˆ_ [0, T ) and sign(Xu(t)) =Ì¸ **sign(Xu0).** _If T_ _>_ _T_ [shift](v1[T] **[u][1][)]** =

_g0[âˆš]11âˆ’Î´/8_ log _âˆšâˆš11âˆ’âˆ’Î´/Î´/88+âˆ’vv11[T][T]_ **[u][u][1][1][ âˆ’]** [log] _âˆšâˆš11âˆ’âˆ’Î´/Î´/88+âˆ’vv00[T][T]_ **[u][u][0][0]** _, then, we have u[T]1_ _[v][1]_ _[>][ 1][ âˆ’]_ _[Î´][.]_

 

**Proposition 6 Consider the sub-gradient flow (23) with s = 1 and the initialization u(0) = u0 and**

**vr(0) =(t) = r0g. Here at initilization the neuron(u(t),Î»[e](t))** **u0 satisfies that âˆ¥u0âˆ¥2 = 1 and y[T]** (Xu0)+ > 0. Let

_âˆ¥g(u(t),Î»[e](t))âˆ¥2_ _[. For any][ Î´ >][ 0][, for sufficiently small][ r][0][, there exists a time][ T][ =][ O][(log(][Î´][âˆ’][1][))]_

_such that u(T_ )[T] **v(T** ) â‰¥ 1 âˆ’ _Î´ and cos âˆ (u(T_ ), g(u(T ), y)) â‰¥ 1 âˆ’ _Î´._

**Remark 2 The statement of proposition is similar to Lemma 4 in (Maennel et al., 2018). However,**
their proof contains a problem because they did not consider the change of sign(Xw) along the
gradient flow. Our proof in Appendix D.4 corrects this error.

We next study the properties of orthogonal separable datasets. DenoteThe following lemma give a sufficient condition on w to satisfy the condition in Proposition 4. B = {w âˆˆ R[d] : âˆ¥wâˆ¥2 â‰¤ 1}.
**Lemma 4 Assume that (X, y) is orthogonal separable. Suppose that w âˆˆB is a local maximizer of**
**wysuch that[T] âˆˆB(Xw is a local minimizer of) y+ inn = B âˆ’ and1.** (Xw)+ Ì¸= 0 y[T]. Then,(Xw)+ âŸ¨ inw, B xn andâŸ© _> ( 0Xw for n)+ âˆˆ Ì¸= 0[N. Then,] such that âŸ¨w, y xnn = 1âŸ©_ _> 0. Suppose that for n âˆˆ_ [N ]

We show an equivalent condition of u âˆˆB being the local maximizer/minimizer of y[T] (Xu)+ in B.
**Proposition 7 Assume that (X, y) is orthogonal separable. Then, u âˆˆB is a local maximizer of**
**y[T]** (Xu)+ in B is equivalent to cos âˆ (u, g(u, y)) = 1. Similarly, u âˆˆB is a local minimizer of
**y[T]** (Xu)+ in B is equivalent to cos âˆ (u, g(u, y)) = âˆ’1.

Based on Proposition 4 and 7, we present the main theorem.
**Theorem 4 Suppose that the dataset is orthogonal separable and Î¸(t) follows the gradient flow.**
_For almost all initializations which are sufficiently close to zero, the limiting point ofSuppose that the neural network is scaled at initialization, i.e., âˆ¥w1,i(0)âˆ¥2 = |w2,i(0)âˆ¥| for allÎ¸Î¸((tt))âˆ¥2_ _[is] i âˆˆâˆ¥Î¸Î¸[[âˆ—]m[âˆ—]âˆ¥2].[,]_

_where Î¸[âˆ—]_ _is a global minimizer of the max-margin problem (2)._

We present a sketch of the proof. According to Proposition 6, for initialization sufficiently close to zero,
there exist two neurons and time T+, T _> 0 such that cos âˆ (w1,i+_ (T+), g(w1,i+ (T+), y)) 1 _Î´_
_âˆ’_ _â‰¥_ _âˆ’_
and cos âˆ (w1,iâˆ’ (Tâˆ’), g(w1,iâˆ’ (Tâˆ’), y)) â‰¤âˆ’(1 âˆ’ _Î´). This implies that w1,i+_ (T+) and w1,iâˆ’ (T+)
are sufficiently close to certain stationary points of gradient flow maximizing/minimizing y[T] (Xu+)
over B, i.e., {u âˆˆB| cos(u, g(u, y)) = Â±1}. As the dataset is orthogonal separable, from Proposition
7 and Lemma 4, the induced masking matrices _D[Ë†]_ _i+_ (T+) and _D[Ë†]_ _iâˆ’_ (Tâˆ’) by w1,i+ (T+)/w1,iâˆ’ (Tâˆ’)
in Proposition 3 satisfy that _D[Ë†]_ _i+_ (T+) **diag(I(y = 1)) and** _D[Ë†]_ _i_ (T ) **diag(I(y =** 1)).
_â‰¥_ _âˆ’_ _âˆ’_ _â‰¥_ _âˆ’_
According to Lemma 3 in (Phuong & Lampert, 2021), for t â‰¥ max{T+, Tâˆ’}, we also have _D[Ë†]_ _i+_ (t) â‰¥
**diag(I(y = 1)) and** _D[Ë†]_ _i_ (t) **diag(I(y =** 1)). According to Theorem 3 and Proposition 4, the
_âˆ’_ _â‰¥_ _âˆ’_
KKT point of the non-convex problem (2) that gradient flow converges to corresponds to the KKT
point of the convex problem (14).

5 CONCLUSION

We provide a convex formulation of the non-convex max-margin problem for two-layer ReLU neural
networks and uncover a primal-dual extreme point relation between non-convex subgradient flow.
Under the assumptions on the training data, we show that flows converge to KKT points of the convex
max-margin problem, hence a global optimum of the non-convex objective.

6 ACKNOWLEDGEMENTS

This work was partially supported by the National Science Foundation under grants ECCS-2037304,
DMS-2134248, and US Army Research Office.


-----

REFERENCES

Burak Bartan and Mert Pilanci. Neural spectrahedra and semidefinite lifts: Global convex optimization of polynomial activation neural networks in fully polynomial-time. arXiv preprint
_arXiv:2101.02429, 2021a._

Burak Bartan and Mert Pilanci. Training quantized neural networks to global optimality via semidefinite programming. International Conference on Machine Learning (ICML), 2021, 2021b.

LÃ©naÃ¯c Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. Advances in Neural Information Processing Systems, 31:3036â€“
3046, 2018.

Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities with
applications in pattern recognition. IEEE transactions on electronic computers, (3):326â€“334, 1965.

Tolga Ergen and Mert Pilanci. Implicit convex regularizers of cnn architectures: Convex optimization
of two-and three-layer networks in polynomial time. International Conference on Learning
_Representations (ICLR), 2021, 2020._

Tolga Ergen and Mert Pilanci. Convex geometry and duality of over-parameterized neural networks.
_Journal of Machine Learning Research, 22(212):1â€“63, 2021a._

Tolga Ergen and Mert Pilanci. Global optimality beyond two layers: Training deep relu networks
via convex programs. In International Conference on Machine Learning, pp. 2993â€“3003. PMLR,
2021b.

Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality. In
_International Conference on Machine Learning, pp. 3004â€“3014. PMLR, 2021c._

Tolga Ergen, Arda Sahiner, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Demystifying batch normalization in relu networks: Equivalent convex optimization models and implicit
regularization. arXiv preprint arXiv:2103.01499, 2021.

Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning, pp. 1832â€“1841.
PMLR, 2018.

Vikul Gupta, Burak Bartan, Tolga Ergen, and Mert Pilanci. Exact and relaxed convex formulations
for shallow neural autoregressive models. In International Conference on Acoustics, Speech, and
_Signal Processing, 2021._

Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural
networks beyond ntk. In Conference on Learning Theory, pp. 2613â€“2682. PMLR, 2020.

Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
_arXiv preprint arXiv:1906.05890, 2019._

Chao Ma, Lei Wu, and Weinan E. The quenching-activation behavior of the gradient descent dynamics
for two-layer neural network models. arXiv preprint arXiv:2006.14450, 2020.

Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient descent quantizes relu network
features. arXiv preprint arXiv:1803.08367, 2018.

Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665â€“E7671,
2018.

Mary Phuong and Christoph H Lampert. The inductive bias of relu networks on orthogonally
separable data. 2021.

Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time
convex optimization formulations for two-layer networks. pp. 7695â€“7705, 2020.


-----

Arda Sahiner, Tolga Ergen, Batu Ozturkler, Burak Bartan, John Pauly, Morteza Mardani, and Mert
Pilanci. Hidden convexity of wasserstein gans: Interpretable generative models with closed-form
solutions. arXiv preprint arXiv:2107.05680, 2021a.

Arda Sahiner, Tolga Ergen, John Pauly, and Mert Pilanci. Vector-output relu neural network problems
are copositive programs: Convex analysis of two layer networks and polynomial-time algorithms.
_International Conference on Learning Representations (ICLR), 2021b._

Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, and John Pauly. Convex regularization
behind neural reconstruction. International Conference on Learning Representations (ICLR),
2021c.

Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822â€“2878, 2018.

Gang Wang, Georgios B Giannakis, and Jie Chen. Learning relu networks on linearly separable
data: Algorithm, optimality, and generalization. IEEE Transactions on Signal Processing, 67(9):
2357â€“2370, 2019.

Qiuling Yang, Alireza Sadeghi, Gang Wang, and Jian Sun. Learning two-layer relu networks is nearly
as easy as learning linear classifiers on separable data. IEEE Transactions on Signal Processing,
69:4416â€“4427, 2021.

Mo Zhou, Rong Ge, and Chi Jin. A local convergence theory for mildly over-parameterized two-layer
neural network. arXiv preprint arXiv:2102.02410, 2021.


-----

A DEFINITIONS AND NOTIONS

We introduce several useful definitions and notions which will be utilized in the proof.

A.1 DEFINITIONS

**Definition 1 Let O âŠ‚** R[n] be an open set and let F : O â†’ R be locally Lipschitz continuous at
_x âˆˆO. Let DF be the differentiable points of F in O. The B-subdifferential of F at x is defined by_

_âˆ‚[B]F_ (x) := lim _._ (28)
kâ†’âˆž _[F][ â€²][(][x][k][)][|][x][k][ âˆˆ]_ _[D][F][, x][k][ â†’]_ _[x]_

The set âˆ‚[â—¦]F (x) = co(âˆ‚BF (x)) is called Clarkeâ€™s subdifferential, where co denotes the convex hull.

**Definition 2there exists** A matrixz 2 1 such that A is spike-free if and only if the following conditions hold: for all âˆ¥uâˆ¥2 â‰¤ 1,
_âˆ¥_ _âˆ¥_ _â‰¤_ (Au)+ = Az. (29)

This is equivalent to say that

max (30)
**u:âˆ¥uâˆ¥2â‰¤1,(Iâˆ’XX[T]** )(Xu)+=0 _[âˆ¥][X][â€ ][(][Xu][)][+][âˆ¥][2][ â‰¤]_ [1][.]

A.2 NOTIONS

We use the following letters for indexing.

-  The index n is for the n-th data sample xn.

-  We use the index i to represent the i-th neuron-pair (w1,i, w2,i).

-  The index j is for the j-th masking matrix Di .
_âˆˆP_

B PROOFS IN SECTION 3

B.1 PROOF FOR PROPOSITION 2

Consider the following loss function [Ëœ]l : R[N] _Ã— R[N]_ _â†’_ R âˆª{+âˆž}

_â„“Ëœ(z, y) =_ 0, _ynzn â‰¥_ 1, âˆ€n âˆˆ [N ], (31)
+ _,_ otherwise.
 _âˆž_

For a given y âˆˆ{1, âˆ’1}[N], _â„“[Ëœ](z, y) is a convex loss function of z. The non-convex max-margin is_
equivalent to

min [Ëœ]l ((XW1)+w2, y) + [1] **W1** _F_ [+][ âˆ¥][w][2][âˆ¥]2[2] _._ (32)

2 _âˆ¥_ _âˆ¥[2]_

According to Appendix A.13 in (Pilanci & Ergen, 2020), the problem (32) is equivalent to  


+ [1]


min [Ëœ]l


**DiX(u[â€²]i**
_i=1_ _[âˆ’]_ **[u][i][)][,][ y]**

X


_âˆ¥W1âˆ¥F[2]_ [+][ âˆ¥][w][2][âˆ¥]2[2]


(33)


s.t. (2Di _I)Xui_ 0, (2Di _I)Xu[â€²]i_
_âˆ’_ _â‰¥_ _âˆ’_ _[â‰¥]_ [0][,][ âˆ€][i][ âˆˆ] [[][p][]][.]

This is equivalent to (14). For fixed y âˆˆ{1, âˆ’1}[N], the Fenchel conjugate function of _â„“[Ëœ](z, y) with_
respect to z can be computed by


_l[âˆ—](Î»[Ë†], y) = max_ _â„“(z, y)_
**z** R[N][ z][T][ x][ âˆ’] [Ëœ]
_âˆˆ_

= max **_Î», s.t. diag(y)z_** **1,**
**z** R[N][ z][T][ Ë†] _â‰¥_
_âˆˆ_

**y[T][ Ë†]Î»,** **diag(y)Î»[Ë†]** 0
= _â‰¤_
+ _,_ otherwise.
 _âˆž_


(34)


-----

According to Theorem 6 in (Pilanci & Ergen, 2020), the dual problem of (14) writes


max _l[âˆ—](Î», y), s.t._ max (35)
_âˆ’[Ëœ]_ **u:** **u** 2 1
_âˆ¥_ _âˆ¥_ _â‰¤_ _[|][Î»][T][ (][Xu][)][+][| â‰¤]_ [1][,]


which is equivalent to


max **y[T]** **_Î», s.t. diag(y)Î»_** 0, max (36)
_âˆ’_ _â‰¤_ **u:** **u** 2 1
_âˆ¥_ _âˆ¥_ _â‰¤_ _[|][Î»][T][ (][Xu][)][+][| â‰¤]_ [1][.]

By taking Î» = âˆ’Î»[Ë†], we derive (15). This completes the proof.

B.2 PROOF FOR PROPOSITION 3

For the non-convex max-margin problem (13), consider the Lagrange function


_L(W1, w2, Î») = [1]_ _F_ [+][ âˆ¥][w][2][âˆ¥]2[2][)][ âˆ’] [(][Y][Î»][)][T][ (][Y][(][XW][1][)][+][w][2]

2 [(][âˆ¥][W][1][âˆ¥][2] _[âˆ’]_ **[1][)]**

where YÎ» âª° 0. The KKT point of the non-convex max-margin problem (13) (in terms of Bsubdifferential) satisfies
0 âˆˆ _âˆ‚W[B]_ 1 _[L][(][W][1][,][ w][2][,][ Î»][)][,]_

**w2 âˆ’** (XW1)[T]+[Î»][ = 0][,] (37)

_Î»n(yn(x[T]n_ **[W][1][)][+][w][2]**

_[âˆ’]_ [1) = 0][.]

The KKT condition on the i-th column of W1 is equivalent to


_w2,iÎ»nxngn,i,_ (38)
_n=1_

X


**w1,i =**


where gn,i âˆˆ _âˆ‚[B](z)+|z=xTn_ **[w][1][,i]** [. In other words, we have]

= I(x[T]n **[w][1][,i]** if x[T]n **[w][1][,i]**

_gn,i_ _[â‰¥]_ [0)][,] _[Ì¸][= 0][,]_ (39)

( 0, 1 _,_ if x[T]n **[w][1][,i]** [= 0][.]

_âˆˆ{_ _}_

Let **D[Ë†]** _i = diag([g1,i, . . ., gN,i]). Then, we can write that_


**w1,i =**


_Î»ngn,ixnw2,i_
_n=1_

X


(40)


=w2,iX[T][ Ë†]DiÎ».

_gn,ix[T]n_ **[w][1][,i]** [= 0][.] (41)


From the definition of gn,i, we have

Therefore, we can compute that


_w2,i =(Xw1,i)[T]+[Î»]_

_N_

= I(x[T]n **[w][1][,i]** _n_ **[w][1][,i][Î»][n]**

_n=1_ _[â‰¥]_ [0)][x][T]

X

_N_

= _gn,ix[T]n_ **[w][1][,i][Î»][n]**

_n=1_

X

=w1[T],i[X][T][ Ë†]DiÎ».


(42)


In summary, we have
**w1,i = w2,iX[T][ Ë†]DiÎ»,** _w2,i = w1[T],i[X][T][ Ë†]DiÎ»._ (43)
Suppose that w2,i = 0. This implies that
_Ì¸_
**w1,i**

= X[T][ Ë†]DiÎ», **X[T][ Ë†]DiÎ»** 2 = 1. (44)
_w2,i_ _âˆ¥_ _âˆ¥_

This completes the proof.


-----

B.3 PROOF FOR THEOREM 3

PROOF We can write the Lagrange function for the convex max-margin problem (14) as
_L({uj}j[p]=1[,][ {][u]j[â€²]_ _[}][p]j=1[,][ Î»][,][ {][z][j][}][p]j=1[,][ {][z]i[â€²]_ _[}][p]j=1[)]_


( **uj** 2 + **u[â€²]j[âˆ¥][2][) +][ Î»][T][ diag][(][y][)]** **1** **diag(y)**
_âˆ¥_ _âˆ¥_ _âˆ¥_ ï£« _âˆ’_
_j=1_

X

_p_ ï£­

(z[T]j [(2][D][j] [+ (][z][â€²]j[)][T][ (2][D][j] _j[)]_

_âˆ’_ _j=1_ _[âˆ’]_ _[I][)][Xu][j]_ _[âˆ’]_ _[I][)][Xu][â€²]_

X


_j=1_ **DjX(uj âˆ’** **u[â€²]j[)]**

X


(45)


=Î»[T] **y +**


_j=1(u[â€²]j[)][T][ (][X][T][ D][j][Î»][ âˆ’]_ **[X][T][ (2][D][j]** _[âˆ’]_ _[I][)][z]j[â€²]_ [)]

X


(âˆ¥ujâˆ¥2 + âˆ¥u[â€²]j[âˆ¥][2][) +]
_j=1_

X


+ (uj)[T] ( **X[T]** **DjÎ»** **X[T]** (2Dj _I)zj)._

_j=1_ _âˆ’_ _âˆ’_ _âˆ’_

X

where zj, z[â€²]j _j_
The KKT point shall satisfy the following KKT conditions:[âˆˆ] [R][N][ satisfies that][ z][j][ â‰¥] [0][,][ z][â€²] _[â‰¥]_ [0][ for][ j][ âˆˆ] [[][p][]][ and][ Î»][ âˆˆ] [R][N][ satisfies that][ diag][(][y][)][Î»][ â‰¥] [0][.]
_âˆ’X[T]_ **DjÎ» + X[T]** (2Dj âˆ’ _I)z[â€²]j_ _[âˆˆ]_ _[âˆ‚][u]j[â€²]_ _[âˆ¥][u]j[â€²]_ _[âˆ¥][2][,]_

**X[T]** **DjÎ» + X[T]** (2Dj _I)zj_ _âˆ‚uj_ **uj** 2,
_âˆ’_ _âˆˆ_ _âˆ¥_ _âˆ¥_


(Dj)nx[T]n [(][u][j] _j[)][ âˆ’]_ _[y][n]_ = 0,
_j=1_ _[âˆ’]_ **[u][â€²]** ï£¶

X

ï£¸

_zj,n(2(Dj)n,n_ 1)x[T]n **[u][j]** [= 0][,]
_âˆ’_

_zj,n[â€²]_ [(2(][D][j][)][n,n] _[âˆ’]_ [1)][x]n[T] **[u]j[â€²]** [= 0][.]


(46)


_Î»n_


Let (W1, w2, Î») be the KKT point of the non-convex problem (2) and Î» satisfies (17). Let **D[Ë†]** _i be_
the diagonal matrix defined in Proposition 3 with respect to w1,i and denote _P[Â¯] = {D[Ë†]_ _i|i âˆˆ_ [m]}.
Without the loss of generality, we may assume that {D[Â¯] _i}i[m]=1_ [are different. (Otherwise, we can merge]
two neurons w1,i1 and w1,i2 with **D[Â¯]** _i1 = D[Â¯]_ _i2 together.)_

Suppose thatuj = âˆ’w1,iw D2,ij and âˆˆ **zP[Ë†]j, i.e., = 0, the following identities hold. Dj = D[Ë†]** _i for certain i âˆˆ_ [m]. By letting u[â€²]j [=][ w][1][,i][w][2][,i][,][ z]j[â€²] [= 0][,]

**X[T]** **DjÎ» + X[T]** (2Di _I)z[â€²]j_ [=][ X][T][ Ë†]DiÎ» = **[w][1][,i]** = **u[â€²]i** (47)
_âˆ’_ _w2,i_ **u[â€²]i[âˆ¥]** _[.]_

_âˆ¥_

**ui**

**X[T]** **DjÎ» + X[T]** (2Di _I)zj =_ **X[T][ Ë†]DiÎ» =** **[w][1][,i]** = (48)
_âˆ’_ _âˆ’_ _âˆ’_ _w2,i_ **ui**

_âˆ¥_ _âˆ¥_ _[.]_

Therefore, for index j satisfying Di _P_, the first two KKT conditions in (46) hold.
_âˆˆ_ [Ë†]

For Dj /âˆˆ _P[Ë†], we can let uj = u[â€²]j_ [= 0][. As][ Î»][ satisfies (17), we have]

max (49)
_âˆ¥uâˆ¥2â‰¤1,(2Dj_ _âˆ’I)Xuâ‰¥0_ _[|][Î»][T][ D][j][Xu][| â‰¤]_ [1][.]

According to Lemma 4 in (Pilanci & Ergen, 2020), this implies that there exist zj, z[â€²]j

_[â‰¥]_ [0][ such that]

**X[T]** **DjÎ» + Z[T]** (2Dj _I)z[â€²]j[âˆ¥â‰¤]_ [1][,][ âˆ¥][X][T][ D][j][Î»][ +][ Z][T][ (2][D][j] (50)
_âˆ¥âˆ’_ _âˆ’_ _[âˆ’]_ _[I][)][z][j][âˆ¥â‰¤]_ [1][.]
Therefore, the first two KKT conditions in (46) hold.

From our choice of uj, zj, u[â€²]j[,][ z][j][, the last two KKT conditions in (46) hold. We also note that]


**DjX(u[â€²]j**
_j=1_ _[âˆ’]_ **[u][j][) =]**

X


(Xw1,i)+w2,i. (51)
_i=1_

X


As (W1, w2, Î») is the KKT point of the non-convex problem, the third KKT condition in (46) holds.
This completes the proof.


-----

C PROOFS IN SECTION 3.1

In this section, we present several proofs for propositions in Section 3.1.

C.1 PROOF FOR PROPOSITION 4

We start with two lemmas.
**Lemma 5 Suppose that u0 = X[T][ Ë†]D0Î» and âˆ¥u0âˆ¥2 â‰¤** 1. For any masking matrix Dj âˆˆP such that
(Dj âˆ’ **D[Ë†]** 0)I(Î» > 0) = 0, we have

max (52)
(2Dj _I)Xu_ 0, **u** 2 1 **_[Î»][T][ D][j][Xu][ â‰¤]_** [1][.]
_âˆ’_ _â‰¥_ _âˆ¥_ _âˆ¥_ _â‰¤_

PROOF According to Lemma 4 in (Pilanci & Ergen, 2020), the constraint (52) is equivalent to that
there exist zj âˆˆ R[N] such that zj â‰¥ 0 and
**X[T]** **DjÎ» + X[T]** (2Dj _I)zj_ 1. (53)
_âˆ¥_ _âˆ’_ _âˆ¥â‰¤_

Consider the index n âˆˆ [N ] such that (Dj âˆ’ **D[Ë†]** 0)nn Ì¸= 0. As (Dj âˆ’ **D[Ë†]** 0)I(Î» > 0) = 0, we have
_Î»n â‰¤_ 0. We let (zj)n = âˆ’Î»n. If ( D[Ë†] 0)nn = 0, then we have (Dj)nn = 1 and

(Dj **D0)nnÎ»nxn = Î»nxn =** **x[T]n** [(2(][D][j][)][nn] (54)
_âˆ’_ [Ë†] _âˆ’_ _[âˆ’]_ [1)(][z][j][)][n][.]

If ( D[Ë†] 0)nn = 1, then we have (Dj)nn = 0 and

(Dj **D0)nnÎ»nxn =** _Î»nxn =_ **x[T]n** [(2(][D][j][)][nn] (55)
_âˆ’_ [Ë†] _âˆ’_ _âˆ’_ _[âˆ’]_ [1)(][z][j][)][n][.]
For other index n âˆˆ [N ], we simply let (zj)n = 0. Then, we have

(Dj **D0)nnÎ»nxn = 0 =** **x[T]n** [(2(][D][j][)][nn] (56)
_âˆ’_ [Ë†] _âˆ’_ _[âˆ’]_ [1)(][z][j][)][n][.]
Based on our choice of zj, we have zj 0 and for n [N ]
_â‰¥_ _âˆˆ_

(Dj **D0)nnÎ»nxn =** **x[T]n** [(2(][D][j][)][nn] (57)
_âˆ’_ [Ë†] _âˆ’_ _[âˆ’]_ [1)(][z][j][)][n][.]
This implies that
**X[T]** (Dj **D0)Î» =** **X[T]** (2Dj _I)zj._ (58)
_âˆ’_ [Ë†] _âˆ’_ _âˆ’_
Hence, we have
**X[T]** **DjÎ» + X[T]** (2Dj âˆ’ _I)zj = X[T][ Ë†]D0Î» = u0._ (59)
Therefore, **X[T]** **DjÎ» + X[T]** (2Dj _I)zj_ 2 1.
_âˆ¥_ _âˆ’_ _âˆ¥_ _â‰¤_

**Lemma 6 Suppose that the data is orthogonal separable and YÎ» â‰¥** 0. Suppose that u0 = X[T][ Ë†]D0Î»
_andu0 âˆ¥2u0âˆ¥12 â‰¤. Therefore,1. For any masking matrix (52) holds._ **Dj such that** **D[Ë†]** 0 âˆ’ **Dj â‰¥** 0, we have âˆ¥X[T] **DjÎ»âˆ¥2 â‰¤**
_âˆ¥_ _âˆ¥_ _â‰¤_

PROOF We note that u0 = X[T] ( D[Ë†] 0 âˆ’ **Dj)Î» + XDjÎ». Denote a = X[T]** ( D[Ë†] 0 âˆ’ **Dj)Î» and b =**
**X[T]** **DjÎ». We note that**

_T_

**a[T]** **b =** _Î»nxn_ _Î»nâ€²_ **xnâ€²** _._ (60)

ï£« ï£¶ [ï£«] ï£¶

_n:( D[Ë†]_ 0)nn=1X,(Dj )n,n=0 _n[â€²]:( D[Ë†]_ 0)nâ€² _nâ€²X=0,(Dj_ )nâ€² _nâ€²_ =0

As diag(y)Î» â‰¥ 0ï£­, Î»n has the same signature withï£¸ _yï£­n. Therefore, from the orthogonal separabilityï£¸_
of the data, we have
_Î»nÎ»nâ€²_ **x[T]n** **[x][T]n[â€²][ â‰¥]** [0][.] (61)
This immediately implies that a[T] **b â‰¥** 0. Therefore,
1 â‰¥âˆ¥u0âˆ¥2[2] [=][ âˆ¥][a][ +][ b][âˆ¥]2[2] [=][ âˆ¥][a][âˆ¥]2[2] [+][ âˆ¥][b][âˆ¥]2[2] [+ 2][a][T][ b][ â‰¥âˆ¥][a][âˆ¥][2][.] (62)
This completes the proof.

Based on Lemma 5 and Lemma 6, we present the proof for Proposition 3. Let u+ = **ww21,i,i++** [. From the]

proof of Proposition 3, we note that âˆ¥u+âˆ¥2 = 1. For any masking matrix Dj âˆˆP, let **D[Ëœ]** = D[Ë†] _i+_ **Dj.**
As **D[Ë†]** _i+_ **D, according to Lemma 6, we have**
_â‰¥_ [Ëœ]

_âˆ¥X[T][ Ëœ]DÎ»âˆ¥2 â‰¤âˆ¥X[T][ Ë†]Di+_ **_Î»âˆ¥2 = âˆ¥u+âˆ¥2 â‰¤_** 1. (63)

AsFrom Lemma 5, we note that YÎ» â‰¥ 0 and **D[Ë†]** _i+ â‰¥_ **diag( Î»I(y satisfies = 1)), we have (52). Similarly, we can show that (Dj âˆ’D[Ëœ]** )I(Î» > 0) = Dj(I âˆ’ âˆ’Î»D[Ë†] also satisfiesi+ )I(Î» > 0) = 0 (52)..
This completes the proof.


-----

C.2 PROOF FOR PROPOSITION 5

PROOF Note that YÎ» 0. Let Y+ = diag(I(y = 1)) and Y = diag(I(y = 1)). We claim
_â‰¥_ _âˆ’_ _âˆ’_
that
max (64)
**u** 1 **_[Î»][T][ (][Xu][)][+][ = max]u_** 1 **_[Î»][T][ Y][+][(][Xu][)][+][.]_**
_âˆ¥_ _âˆ¥â‰¤_ _âˆ¥_ _âˆ¥â‰¤_

Firstly, we note that


**_Î»[T]_** (Xu)+ =


(Î»n)+(x[T]n **[u][)][+]** [=][ Î»][T][ Y][+][(][Xu][)][+][.] (65)
_n=1_

X


_Î»n(x[T]n_ **[u][)][+]**
_n=1_ _[â‰¤]_

X


This implies that max **u** 1 Î»[T] (Xu)+ max **u** 1 Î»[T] **Y+(Xu)+.**
_âˆ¥_ _âˆ¥â‰¤_ _â‰¤_ _âˆ¥_ _âˆ¥â‰¤_

On the other hand, suppose that u arg max **u** 1 Î»[T] **Y+(Xu)+. As X is spike-free, there exists z**
_âˆˆ_ _âˆ¥_ _âˆ¥â‰¤_
such that **z** 2 1 and Xz = (Xu)+. Therefore, we have
_âˆ¥_ _âˆ¥_ _â‰¤_

**_Î»[T]_** **Y+(Xu)+ = Î»[T]** **Y+Xz = Î»[T]** **Xz = Î»[T]** (Xz)+. (66)

This implies that max **u** 1 Î»[T] (Xu)+ max **u** 1 Î»[T] **Y+(Xu)+.**
_âˆ¥_ _âˆ¥â‰¤_ _â‰¥_ _âˆ¥_ _âˆ¥â‰¤_

For any Dj with Dj **Y+. We note that**
_âˆˆP_ _â‰¥_

**_Î»[T]_** (Xu)+ **_Î»[T]_** **Dj(Xu)+** **_Î»[T]_** **Y+(Xu)+.** (67)
_â‰¤_ _â‰¤_

Combining with (65), this implies that max **u** 1 Î»[T] (Xu)+ = max **u** 1 Î»[T] **D(Xu)+.**
_âˆ¥_ _âˆ¥â‰¤_ _âˆ¥_ _âˆ¥â‰¤_

Let us go back to the original problem. Let u+ = **ww21,i,i++** [. We note that][ (][Xw][+][) = Ë†]Di+ **Xw+ =**
**DË†** _i+_ **XX[T][ Ë†]Di+** **_Î». Therefore, we have_**

**_Î»[T]_** (Xw+) = Î»[T][ Ë†]Di+ **XX[T][ Ë†]Di+** **_Î» = âˆ¥X[T][ Ë†]Di+_** **_Î»âˆ¥2[2]_** [=][ âˆ¥][u][+][âˆ¥]2[2] [= 1][.] (68)

Thus, for any âˆ¥uâˆ¥2 â‰¤ 1, suppose that (Xu)+ = Xz, where âˆ¥zâˆ¥2 â‰¤ 1. Then, we have

**_Î»[T][ Ë†]Di+_** (Xu)+ = Î»[T][ Ë†]Di+ **Xz â‰¤âˆ¥zâˆ¥2 â‰¤** 1. (69)

Therefore, max **u** 1 Î»[T] (Xu)+ = max **u** 1 Î»[T] **D+(Xu)+** 1. Similarly, we have
_âˆ¥_ _âˆ¥â‰¤_ _âˆ¥_ _âˆ¥â‰¤_ _â‰¤_

min
**u** 1 **_[Î»][T][ (][Xu][)][+][ = min]u_** 1 **_[Î»][T][ D][âˆ’][(][Xu][)][+][ â‰¥âˆ’][1][.]_**
_âˆ¥_ _âˆ¥â‰¤_ _âˆ¥_ _âˆ¥â‰¤_

This completes the proof.

D PROOFS IN SECTION 4

D.1 PROOF FOR LEMMA 1

PROOF According to the sub-gradient flow (22), we can compute that


_âˆ‚_

**w1,i** 2 2,i = 2w1[T],i _w2,ig(u,_ **_Î»)_** 2w2,iw1[T],i[g][(][u][,][ e]Î») = 0. (70)
_âˆ‚t_ _âˆ¥_ _âˆ¥[2]_ _[âˆ’]_ _[w][2]_ _âˆ’_
    

Let T0 = sup{T _|âˆ¥w1,i(t)âˆ¥2 = |w2,i(t)| > 0, âˆ€i âˆˆ_ [n][e], t âˆˆ [0, T )}. For t âˆˆ [0, T0), as the neural
network is scaled, it is sufficient study the dynamics of w1,i in the polar coordinate. Let us write
**w1,i(t) = e[r][i][(][t][)]ui(t), where âˆ¥ui(t)âˆ¥2 = 1. Then, in terms of polar coordinate, the projected gradient**
flow follows
_âˆ‚_

_i_ **[g][(][u][i][,][ e]Î»),**
_âˆ‚t_ _[r][i][ =][sign][(][w][2][,i][)][u][T]_

(71)

_âˆ‚_

**g(ui,** **_Î»)_** **u[T]i** **[g][(][u][i][,][ e]Î»)** **ui** _._
_âˆ‚t_ **[u][i][ =][sign][(][w][2][,i][)]** _âˆ’_

Without the loss of generality, we may assume that _w2,i(0)_ = 0 for i  [m]. Denote

[e] _Ì¸_ _âˆˆ_

_xmax = max_ (72)
_iâˆˆ[n]_ _[âˆ¥][x][i][âˆ¥][2][.]_


-----

From the definition of **_Î», we have_** **_Î»_** 1/4. Therefore, we have
_âˆ¥[e]âˆ¥âˆž_ _â‰¤_

[e]âˆ‚ **_Î»)_** 2 _Î»Ëœjxj_

_âˆ‚t_ _[r][i]_ _[â‰¤âˆ¥][g][(][u][i][,][ e]_ _âˆ¥_ _â‰¤_ _j:xX[T]j_ **[u][>][0]** 2 _â‰¤_ _[nx]2[max]_

Therefore, for finite t > 0, we have

_ri(t)_ _ri(0)_ _,_
_â‰¥_ _âˆ’_ _[nx][max]4_ _[t]_

which implies that |w2,i(t)| > 0. This implies that T0 = âˆž.

D.2 PROOF OF LEMMA 2

PROOF As we have âˆ¥w1,iâˆ¥2 = |w2,i|, for n âˆˆ [N ], we can compute that

_|qn| =|(x[T]n_ **[W][1][)][+][w][2][|]**

_m_

_â‰¤_ _|(x[T]n_ **[w][1][,i][)][+][w][2][,i][|]**

_i=1_

Xm

= _âˆ¥w1,iâˆ¥2|(x[T]n_ **[w][1][,i][)][+][|]**

_i=1_

Xm

_â‰¤_ _âˆ¥w1,iâˆ¥2|x[T]n_ **[w][1][,i][|]**

_i=1_

X


(73)

(74)

(75)


_âˆ¥w1,iâˆ¥2[2][.]_
_i=1_

X


**xn** 2
_â‰¤âˆ¥_ _âˆ¥_


Note that _Î»[Ëœ]n = âˆ’ynâ„“[â€²](qn) and_ _[y]4[n]_ [=][ âˆ’][y][n][â„“][â€²][(0)][. As][ â„“][â€²][ is][ 1]4 [-Lipschitz continuous, we have]

_m_

_Î»n_ _yn/4_ **w1,i** 2[.] (76)
_âˆ’_ _â‰¤_ [1]4 _[|][q][n][| â‰¤âˆ¥][x]4[n][âˆ¥][2]_ _i=1_ _âˆ¥_ _âˆ¥[2]_

X

For any Ë†Ïƒ, as _Î»[Ëœ]n_ [0, 1[Ëœ]/4] for n [N ], we have
_âˆˆQ_ _âˆˆ_ _âˆˆ_

**g(Ë†Ïƒ,** **_Î»)_** **g(Ë†Ïƒ, y/4)** 2
_âˆ¥_ [e] _âˆ’(Î»[Ëœ]n_ _yn/4)âˆ¥xn_

_â‰¤_ _âˆ’_

_n:(Ë†Ïƒ)n>0_

X


_Î»k_ _Î»k_ **xk** 2
_|_ _âˆ’_ [Ëœ] _| âˆ¥_ _âˆ¥_
_k:(Ë†Ïƒ)k>0_

X


(77)


**xk** 2
_âˆ¥_ _âˆ¥[2]_


**w1,j** 2
_âˆ¥_ _âˆ¥[2]_
_j=1_

X


_k=1_


=c1 _âˆ¥w1,iâˆ¥2[2][,]_

_i=1_

X


where c1 = [1]4 _[âˆ¥][X][âˆ¥]F[2]_ _[>][ 0][ is a constant. Therefore, we can bound][ âˆ¥][g][(Ë†]Ïƒ,_ **_Î»(t))âˆ¥_** by

_m_ _m_

**g(Ë†Ïƒ,** **_Î»(t))_** 2 **g(Ë†Ïƒ, y/4)** 2 + c1 **w1,i(t)** 2 [e][+][ c][1] _e[2][r][i][(][t][)],_ (78)
_âˆ¥_ _âˆ¥_ _â‰¤âˆ¥_ _âˆ¥_ _i=1_ _âˆ¥_ _âˆ¥[2]_ _[â‰¤]_ _[d][max]_ _i=1_

X X

where we let [e]
_gmax = max_ (79)
**_ÏƒâˆˆQ_** _[âˆ¥][g][(][Ïƒ,][ y][/][4)][âˆ¥][2][.]_


-----

Let r(t) = maxi [m] ri(t). We note that
_âˆˆ_

_âˆ‚_

(80)
_âˆ‚t_ _[r][(][t][)][ â‰¤]_ _[d][max][ +][ c][1][ne][2][r][(][t][)][ â‰¤]_ _[c][2][(1 +][ e][2][r][(][t][)][)][,]_

where c2 = max{nc1, dmax} > 0 is a constant. If we start with r(0) â‰ª 0, then, r(t) cannot grow
much faster than c2t. Let Ëœr(t) satisfy the following ODE:

_âˆ‚_ _r(t) = c2(1 + e[2Ëœ]r(t))._ (81)

_âˆ‚t_ [Ëœ]

The solution is given by

_rËœa(t) = c2(t_ _a)_ (82)
_âˆ’_ _âˆ’_ 2 [1] [log(1][ âˆ’] _[e][2][c][2][(][t][âˆ’][a][)][)][,]_


where a > 0 is a parameter depending on the initialization. For any initial r(0), we have a unique a
satisfying Ëœra(0) = r(0). Therefore, we have r(t) _rËœa(t) and_
_â‰¤_

**g(Ïƒ,** **_Î»(t))_** **g(Ïƒ, y/4)** 2 _c1ne[2Ëœ]ra(t)._ (83)
_âˆ¥_ _âˆ’_ _âˆ¥_ _â‰¤_

According to the bound (83), by choosing a sufficiently small r0, (which leads to a sufficiently small
_a), such that_ [e]

_e[2Ëœ]ra(T )_ min _dminÎ´_ _,_ _d[2]min[Î´]_ _._ (84)
_â‰¤_ 16c1 4n[2]x[3]max
 

Therefore, for t â‰¤ _T_, we have

**g(Ë†Ïƒ,** **_Î»(t))_** **g(Ë†Ïƒ, y/4)** 2 _c1ne[2Ëœ]ra(t)_ _c1ne2Ëœra(T )_ _._ (85)
_âˆ¥_ _âˆ’_ _âˆ¥_ _â‰¤_ _â‰¤_ _â‰¤_ _[d][min]8_ _[Î´]_

Hence, we have

[e]

**g(Ïƒ(t),** **_Î»(t))_** **g(Ïƒ(t), y/4)** 2 _c1ne[2Ëœ]ra(t)_ _c1ne2Ëœra(T )_ _._ (86)
_âˆ¥_ _âˆ’_ _âˆ¥_ _â‰¤_ _â‰¤_ _â‰¤_ _[d][min]8_ _[Î´]_

We can compute that

[e] _d_

_Î»Ëœi =_ _yil[(2)](qi)_ _[d]_ (87)
_dt_ _âˆ’_ _dt_ _[q][i][.]_

As l[(2)](q) âˆˆ (0, 1/4], we can compute that


_w2,j_ **xi** 2
_|_ _|âˆ¥_ _âˆ¥_
_j=1_

X


_dt_ _[q][i]_


_dt_ **[w][1][,j]**


**w1,j** 2 **xi** 2
_âˆ¥_ _âˆ¥_ _âˆ¥_ _âˆ¥_
_j=1_

X


_dt_ _[w][2][,j]_


(88)


_â‰¤_ _[n]4_


_m_

**w1,j** 2[x]max[2] [+][ n]
_âˆ¥_ _âˆ¥[2]_ 4
_j=1_

X


_w2[2],j[x]max[2]_
_j=1_

X


max
_â‰¤_ _[nx]2[2]_


_e[2][r][(][t][)]._


Therefore, we have
_d_ _Î»Ëœi_ [=][ |][â„“][â€²â€²][(][q][i][)][|] _d_ _d_ max _e[2][r][(][t][)]._ (89)

_dt_ _dt_ _[q][i]_ 4 _dt_ _[q][i]_ 8

_[â‰¤]_ [1] _[â‰¤]_ _[nx][2]_

Suppose that sign(Xu(s)) = Ïƒ(t) holds for s in a small neighbor of t. Then, we have


_d_

**_Î»(t))_**
_dt_ **[g][(][u][(][t][)][,][ e]**

This completes the proof.


_d_
= **_Î»(t))_**

_dt_ **[g][(][Ïƒ][(][t][)][,][ e]** 2

_n_

**xi** 2 _d_ _Î»Ëœi_ max

_â‰¤_ _i=1_ _âˆ¥_ _âˆ¥_ _dt_ _[â‰¤]_ _[n][2][x]8[3]_

X

max _e[2Ëœ]ra(T )_ min[Î´] _._
_â‰¤_ _[n][2][x]8[3]_ _â‰¤_ _[d][2]16_


_e[2][r][(][t][)]_


(90)


-----

D.3 PROOF OF LEMMA 3

PROOF Let T0 = sup{T _|sign(Xu(t)) = sign(Xu0), âˆ€t âˆˆ_ [0, T )}. We analyze the dynamics of
**u(t) in the interval [0, min** _T0, T_ ]. For t min _T0, T_, as the statements in Lemma 2 hold, we
_{_ _[âˆ—]}_ _â‰¤_ _{_ _[âˆ—]}_
can compute that

_d_

_dt_ **[v][(][t][)][T][ u][(][t][)]**

_T_

**g(Ïƒ0,** **_Î»(t))_**

= _[d]_ **u(t)**

_dt_ **g(Ïƒ0,** **_Î»(t))_** 2 !

_âˆ¥_ _âˆ¥_

[e] _T_

**g(Ïƒ0,** **_Î»(t))_** _d_ 1 _d_

= [e] **_Î»(t))_**

**g(Ïƒ0,** **_Î»(t))_** 2 ! _dt_ **[u][(][t][) +][ u][(][t][)][T]** **g(Ïƒ0,** **_Î»(t))_** 2 _dt_ **[g][(][Ïƒ][0][,][ e]**
_âˆ¥_ _âˆ¥_ _âˆ¥_ _âˆ¥_

[e]

**_Î»(t))[T d]dt_** **[g][(][Ïƒ][0][,][ e]Î»(t))**
**u(t)[T]** **g[e](Ïƒ0,** **_Î»(t))_** **[g][(][Ïƒ][0][,][ e]** [e] (91)
_âˆ’_ **g(Ïƒ0,** **_Î»(t))_** 2

_âˆ¥_ _âˆ¥[3]_

2 _d_

**g(Ïƒ0,** **_Î»(t))[T][ d][e]_** **_Î»(t))_**
_â‰¥_ _dt_ **[u][t][ âˆ’]** _gmin_ _dt_ **[g][(][Ïƒ][e][0][,][ e]** 2

**g(Ïƒ0,[e]Î»(t))** 2 1 (v(t)[T] **u(t))[2][]**
_â‰¥âˆ¥_ _âˆ¥_   _âˆ’_ _âˆ’_ _[g][min]8_ _[Î´]_

_g0 (1_ [e]Î´/8) 1 (v(t)[T] **u(t))[2][]**
_â‰¥g0_ 1 âˆ’ _Î´/4_   ( âˆ’v(t)[T] **u(t))[2][]** _._ _âˆ’_ _[g][min]8_ _[Î´]_

_â‰¥_ _âˆ’_ _âˆ’_

Here we utilize that  g0 â‰¥ _gmin, where gmin is defined in (24). Let z(t) satisfies the ODE_

_dz(t)_

= (1 _Î´/4_ _z(t)[2])g0,_ (92)
_dt_ _âˆ’_ _âˆ’_

with initialization z(0) = v0[T] **[u][0][. Then, we note that]**


2 1 _Î´/8_

_z(t) =_ 1 âˆ’ _Î´/8 âˆ’_ 1 + c3 exp(2pg0 âˆ’t/( 1 _Î´/8))_ _,_ (93)
p _âˆ’_

_âˆš1_ _Î´/8_ _âˆ’1_ p
where c3 = **v0[T]âˆ’[u][0]** _âˆ’_ 1 . We can compute that
 

_z(T3) = 1_ _Î´._ (94)
_âˆ’_

According to the comparison theorem, for t min _T0, T3_, we have
_â‰¤_ _{_ _}_

**v(t)[T]** **u(t) â‰¥** _z(t)._ (95)

We first consider the case where T0 = âˆž. As T0 = âˆž, we have

**v(T** _[âˆ—])[T]_ **u(T** _[âˆ—]) â‰¥_ _z(T_ _[âˆ—]) = 1 âˆ’_ _Î´._ (96)

Therefore, the second event holds for T â‰¤ _T_ _[âˆ—]._

Otherwise, we have T0 < . Recall that u1 = u(T0) and v1 = limt _T0 v(t). Let T1 =_
_âˆž_ _â†‘_
sup _T_ **v(t)[T]** **u(t) < v1[T]** **[u][1][,][ âˆ€][t][ âˆˆ]** [[0][, T] [)][}][ and][ T][2] [= sup][{][T] _[|][v][(][t][)][T][ u][(][t][)][ <][ 1][ âˆ’]_ _[Î´,][ âˆ€][t][ âˆˆ]_ [[0][, T] [)][}][.]
_{_ _|_
If T2 _T0, for t_ [0, T2], we have
_â‰¤_ _âˆˆ_

_d_

1 _Î´/4_ (1 _Î´)[2][]_ _g0 > 0._ (97)
_dt_ **[v][(][t][)][T][ u][(][t][)][ â‰¥]** _âˆ’_ _âˆ’_ _âˆ’_


Therefore, v(t)[T] **u(t) monotonically increases in [0, T2]. As v(t)[T]** **u(t)** _z(t) for t_ [0, T0], we
_â‰¥_ _âˆˆ_
have that z(T2) **v(T2)[T]** **u(T2) = 1** _Î´ = z(T3). Hence, we have T2_ _T_ . Therefore, the second
condition of the first event holds at â‰¤ _T = âˆ’_ _T2._ _â‰¤_ _[âˆ—]_

Then, we consider the case where T2 _T0. For t_ _T0, we have v(t)[T]_ **u(t)** 1 _Î´. This implies that_
**v1[T]** **[u][1]** _â‰¥_ _â‰¤_ _[< T][0][, as][ T][0]_ _â‰¤_ _âˆ’_

_[â‰¤]_ [1][ âˆ’] _[Î´][. Apparently, we have][ T][1]_ _[â‰¤]_ _[T][0][. If][ T][1]_ _[â‰¤]_ _[T][2][, for][ t][ âˆˆ]_ [[0][, T][0][]][, the inequality]


-----

(97) holds. This implies that limt _T0_ 0 v(t)[T] **u(T0) > v(T1)[T]** **u(T1) = limt** _T0_ 0 v(t)[T] **u(T0),**
_â†’_ _âˆ’_ _â†’_ _âˆ’_
which leads to a contradiction. Therefore, we have T0 = T1. We note that

_z(T_ [shift](u[T]1 **[v][1][)) =][ u]1[T]** **[v][1][.]** (98)

As u(t)[T] **g(u(t),** **_Î»(t))_** _z(t) for t_ [0, T0], we have that z(T1) **u[T]1** **[v][1]**
_T0 = T1 â‰¤_ _T_ [shift](u[T]1 **[v] â‰¥[1][)][. This completes the proof.] âˆˆ** _â‰¤_ _[â‰¤]_ _[z][(][T][4][)][. Hence, we have]_

[e]

D.4 PROOF OF PROPOSITION 6

We first introduce a lemma.

**Lemma 7 Let a, b âˆˆ** R[d] _and 0 < Î´ < c. Suppose that âˆ¥a âˆ’_ **bâˆ¥2 â‰¤** _Î´ and âˆ¥aâˆ¥2 â‰¥_ _c. Then, we have_
**a** **b**

(99)

**a** 2 _âˆ’_ **b** 2 2 _â‰¤_ [2]c [Î´] _[.]_
_âˆ¥_ _âˆ¥_ _âˆ¥_ _âˆ¥_

PROOF As Î´ < c, we have âˆ¥bâˆ¥2 > âˆ¥aâˆ¥2 âˆ’âˆ¥a âˆ’ _bâˆ¥2 â‰¥_ _c âˆ’_ _Î´ > 0. We first note that_

_Î´_

_âˆ¥aâˆ¥2[âˆ’][1]_ _âˆ’âˆ¥bâˆ¥2[âˆ’][1]_ = _[|âˆ¥][a][âˆ¥]a[2][ âˆ’âˆ¥]2_ **b[b]2[âˆ¥][2][|]** _â‰¤_ _c_ **b** 2 _._ (100)

_âˆ¥_ _âˆ¥_ _âˆ¥_ _âˆ¥_ _âˆ¥_ _âˆ¥_

Therefore, we can compute that


**a** 2
_âˆ¥_ _âˆ¥_


**b** 2
_âˆ¥_ _âˆ¥_


(101)


2

_[âˆ¥][b][âˆ¥]_


_â‰¤_ **a** 2 _âˆ’_ **a** 2

_âˆ¥_ _âˆ¥_ _âˆ¥_ _âˆ¥_

_â‰¤_ _[Î´]c_ [+][ Î´]c [= 2]c [Î´] _[.]_

This completes the proof.

Then we present the proof of Proposition 6.


**a** 2
_âˆ¥_ _âˆ¥_


**b** 2
_âˆ¥_ _âˆ¥_


PROOF As y[T] (Xu0)+ > 0, with sufficiently small initialization and sufficiently small Î´ > 0, we
also havetime T such thatÎ»(0)[T] ( uXu(T0))[T]+v â‰¥(Ty) â‰¥[T] (Xu1 âˆ’0)[3]4+[Î´][ by contradiction. Denote]/4 âˆ’âˆ¥Xâˆ¥2âˆ¥Î»[e](0) âˆ’ **y/4âˆ¥[ v]2 >[0][ =] 0[ v]. We prove that there exists a[(0)][. For all possible values]**

of **g(u, y/4)** 2, we can arrange them from the smallest to the largest by g(1) < g(2) < _< g(p)._
_âˆ¥_ [e] _âˆ¥_ _Â· Â· Â·_

Let Ti = 2[âˆš]1âˆ’1Î´/8g(i) log _âˆšâˆš11âˆ’âˆ’Î´/Î´/8+18âˆ’1+âˆ’Î´/Î´/22_ _[âˆ’]_ [log] _âˆšâˆš11âˆ’âˆ’Î´/Î´/88+âˆ’gg(([âˆ’][âˆ’]ii))[1][1][v][v]00[T][T]_ **[u][u][0][0]**  and T = _i=1_ _[T][i][. Suppose]_

that r0 is sufficiently small such that statements in Lemma 2 holds for T . According to Lemma 3,
we can find 0 = t0 < t1 < . . . such that for i = 1, . . ., sign(Xu(t)) is constant on[P][p] [ti 1, ti) and
_âˆ’_
**sign(Xu(ti** 1)) = sign(Xu(ti)). We write ui = u(ti), gi = **g(u(ti), y/4)** 2,
_âˆ’_ _Ì¸_ _âˆ¥_ _âˆ¥_

**gi[âˆ’]** [= lim]t _ti_ **[g][(][u][(][t][)][,][ e]Î»(t)),** **gi = g(u(ti),** **_Î»(t)),_** (102)
_â†‘_

**vi[âˆ’]** [=] _âˆ¥ggi[âˆ’]i[âˆ’][âˆ¥][2][ and][ v][i][ =]_ _âˆ¥ggiiâˆ¥2_ [. We note that][ g]i[âˆ’] [=][ g]i[+]âˆ’1[. According to Lemma 3, we have][e]

_ti âˆ’_ _tiâˆ’1 â‰¤_ 2 1 âˆ’1Î´/8giâˆ’1 log p11 âˆ’ âˆ’ _Î´/Î´/8 + (8 âˆ’_ (vvii[âˆ’][âˆ’][)][)][T][T][ u][ u][i][i] _âˆ’_ log p11 âˆ’ âˆ’ _Î´/Î´/8 +8 âˆ’ vvii[T][T]âˆ’âˆ’11[u][u][i][i][âˆ’][âˆ’][1][1]_ !

p 1 log p1 âˆ’ _Î´/8 + (vi[âˆ’][)][T][ u][i]_ log p1 âˆ’ _Î´/8 + vi[T]âˆ’1[u][i][âˆ’][1]_ _._

_â‰¤_ 2 1 _Î´/8gmin_ p1 _Î´/8_ (vi[âˆ’][)][T][ u][i] _âˆ’_ p1 _Î´/8_ **vi[T]** 1[u][i][âˆ’][1] !

_âˆ’_ _âˆ’_ _âˆ’_ _âˆ’_ _âˆ’_ _âˆ’_

(103)

p p p

Here we utilize that gi 1 _gmin, where gmin is defined in (24). This implies that_
_âˆ’_ _â‰¥_

1 _Î´/8 + (vi[âˆ’][)][T][ u][i]_ 1 _Î´/8gmin(ti_ _ti_ 1) 1 _Î´/8 + vi[T]_ 1[u][i][âˆ’][1]

p1 âˆ’ _Î´/8_ (vi[âˆ’][)][T][ u][i] _â‰¥_ _e[2][âˆš]_ _âˆ’_ _âˆ’_ _âˆ’_ p1 âˆ’ _Î´/8_ **vi[T]âˆ’1[u][i][âˆ’][1]** _._ (104)

_âˆ’_ _âˆ’_ _âˆ’_ _âˆ’_ _âˆ’_

p p


-----

We can show that for t satisfying t 1 log _âˆš1âˆ’Î´/8+1âˆ’Î´/2_ min[v]0[T] **[u][0]** and
_â‰¥_ 2[âˆš]1âˆ’Î´/8gmin  _âˆš1âˆ’Î´/8âˆ’1+Î´/2_ _[âˆ’]_ [log][ 1+]1âˆ’[g]gmin[âˆ’][âˆ’][1][1] **[v]0[T]** **[u][0]** 

_t â‰¤_ _T_, we have âˆ¥g(u(t), Î»)âˆ¥2 > gmin. According to Lemma 3, as gi â‰¥ _gmin, we have_

1 _Î´/8 + gmin[âˆ’][1]_ [(][g]i[âˆ’][)][T][ u][i] 1 _Î´/8gmin(ti_ _ti_ 1) 1 _Î´/8 + gmin[âˆ’][1]_ **[g]i[T]** 1[u][i][âˆ’][1]

p1 âˆ’ _Î´/8_ _gmin[âˆ’][1]_ [(][g]i[âˆ’][)][T][ u][i] _â‰¥_ _e[2][âˆš]_ _âˆ’_ _âˆ’_ _âˆ’_ p1 âˆ’ _Î´/8_ _gmin[âˆ’][1]_ **[g]i[T]âˆ’1[u][i][âˆ’][1]** _._ (105)

_âˆ’_ _âˆ’_ _âˆ’_ _âˆ’_ _âˆ’_

This implies thatp p


1 _Î´/8 + gmin[âˆ’][1]_ [(][g]i[âˆ’][)][T][ u][i] 1 _Î´/8gminti_
_âˆ’_ _e[2][âˆš]_ _âˆ’_

p1 _Î´/8_ _gmin[âˆ’][1]_ [(][g]i[âˆ’][)][T][ u][i] _â‰¥_

_âˆ’_ _âˆ’_

or equivalently, for anyp _t > 0, we have_


1 _Î´/8 + gmin[âˆ’][1]_ **[v]0[T]** **[u][0]**
_âˆ’_ _,_ (106)

1 _Î´/8_ _gmin[âˆ’][1]_ **[v]0[T]** **[u][0]**
_âˆ’_ _âˆ’_


p11 âˆ’ _Î´/Î´/8 +8_ _ggmin[âˆ’]min[âˆ’][1][1][(][g][g][(][(][u][u][(][(][t][t][))][)][,][ e][,]Î»[ e]Î»((t))t))[T][T]uu((t)t)_ _â‰¥_ _e[2][âˆš]1âˆ’Î´/8gmint_ p11 âˆ’ _Î´/Î´/8 +8_ _ggminmin[âˆ’][âˆ’][1][1]_ **[v][v]00[T][T]** **[u][u][0][0]** _._ (107)

_âˆ’_ _âˆ’_ _âˆ’_ _âˆ’_

Here we utilize thatp **g(u(t), Î»(t))[T]** **u(t) is continuous w.r.t.p** _t._ Therefore, for t _â‰¥_

2g1min log [2][âˆ’]Î´ _[Î´]_ _âˆ’_ log 1[1+]âˆ’[g]gminmin[âˆ’][âˆ’][1][1] **[v][v]00[T][T]** **[u][u][0][0]**, we have

 

1 + gmin[âˆ’][1] [(][g][(][u][(][t][)][,][ e]Î»(t)))[T] **u(t)** 1 _Î´/8 + 1_ _Î´/2_

1 _gmin[âˆ’][1]_ **[g][(][u][(][t][)][,][ e]Î»(t))[T]** **u(t)** _â‰¥_ p1 âˆ’ _Î´/8_ 1 + âˆ’ Î´/2 _._ (108)
_âˆ’_ _âˆ’_ _âˆ’_

This implies that p
_gmin[âˆ’][1]_ [(][g][(][u][(][t][)][,][ e]Î»(t)))[T] **u(t) â‰¥** 1 âˆ’ _Î´/2._ (109)

If âˆ¥g(u(t), Î»)âˆ¥2 = gmin, as the statements in Lemma 2 hold, we can compute that

**g(u(t), Î»)** **g(u(t),** **_Î»(t))_** 2 = _[Î´]_ (110)
_âˆ¥_ _âˆ’_ _âˆ¥_ _â‰¤_ _[g][min]4_ _[Î´]_ 4 _[âˆ¥][g][(][u][(][t][)][,][ Î»][)][âˆ¥][2][,]_

which implies that

[e]

**g(u(t),** **_Î»(t))_** 2 (1 + Î´/4) **g(u(t), Î»)** 2. (111)
_âˆ¥_ _âˆ¥_ _â‰¤_ _âˆ¥_ _âˆ¥_

Therefore, we have

[e] **_Î»(t)))[T]_** **u(t)**

**v(t)[T]** **u(t) =** [(][g][(][u][(][t][)][,][ e]

**g(u(t),** **_Î»(t))_** 2
_âˆ¥_ _âˆ¥_

1 (g(u(t), **_Î»(t)))[T]_** **u(t)** (112)
_â‰¥_ 1 + Î´/4 [e] _gmin_

[e]

_â‰¥_ 1 +[1][ âˆ’] Î´/[Î´/]4[2] 4 _[Î´.]_

_[â‰¥]_ [1][ âˆ’] [3]

This leads to a contradiction.

Analogously, we can show that for t â‰¥ [P]j[i] =1 _[T][i][, we have][ âˆ¥][g][(][u][(][t][)][,][ y][/][4)][âˆ¥][2][ > g][(][i][)][. Thus, by taking]_
_t â‰¥_ [P]i[p]=1 _[T][i][, we have][ âˆ¥][g][(][u][(][t][)][,][ y][/][4)][âˆ¥][2][ > g][(][p][)][ =][ g][max][. However, from the definition of][ g][max][,]_
we have **g(u(t), y/4)** 2 _gmax. This leads to a contradiction. Therefore, there exists a time_
_T =_ _i âˆ¥=1_ _[T][i][ =][ O][(log][ Î´]âˆ¥_ _[âˆ’] â‰¤[1][)][ such that][ v][(][T]_ [)][T][ u][(][T] [)][ â‰¥] [1][ âˆ’] [3]4 _[Î´][.]_

We note that **g(u(T** ), y/4) 2 _gmin. As the statements in Lemma (2) hold, we have_

[P][p] _âˆ¥_ _âˆ¥_ _â‰¥_

**g(u(T** ), y/4) **g(u(T** ), **_Î»(T_** )) 2 (113)
_âˆ¥_ _âˆ’_ _âˆ¥_ _â‰¤_ _[Î´g]8[min]_

According to Lemma 7, we have

[e]

**g(u(T** ), y/4) **g(u(T** ), **_Î»(T_** )) **_Î»(T_** )) 2

_âˆ¥_

_âˆ¥g(u(T_ ), y/4)âˆ¥2 _âˆ’_ _âˆ¥g(u(T_ ), **_Î»(T_** ))âˆ¥2 2 _â‰¤_ [2][âˆ¥][g][(][u][(][T] [)][,][ y][/][4)]g[ âˆ’]min[g][(][u][(][T] [)][,][ e] _â‰¤_ 4[Î´] _[.]_

[e] (114)

[e]


-----

This implies that

Hence, we have


**g(u(T** ), y/4)
**u(T** )[T]

**g(u(T** ), y/4) 2
_âˆ¥_ _âˆ¥_

**g(u(T** ), Î») **g(u(T** ), **_Î»(T_** ))

**u(T** )[T] **v(T** )
_â‰¥_ _âˆ’_ **g(u(T** ), Î») 2 _âˆ’_ **g(u(T** ), **_Î»(T_** )) 2

_âˆ¥_ _âˆ¥_ _âˆ¥_ _âˆ¥_

[e]

_â‰¥1 âˆ’_ _Î´._

[e]


(115)


**g(u(T** ), y) **g(u(T** ), y/4)
cos âˆ (u(T ), g(u(T ), y)) = u(T )[T] = u(T )[T] 1 _Î´._

**g(u(T** ), y) 2 **g(u(T** ), y/4) 2 _â‰¥_ _âˆ’_
_âˆ¥_ _âˆ¥_ _âˆ¥_ _âˆ¥_

This completes the proof.

D.5 PROOF OF LEMMA 4


PROOF This is proved in Lemma 2 in (Phuong & Lampert, 2021). Here we provide an alternative
proof. It is sufficient to prove for the case of local maximizer. Suppose that w is a local maximizer of
**y[T]** (Xw)+ in B. We first consider the case where y[T] (Xw)+ > 0.

If there exists n [N ] such that **w, xn** 0 and yn = 1. Consider v = xn/ **xn** 2 and let
**w+Ïµv** _âˆˆ_ _âŸ¨_ _âŸ©â‰¤_ _âˆ¥_ _âˆ¥_
**wÏµ =** _âˆ¥w+Ïµvâˆ¥2_ [, where][ Ïµ >][ 0][. For index][ n][â€²][ âˆˆ] [[][N] []][ such that][ y][n][â€²][ = 1][, as the dataset is orthogonal]

separable, we have x[T]n[â€²] **[x][n][ >][ 0][ and]**

_Ïµ_
**x[T]n[â€²]** [(][w][ +][ Ïµ][v][) =][ x]n[T][â€²] **[w][ +]** **x[T]n[â€²]** **[x][n]** _[>][ x][T]n[â€²]_ **[w][.]** (116)

**xn** 2
_âˆ¥_ _âˆ¥_

This implies that (x[T]n[â€²] **[w][Ïµ][)][+][ â‰¥]** [(][x][T]n[â€²] **[w][)][+][. For][ y][n][â€²][ =][ âˆ’][1][, as the data is orthogonal separable, we note]**
that x[T]n[â€²] **[x][n][ â‰¤]** [0][ and]
_Ïµ_
**x[T]n[â€²]** [(][w][ +][ Ïµ][v][) =][ x]n[T][â€²] **[w][ +]** **x[T]n[â€²]** **[x][n]** _n[â€²]_ **[w][.]** (117)

**xn** 2 _[â‰¤]_ **[x][T]**
_âˆ¥_ _âˆ¥_

This implies that (x[T]j **[w][Ïµ][)][+][ â‰¤]** [(][x]j[T] **[w][)][+][. In summary, we have]**


**y[T]** (X(w + Ïµv))+ =


_yn(x[T]j_ **[w][)][+]** [=][ y][T][ (][Xw][)][+] _[>][ 0]_ (118)
_n=1_

X


_yn(x[T]j_ [(][w][ +][ Ïµ][v][))][+]
_n=1_ _[â‰¥]_

X


If âŸ¨w, xnâŸ© _< 0, then w[T]_ **v < 0. This implies that with sufficiently small Ïµ, we have âˆ¥w + Ïµvâˆ¥2 <**
_âˆ¥wâˆ¥2 = 1. Therefore,_

1
**y[T]** (XwÏµ))+ = **y[T]** (X(w + Ïµv))+ > y[T] (X(w + Ïµv))+ **y[T]** (Xw)+, (119)

**w + Ïµv** 2 _â‰¥_
_âˆ¥_ _âˆ¥_

which leads to a contradiction. If **w, xn** = 0, we note that
_âŸ¨_ _âŸ©_

(x[T]n [(][w][ +][ Ïµ][v][))][+] [=][ Ïµ >][ (][x][T]n **[w][)][+][.]** (120)

This implies that
**y[T]** (X(w + Ïµv))+ â‰¥ **y[T]** (Xw)+ + Ïµ. (121)
We also note that âˆ¥w + Ïµvâˆ¥2 = _âˆš1 + Ïµ[2]_ = 1 + O(Ïµ[2]). Therefore, with sufficiently small Ïµ, we have

**y[T]** (XwÏµ)+ _> y[T]_ (Xw)+. (122)
_â‰¥_ **[y][T][ (]âˆš[Xw]1 +[)] Ïµ[+][2][ +][ Ïµ]**

We then consider the case where y[T] (Xw)+ < 0. Apparently, we can make y[T] (Xw)+ larger by
replacing w by (1 âˆ’ _Ïµ)w, where Ïµ âˆˆ_ (0, 1), which leads to a contradiction.

Finally, we consider the case where y[T] (Xw)+ = 0. This implies that

(x[T]j **[w][)][+]** [=] (x[T]j **[w][)][+][.]** (123)
_n:Xyn=1_ _n:yXn=âˆ’1_

As (Xw)+ = 0, this implies that there exists at least for one index n [N ] such that yn = 1 and
**x[T]n** **[w][ >][ 0][. Let] Ì¸** **[ v][ =][ x][n][/][âˆ¥][x][n][âˆ¥][2][. We note that]** **w+1Ïµv** 2 **[y][T][ (][X][(][w][ +][ Ïµ][v][))] âˆˆ[+][ >][ 0][ for][ Ïµ >][ 0][. This leads]**

_âˆ¥_ _âˆ¥_
to a contradiction.


-----

D.6 PROOF OF PROPOSITION 7

It is sufficient to consider the case of the local maximizer. Denote Q = {Ïƒ âˆˆ{âˆ’1, 0, 1}[N] _|diag(Ïƒ) âˆˆ_
_P}is open if. For Ïƒ Ïƒ, Ïƒn_ _[â€²]= 0âˆˆQ for, we say n_ [N Ïƒ] âŠ†. DefineÏƒ[â€²] if for all index n âˆˆ [N ] with Ïƒn Ì¸= 0, Ïƒn[â€²] [=][ Ïƒ][n][. We say][ Ïƒ][ âˆˆQ]
_Ì¸_ _âˆˆ_

_SÏƒ = {u|sign(Xu) = Ïƒ}._ (124)

We start with the two lemmas.

**Lemma 8 Let Î» âˆˆ** R[N] _. Suppose that u0 satisfies that u0 =_ _âˆ¥gg((uu00,,Î»Î»))âˆ¥2_ _[. Let][ Ïƒ][ =][ sign][(][u][0][)][. Then,]_

**v âˆˆB2 is a local maximizer of Î»[T]** (Xu)+ in B2 if for any open Ïƒ[â€²] _satisfying Ïƒ âŠ†_ **_Ïƒ[â€²], we have_**
_âˆ¥g(Ïƒ, y)âˆ¥2 = âˆ¥g(Ïƒ[â€²], y)âˆ¥2._

PROOF Suppose that Ïƒ is open. Then, SÏƒ is an open set. In a small neighbor around u0 =
**g(u0,Î»)** **g(Ïƒ,Î»)**

_âˆ¥g(u0,Î»)âˆ¥2_ [=] _âˆ¥g(Ïƒ,Î»)âˆ¥2_ [,][ Î»][T][ (][Xu][)][+][ =][ u][T][ g][(][Ïƒ][,][ Î»][)][ is a linear function of][ u][. The Riemannian]

gradient of u[T] **g(Ïƒ, Î») at v is zero. This implies that v locally maximizes Î»[T]** (Xu)+.

Suppose that there exists at least one zero in Ïƒ. Consider any v satisfying u[T]0 **[v][ = 0][. Let][ Ïµ >][ 0]**
_âˆˆB_ **u+sv**
be a small constant such that for any s âˆˆ (0, Ïµ], u0 + sv âˆˆ _SÏƒ[â€²] where Ïƒ âŠ†_ _Ïƒ[â€²]. Let us =_ _âˆš1+s[2][ .]_

Suppose thatÏƒ âŠ† **_Ïƒ[â€²], we construct âˆ¥g(Ïƒ[â€²â€²], Î») Ïƒâˆ¥2[â€²â€²] â‰¤âˆ¥by Ïƒg([â€²â€²]iÏƒ,[=] Î»[ âˆ’])âˆ¥2[1] for all open[ for][ n][ âˆˆ]_** [[][N] Ïƒ[]][ such that][â€²â€²] satisfying[ Ïƒ] Ïƒn[â€²] _âŠ†[= 0]Ïƒ[ and][â€²â€²]. For any[ Ïƒ]n[â€²â€²]_ [=] Ïƒ[ Ïƒ][â€²] _n[â€²]with[for]_
_n_ [N ] such that Ïƒn[â€²] [= 0][. We note that][ âˆ¥][g][(][Ïƒ][â€²â€²][,][ Î»][)][âˆ¥][2]
_âˆˆ_ _[â‰¥âˆ¥][g][(][Ïƒ][â€²][,][ Î»][)][âˆ¥][2][. Thus,][ âˆ¥][g][(][Ïƒ][â€²][,][ Î»][)][âˆ¥][2]_ _[â‰¤]_
**g(Ïƒ[â€²â€²], Î»)** 2 **g(Ïƒ, Î»)** 2. As **_Î»[T]_** (Xus)+ = **g(Ïƒ[â€²], Î»)[T]** **us** **g(Ïƒ[â€²], Î»)** 2, we have
_âˆ¥_ _âˆ¥_ _â‰¤âˆ¥_ _âˆ¥_ _|_ _|_ _|_ _| â‰¤âˆ¥_ _âˆ¥_
**_Î»[T]_** (Xus)+ **g(Ïƒ[â€²], Î»)** 2 **g(Ïƒ, Î»)** 2. Therefore, u is a local maximizer of Î»[T] (Xu)+.
_|_ _| â‰¤âˆ¥_ _âˆ¥_ _â‰¤âˆ¥_ _âˆ¥_

_Suppose thatLemma 9 Suppose that the dataset is orthogonal separable. Let u0 satisfies that u0 =_ _âˆ¥gg((uu00,,Î»Î»))âˆ¥2_ _[. Then, for any] Î» âˆˆ[ Ïƒ][â€²]R[ satisfying][N]_ _satisfy that[ Ïƒ][ âŠ†] diag[Ïƒ][â€²][, we have](y)Î» â‰¥_ 0.

_âˆ¥g(Ïƒ[â€²], Î»)âˆ¥2 = âˆ¥g(Ïƒ, Î»)âˆ¥2._

PROOF If there exists n âˆˆ [N ] such that Ïƒn = 1 and yn = âˆ’1, as the data is orthogonal separable,
we note that


**x[T]n** **[g][(][Ïƒ][,][ Î»][) =][ x]n[T]** _Î»nâ€²_ **xnâ€²** = yn(ynxn)[T] (Î»nâ€² _ynâ€²_ )ynâ€² **xnâ€²**

ï£« ï£¶ ï£«

_n[â€²]:Ïƒnâ€²_ _>0_ _n[â€²]:Ïƒnâ€²_ _>0_

X X
ï£­ ï£¸ ï£­

which contradicts with sign(x[T]n **[g][(][Ïƒ][,][ Î»][)) =][ sign][(][x][T]n** **[u][0][) =][ Ïƒ][n]** [= 1][.]


**x[T]n** **[g][(][Ïƒ][,][ Î»][) =][ x]n[T]**


0, (125)

ï£¶ï£¸ _â‰¤_


_Î»nâ€²_ **xnâ€²**
_n[â€²]:Ïƒnâ€²_ _>0_

X


Suppose that there exists n âˆˆ [N ] such that Ïƒn and yn = 1. Then, as the dataset is orthogonal
separable, then, for index n1 âˆˆ [N ] such that Ïƒn1 = 0, we note that yn1 Ì¸= 1. Otherwise,

**x[T]n1** **[g][(][Ïƒ][,][ Î»][) =][ x]n[T]1** _Î»n2_ **xn2** = x[T]n1 (Î»n2 _yn2_ )yn2 **xn2** _> 0,_ (126)

ï£« ï£¶ ï£« ï£¶

_n2:Ïƒn2_ _>0_ _n2:Ïƒn2_ _>0_

X X
ï£­ ï£¸ ï£­ ï£¸

which contradicts with sign(x[T]n1 **[g][(][Ïƒ][,][ Î»][)) =][ sign][(][x]n[T]1** **[u][0][) =][ Ïƒ][n]1** [= 0][. This also implies that the]
index set {n âˆˆ [N ]|Ïƒn > 0} include all data with yn = 1.

If there exists Ïƒ[â€²] such that Ïƒ âŠ† **_Ïƒ[â€²]_** and âˆ¥g(Ïƒ[â€²], Î»)âˆ¥2 > âˆ¥g(Ïƒ, Î»)âˆ¥2. Then, there exists at least one
indexyn = âˆ’ n âˆˆ1 and[N ] such that Ïƒn â‰¤ 0 and Ïƒn[â€²] [= 1][. However, from the previous derivation, we note that]


= x[T]n

ï£¶

ï£¸


_< 0,_ (127)

ï£¶

ï£¸


**x[T]n** **[g][(][Ïƒ][â€²][,][ Î»][) =][ x][T]n**


_Î»n1_ **xn1**
_j:Ïƒ[â€²]n1_ _[>][0]_

X


(Î»n1 _yn1_ )yn1 **xn1**
_n1:Ïƒ[â€²]n1_ _[>][0]_

X


which contradicts with Ïƒn[â€²] [= 1][.]


By combining Lemma 8 and 9, we complete the proof.


-----

D.7 PROOF OF THEOREM 4

PROOF For almost all initialization, we can find two neurons such that sign(w2,i+ ) =
**sign(y[T]** (Xw1,i+ )+) = 1 and sign(w2,iâˆ’ ) = **sign(y[T]** (Xw1,iâˆ’ )+) = _âˆ’1 at initializa-_
tion. By choosing a sufficiently small Î´ _> 0 in Proposition 6, there exist two neurons_
**w1,i+** _, w1,iâˆ’_ and times T+, Tâˆ’ _> 0 such that cos âˆ (w1,i+_ (T+), g(w1,i+ (T+), y)) > 1 âˆ’ _Î´ and_
cos âˆ (w1,i+ (T+), g(w1,i+ (T+), y)) < âˆ’(1 âˆ’ _Î´). This implies that w1,i+_ (T+) and w1,iâˆ’ (T+) are
sufficiently close to certain stationary points of gradient flow maximizing/minimizing y[T] (Xu+)
over B, i.e., {u âˆˆB| cos(u, g(u, y)) = Â±1}. As the dataset is orthogonal separable, according to
Lemma 4 and Proposition 7, the corresponding diagonal matrices **D[Ë†]** _i+_ (T+) and **D[Ë†]** _iâˆ’_ (Tâˆ’) satisfy
that **D[Ë†]** _i+_ (T+) **diag(I(y = 1)) and** **D[Ë†]** _i_ (T ) **diag(I(y =** 1)). According to Lemma 3 in
_â‰¥_ _âˆ’_ _âˆ’_ _â‰¥_ _âˆ’_
(Phuong & Lampert, 2021), we have **D[Ë†]** _i+_ (t) **diag(I(y = 1)) and Di** (t) **diag(I(y =** 1))
_â‰¥_ _âˆ’_ _â‰¥_ _âˆ’_
hold for t â‰¥ max{T+, Tâˆ’}.

With t â†’âˆž, according to Proposition 4, the dual variable Î» in the KKT point of the non-convex
max-margin problem (13) is dual feasible, i.e., Î» satisfies (16). Suppose that Î¸[âˆ—] is a limiting point
of _âˆ¥Î¸Î¸((tt))âˆ¥2_ _t_ 0 [and][ Î»][âˆ—] [is the corresponding dual variable. From Theorem 1, we note that the pair]

_â‰¥_

(Î¸[âˆ—]n, Î»[âˆ—]) corresponds to the KKT point of the convex max-margin problem (14).o


E PROOFS OF MAIN RESULTS ON MULTI-CLASS CLASSIFICATION

E.1 PROOF OF PROPOSITION 1

The neural network training problem (4) can be separated into K subproblems. Each of these
subproblems corresponds to the neural network training problem (3) for binary classification. For
each subproblem, by applying Proposition 2, we complete the proof.

E.2 PROOF OF THEOREM 1

We note that the neural network training problem (4) can be separated into K subproblems. Each of
these subproblems corresponds to the neural network training problem (3) for binary classification.
By applying Proposition 6 with to each subproblem with y = yk, we complete the proof.

E.3 PROOF OF THEOREM 2

Similarly, the corresponding non-convex max-margin problem (5) and the convex max-margin
problem (7) can be separated into K subproblems. Each of these subproblems corresponds to the nonconvex max-margin problem (2) and the convex max-margin problem (14) for binary classification.
By applying Theorem 4 to each subproblem with y = yk, we complete the proof.

F NUMERICAL EXPERIMENT

F.1 DETAILS ON FIGURE 5

We provide the experiment setting in Figure 1 and 5 as follows. The dataset is given by X =
1.65 0.47 1
_âˆ’_ R[2][Ã—][2] and y = R[2]. Here we have N = 2 and d = 2. We note that this
0.47 1.35 _âˆˆ_ 1 _âˆˆ_
âˆ’  âˆ’ 

dataset is orthogonal separable but not spike-free. We plot the ellipsoid set and the rectified ellipsoid
set in Figure 6.


-----

2 2

1 1

0 0

1 1

2 2

2 1 0 1 2 2 1 0 1 2


Figure 6: The ellipsoid set and the rectified ellipsoid set. Orthogonal separable dataset.

We enumerate all possible hyperplane arrangements in the set P and solve the convex max-margin
problem (14) via CVXPY to obtain the following non-zero neurons


0.58
**u1,3 =** _,_
âˆ’0.16

We note that the dual problem (15) is equivalent to

max Î»[T] **y,**


0.23
**w1[â€²]** _,2_ [=] _âˆ’0.66_



(128)

(129)


s.t. **X[T]** **DjÎ»** **X[T]** (2Dj _I)zj,+_ 2 1, _j_ [p],
_âˆ¥_ _âˆ’_ _âˆ’_ _âˆ¥_ _â‰¤_ _âˆ€_ _âˆˆ_

_âˆ¥âˆ’_ **X[T]** **DjÎ» âˆ’** **X[T]** (2Dj âˆ’ _I)zj,âˆ’âˆ¥2 â‰¤_ 1, âˆ€j âˆˆ [p],
**zj,+ â‰¥** 0, zj,âˆ’ _â‰¥_ 0, âˆ€j âˆˆ [p], diag(y)Î» â‰¥ 0.


The above problem is a second-order cone program (SOCP) and can be solved via standard convex
optimization frameworks such as CVX and CVXPY. We solve (129) to obtain the optimal dual
variable Î». For the geometry of the dual problem, as the dataset is orthogonal separable, the set
**_Î» : max_** **u** 2 1 **_Î»[T]_** (Xu)+ 1 reduces to **_Î» : max_** **u** 2 1 **_Î»[T]_** (Xu[âˆ—]1[)][+][| â‰¤] [1][,][ Î»][T][ (][Xu][âˆ—]2[)][+][| â‰¤]
_{_ _âˆ¥_ _âˆ¥_ _â‰¤_ _|_ _| â‰¤_ _}_ _{_ _âˆ¥_ _âˆ¥_ _â‰¤_ _|_
1}, where u[âˆ—]1[,][ u][âˆ—]2 [correspond to two vectors at the spikes of the rectified ellipsoid set. We draw the]
setsFigure 2. {Î» : maxâˆ¥uâˆ¥2â‰¤1 |Î»[T] (Xu)+| â‰¤ 1}, {Î» :, the optimal dual variable Î» and the direction of y in

For eachconstraints Dju âˆˆPj 2, we solve for the vector1 and (2Dj _I)Xuj_ **u0. We plot the rectified ellipsoid setj which maximize/minimize Î»[T]** **D(jXuXu)j+ with theu** 2
1, vectors âˆ¥ uj, neurons in the optimal solution toâˆ¥ _â‰¤_ _âˆ’_ _â‰¥_ (14) scaled to unit â„“2-norm and the direction of { _|âˆ¥_ _âˆ¥_ _â‰¤ Î»_
_}_
in Figure 1. We note that each neuron u[âˆ—]j [in the optimal solution from][ (14)][ (scaled to unit][ â„“][2][-norm)]
maximize/minimize the corresponding Î»[T] **DjXuj given (2Dj** _I)Xu[âˆ—]j_
_âˆ’_ _[â‰¥]_ [0][.]

Then, we consider a two-layer ReLU network with m = 10 neurons and apply the gradient descent
method to train on the logistic loss (3). Let Ë†w1,i = _âˆ¥ww11,i,iâˆ¥2_ [for][ i][ âˆˆ] [[][m][]][. We plot][ Ë†]w1,i and (X Ë†w1,i)+

at iteration {10[l]|l = 0, . . ., 4} along with neurons in the optimal solution to (14) scaled to unit
_â„“2-norm in Figure 5. Certain neurons do not move, while the activated neurons trained by gradient_
descent tend to converge to the direction of the neurons in the optimal solution to (14).

We repeat the training on the logistic loss (3) with the gradient descent method several times and we
plot the trajectories in Figure 7.


-----

|Col1|cone boundary 1-th neuron 2-th neuron 3-th neuron 4-th neuron 5-th neuron 6-th neuron 7-th neuron 8-th neuron 9-th neuron 10-th neuron optimal neuron|
|---|---|

|Col1|cone boundary 1-th neuron 2-th neuron 3-th neuron 4-th neuron 5-th neuron 6-th neuron 7-th neuron 8-th neuron 9-th neuron 10-th neuron optimal neuron|
|---|---|

|Col1|cone boundary 1-th neuron 2-th neuron 3-th neuron 4-th neuron 5-th neuron 6-th neuron 7-th neuron 8-th neuron 9-th neuron 10-th neuron optimal neuron|
|---|---|

|1.0 0.5 0.0 0.5 1.0 1.0 0. 1.0 0.5 0.0 0.5 1.0 1.0 0. 1.0 0.5 0.0 0.5 1.0 1.0 0. 1.0 0.5 0.0 0.5 1.0 1.0 0.|Col2|cone boundary 1-th neuron 2-th neuron 3-th neuron 4-th neuron 5-th neuron 6-th neuron 7-th neuron 8-th neuron 9-th neuron 10-th neuron optimal neuron|
|---|---|---|


2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00


Figure 7: Multiple independent random initializations of gradient descent trajectories on the same
h l bl d


-----

F.2 EXPERIMENT ON SPIKE-FREE DATASET

1.65 0.47
We repeat the previous numerical experiment on a non-spike-free dataset: X =
0.47 1.35



R[2][Ã—][2] and y =


_âˆˆ_ R[2]. Similarly, we plot the ellipsoid set and the rectified set in Figure 8.


2 2

1 1

0 0

1 1

2 2

2 1 0 1 2 2 1 0 1 2


Figure 8: The ellipsoid set and the rectified ellipsoid set for a non-spike-free dataset.

We enumerate all possible hyperplane arrangements in the set P and solve the convex max-margin
problem (14) via CVXPY to obtain the following non-zero neuron


0.43
**u1,4 =**
0.59



(130)


We plot the rectified ellipsoid set (Xu)+ **u** 2 1, vectors uj, neurons in the optimal solution to
(14) scaled to unit â„“2-norm and the direction of { _|âˆ¥_ _âˆ¥_ _â‰¤ Î» in Figure 9. We also plot}_ Ë†w1,i and (X Ë†w1,i)+ at
iteration 10[l] _l = 0, . . ., 4_ along with neurons in the optimal solution to (14) scaled to unit â„“2-norm
_{_ _|_ _}_
in Figure 10.

2.0

1.5

1.0

0.5

0.0

optimal

y[T]u = 0

0.5 maximal

minimal

optimal (Xw1,[*] i[)][ +]

1.0

1.0 0.5 0.0 0.5 1.0 1.5 2.0


Figure 9: Recitified Ellipsoidal set and corresponding extreme points for a non-spike-free dataset.


-----

|cone b 1-th n 2-th n 3-th n 4-th n 5-th n 6-th n 7-th n 8-th n 9-th n 10-th optim|oundary euron euron euron euron euron euron euron euron euron neuron al neuron|
|---|---|

|cone b 1-th n 2-th n 3-th n 4-th n 5-th n 6-th n 7-th n 8-th n 9-th n 10-th optim|oundary euron euron euron euron euron euron euron euron euron neuron al neuron|
|---|---|

|1.0 cone b 0.5 1-th n 2-th n 3-th n 4-th n 5-th n 0.0 6-th n 7-th n 8-th n 9-th n 0.5 10-th optim 1.0 1.0 0.5 0.0 1.0 cone b 0.5 1-th n 2-th n 3-th n 4-th n 5-th n 0.0 6-th n 7-th n 8-th n 9-th n 0.5 10-th optim 1.0 1.0 0.5 0.0 1.0 cone b 0.5 1-th n 2-th n 3-th n 4-th n 5-th n 0.0 6-th n 7-th n 8-th n 9-th n 0.5 10-th optim 1.0 1.0 0.5 0.0|cone b 1-th n 2-th n 3-th n 4-th n 5-th n 6-th n 7-th n 8-th n 9-th n 10-th optim|oundary euron euron euron euron euron euron euron euron euron neuron al neuron|
|---|---|---|


2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00


Figure 10: Multiple independent random initializations of gradient descent trajectories on the same
non-spike-free dataset. Note that the optimal extreme point (star), which is the uniquely optimal single
neuron is on the boundary of the main two-dimensional ellipsoid and not on the one-dimensional
spikes (projected ellipsoids). Also note that some neurons are stuck at spurious stationary points.


-----

