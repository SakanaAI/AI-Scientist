# XI-LEARNING: SUCCESSOR FEATURE TRANSFER LEARNING FOR GENERAL REWARD FUNCTIONS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Transfer in Reinforcement Learning aims to improve learning performance on
target tasks using knowledge from experienced source tasks. Successor features
(SF) are a prominent transfer mechanism in domains where the reward function
changes between tasks. They reevaluate the expected return of previously learned
policies in a new target task and to transfer their knowledge. A limiting factor of
the SF framework is its assumption that rewards linearly decompose into successor
features and a reward weight vector. We propose a novel SF mechanism, Î¾learning, based on learning the cumulative discounted probability of successor
features. Crucially, Î¾-learning allows to reevaluate the expected return of policies
for general reward functions. We introduce two Î¾-learning variations, prove its
convergence, and provide a guarantee on its transfer performance. Experimental
evaluations based on Î¾-learning with function approximation demonstrate the
prominent advantage of Î¾-learning over available mechanisms not only for general
reward functions, but also in the case of linearly decomposable reward functions.

1 INTRODUCTION

Reinforcement Learning (RL) successfully addressed many complex problems such as playing
computer games, chess, and even Go with superhuman performance (Mnih et al., 2015; Silver et al.,
2018). These impressive results are possible thanks to a vast amount of interactions of the RL agent
with its environment/task. Such strategy is unsuitable in settings where the agent has to perform and
learn at the same time. Consider, for example, a care giver robot in a hospital that has to learn a new
task, such as a new route to deliver meals. In such a setting, the agent can not collect a vast amount
of training samples but has to adapt quickly instead. Transfer learning aims to provide mechanisms
quickly to adapt agents in such settings (Taylor and Stone, 2009; Lazaric, 2012; Zhu et al., 2020).
The rationale is to use knowledge from previously encountered source tasks for a new target task
to improve the learning performance on the target task. The previous knowledge can help reducing
the amount of interactions required to learn the new optimal behavior. For example, the care giver
robot could reuse knowledge about the layout of the hospital it learned in previous source tasks (e.g.
guiding a person) to learn to deliver meals.

The Successor Feature (SF) and General Policy Improvement (GPI) framework (Barreto et al., 2020)
is a prominent transfer learning mechanism for tasks where only the reward function differs. Its
basic premise is that the rewards which the RL agent tries to maximize are defined based on a lowdimensional feature descriptor Ï† âˆˆ R[n]. For our care-giver robot this could be IDâ€™s of beds or rooms
that it is visiting, in difference to its high-dimensional visual state input from a camera. The rewards
are then computed not based on its visual input but on the IDâ€™s of the beds or rooms that it visits. The
expected cumulative discounted successor features (Ïˆ) are learned for each behavior that the robot
learned in the past. It represents the dynamics in the feature space that the agent experiences for a
behavior. This corresponds to the rooms or beds the care-giver agent would visit if using the behavior.
This representation of feature dynamics is independent from the reward function. A behavior learned
in a previous task and described by this SF representation can be directly re-evaluated for a different
reward function. In a new task, i.e. for a new reward function, the GPI procedure re-evaluates the
behaviors learned in previous tasks for it. It then selects at each state the behavior of a previous task
if it improves the expected reward. This allows to reuse behaviors learned in previous source tasks

[Source code at https://tinyurl.com/3xuzxff3](https://tinyurl.com/3xuzxff3)


-----

for a new target task. A similar transfer strategy can also be observed in the behavior of humans
(Momennejad et al., 2017; Momennejad, 2020; Tomov et al., 2021) .

The classical SF&GPI framework (Barreto et al., 2017; 2018) makes the assumption that rewards r
are a linear composition of the featureson the task i: ri = Ï†[âŠ¤]wi. This assumption allows to effectively separate the feature dynamics of a Ï† âˆˆ R[n] via a reward weight vector wi âˆˆ R[n] that depends
behavior from the rewards and thus to re-evaluate previous behaviors given a new reward function, i.e.
a new weight vector wj. Nonetheless, this assumption also restricts successful application of SF&GPI
only to problems where such a linear decomposition is possible. We investigate the application of the
SF&GPI framework to general reward functions: ri = Ri(Ï†) over the feature space. We propose
to learn the cumulative discounted probability over the successor features, named Î¾-function, and
refer to the proposed framework as Î¾-learning. Our work is related to Janner et al. (2020); Touati and
Ollivier (2021), and brings two important additional contributions. First, we provide mathematical
proof of the convergence of Î¾-learning. Second, we demonstrate how Î¾-learning can be used for
meta-RL, using the Î¾-function to re-evaluate behaviors learned in previous tasks for a new reward
function Rj. Furthermore, Î¾-learning can also be used to transfer knowledge to new tasks using GPI.

The contribution of our paper is three-fold:

-  We introduce a new RL algorithm, Î¾-learning, based on a cumulative discounted probability
of successor features, and two variants of its update operator.

-  We provide theoretical proofs of the convergence of Î¾-learning to the optimal policy and for
a guarantee of its transfer learning performance under the GPI procedure.

-  We experimentally compare Î¾-learning in tasks with linear and general reward functions,
and for tasks with discrete and continuous features to standard Q-learning and the classical
SF framework, demonstrating the interest and advantage of Î¾-learning.

2 BACKGROUND

2.1 REINFORCEMENT LEARNING

RL investigates algorithms to solve multi-step decision problems, aiming to maximize the sum
over future rewards (Sutton and Barto, 2018). RL problems are modeled as Markov Decision
_Processes (MDPs) which are defined as a tuple M â‰¡_ (S, A, p, R, Î³), where S and A are the state
and action set. An agent transitions from a state st to another state st+1 using action at at time point
_t collecting a reward rt: st_ _at,rt_ _st+1. This process is stochastic and the transition probability_
_âˆ’âˆ’âˆ’â†’_
_p(st+1|st, at) describes which state st+1 is reached. The reward function R defines the scalar reward_
_rt = R(st, at, st+1) âˆˆ_ R for the transition. The goal in an MDP is to maximize the expected return
_Gt = E_ _âˆžk=0_ _[Î³][k][R][t][+][k]_, where Rt = R(St, At, St+1). The discount factor Î³ [0, 1) weights
_âˆˆ_
collected rewards by discounting future rewards stronger. RL provides algorithms to learn a policy
_Ï€ :_ P defining which action to take in which state to maximise _Gt._
_S â†’A_

Value-based RL methods use the concept of value functions to learn the optimal policy. The stateaction value function, called Q-function, is defined as the expected future return taking action at in st
and then following policy Ï€:


_Q[Ï€](st, at) = EÏ€_ _rt + Î³rt+1 + Î³[2]rt+2 + . . ._ = EÏ€



_rt + Î³ max_ _._ (1)
_at+1_ _[Q][Ï€][(][S][t][+1][, a][t][+1][)]_



The Q-function can be recursively defined following the Bellman equation such that the current
Q-value Q[Ï€](st, at) depends on the maximum Q-value of the next state Q[Ï€](st+1, at+1). The optimal
policy for an MDP can then be expressed based on the Q-function, by taking at every step the
maximum action: Ï€[âˆ—](s) âˆˆ argmaxa Q[âˆ—](s, a).

The optimal Q-function can be learned using a temporal difference method such as Q-learning
(Watkins and Dayan, 1992). Given a transition (st, at, rt, st+1), the Q-value is updated according to:


_rt + max_ _,_ (2)
_at+1_ _[Q][k][(][s][t][+1][, a][t][+1][)][ âˆ’]_ _[Q][k][(][s][t][, a][t][)]_



_Qk+1(st, at) = Qk(st, at) + Î±k_


where Î±k (0, 1] is the learning rate at iteration k.
_âˆˆ_


-----

2.2 TRANSFER LEARNING AND THE SF&GPI FRAMEWORK

We are interested in the transfer learning setting where the agent has to solve a set of tasks M =
_M1, M2, . . ., Mm_, that in our case differ only in their reward function. The Successor Feature
_{_ _}_
(SF) framework provides a principled way to perform transfer learning (Barreto et al., 2017; 2018).
SF assumes that the reward function can be decomposed into a linear combination of features
_Ï† âˆˆ_ Î¦ âŠ‚ R[n] and a reward weight vector wi âˆˆ R[n] that is defined for a task Mi:

_ri(st, at, st+1) â‰¡_ _Ï†(st, at, st+1)[âŠ¤]wi ._ (3)

We refer to such reward functions as linear reward functions. Since the various tasks differ only in
their reward functions, the features are the same for all tasks in M.

Given the decomposition above, it is also possible to rewrite the Q-function into an expected
discounted sum over future features Ïˆ[Ï€][i] (s, a) and the reward weight vector wi:

_Q[Ï€]i_ _[i]_ [(][s, a][)] = E _rt + Î³[1]rt+1 + Î³[2]rt+2 + . . ._ = E _Ï†[âŠ¤]t_ **[w][i]** [+][ Î³][1][Ï†][âŠ¤]t+1[w][i] [+][ Î³][2][Ï†][âŠ¤]t+2[w][i] [+][ . . .]

= E  _âˆžk=0_ _[Î³][k][Ï†][t][+][k]_ _âŠ¤_ **wi** _ÏˆÏ€i_ (s, a)âŠ¤wi .
_â‰¡_ (4)
This decouples the dynamics of the policyP _Ï€i in the feature space of the MDP from the expected_
rewards for such features. Thus, it is now possible to evaluate the policy Ï€i in a different task Mj using
a simple multiplication of the weight vector wj with the Ïˆ-function: Q[Ï€]j _[i]_ [(][s, a][) =][ Ïˆ][Ï€][i] [(][s, a][)][âŠ¤][w][j][.]
Interestingly, the Ïˆ function also follows the Bellman equation:

_Ïˆ[Ï€](s, a) = E {Ï†t+1 + Î³Ïˆ[Ï€](st+1, Ï€(st+1))|st, at},_ (5)

and can therefore be learned with conventional RL methods. Moreover, (Lehnert and Littman, 2019)
showed the equivalence of SF-learning to Q-learning.

Being in a new task Mj the Generalized Policy Improvement (GPI) can be used to select the action
over all policies learned so far that behaves best:

_Ï€(s) âˆˆ_ argmaxa maxi _Q[Ï€]j_ _[i]_ [(][s, a][) = argmax]a maxi _Ïˆ[Ï€][i]_ (s, a)[âŠ¤]wj . (6)

(Barreto et al., 2018) proved that under the appropriate conditions for optimal policy approximates,
the policy constructed in (6) is close to the optimal one, and their difference is upper-bounded:


2
_Q[âˆ—]_ _Q[Ï€]_ _r_ _ri_ + min _ri_ _rj_ + Ïµ _,_ (7)
_||_ _âˆ’_ _||âˆž_ _â‰¤_ 1 _Î³_ _||_ _âˆ’_ _||âˆž_ _j_ _||_ _âˆ’_ _||âˆž_

_âˆ’_  

whereinterpreted in the following manner. Given the arbitrary task âˆ¥f âˆ’ _gâˆ¥âˆž_ = maxs,a |f (s, a) âˆ’ _g(s, a)|. For an arbitrary reward function M_, we identify the theoretically closest r the result can be
possible linear reward task Mi with ri. For this theoretically closest task, we search the linear task
_Mj in our set of task M (from which we also construct the GPI optimal policy (6)) which is closest to_
it. The upper bound between Q[âˆ—] and Q is then defined by 1) the difference between task M and the
theoretically closest possible linear task Mi: ||r âˆ’ _ri||âˆž; and by 2) the difference between theoretical_
task Mi and the closest task Mj: minj _ri_ _rj_ . If our new task M is also linear then r = ri and
the first term in (7) would vanish. _||_ _âˆ’_ _||âˆž_

Very importantly, this result shows that the SF framework will only provide a good approximation of
the true Q-function if the reward function in a task can be represented using a linear decomposition.
If this is not the case then the error in the approximation increases with the distance between the true
reward function r and the best linear approximation of it ri as stated by _r_ _ri_ .
_||_ _âˆ’_ _||âˆž_

3 METHOD: Î¾-LEARNING

3.1 DEFINITION AND FOUNDATIONS OF Î¾-LEARNING

The goal of this paper is to investigate the application of SF&GPI to tasks with general reward
_functions R : Î¦ 7â†’_ R over state features Ï† âˆˆ Î¦:

_r(st, at, st+1)_ _R(Ï†(st, at, st+1)) = R(Ï†t),_ (8)
_â‰¡_


-----

where we define Ï†t _Ï†(st, at, st+1). Under this assumption the Q-function can not be linearly_
decomposed into a part that describes feature dynamics and one that describes the rewards as in the â‰¡
linear SF framework (4). To overcome this issue, we propose to define the expected cumulative discounted probability of successor features or Î¾-function, which is going to be the central mathematical
object of the paper, as:


_Î¾[Ï€](s, a, Ï†) =_


_Î³[k]p(Ï†t+k = Ï†|st = s, at = a; Ï€),_ (9)
_k=0_

X


where p(Ï†t+k = Ï†|st = s, at = a; Ï€), or in short p(Ï†t+k = Ï†|st, at; Ï€), is the probability density
function of the features at time t + k, following policy Ï€ and conditioned to s and a being the state
and action at time t respectively. Note that Î¾[Ï€] depends not only on the policy Ï€ but also on the state
transition (constant through the paper). With the definition of the Î¾-function, the Q-function rewrites
(this is compatible with SFQL in the linear reward case, see Appendix A.6):


_Q[Ï€](st, at) =_


_k=0_ _Î³[k]Ep(Ï†t+k|st,at;Ï€) {R(Ï†t+k)} =_

X


_Î³[k]_

_k=0_

X


_p(Ï†t+k = Ï†|st, at; Ï€)R(Ï†)dÏ†_

_R(Ï†)Î¾[Ï€](st, at, Ï†)dÏ† ._


(10)


_Î³[k]p(Ï†t+k = Ï†|st, at; Ï€)dÏ† =_
_k=0_

X


_R(Ï†)_


Depending on the reward function R, there are several Î¾-functions that correspond to the same Q
function. Formally, this is an equivalence relationship, and the quotient space has a one-to-one
correspondence with the Q-function space.
**Proposition 1. (Equivalence between functions Î¾ and Q) Let** = _Q :_ R s.t. _Q_ _<_
_Q_ _{_ _S Ã— A â†’_ _âˆ¥_ _âˆ¥âˆž_
_âˆž}there is a bijective correspondence between the quotient space. Let âˆ¼_ _be defined as Î¾1 âˆ¼_ _Î¾2 â‡”_ Î¦ _[RÎ¾][1][ =]_ Î¦ _[RÎ¾][2][. Then,][ âˆ¼] Îž[is an equivalence relationship, and]and_ _._
R R _âˆ¼_ _Q_

**Corollary 1. The bijection between Îž** _and_ _allows to induce a norm_ _into Îž_ _from the_
_âˆ¼_ _Q_ _âˆ¥Â· âˆ¥âˆ¼_ _âˆ¼_
_supremum norm in_ _, with which Îž_ _is a Banach space (since_ _is Banach with_ _):_
_Q_ _âˆ¼_ _Q_ _âˆ¥Â· âˆ¥âˆž_

_âˆ¥Î¾âˆ¥âˆ¼_ = sups,a Î¦ _R(Ï†)Î¾(s, a, Ï†)dÏ†_ [= sup]s,a (11)

Z _[|][Q][(][s, a][)][|][ =][ âˆ¥][Q][âˆ¥][âˆž]_ _[.]_

Similar to the Bellman equation for the Q-function, we can define a Bellman operator for the
_Î¾-function, denoted by TÎ¾, as:_
_TÎ¾(Î¾[Ï€]) = p(Ï†t = Ï†|st, at) + Î³Ep(st+1,at+1|st,at;Ï€) {Î¾[Ï€](st+1, at+1, Ï†)} ._ (12)

As in the case of the Q-function, we can use TÎ¾ to construct a contractive operator:
**Proposition 2. (Î¾-learning has a fixed point) The operator TÎ¾ is well-defined w.r.t. the equivalence**
_, and therefore induces an operator T_ _defined over Îž_ _. T_ _is contractive w.r.t._ _. Since Îž_
_âˆ¼_ _âˆ¼_ _âˆ¼_ _âˆ¼_ _âˆ¥Â· âˆ¥âˆ¼_ _âˆ¼_
_is Banach, T_ _has a unique fixed point and iterating T_ _starting anywhere converges to that point._
_âˆ¼_ _âˆ¼_

In other words, successive applications of the operator T converge towards the class of optimal Î¾
_âˆ¼_
functions [Î¾[âˆ—]] or equivalently to an optimal Î¾ function defined up to an additive function k satisfying

Î¦ _[k][(][s, a, Ï†][)][R][(][Ï†][)][d][Ï†][ = 0][,][ âˆ€][(][s, a][)][ âˆˆS Ã— A][ (i.e.][ k][ âˆˆ]_ [Ker][(][Î¾][ â†’] Î¦ _[RÎ¾][)][).]_

While these two results state (see Appendix A for the proofs) the theoretical links to standardR R
Q-learning formulations, the TÎ¾ operator defined in (12) is not usable in practice, because of the
expectation. In the next section, we define the optimisation iterate, prove its convergence, and provide
two variants to perform the Î¾ updates.

3.2 _Î¾-LEARNING ALGORITHMS_

In order to learn the Î¾-function, we introduce the Î¾-learning update operator, which is an offpolicy temporal difference method analogous to Q-learning. Given a transition (st, at, st+1, Ï†t) the
_Î¾-learning update operator is defined as:_

_Î¾k[Ï€]+1[(][s][t][, a][t][, Ï†][)][ â†]_ _[Î¾]k[Ï€][(][s][t][, a][t][, Ï†][) +][ Î±][k]_ [[][p][(][Ï†][t] [=][ Ï†][|][s][t][, a][t][) +][ Î³Î¾]k[Ï€][(][s][t][+1][,][ Â¯]at+1, Ï†) âˆ’ _Î¾k[Ï€][(][s][t][, a][t][, Ï†][)]][,][ (13)]_

where Â¯at+1 = argmaxa Î¦ _[R][(][Ï†][)][Î¾][Ï€][(][s][t][+1][, a, Ï†][)][d][Ï†][.]_

The following is one of the main results of the manuscript, stating the convergence ofR _Î¾-learning:_


-----

**Theorem 1. (Convergence of Î¾-learning) For a sequence of state-action-feature** _st, at, st+1, Ï†t_ _t=0_
_{_ _}[âˆž]_
_consider the Î¾-learning update given in (13). If the sequence of state-action-feature triples visits_
_each state, action infinitely often, and if the learning rate Î±k is an adapted sequence satisfying the_
_Robbins-Monro conditions:_


_Î±k[2]_ _[<][ âˆž]_ (14)
_k=1_

X


_Î±k = âˆž,_
_k=1_

X


_then the sequence of function classes corresponding to the iterates converges to the optimum, which_
_corresponds to the optimal Q-function to which standard Q-learning updates would converge to:_



[Î¾n] [Î¾[âˆ—]] _with_ _Q[âˆ—](s, a) =_
_â†’_


_R(Ï†)Î¾[âˆ—](s, a, Ï†)dÏ†._ (15)


The proof is provided in Appendix A and follows the same flow as for Q-learning.

The previous theorem provides convergence guarantees under the assumption that either p(Ï†t =
_Ï†_ _st, at; Ï€) is known, or an unbiased estimate can be constructed. We propose two different ways_
_|_
to approximate p(Ï†t = Ï†|st, at; Ï€) from a given transition (st, at, st+1, Ï†t) so as to perform the
_Î¾-update (13). The first instance is a model-free version and detailed in the following section. A_
second instance uses a one-step SF model, called One-Step Model-based (MB) Î¾-learning, which is
further described in Sec. B.

**Model-free (MF) Î¾-Learning:** MF Î¾-learning uses the same principle as standard model-free
temporal difference learning methods. The update assumes for a given transition (st, at, st+1, Ï†t)
that the probability for the observed feature is p(Ï† = Ï†t _st, at) = 1. Whereas for all other features_
_|_
( _Ï†[â€²]_ Î¦, Ï†[â€²] = Ï†t) the probability is p(Ï†[â€²] = Ï†t _st, at) = 0, see Appendix D for continuous features._
_âˆ€_ _âˆˆ_ _Ì¸_ _|_
The resulting updates are:

_Ï† = Ï†t :_ _Î¾[Ï€](st, at, Ï†)_ (1 _Î±)Î¾[Ï€](st, at, Ï†) + Î± (1 + Î³Î¾[Ï€](st+1, Â¯at+1, Ï†))_
_â†_ _âˆ’_ (16)
_Ï†[â€²]_ =Ì¸ _Ï†t :_ _Î¾[Ï€](st, at, Ï†[â€²])_ _â†_ (1 âˆ’ _Î±)Î¾[Ï€](st, at, Ï†[â€²]) + Î±Î³Î¾[Ï€](st+1, Â¯at+1, Ï†[â€²]) ._

Due to the stochastic update of the Î¾-function and if the learning rate Î± âˆˆ (0, 1] discounts over time,
the Î¾-update will learn the true probability of p(Ï† = Ï†t _st, at). A potential problem with the MF_
_|_
procedure is that it might induce a high variance when the true feature probabilities are not binary.

3.3 META Î¾-LEARNING

After discussing Î¾-learning on a single task and showing its theoretical convergence, we can now
investigate how it can be applied in transfer learning. Similar to the linear SF framework the Î¾-function
allows to reevaluate a policy learned for task Mi, Î¾[Ï€][i], in a new environment Mj:


_Q[Ï€]j_ _[i]_ [(][s, a][) =]


_Rj(Ï†)Î¾[Ï€][i]_ (s, a, Ï†)dÏ†. (17)


This allows us to apply GPI in (6) for arbitrary reward functions in a similar manner to what was
proposed for linear reward functions in (Barreto et al., 2018). We extend the GPI result to the
_Î¾-learning framework as follows:_
**Theorem 2.associated to a (possibly different) weighting function (Generalised policy improvement in Î¾-learning) Let Ri** _L[1](Î¦) M. Let be the set of tasks, each one Î¾[Ï€]i[âˆ—] be a representative of_
_âˆˆ_
_the optimal class of Î¾-functions for task Mi, i_ 1, . . ., I _, and let_ _Î¾[Ëœ][Ï€][i]_ _be an approximation to the_
_optimal Î¾-function,_ _Î¾[Ï€]i[âˆ—]_ _Î¾[Ï€][i]_ _Ri_ _Îµ,_ _i. Then, for another task âˆˆ{_ _}_ _M with weighting function R, the_
_policy defined as:_ _âˆ¥_ _âˆ’_ [Ëœ] _âˆ¥_ _â‰¤_ _âˆ€_


_R(Ï†)Î¾[Ëœ][Ï€][i]_ (s, a, Ï†)dÏ†, (18)


_Ï€(s) = arg max_ max


_satisfies:_

_where_ _f_ _g = sups,a_
_âˆ¥_ _âˆ¥_


_Î¾[âˆ—]_ _Î¾[Ï€]_ _R_
_âˆ¥_ _âˆ’_ _âˆ¥_ _â‰¤_


2

_R_ _Ri_ _p(Ï†_ _s,a) + Îµ),_ (19)
1 _Î³_ [(min]i _âˆ¥_ _âˆ’_ _âˆ¥_ _|_
_âˆ’_


_where_ _f_ _g = sups,a_ Î¦
_âˆ¥_ _âˆ¥_ _[|][f][ Â·][ g][|][ d][Ï†][.]_

The proof is provided in Appendix A.R


-----

4 EXPERIMENTS

We evaluated Î¾-learning in two environments. The first has discrete features. It is a modified version
of the object collection task by Barreto et al. (2017) having more complex features to allow general
reward functions. See Appendix E.1 for experimental results in the original environment. The second
environment, the racer environment, evaluates the agents in tasks with continuous features.


4.1 DISCRETE FEATURES - OBJECT COLLECTION ENVIRONMENT

**Environment:** The environment consist of 4 rooms (Fig. 1 - a). The agent starts an episode in
position S and has to learn to reach the goal position G. During an episode, the agent can collect
objects to gain further rewards. Each object has 2 properties: 1) color: orange or blue, and 2) form:
box or triangle. The state space is a high-dimensional vector s âˆˆ R[112]. It encodes the agentâ€™s position
using a 10 Ã— 10 grid of two-dimensional Gaussian radial basis functions. Moreover, it includes
a memory about which object has been already collected. Agents can move in 4 directions. The
features Ï† âˆˆ Î¦ = {0, 1}[5] are binary vectors. The first 2 dimensions encode if an orange or a blue
object was picked up. The 2 following dimensions encode the form. The last dimension encodes
if the agent reached goal G. For example, Ï†[âŠ¤] = [1, 0, 1, 0, 0] encodes that the agent picked up an
orange box.

**Tasks:** Each agent learns sequentially 300 tasks which differ in their reward for collecting objects.
We compared agents in two settings: either in tasks with linear or general reward functions. For each
linear task Mi, the rewards r = Ï†[âŠ¤]wi are defined by a linear combination of features and a weight
with a specific property. They are randomly sampled from a uniform distribution:vector wi âˆˆ R[5]. The weights wi,k for the first 4 dimensions define the rewards for collecting an object wi,k ( 1, 1).
The final weight defines the reward for reaching the goal position which is wi,5 = 1 for each task. The âˆ¼U _âˆ’_
general reward functions are sampled by assigning a different reward to each possible combination
of object properties Ï†j Î¦ using uniform sampling: Ri(Ï†j) ( 1, 1), such that picking up an
_âˆˆ_ _âˆ¼U_ _âˆ’_
orange box might result in a reward of Ri(Ï†[âŠ¤] = [1, 0, 1, 0, 0]) = 0.23.


(a) Collection Environment (b) Tasks with Linear Reward Functions

QL SFQL MF Xi MB Xi


600

400

200


20 40 60 80 100 120 140 160 180 200 220 240 260 280 300


S Tasks Trained

G

S


(c) Effect of Non-Linearity (d) Tasks with General Reward Functions


QL / Xi SFQL / Xi

0.1250.3750.6250.8751.1251.375

Mean Error of Linear Model


QL SFQL MF Xi MB Xi

20 40 60 80 100 120 140 160 180 200 220 240 260 280 300


800

600

400

200


0.5


Tasks Trained


Figure 1: In the (a) object collection environment, Î¾-learning reached the highest average reward
per task for (b) linear, and (d) general reward functions. The average over 10 runs per algorithm and
the standard error of the mean are depicted. (c) The performance difference between Î¾-learning and
SFQL is stronger for general reward tasks that have high non-linearity, i.e. where a linear reward
model yields a high error. SFQL can only reach less than 50% of MF Î¾-learningâ€™s performance in
tasks with a mean linear reward model error of 1.625.


-----

**Agents:** We compared Î¾-learning to Q-learning (QL), and classical SF Q-learning (SFQL) (Barreto
et al., 2017). All agents use function approximation for their state-action functions (Q, Ïˆ, or Î¾function). An independent linear mapping is used to map the values from the state for each of the 4
actions. As the features are discrete, the Î¾-function and Ë†pÏ†-model are approximated by an independent
mapping for each action and possible feature Ï† âˆˆ Î¦. The Q-value Q(s, a) for the Î¾-agents (Eq. 10) is
computed by: Q[Ï€](s, a) = _Ï†_ Î¦ _[R][(][Ï†][)][Î¾][Ï€][(][s, a, Ï†][)][. The reward functions of each task are given to the]_

_âˆˆ_
_Î¾-agents. For SFQL, the sampled reward weights wi were given in tasks with linear reward functions._
For general reward functions, a linear model[P] _r = Ï†[âŠ¤]wËœ_ _i approximating the rewards was learned_
for each task and its weights Ëœwi given to SFQL (see Appendix C.3 for details). Each tasks was
executed for 20, 000 steps, and the average performance over 10 runs per algorithm was measured.
We performed a grid-search over the parameters of each agent, reporting here the performance of the
parameters with the highest total reward over all tasks.

**Results:** _Î¾-learning outperformed SFQL and QL for tasks with linear and general reward functions_
(Fig. 1 - b; d). MF showed a slight advantage over MB Î¾-learning in both settings. We further studied
the effect non-linearity of general reward functions on the performance of classical SF compared to
_Î¾-learning by evaluating them in tasks with different levels of non-linearity. We sampled general_
reward functions that resulted in different levels of mean absolute model error if they are linearly
approximated with min Ëœw **w** . We trained SFQL and MF Î¾-learning in each of these
conditions on 300 tasks and measured the ratio between the total return of SFQL and MF[|][r][(][Ï†][)][ âˆ’] _[Ï†][âŠ¤]_ [Ëœ] _|_ _Î¾ (Fig. 1)._
The relative performance of SFQL compared to MF Î¾ reduces with higher non-linearity of the reward
functions. For reward functions that are nearly linear (mean error of 0.125), both have a similar
performance. Whereas, for reward functions that are difficult to model with a linear relation (mean
error of 1.625) SFQL reaches only less than 50% of the performance of Î¾-learning. This follows
SFQLâ€™s theoretical limitation in (7) and shows the advantage of Î¾ learning over SFQL in non-linear
reward tasks.

4.2 CONTINUOUS FEATURES - RACER ENVIRONMENT


**Environment and Tasks:** We further evaluated the agents in an environment with continuous
features (Fig. 2 - a). The agent is randomly placed in the environment and has to drive around for 200
timesteps before the episode ends. Similar to a car, the agent has an orientation and momentum, so
that it can only drive straight, or in a right or left curve. The agent reappears on the opposite side if
it exits one side. The distance to 3 markers are provided as features Ï† âˆˆ R[3]. Rewards depend on
the distances r = _k=1_ _[r][k][Ï†][k][, where each component][ r][k][ has 1 or 2 preferred distances defined by]_
Gaussian functions. For each of the 37 tasks, the number of Gaussians and their properties (Âµ, Ïƒ) are
randomly sampled for each feature dimension. Fig. 2 (a) shows a reward function with dark areas

[P][3]
depicting higher rewards. The agent has to learn to drive around in such a way as to maximize its
trajectory over positions with high rewards. The state space is a high-dimensional vector s âˆˆ R[120]
encoding the agentâ€™s position and orientation. As before, the 2D position is encoded using a 10 Ã— 10
grid of two-dimensional Gaussian radial basis functions. Similarly, the orientation is also encoded
using 20 Gaussian radial basis functions.

(a) Racer Environment (b) Tasks with General Reward Functions


QL SFQL CMF Xi

10 15 20 25 30 35


80k

60k


40k


Tasks Trained

Figure 2: (a) Example of a reward function for the racer environment based on distances to its 3
markers. (b) Î¾-learning reaches the highest average reward per task. SFQL yields a performance even
below QL as it is not able to model the reward function with its linear combination of weights and
features. The average over 10 runs per agent and the standard error of the mean are depicted.


-----

**Agents:** We introduce a MF Î¾-agent for continuous features (CMF Î¾) (Appendix D.2.3). CMF Î¾ discretizes each feature dimension Ï†k [0, 1] in 11 bins with the bin centers: X = 0.0, 0.1, . . ., 1.0 .
It learns for each dimension k and bin âˆˆ _i the Î¾-value Î¾k[Ï€][(][s, a, X][i][)][. Q-values (Eq. 10) are computed] {_ _}_
by: Q[Ï€](s, a) = _k=1_ 11i=1 _[r][k][(][X][i][)][Î¾]k[Ï€][(][s, a, X][i][)][. SFQL learns][ Ïˆ][ for the continuous, non-discretized]_
feature space. It received an approximated weight vector Ëœwi that was trained before the task started
on several uniformly sampled features and rewards.P

[P][3]

**Results:** _Î¾-learning reached the highest performance of all agents (Fig. 2 - b) outperforming QL_
and SFQL. SFQL reaches only a low performance below QL, because it is not able to sufficiently well
approximate the general reward functions with its linear reward model. This shows the advantage of
_Î¾-learning over SFQL in environments with general reward functions._

5 DISCUSSION

**Performance of Î¾-learning compared to classical SF&GPI:** _Î¾-learning allows to disentangle_
the dynamics of policies in the feature space of a task from the associated reward, see (10). The
experimental evaluation in tasks with general reward functions (Fig. 1 - d, and Fig. 2) shows that
_Î¾-learning can therefore successfully apply GPI to transfer knowledge from learned tasks to new_
ones. Given a general reward function it can re-evaluate successfully learned policies for knowledge
transfer. Instead, classical SFQL based on a linear decomposition (3) can not be directly applied
given a general reward function. In this case a linear approximation has to be learned which shows
inferior performance to Î¾-learning that directly uses the true reward function.

_Î¾-learning also shows an increased performance over SFQL in environments with linear reward_
functions (Fig. 1 - a). This effect can not be attributed to differences in their computation of a policyâ€™s
expected return as both are correct (Appendix A.6). A possible explanation is that Î¾-learning reduces
the complexity for the function approximation of the Î¾-function compared to the Ïˆ-function in SFQL.

**Continuous Feature Spaces:** For tasks with continuous features (racer environment), Î¾-learning
used successfully a discretization of each feature dimension, and learned the Î¾-values independently
for each dimension. This strategy is viable for reward functions that are cumulative over the feature
dimensions: r(Ï†) = _k_ _[r][k][Ï†][k][. The Q-value can be computed by summing over the independent]_

dimensions and the bins X: Q[Ï€](s, a) = _k_ _x_ _X_ _[r][k][(][x][)][Î¾][Ï€][(][s, a, x][)][. For more general reward func-]_

_âˆˆ_
tions, the space of all feature combinations would need to be discretized, which grows exponentially

[P]

with each new dimension. As a solution the Î¾P-function could be directly defined over the continuous

[P]

feature space, but this yields some problems. First, the computation of the expected return requires an
integral Q(s, a) = _Ï†_ Î¦ _[R][(][Ï†][)][Î¾][(][s, a, Ï†][)][ over features instead of a sum, which is a priori intractable.]_

_âˆˆ_
Second, the representation and training of the Î¾-function, which would be defined over a continuum

R

thus increasing the difficulty of approximating the function. Janner et al. (2020) and Touati and
Ollivier (2021) propose methods that might allow to represent a continuous Î¾-function, but it is
unclear if they converge and if they can be used for transfer learning.

**Learning of Features:** In principle, classical SF&GPI can also optimize general reward functions
if features and reward weights are learned. This is possible if the learned features describe the
non-linear effects in the reward functions. Nonetheless, learning of features adds further challenges
and shows to reduce performance. Barreto et al. (2017) learns features from observations sampled
from several tasks before the SF&GPI starts. Therefore, novel non-linearities potentially introduced
at later tasks are not well represent by the learned features. If instead features are learned alongside
the SF&GPI procedure, the problem on how to coordinate both learning processes needs to be
investigated. Importantly, Ïˆ-functions for older tasks would become unusable for the GPI procedure
on newer task, because the feature representation changed between them.

Moreover, our replication (Sec. E.1) of the object collection task from Barreto et al. (2017) shows the
performance of learned features is below the performance of given features. MF Xi reaches a final
average reward per task of 850 with given features and reward functions. The best performance of
SFQL with learned features only reaches a final performance of 575 (Fig. 2 in (Barreto et al., 2017)).

In summary, if features and reward functions are known then Î¾-learning outperforms SFQL. And
using given features and reward functions is natural for many applications as these are often known,
for example in robotic tasks where they are usually manually designed (Akalin and Loutfi, 2021).


-----

**Computational Complexity:** The improved performance of SFQL and Î¾-learning over QL in
the transfer learning setting comes at the cost of an increased computational complexity. The GPI
procedure (6) of both approaches requires to evaluate at each step the Ïˆ[Ï€][i]-function or Î¾[Ï€][i]-function
over all previous experienced tasks in M. As a consequence, the computational complexity increases
linearly with each new environment that is added. A solution is to apply GPI only over a subset of
learned policies. Nonetheless, an open question is still how to optimally select this subset.

6 RELATED WORK

**Transfer Learning:** Transfer methods in RL can be generally categorized according to the type of
tasks between which transfer is possible and the type of transferred knowledge (Taylor and Stone,
2009; Lazaric, 2012; Zhu et al., 2020). In the case of SF&GPI which Î¾-learning is part of, tasks only
differ in their reward functions. The type of knowledge that is transferred are policies learned in
source tasks which are re-evaluated in the target task and recombined using the GPI procedure. A
natural use-case for Î¾-learning are continual problems (Khetarpal et al., 2020) where an agent has
continually adapt to changing tasks, which are in our setting different reward functions.

**Successor Features:** SF are based on the concept of successor representations (Dayan, 1993;
Momennejad, 2020). Successor representations predict the future occurrence of all states for a
policy in the same manner as SF for features. Their application is restricted to low-dimensional
state spaces using tabular representations. SF extended them to domains with high-dimensional state
spaces (Kulkarni et al., 2016; Zhang et al., 2017; Barreto et al., 2017; 2018), by predicting the future
occurrence of low-dimensional features that are relevant to define the return. Several extensions
to the SF framework have been proposed. One direction aims to learn appropriate features from
data such as by optimally reconstruct rewards (Barreto et al., 2017), using the concept of mutual
information (Hansen et al., 2019), or the grouping of temporal similar states (Madjiheurem and
Toni, 2019). Another direction is the generalization of the Ïˆ-function over policies (Borsa et al.,
2018) analogous to universal value function approximation (Schaul et al., 2015). Similar approaches
use successor maps (Madarasz, 2019), goal-conditioned policies (Ma et al., 2020), or successor
feature sets (Brantley et al., 2021). Other directions include their application to POMDPs (VÃ©rtes
and Sahani, 2019), combination with max-entropy principles (Vertes, 2020), or hierarchical RL
(Barreto et al., 2021). In difference to Î¾-learning all these approaches build on the assumption of
linear reward functions, whereas Î¾-learning allows the SF&GPI framework to be used with general
reward functions. Nonetheless, most of the extensions for linear SF can be combined with Î¾-learning.

**Model-based RL:** SF represent the dynamics of a policy in the feature space that is decoupled
from the rewards allowing to reevaluate them under different reward functions. It shares therefore
similar properties with model-based RL (Lehnert and Littman, 2019). In general, model-based RL
methods learn a one-step model of the environment dynamics p(st+1 _st, at). Given a policy and an_
_|_
arbitrary reward function, rollouts can be performed using the learned model to evaluate the return.
In practice, the rollouts have a high variance for long-term predictions rendering them ineffective.
Recently, (Janner et al., 2020) proposed the Î³-model framework that learns to represent Î¾-values
in continuous domains. Nonetheless, the application to transfer learning is not discussed and no
convergence is proven as for Î¾-learning. This is the same case for the forward-backward MPD
representation proposed in Touati and Ollivier (2021). (Tang et al., 2021) also proposes to decouple
the dynamics in the state space from the rewards, but learn an internal representation of the rewards.
This does not allow to reevaluate an policy to a new reward function without relearning the mapping.

7 CONCLUSION

The introduced Î¾-learning framework learns the expected cumulative discounted probability of
successor features which disentangles the dynamics of a policy in the feature space of a task from the
expected rewards. This allows Î¾-learning to reevaluate the expected return of learned policies for
general reward functions and to use it for transfer learning utilizing GPI. We proved that Î¾-learning
converges to the optimal policy, and showed experimentally its improved performance over Q-learning
and the classical SF framework for tasks with linear and general reward functions.


-----

ETHICS STATEMENT

_Î¾-learning and its associated optimization algorithms represent general RL procedures similar to_
Q-learning. Their potential negative societal impact depends on their application domains which
range over all possible societal areas in a similar manner as for other general RL procedures.

Beyond the topic of the paper, we did our best to cite the relevant literature and to fairly compare
with previous ideas, concepts and methods. To that aim, all agents are trained and evaluated within
the same software environment, and under the very same experimental settings.

REPRODUCIBILITY STATEMENT

In order to ensure high changes of reproducibility we provided lots of details of the method and
experiments associated to the paper. In particular, we have provided the proofs for all mathematical
results announced in the main paper (see Appendix A). These constitute the theoretical foundation of
the proposed Î¾-learning methodology. Secondly, we have provided all experimental details (methods,
and environments) required for reproducing our experiments, namely: appendix C for the object
collection and D for the racer environment respectively. In addition, we provide additional results
in appendix E, to completely illustrate the interest of the proposed method. Finally, we provided an
anonymous link to the source code, so that reviewers can run it if necessary.

REFERENCES

N. Akalin and A. Loutfi. Reinforcement learning approaches in social robotics. Sensors, 21(4):1292,
2021.

A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. P. van Hasselt, and D. Silver. Successor
features for transfer in reinforcement learning. In Advances in neural information processing
_systems, pages 4055â€“4065, 2017._

A. Barreto, D. Borsa, J. Quan, T. Schaul, D. Silver, M. Hessel, D. Mankowitz, A. Zidek, and
R. Munos. Transfer in deep reinforcement learning using successor features and generalised policy
improvement. In International Conference on Machine Learning, pages 501â€“510. PMLR, 2018.

A. Barreto, S. Hou, D. Borsa, D. Silver, and D. Precup. Fast reinforcement learning with generalized
policy updates. Proceedings of the National Academy of Sciences, 117(48):30079â€“30087, 2020.

A. Barreto, D. Borsa, S. Hou, G. Comanici, E. AygÃ¼n, P. Hamel, D. Toyama, J. Hunt, S. Mourad,
D. Silver, et al. The option keyboard: Combining skills in reinforcement learning. arXiv preprint
_arXiv:2106.13105, 2021._

D. Borsa, A. Barreto, J. Quan, D. Mankowitz, R. Munos, H. van Hasselt, D. Silver, and T. Schaul.
Universal successor features approximators. arXiv preprint arXiv:1812.07626, 2018.

K. Brantley, S. Mehri, and G. J. Gordon. Successor feature sets: Generalizing successor representations across policies. arXiv preprint arXiv:2103.02650, 2021.

P. Dayan. Improving generalization for temporal difference learning: The successor representation.
_Neural Computation, 5(4):613â€“624, 1993._

S. Hansen, W. Dabney, A. Barreto, T. Van de Wiele, D. Warde-Farley, and V. Mnih. Fast task
inference with variational intrinsic successor features. arXiv preprint arXiv:1906.05030, 2019.

M. Janner, I. Mordatch, and S. Levine. Î³-models: Generative temporal difference learning for
infinite-horizon prediction. In NeurIPS, 2020.

K. Khetarpal, M. Riemer, I. Rish, and D. Precup. Towards continual reinforcement learning: A review
and perspectives. arXiv preprint arXiv:2012.13490, 2020.

T. D. Kulkarni, A. Saeedi, S. Gautam, and S. J. Gershman. Deep successor reinforcement learning.
_arXiv preprint arXiv:1606.02396, 2016._


-----

A. Lazaric. Transfer in reinforcement learning: a framework and a survey. In Reinforcement Learning,
pages 143â€“173. Springer, 2012.

L. Lehnert and M. L. Littman. Successor features support model-based and model-free reinforcement
learning. CoRR abs/1901.11437, 2019.

C. Ma, D. R. Ashley, J. Wen, and Y. Bengio. Universal successor features for transfer reinforcement
learning. arXiv preprint arXiv:2001.04025, 2020.

T. J. Madarasz. Better transfer learning with inferred successor maps. _arXiv preprint_
_arXiv:1906.07663, 2019._

S. Madjiheurem and L. Toni. State2vec: Off-policy successor features approximators. arXiv preprint
_arXiv:1910.10277, 2019._

V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement
learning. nature, 518(7540):529â€“533, 2015.

I. Momennejad. Learning structures: Predictive representations, replay, and generalization. Current
_Opinion in Behavioral Sciences, 32:155â€“166, 2020._

I. Momennejad, E. M. Russek, J. H. Cheong, M. M. Botvinick, N. D. Daw, and S. J. Gershman.
The successor representation in human reinforcement learning. Nature Human Behaviour, 1(9):
680â€“692, 2017.

T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In
_International conference on machine learning, pages 1312â€“1320, 2015._

D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and
go through self-play. Science, 362(6419):1140â€“1144, 2018.

R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.

H. Tang, J. Hao, G. Chen, P. Chen, C. Chen, Y. Yang, L. Zhang, W. Liu, and Z. Meng. Foresee then evaluate: Decomposing value estimation with latent future prediction. arXiv preprint
_arXiv:2103.02225, 2021._

M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. Journal
_of Machine Learning Research, 10(7), 2009._

M. S. Tomov, E. Schulz, and S. J. Gershman. Multi-task reinforcement learning in humans. Nature
_Human Behaviour, pages 1â€“10, 2021._

A. Touati and Y. Ollivier. Learning one representation to optimize all rewards. arXiv preprint
_arXiv:2103.07945, 2021._

J. N. Tsitsiklis. Asynchronous stochastic approximation and q-learning. Machine learning, 16(3):
185â€“202, 1994.

E. Vertes. Probabilistic learning and computation in brains and machines. PhD thesis, UCL
(University College London), 2020.

E. VÃ©rtes and M. Sahani. A neurally plausible model learns successor representations in partially
observable environments. arXiv preprint arXiv:1906.09480, 2019.

C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279â€“292, 1992.

J. Zhang, J. T. Springenberg, J. Boedecker, and W. Burgard. Deep reinforcement learning with
successor features for navigation across similar environments. In 2017 IEEE/RSJ International
_Conference on Intelligent Robots and Systems (IROS), pages 2371â€“2378. IEEE, 2017._

Z. Zhu, K. Lin, and J. Zhou. Transfer learning in deep reinforcement learning: A survey. arXiv
_preprint arXiv:2009.07888, 2020._


-----

A THEORETICAL PROOFS

A.1 PROOF OF PROPOSITION 1

Let us start by recalling the original statement in the main paper.

**Proposition 1. (Equivalence between functions Î¾ and Q) Let** = _Q :_ R s.t. _Q_ _<_
_Q_ _{_ _S Ã— A â†’_ _âˆ¥_ _âˆ¥âˆž_
_âˆž}there is a bijective correspondence between the quotient space. Let âˆ¼_ _be defined as Î¾1 âˆ¼_ _Î¾2 â‡”_ Î¦ _[RÎ¾][1][ =]_ Î¦ _[RÎ¾][2][. Then,][ âˆ¼] Îž[is an equivalence relationship, and]and_ _._
R R _âˆ¼_ _Q_


_Proof. We will proof the statements sequentially._

_âˆ¼_ **is an equivalence relationship:** To prove this we need to demonstrate that âˆ¼ is symmetric,
reciprocal and transitive. The three are quite straightforward since: Î¾ âˆ¼ _Î¾, âˆ€Î¾, Î¾ âˆ¼_ _Î· â‡”_ _Î· âˆ¼_ _Î¾, âˆ€Î¾, Î·_
and Î¾ âˆ¼ _Î·, Î· âˆ¼_ _Î½ â‡’_ _Î¾ âˆ¼_ _Î½._

**Bijective correspondence:** To prove the bijectivity, we will first prove that it is injective, then
surjective. Regarding the injectivity: [Î¾] = [Î·] _QÎ¾_ = QÎ·, we prove it by contrapositive:
_Ì¸_ _â‡’_ _Ì¸_


_R(Ï†)Î¾(s, a, Ï†)dÏ† =_


_R(Ï†)Î·(s, a, Ï†)dÏ† â‡’_ [Î¾] = [Î·]. (20)


_QÎ¾ = QÎ· â‡’_


In order to prove the surjectivity, we start from a function Q âˆˆQ and select an arbitrary Î¾ âˆˆ Îž, then
the following function:

_Q(s, a)_
_Î¾Q(s, a, Ï†) =_ (21)

Î¦ _[R][(Â¯]Ï†)Î¾(s, a,_ _Ï†[Â¯])dÏ†[Â¯][Î¾][(][s, a, Ï†][)]_

R


satisfies thatthere is a bijective correspondence between the elements of Î¾Q âˆˆ Îž and that Î¦ _[R][(][Ï†][)][Î¾][Q][(][s, a, Ï†][)][d][Ï†][ =][ Q][(][s, a] Îž[)][,][ âˆ€]and of[(][s, a][)][ âˆˆS Ã— A]._ [. We conclude that]
R _âˆ¼_ _Q_

A.2 PROOF OF COROLLARY 1

Let us recall the result:

**Corollary 1. The bijection between Îž** _and_ _allows to induce a norm_ _into Îž_ _from the_
_âˆ¼_ _Q_ _âˆ¥Â· âˆ¥âˆ¼_ _âˆ¼_
_supremum norm in_ _, with which Îž_ _is a Banach space (since_ _is Banach with_ _):_
_Q_ _âˆ¼_ _Q_ _âˆ¥Â· âˆ¥âˆž_

_âˆ¥Î¾âˆ¥âˆ¼_ = sups,a Î¦ _R(Ï†)Î¾(s, a, Ï†)dÏ†_ [= sup]s,a (22)

Z _[|][Q][(][s, a][)][|][ =][ âˆ¥][Q][âˆ¥][âˆž]_ _[,]_

_Proof. The norm induced in the quotient space is defined from the correspondence between Îž_ and
_âˆ¼_
_Q and is naturally defined as in the previous equation. The norm is well defined since it does not_
depend on the class representative. Therefore, all the metric properties are transferred, and Îž is
_âˆ¼_
immediately Banach with the norm .
_âˆ¥Â· âˆ¥âˆ¼_

A.3 PROOF OF PROPOSITION 2

Letâ€™s restate the result:

**Proposition 2. (Î¾-learning has a fixed point) The operator TÎ¾ is well-defined w.r.t. the equivalence**
_, and therefore induces an operator T_ _defined over Îž_ _. T_ _is contractive w.r.t._ _. Since Îž_
_âˆ¼_ _âˆ¼_ _âˆ¼_ _âˆ¼_ _âˆ¥Â· âˆ¥âˆ¼_ _âˆ¼_
_is Banach, T_ _has a unique fixed point and iterating T_ _starting anywhere converges to that point._
_âˆ¼_ _âˆ¼_

_Proof. We prove the statements above one by one:_


-----

**The operator T** **is well defined:** Let us first recall the definition of the operator TÎ¾ in (12), where
_âˆ¼_
we removed the dependency on Ï€ for simplicity:

_TÎ¾(Î¾) = p(Ï†t = Ï†|st, at) + Î³Ep(st+1,at+1|st,at) {Î¾(st+1, at+1, Ï†)}_

Let Î¾1, Î¾2 [Î¾] two different representatives of class [Î¾], we can write:
_âˆˆ_

_R(Ï†)(TÎ¾(Î¾1)(s, a, Ï†)_ _TÎ¾(Î¾2)(s, a, Ï†))dÏ†_
Î¦ _âˆ’_

Z


_p(s[â€²], a[â€²]_ _s, a)Î³(Î¾1(s[â€²], a[â€²], Ï†)_ _Î¾2(s[â€²], a[â€²], Ï†))ds[â€²]dÏ†_
_|_ _âˆ’_


_R(Ï†)_


(23)


_p(s[â€²], a[â€²]|s, a)_


_R(Ï†)(Î¾1(s[â€²], a[â€²], Ï†)_ _Î¾2(s[â€²], a[â€²], Ï†))dÏ†ds[â€²]_
_âˆ’_


= Î³

= 0


becausesince the image of class does not depend on the function chosen to represent the class. Î¾1, Î¾2 âˆˆ [Î¾]. Therefore the operator Tâˆ¼([Î¾]) = TÎ¾(Î¾) is well defined in the quotient space,

**Contractive operator T** **:** The contractiveness of T can be proven directly:
_âˆ¼_ _âˆ¼_


_R(Ï†)_ _p(Ï†|s, a) + Î³Ep(sâ€²,aâ€²|s,a){Î¾(s[â€²], a[â€²], Ï†)}_
 


_T_ ([Î¾]) _T_ ([Î·]) = sup
_âˆ¥_ _âˆ¼_ _âˆ’_ _âˆ¼_ _âˆ¥âˆ¼_ _s,a_


_âˆ’p(Ï†|s, a) âˆ’_ _Î³Ep(sâ€²,aâ€²|s,a){Î·(s[â€²], a[â€²], Ï†)}_ dÏ†



_R(Ï†)Ep(sâ€²,aâ€²|s,a){Î¾(s[â€²], a[â€²], Ï†) âˆ’_ _Î·(s[â€²], a[â€²], Ï†)}dÏ†_


=Î³ sup
_s,a_


(24)


_R(Ï†)(Î¾(s[â€²], a[â€²], Ï†) âˆ’_ _Î·(s[â€²], a[â€²], Ï†))dÏ†_


=Î³ sup
_s,a_ [E][p][(][s][â€²][,a][â€²][|][s,a][)]


_R(Ï†)(Î¾(s[â€²], a[â€²], Ï†) âˆ’_ _Î·(s[â€²], a[â€²], Ï†))dÏ†_


_â‰¤Î³ sups[â€²],a[â€²]_


=Î³ [Î¾] [Î·]
_âˆ¥_ _âˆ’_ _âˆ¥âˆ¼_

The contractiveness of T can also be understood as being inherited from the standard Bellmann
_âˆ¼_
operator on Q. Indeed, given a Î¾ function, one can easily see that applying the standard Bellman
operator to the Q function corresponding to Î¾ leads to the Q function corresponding to T ([Î¾]).
_âˆ¼_

**Fixed point of T** **:** To conclude the proof, we use the fact that any contractive operator on a Banach
_âˆ¼_
space, in our case: Tâˆ¼ : Îžâˆ¼ _â†’_ Îžâˆ¼ has a unique fixed point [Î¾[âˆ—]], and that for any starting point [Î¾0],
the sequence [Î¾n] = Tâˆ¼([Î¾nâˆ’1]) converges to [Î¾[âˆ—]] w.r.t. to the corresponding norm âˆ¥[Î¾]âˆ¥âˆ¼.

A.4 PROOF OF THEOREM 1

These two propositions will be useful to prove that the Î¾ learning iterates converge in Îž . Let us
_âˆ¼_
restate the definition of the operator from (13):

_Î¾k[Ï€]+1[(][s][t][, a][t][, Ï†][)][ â†]_ _[Î¾]k[Ï€][(][s][t][, a][t][, Ï†][) +][ Î±][k]_ [[][p][(][Ï†][t] [=][ Ï†][|][s][t][, a][t][;][ Ï€][) +][ Î³Î¾]k[Ï€][(][s][t][+1][,][ Â¯]at+1, Ï†) âˆ’ _Î¾k[Ï€][(][s][t][, a][t][, Ï†][)]]_

and the theoretical result:

**Theorem 1. (Convergence of Î¾-learning) For a sequence of state-action-feature** _st, at, st+1, Ï†t_ _t=0_
_{_ _}[âˆž]_
_consider the Î¾-learning update given in (13). If the sequence of state-action-feature triples visits_
_each state, action infinitely often, and if the learning rate Î±k is an adapted sequence satisfying the_
_Robbins-Monro conditions:_


_Î±k[2]_ _[<][ âˆž]_ (25)
_k=1_

X


_Î±k = âˆž,_
_k=1_

X


-----

_then the sequence of function classes corresponding to the iterates converges to the optimum, which_
_corresponds to the optimal Q-function to which standard Q-learning updates would converge to:_



[Î¾n] [Î¾[âˆ—]] _with_ _Q[âˆ—](s, a) =_
_â†’_


_R(Ï†)Î¾[âˆ—](s, a, x)dÏ†._ (26)


_Proof. The proof re-uses the flow of the proof used for Q-learning (Tsitsiklis, 1994). Indeed, we_
rewrite the operator above as:

_Î¾k[Ï€]+1[(][s][t][, a][t][, Ï†][)][ â†]_ _[Î¾]k[Ï€][(][s][t][, a][t][, Ï†][) +][ Î±][k]_ [[][T][Î¾][(][Î¾]k[Ï€][)(][s][t][, a][t][, Ï†][)][ âˆ’] _[Î¾]k[Ï€][(][s][t][, a][t][, Ï†][) +][ Îµ][(][s][t][, a][t][, Ï†][)]]_

with Îµ defined as:
_Îµ(st, at, Ï†) =_ _p(Ï†t = Ï†|st, at; Ï€) + Î³Î¾k[Ï€][(][s][t][+1][,][ Â¯]at+1, Ï†)_
_âˆ’E {p(Ï†t = Ï†|st, at; Ï€) + Î³Î¾k[Ï€][(][s][t][+1][,][ Â¯]at+1, Ï†)} ._

Obviously Îµ satisfies E _Îµ_ = 0, which, together with the contractiveness of T, is sufficient to
_{_ _}_ _âˆ¼_
demonstrate the convergence of the iterative procedure as done for Q-learning. In our case, the
optimal function Î¾[âˆ—] is defined up to an additive kernel function Îº âˆˆ Ker. The correspondence with
the optimal Q learning function is a direct application of the correspondence between the Î¾- and
Q-learning problems.

A.5 PROOF OF THEOREM 2

Let us restate the result.
**Theorem 2.associated to a (possibly different) weighting function (Generalised policy improvement in Î¾-learning) Let Ri** _L[1](Î¦) M. Let be the set of tasks, each one Î¾[Ï€]i[âˆ—] be a representative of_
_âˆˆ_
_the optimal class of Î¾-functions for task Mi, i_ 1, . . ., I _, and let_ _Î¾[Ëœ][Ï€][i]_ _be an approximation to the_
_optimal Î¾-function,_ _Î¾[Ï€]i[âˆ—]_ _Î¾[Ï€][i]_ _Ri_ _Îµ,_ _i. Then, for another task âˆˆ{_ _}_ _M with weighting function R, the_
_policy defined as:_ _âˆ¥_ _âˆ’_ [Ëœ] _âˆ¥_ _â‰¤_ _âˆ€_


_R(Ï†)Î¾[Ëœ][Ï€][i]_ (s, a, Ï†)dÏ†, (27)


_Ï€(s) = arg max_ max


_satisfies:_

_where_ _f_ _g = sups,a_
_âˆ¥_ _âˆ¥_


_Î¾[âˆ—]_ _Î¾[Ï€]_ _R_
_âˆ¥_ _âˆ’_ _âˆ¥_ _â‰¤_

Î¦

_[|][f][ Â·][ g][|][ d][Ï†][.]_


2

_R_ _Ri_ _p(Ï†_ _s,a) + Îµ),_ (28)
1 _Î³_ [(min]i _âˆ¥_ _âˆ’_ _âˆ¥_ _|_
_âˆ’_


_Proof. The proof is stated in two steps. First, we exploit the proof of Proposition 1 of (Barreto et al.,_
2017), and in particular (13) that states:

2
_Q[âˆ—]_ _Q[Ï€]_ sup _,_ _i_ 1, . . ., I _,_ (29)
_âˆ¥_ _âˆ’_ _âˆ¥âˆž_ _â‰¤_ 1 _Î³_ _s,a_ _âˆ€_ _âˆˆ{_ _}_

_âˆ’_  _[|][r][(][s, a][)][ âˆ’]_ _[r][i][(][s, a][)][|][ +][ Îµ]_

where Q[âˆ—] and Q[Ï€] are the Q-functions associated to the optimal and Ï€ policies in the environment
_R. The conditions on the Q functions required in the original proposition are satisfied because the_
_Î¾-functions satisfy them, and there is an isometry between Q and Î¾ functions._

Because the above inequality is true for all training environments i, we can rewrite as:


2
_Q[âˆ—]_ _Q[Ï€]_ min sup _._ (30)
_âˆ¥_ _âˆ’_ _âˆ¥âˆž_ _â‰¤_ 1 _Î³_ _i_ _s,a_

_âˆ’_  _[|][r][(][s, a][)][ âˆ’]_ _[r][i][(][s, a][)][|][ +][ Îµ]_

We now realise that, in the case of Î¾-learning, the reward functions rewrite as:


_R(Ï†)p(Ï†_ _s, a)dÏ†_ _ri(s, a) =_
_|_


_Ri(Ï†)p(Ï†_ _s, a)dÏ†,_ (31)
_|_


_r(s, a) =_

and therefore we have:


(R(Ï†) _Ri(Ï†))p(Ï†_ _s, a)dÏ†_
_âˆ’_ _|_

_R(Ï†)_ _Ri(Ï†)_ _p(Ï†_ _s, a)dÏ†_
_|_ _âˆ’_ _|_ _|_


sup
_s,a_ _s,a_

_[|][r][(][s, a][)][ âˆ’]_ _[r][i][(][s, a][)][|][ = sup]_

_â‰¤_ sups,a


(32)


= âˆ¥R âˆ’ _Riâˆ¥p(Ï†|s,a)_


-----

Similarly, due to the isometry between Î¾ and Q-learning, i.e. Proposition 2, we can write that:

_Î¾[âˆ—]_ _Î¾[Ï€]_ _R =_ [Î¾[âˆ—]] [Î¾[Ï€]] = _Q[âˆ—]_ _Q[Ï€]_
_âˆ¥_ _âˆ’_ _âˆ¥_ _âˆ¥_ _âˆ’_ _âˆ¥âˆ¼_ _âˆ¥_ _âˆ’_ _âˆ¥âˆž_


min sup
_i_ _s,a_

_[|][r][(][s, a][)][ âˆ’]_ _[r][i][(][s, a][)][|][ +][ Îµ]_


(33)


1 âˆ’ _Î³_


_R_ _Ri_ _p(Ï†_ _s,a) + Îµ),_

_â‰¤_ 1 _Î³_ [(min]i _âˆ¥_ _âˆ’_ _âˆ¥_ _|_

_âˆ’_

which proves the generalised policy improvement for Î¾-learning.

A.6 RELATION BETWEEN CLASSICAL SF AND Î¾-LEARNING FOR LINEAR REWARD FUNCTIONS

In the case of linear reward functions, i.e. where assumption (3) holds, it is possible to show that Î¾learning can be reduced to classical SF. Classical SF represents therefore a specific case of Î¾-learning
under this assumption.
**Theorem 3. (Equality of classical SF and Î¾-learning for linear reward functions) Given the assump-**
_tion that reward functions are linearily decomposable with_

_ri(st, at, st+1) â‰¡_ _Ï†(st, at, st+1)[âŠ¤]wi,_

_wheremi_ _Ï† âˆˆ, then the classical SF andR[n]_ _are features for a transition and Î¾-learning framework are equivalent. wi âˆˆ_ R[n] _are the reward weight vector of task_
_âˆˆM_

_Proof. We start with the definition of the Q-value according to Î¾-learning from (10). After replacing_
the reward function Ri with our linear assumption, the definition of the Q-function according to
classical SF with the Ïˆ-function can be recovered:


_Î¾[Ï€](st, at, Ï†)Ri(Ï†)dÏ†_

_Î¾[Ï€](st, at, Ï†)Ï†[âŠ¤]widÏ†_


_Qi(s, a) =_


= wi[âŠ¤]

= wi[âŠ¤]

= wi[âŠ¤]


_Î³[k]p(Ï†t+k = Ï†|st = s, at = a; Ï€)Ï† dÏ†_
_k=0_

X


_Î³[k]_

_k=0_

X


_p(Ï†t+k = Ï†|st = s, at = a; Ï€)Ï† dÏ†_


_i[âŠ¤]_ _Î³[k]E_ _Ï†t+k_

_{_ _}_
_k=0_

X

_âˆž_ _âŠ¤_

_Î³[k]Ï†t+k_ **wi** = _Ïˆ(s, a)[âŠ¤]wi ._

(k=0 )
X


= E


Please note, although both methods are equal in terms of their computed values, how these are
represented and learned differs between them. Thus, it is possible to see a performance difference of
the methods in the experimental results where Î¾-learning outperforms SFQL in our environments.

B ONE-STEP SF MODEL-BASED (MB) Î¾-LEARNING

Besides the MF Î¾-learning update operator (16), we introduce a second Î¾-learning procedure called
_One-step SF Model-based (MB) Î¾-Learning that attempts to reduce the variance of the update._
To do so, MB Î¾-learning estimates the distribution over the successor features over time. Let
_pËœ(Ï†t = Ï†|st, at; Ï€) denote the current estimate of the feature distribution. Given a transition_
(st, at, st+1, Ï†t) the model is updated according to:

_Ï† = Ï†t :_ _pËœÏ†(Ï†|st, at; Ï€)_ _â†_ _pËœÏ†(Ï†|st, at; Ï€) + Î² (1 âˆ’_ _pËœÏ†(Ï†|st, at; Ï€))_
_Ï†[â€²]_ =Ì¸ _Ï†t :_ _pËœÏ†(Ï†[â€²]|st, at; Ï€)_ _â†_ _pËœÏ†(Ï†[â€²]|st, at; Ï€) âˆ’_ _Î²pËœÏ†(Ï†[â€²]|st, at; Ï€),_


-----

where Î² [0, 1] is the learning rate. After updating the model ËœpÏ†, it can be used for the Î¾-update as
_âˆˆ_
defined in (13). Since the learned model ËœpÏ† is independent from the reward function and from the
policy, it can be learned and used over all tasks.

C EXPERIMENTAL DETAILS: OBJECT COLLECTION ENVIRONMENT

The object collection environment (Fig. 1 - a) was briefly introduced in Section 4.1. This section
provides a formal description.

C.1 ENVIRONMENT

The environment is a continuous two-dimensional area in which the agent moves. The position of
the agent is a point in the 2D space: (x, y) âˆˆ [0, 1][2]. The action space of the agent consists of four
movement directions: A = {up, down, left, right}. Each action changes the position of the agent in a
certain direction and is stochastic by adding a Gaussian noise. For example, the action for going right
updates the position according to xt+1 = xt + N (Âµ = 0.05, Ïƒ = 0.005). If the new position ends in
a wall (black areas in Fig. 1 - a) that have a width of 0.04) or outside the environment, the agent is set
back to its current position. Each environment has 12 objects. Each object has two properties with
two possible values: color (orange, blue) and shape (box, triangle). If the agent reaches an object, it
collects the object which then disappears. The objects occupy a circular area with radius 0.04. At the
beginning of an episode the agent starts at location S with (x, y)S = (0.05, 0.05). An episode ends if
the agent reaches the goal area G which is at position (x, y)G = (0.86, 0.86) and has a circular shape
with radius 0.1. After an episode the agent is reset to the start position S and all collected objects
reappear.

The state space of the agents consist of their position in the environment and the information about
which objects they already collected during an episode. Following (Barreto et al., 2017), the position
is encoded using a radial basis function approach. This upscales the agentâ€™s (x, y) position to a
high-dimensional vectordifferent functions such as the spos âˆˆ ÏˆR[100] or Î¾providing a better signal for the function approximation of the-function. The vector spos is composed of the activation of
two-dimensional Gaussian functions based on the agents position (x, y):


_spos = exp_
_âˆ’_ [(][x][ âˆ’] _[c][j,][1][)][2][ + (]Ïƒ_ _[y][ âˆ’]_ _[c][j,][2][)][2]_



(34)


whereover the area of the environment. The state also encodes the memory about the objects that the agent cj âˆˆ R[2] is the center of the j[th] Gaussian. The centers are laid out on a regular 10 Ã— 10 grid
has already collected using a binary vectorobject has been taken (smem,j = 1) or not (s smemmem,j âˆˆ{ = 00). An additional constant term was added to, 1}[12]. The j[th] dimension encodes if the j[th]
the state to aid the function approximation. As a result, the state received by the agents is a column
vector with s = [s[âŠ¤]pos[, s][âŠ¤]mem[,][ 1]][âŠ¤] _[âˆˆ]_ [R][113][.]

The features Ï†(st, at, st+1) 0, 1 in the environment describe the type of object that was collected
_âˆˆ{_ _}[5]_
by an agent during a step or if it reached the goal position. The first four feature dimensions encode
binary the properties of a collected object and the last dimension if the goal area was reached. In total
_|Î¦| = 6 possible features exists: Ï†1 = [0, 0, 0, 0, 0][âŠ¤]- standard observation, Ï†2 = [1, 0, 1, 0, 0][âŠ¤]-_
collected an orange box, Ï†3 = [1, 0, 0, 1, 0][âŠ¤]- collected an orange triangle, Ï†4 = [0, 1, 1, 0, 0][âŠ¤]collected a blue box, Ï†5 = [0, 1, 0, 1, 0][âŠ¤]- collected a blue triangle, and Ï†6 = [0, 0, 0, 0, 1][âŠ¤]- reached
the goal area.

Two types of tasks were evaluated in this environment that have either 1) linear or 2) general reward
functions. 300 tasks, i.e. reward functions, were sampled for each type. For linear tasks, the rewards
_r = Ï†[âŠ¤]wi are defined by a linear combination of discrete features Ï† âˆˆ_ N[5] and a weight vector
**wobjects having specific properties, e.g. being blue or being a box. The weights for each of the fouri âˆˆ** R[5]. The first four dimensions in wi define the reward that the agent receives for collecting
dimensions are randomly sampled from a uniform distribution:task. The final weight defines the reward for reaching the goal state which is wkâˆˆ[1,2,3,4] âˆ¼U w(5âˆ’ = 11, 1) for each for each
task. For training agents in general reward tasks, general reward functions Ri for each task Mi were
sampled. These reward functions define for each of the four features (Ï†2, . . ., Ï†5) that represent the
collection of a specific object type an individual reward. Their rewards were sampled from a uniform


-----

distribution: Ri(Ï†kâˆˆ{2,...,5}) âˆ¼U(âˆ’1, 1). The reward for collecting no object is Ri(Ï†1) = 0 and for
reaching the goal area is Ri(Ï†6) = 1 for all tasks. Reward functions of this form can not be linearly
decomposed in features and a weight vector.

C.2 ALGORITHMS

This section introduces the details of each evaluated algorithm. First the common elements are
discussed before introducing their specific implementations.

All agents experience the tasks M âˆˆM of an environment sequentially. They are informed when
a new task starts. All algorithms receive the features Ï†(s, a, s[â€²]) of the environment. For the action
selection and exploration, all agents use a Ïµ-greedy strategy. With probability Ïµ âˆˆ [0, 1] the agent
performs a random action. Otherwise it selects the action that maximizes the expected return.

As the state space (s âˆˆ R[113]) of the environments is high-dimensional and continuous, all agents use
an approximation of their respective functions such as for the Q-function (Q[Ëœ](s, a) â‰ˆ _Q(s, a)) or the_
_Î¾-function (Î¾[Ëœ](s, a, Ï†) â‰ˆ_ _Î¾(s, a, Ï†)). We describe the general function approximation procedure on_
the example of Î¾-functions. If not otherwise mentioned, all functions are approximated by a single
linear mapping from the states to the function values. The parameters Î¸[Î¾] _âˆˆ_ R[113][Ã—|A|Ã—|][Î¦][|] of the
mapping have independent components Î¸a,Ï†[Î¾]

_[âˆˆ]_ [R][113][ for each action][ a][ âˆˆA][ and feature][ Ï†][ âˆˆ] [Î¦][:]

_Î¾Ëœ(s, a, Ï†; Î¸[Î¾]) = s[âŠ¤]Î¸a,Ï†[Î¾]_ (35)

To learn _Î¾[Ëœ] we update the parameters Î¸[Î¾]_ using stochastic gradient descent following the gradients
_Î¸Î¾_ _Î¾(Î¸[Î¾]) of the loss based on the Î¾-learning update (13):_
_âˆ‡_ _L_

2[]
_âˆ€Ï† âˆˆ_ Î¦ : LÎ¾(Î¸[Î¾]) = E _p(Ï†t = Ï†|st, at) + Î³Î¾[Ëœ](st+1, Â¯at+1, Ï†; Î¸[Â¯][Î¾]) âˆ’_ _Î¾[Ëœ](st, at, Ï†; Î¸[Î¾])_
  (36)

with Â¯at+1 = argmax _R(Ï†)Î¾[Ëœ](st+1, a, Ï†; Î¸[Â¯][Î¾]),_
_a_

_Ï†XâˆˆÎ¦_

where _Î¸[Â¯][Î¾]_ = Î¸[Î¾] but _Î¸[Â¯][Î¾]_ is treated as a constant for the purpose of calculating the gradients _Î¸Î¾_ _Î¾(Î¸[Î¾])._
_âˆ‡_ _L_
We used PyTorch[2] for the computation of gradients and its stochastic gradient decent procedure
(SGD) for updating the parameters.

C.2.1 QL

The Q-learning (QL) agent (Algorithm 1) represents standard Q-learning (Watkins and Dayan, 1992).
The Q-function is approximated and updated using the following loss after each observed transition:

2[)]

_Q(Î¸[Q]) = E_ _r(st, at, st+1) + Î³ max_ _QËœ(st+1, at+1; Î¸[Â¯][Q])_ _Q(st, at; Î¸[Q])_ (37)
_L_ _at+1_ _âˆ’_ [Ëœ]

( 

where _Î¸[Â¯][Q]_ = Î¸[Q] but _Î¸[Â¯][Q]_ is treated as a constant for the purpose of optimization, i.e no gradients flow
through it. Following (Barreto et al., 2017) the parameters Î¸[Q] are reinitialized for each new task.

C.2.2 SFQL

The classical successor feature algorithm (SFQL) is based on a linear decomposition of rewards in
features and reward weights (Barreto et al., 2017) (Algorithm 2). If the agent is learning the reward
weights Ëœwi for a task Mi then they are randomly initialized at the beginning of a task. For the case
of general reward functions and where the reward weights are given to the agents, the weights are
learned to approximate a linear reward function before the task. See Section C.3 for a description of
the training procedure. After each transition the weights are updated by minimizing the error between
the predicted rewards Ï†(st, at, st+1)[âŠ¤]wËœi and the observed reward rt:

2[o]
**wi** ( Ëœwi) = E _r(s, a, s[â€²])_ _Ï†(st, at, st+1)[âŠ¤]wËœ_ _i_ _._ (38)
_L_ _âˆ’_

2 n  
[PyTorch v1.4: https://pytorch.org](https://pytorch.org)


-----

**Algorithm 1: Q-learning (QL)**
**Input : exploration rate: Ïµ**
learning rate for the Q-function: Î±

**for i â†** 1 to num_tasks do

initialize _Q[Ëœ]: Î¸[Q]_ _â†_ small random initial values
new_episode â† true
**for t â†** 1 to num_steps do

**if new_episode then**

new_episode â† false
_st_ initial state
_â†_

With probabilityTake action at and observe reward Ïµ select a random action rt and next state at, otherwise st+1 at â† argmaxa _Q[Ëœ](st, a)_
**if st+1 is a terminal state then**

new_episode â† true
_Î³t_ 0
_â†_

**else**
_Î³t_ _Î³_
_â†_

_y â†_ _rt + Î³t maxat+1_ _Q[Ëœ](st+1, at+1)_
Update Î¸[Q] using SGD(Î±) with LQ = (y âˆ’ _Q[Ëœ](st, at))[2]_
_st_ _st+1_
_â†_


SFQL learns an approximated _Ïˆ[Ëœ]i-function for each task Mi. The parameters of the_ _Ïˆ[Ëœ]-function for_
the first task Î¸1[Ïˆ] [are randomly initialized. For consecutive tasks, they are initialized by copying them]
from the previous task (Î¸i[Ïˆ] _i_ 1[). The][ Ëœ]Ïˆi-function of the current task Mi is updated after each
observed transition with the loss based on (5):[â†] _[Î¸][Ïˆ]âˆ’_

2[]
_LÏˆ(Î¸i[Ïˆ][) = E]_ _Ï†(st, at, st+1) + Î³Ïˆ[Ëœ]i(st+1, Â¯at+1; Î¸[Â¯]i[Ïˆ][)][ âˆ’]_ _Ïˆ[Ëœ]i(st, at; Î¸i[Ïˆ][)]_

(39)

 

with Â¯at+1 = argmax max _ÏˆËœk(st+1, a; Î¸[Â¯]k[Ïˆ][)][âŠ¤]w[Ëœ]_ _i,_
_a_ _kâˆˆ{1,2,...,i}_

where _Î¸[Â¯]i[Ïˆ]_ [=][ Î¸]i[Ïˆ] [but][ Â¯]Î¸i[Ïˆ] [is treated as a constant for the purpose of optimization, i.e no gradients flow]
through it. Besides the current _Ïˆ[Ëœ]i-function, SFQL also updates the_ _Ïˆ[Ëœ]c-function which provided_
the GPI optimal action for the current transition: c = argmaxk 1,2,...,i maxb _Ïˆ[Ëœ]k(s, b)[âŠ¤]wËœ_ _i. The_
_âˆˆ{_ _}_
update uses the same loss as for the update of the active _Ïˆ[Ëœ]i-function (39), but instead of using the_
GPI optimal action as next action, it uses the optimal action according to its own policy: Â¯at+1 =
argmaxa _Ïˆ[Ëœ]c(st+1, a)[âŠ¤]wËœ_ _c_

C.2.3 _Î¾-LEARNING_

The Î¾-learning agents (Algorithms 3, 4) allow to reevaluate policies in tasks with general reward
functions. If the reward function is not given, an approximation _R[Ëœ]i of the reward function for each_
task Mi is learned. The parameters for the approximation are randomly initialized at the beginning of
each task. After each observed transition the approximation is updated according to the following
loss:

2[]
_LR(Î¸i[R][) = E]_ _r(st, at, st+1) âˆ’_ _R[Ëœ]i(Ï†(st, at, st+1); Î¸i[R][)]_ (40)
 

In the case of tasks with linear reward functions the reward approximation becomes
_RËœi(Ï†(st, at, st+1); Î¸i[R][) =][ Ï†][(][s][t][, a][t][, s][t][+1][)][âŠ¤][Î¸]i[R][. Thus with][ Î¸]i[R]_ = Ëœwi we recover the same procedure as for SFQL (38). For non-linear, general reward functions we represented _R[Ëœ] with a neural_
network. The input of the network is the feature Ï†(st, at, st+1). The network has one hidden layer
with 10 neurons having ReLu activations. The output is a linear mapping to the scalar reward rt âˆˆ R.

All Î¾-learning agents learn an approximation of the _Î¾[Ëœ]i-function for each task Mi. Analogous to_
SFQL, the parameters of the _Î¾[Ëœ]-function for the first task Î¸1[Î¾]_ [are randomly initialized. For con-]


-----

**Algorithm 2: Classical SF Q-learning (SFQL) (Barreto et al., 2017)**
**Input : exploration rate: Ïµ**
learning rate for Ïˆ-functions: Î±
learning rate for reward weights w: Î±w
features Ï†
optional: reward weights for tasks: { Ëœw1, Ëœw2, . . ., Ëœwnum_tasks}

**for i â†** 1 to num_tasks do

**if Ëœwi not provided then Ëœwi â†** small random initial values
**ifnew_episode i = 1 then initialize â†** true _Ïˆ[Ëœ]i: Î¸i[Ïˆ]_ _[â†]_ [small random initial values][ else][ Î¸]i[Ïˆ] _[â†]_ _[Î¸]i[Ïˆ]âˆ’1_
**for t â†** 1 to num_steps do

**if new_episode then**

new_episode â† false
_st_ initial state
_â†_

_c_ argmaxk 1,2,...,i maxa _Ïˆ[Ëœ]k(st, a)[âŠ¤]wËœ_ _i_ // GPI optimal policy
_â†_ _âˆˆ{_ _}_
With probabilityTake action at and observe reward Ïµ select a random action rt and next state at, otherwise st+1 at â† argmaxa _Ïˆ[Ëœ]c(st, a)[âŠ¤]wËœ_ _i_
**ifUpdate st+1 is a terminal state Ëœwi using SGD(Î±w then) with Lw = (rt âˆ’** _Ï†(st, at, st+1)[âŠ¤]wËœ_ _i)[2]_

new_episode â† true
_Î³t_ 0
_â†_

**else**
_Î³t_ _Î³_
_â†_

// GPI optimal next action for task i
_aÂ¯t+1_ argmaxa argk 1,2,...,i _Ïˆ[Ëœ]k(st+1, a)[âŠ¤]wËœ_ _i_
_â†_ _âˆˆ{_ _}_
_y â†_ _Ï†(st, at, st+1) + Î³tÏˆ[Ëœ]i(st+1, Â¯at+1)_
Update Î¸i[Ïˆ] [using SGD(][Î±][) with][ L][Ïˆ][ = (][y][ âˆ’] _Ïˆ[Ëœ]i(st, at))[2]_
**if c Ì¸= i then**

_aÂ¯ct+1 â†_ argmaxa _Ïˆ[Ëœ]c(st+1, a)[âŠ¤]wËœ_ _c_ // optimal next action for task
_y â†_ _Ï†(st, at, st+1) + Î³tÏˆ[Ëœ]c(st+1, Â¯at+1)_
Update Î¸c[Ïˆ] [using SGD(][Î±][) with][ L][Ïˆ] [= (][y][ âˆ’] _Ïˆ[Ëœ]c(st, at))[2]_

_st_ _st+1_
_â†_

secutive tasks, they are initialized by copying them from the previous task (Î¸i[Î¾] _i_ 1[). The]
_Î¾Ëœi-function of the current task Mi is updated after each observed transition with the loss given[â†]_ _[Î¸][Î¾]âˆ’_
in (46). The Î¾-learning agents differ in their setting for p(Ï†t = Ï†|st, at) in the updates which
is described in the upcoming sections. Besides the current _Î¾[Ëœ]i-function, the Î¾-learning agents_
also update the _Î¾[Ëœ]c-function which provided the GPI optimal action for the current transition:_
_c = argmaxk_ 1,2,...,i maxat _Ï†_ Î¦ _Î¾[Ëœ]k(st, at, Ï†) R[Ëœ]i(Ï†). The update uses the same loss as for_
_âˆˆ{_ _}_ _âˆˆ_

the update of the active _Î¾[Ëœ]i-function (46), but instead of using the GPI optimal action as next action, it_

P

uses the optimal action according to its own policy: Â¯at+1 = maxa _Ï†_ Î¦ _Î¾[Ëœ]c(st+1, a, Ï†) R[Ëœ]c(Ï†)._

_âˆˆ_

P


**MF Î¾-learning:** The model-free Î¾-learning agent (Algorithm 3) uses a stochastic update for the _Î¾[Ëœ]-_
functions. Given a transition, we set p(Ï†t = Ï†|st, at) â‰¡ 1 for the observed feature Ï† = Ï†(st, at, st+1)
and p(Ï†t = Ï†|st, at) â‰¡ 0 for all other features Ï† Ì¸= Ï†(st, at, st+1).

**MB Î¾-learning:** The one-step SF model-based Î¾-learning agent (Algorithm 4) uses an approximated
model Ëœp to predict p(Ï†t = Ï†|st, at) to reduce the variance of the Î¾-function update. The model is by


-----

a linear mapping for each action. It uses a softmax activation to produce a valid distribution over Î¦:

exp(s[âŠ¤]Î¸a,Ï†[p] [)]
_pËœ(s, a, Ï†; Î¸[p]) =_ (41)

_Ï†[â€²]_ Î¦ [exp(][s][âŠ¤][Î¸]a,Ï†[p] _[â€²]_ [)]
_âˆˆ_

where Î¸a,Ï†[p] _p is valid for each task inP_, its weights Î¸[p] are only randomly initialized at
the beginning of the first task. For each observed transition, the model is updated using the following[âˆˆ] [R][113][. As][ Ëœ] _M_
loss:
_Ï†_ Î¦ : _p(Î¸i[p][) = E]_ (p(Ï†t = Ï† _st, at)_ _pËœ(st, at, Ï†; Î¸i[p][))][2][o]_ _,_ (42)
_âˆ€_ _âˆˆ_ _L_ _|_ _âˆ’_
n

where we set p(Ï†t = Ï†|st, at) â‰¡ 1 for the observed feature Ï† = Ï†(st, at, st+1) and p(Ï†t =
_Ï†_ _st, at)_ 0 for all other features Ï† = Ï†(st, at, st+1).
_|_ _â‰¡_ _Ì¸_

C.3 EXPERIMENTAL PROCEDURE

All agents were evaluated in both task types (linear or general reward function) on 300 tasks. The
agents experienced the tasks sequentially, each for 20.000 steps. The agents had knowledge when
a task change happened. Each agent was evaluated for 10 repetitions to measure their average
performance. Each repetition used a different random seed that impacted the following elements: a)
the sampling of the tasks, b) the random initialization of function approximator parameters, c) the
stochastic behavior of the environments when taking steps, and d) the Ïµ-greedy action selection of
the agents. The tasks, i.e. the reward functions, were different between the repetitions of a particular
agent, but identical to the same repetition of a different agent. Thus, all algorithms were evaluated
over the same tasks.

The SF agents (SFQL, Î¾-learning) were evaluated under two conditions. First, that they have to learn
the reward weights or the reward function online during the training (indicated by (O) in figures).
Second, the reward weights or the reward function is given to them. As the SFQL does not support
general reward functions, it is not possible to provide the SFQL agent with the reward function in
the second condition. As a solution, before the agent was trained on a new task _i, a linear model_
_M_
of the reward Ri(Ï†) = Ï†[âŠ¤]wËœi was fitted. The initial approximation Ëœwi was randomly initialized and
then fitted for 10.000 iterations using a gradient descent procedure based on the absolute mean error
(L1 norm):

âˆ†Ëœwi = Î· [1] _Ri(Ï†)_ _Ï†[âŠ¤]wËœi,_ (43)

Î¦ _âˆ’_
_|_ _|_ _Ï†XâˆˆÎ¦_

with a learning rate of Î· = 1.0 that yielded the best results tested over several learning rates.

**Hyperparameters:** The hyperparameters of the algorithms were set to the same values as in
(Barreto et al., 2017). A grid search over the learning rates of all algorithms was performed. Each
learning rate was evaluated for three different settings which are listed in Table 1. If algorithms had
several learning rates, then all possible combinations were evaluated. This resulted in a different
number of evaluations per algorithm and condition: QL - 3, SFQL (O) - 9, MF Xi (O) - 9, MB Xi
(O) - 27, SFQL - 3, MF Xi - 3, MB Xi - 9. In total, 63 parameter combinations were evaluated. The
reported performances in the figures are for the parameter combination that resulted in the highest
cumulative total reward averaged over all 10 repetitions in the respective environment. Please note,
the learning rates Î± and Î±w are set to half of the rates defined in (Barreto et al., 2017). This is
necessary due to the differences in calculating the loss and the gradients in the current paper. We use
mean squared error loss formulations, whereas (Barreto et al., 2017) uses absolute error losses. The
probability for random actions of the Ïµ-Greedy action selection was set to Ïµ = 0.15 and the discount
rate to Î³ = 0.95. The initial weights Î¸ for the function approximators were randomly sampled from a
standard distribution with Î¸init âˆ¼N (Âµ = 0, Ïƒ = 0.01).

**Computational Resources:** Experiments were conducted on a cluster with a variety of node types
(Xeon SKL Gold 6130 with 2.10GHz, Xeon SKL Gold 5218 with 2.30GHz, Xeon SKL Gold 6126
with 2.60GHz, Xeon SKL Gold 6244 with 3.60GHz, each with 192 GB Ram, no GPU). The time
for evaluating one repetition of a certain parameter combination over the 300 tasks depended on the
algorithm and the task type. Linear reward function tasks: QL â‰ˆ 1h, SFQL (O) â‰ˆ 4h, MF Î¾ (O)
_â‰ˆ_ 42h, MB Î¾ (O) â‰ˆ 43h, SFQL â‰ˆ 4h, MF Î¾ â‰ˆ 15h, and MB Î¾ â‰ˆ 16h. General reward function


-----

Table 1: Evaluated Learning Rates in the Object Collection Environment

Parameter Description Values

_Î±_ Learning rate of the Q, Ïˆ, and Î¾-function _{0.0025, 0.005, 0.025}_
_Î±w, Î±R_ Learning rate of the reward weights or the reward model 0.025, 0.05, 0.075
_{_ _}_
_Î²_ Learning rate of the One-Step SF Model _{0.2, 0.4, 0.6}_

tasks: QL â‰ˆ 1h, SFQL (O) â‰ˆ 4h, MF Î¾ (O) â‰ˆ 68h, MB Î¾ (O) â‰ˆ 67h, SFQL â‰ˆ 5h, MF Î¾ â‰ˆ 14h,
and MB Î¾ â‰ˆ 18h. Please note, the reported times do not represent well the computational complexity
of the algorithms, as the algorithms were not optimized for speed, and some use different software
packages (numpy or pytorch) for their individual computations.

C.4 EFFECT OF INCREASING NON-LINEARITY IN GENERAL REWARD TASK

We further studied the effect of general reward functions on the performance of classical SF compared
to Î¾-learning (Fig. 1 - c). We evaluated the agents in tasks with different levels of difficulty in relation
to how well their reward functions can be approximated by a linear model. Seven difficulty levels
have been evaluated. For each level, the agents were trained sequentially on 300 tasks as for the
experiments with general reward functions. The reward functions for each level were sampled with
the following procedure. Several general reward functions were randomly sampled as previously
described. For each reward function, a linear model of a reward weight vector Ëœw was fitted using
the same gradient descent procedure as in Eq. 43. The final average absolute model error after
10.000 iterations was measured. Each of the seven difficulty levels defines a range of model errors its
tasks have with the following increasing ranges: {[0.0, 0.25], [0.25, 0.5], . . ., [1.5, 1.75]}. For each
difficulty level, 300 reward functions were selected that yield a linear model are in the respective
range of the level.

Q-Learning, SFQL, and MF Xi-learning were each trained on 300 tasks, i.e. reward functions, on
each difficulty level. As hyperparameters were the best performing parameters from the previous
general reward task experiments used. We measured the ratio between the total return over 300 tasks
of QL and MF Xi-learning (QL/MF Xi), and SFQL and MF Xi-learning (SFQL/MF Xi). Fig. 1 c shows the results, using as x-axis the mean average absolute model error defined by the bracket
of each difficulty level. The results show that the relative performance of SFQL compared to MF
Xi reduces with higher non-linearity of the reward functions. For reward functions that are nearly
linear (mean error of 0.125), both have a similar performance. Whereas, for reward functions that are
difficult to model with a linear relation (mean error of 1.625) SFQL reaches only less than 50% of
the performance of MF Xi-learning.


-----

**Algorithm 3: Model-free Î¾-learning (MF Î¾)**
**Input : exploration rate: Ïµ**
learning rate for Î¾-functions: Î±
learning rate for reward models R: Î±R
features Ï†
optional: reward functions for tasks: {R[Ëœ]1, _R[Ëœ]2, . . .,_ _R[Ëœ]num_tasks}_

**for i â†** 1 to num_tasks do

**if** _R[Ëœ]i not provided then initialize_ _R[Ëœ]i: Î¸i[R]_

_[â†]_ [small random initial values]
new_episodeif i = 1 then initialize â† true _Î¾[Ëœ]i: Î¸i[Î¾]_ _[â†]_ [small random initial values][ else][ Î¸]i[Î¾] _[â†]_ _[Î¸]i[Î¾]âˆ’1_
**for t â†** 1 to num_steps do

**if new_episode then**

new_episode â† false
_st_ initial state
_â†_

_c_ argmaxk 1,2,...,i maxa _Ï†_ _Î¾[Ëœ]k(st, a, Ï†) R[Ëœ]i(Ï†)_ // GPI optimal policy
_â†_ _âˆˆ{_ _}_
With probability Ïµ select a random action at, otherwise
P
_at â†_ argmaxa _Ï†_ _Î¾[Ëœ]c(st, a, Ï†) R[Ëœ]i(Ï†)_

Take action at and observe reward rt and next state st+1

P

**if** _R[Ëœ]i not provided then_

Update Î¸i[R] [using SGD(][Î±][R][) with][ L][R][ = (][r][t][ âˆ’] _R[Ëœ]i(Ï†(st, at, st+1))[2]_

**if st+1 is a terminal state then**

new_episode â† true
_Î³t_ 0
_â†_

**else**
_Î³t_ _Î³_
_â†_

// GPI optimal next action for task i
_aÂ¯t+1_ argmaxa argk 1,2,...,i _Ï†_ _Î¾[Ëœ]k(st+1, a, Ï†) R[Ëœ]i(Ï†)_
_â†_ _âˆˆ{_ _}_
**foreach Ï†** Î¦ do
_âˆˆ_ P

**if Ï† = Ï†(st, at, st+1) then yÏ† â†** 1 + Î³tÎ¾[Ëœ]i(st+1, Â¯at+1, Ï†)
**else yÏ† â†** _Î³tÎ¾[Ëœ]i(st+1, Â¯at+1, Ï†)_

Update Î¸i[Î¾] [using SGD(][Î±][) with][ L][Î¾][ =][ P]Ï†[(][y][Ï†][ âˆ’] _Î¾[Ëœ]i(st, at, Ï†))[2]_

**if c Ì¸= i then**

// optimal next action for task c
_aÂ¯t+1 â†_ argmaxa _Ï†_ _Î¾[Ëœ]c(st+1, a, Ï†) R[Ëœ]c(Ï†)_

**foreach Ï†** Î¦ do
_âˆˆ_ P

**if Ï† = Ï†(st, at, st+1) then yÏ† â†** 1 + Î³tÎ¾[Ëœ]c(st+1, Â¯at+1, Ï†)
**else yÏ† â†** _Î³tÎ¾[Ëœ]c(st+1, Â¯at+1, Ï†)_

Update Î¸c[Î¾] [using SGD(][Î±][) with][ L][Î¾] [=][ P]Ï†[(][y][Ï†][ âˆ’] _Î¾[Ëœ]c(st, at, Ï†))[2]_

_st_ _st+1_
_â†_


-----

**Algorithm 4: One Step SF-Model Î¾-learning (MB Î¾)**
**Input : exploration rate: Ïµ**
learning rate for Î¾-functions: Î±
learning rate for reward models R: Î±R
learning rate for the one-step SF model Ëœp: Î²
features Ï†
optional: reward functions for tasks: {R[Ëœ]1, _R[Ëœ]2, . . .,_ _R[Ëœ]num_tasks}_

initialize Ëœp: Î¸[p] _â†_ small random initial values
**for i â†** 1 to num_tasks do

**if** _R[Ëœ]i not provided then initialize_ _R[Ëœ]i: Î¸i[R]_

_[â†]_ [small random initial values]
new_episodeif i = 1 then initialize â† true _Î¾[Ëœ]i: Î¸i[Î¾]_ _[â†]_ [small random initial values][ else][ Î¸]i[Î¾] _[â†]_ _[Î¸]i[Î¾]âˆ’1_
**for t â†** 1 to num_steps do

**if new_episode then**

new_episode â† false
_st_ initial state
_â†_

_c_ argmaxk 1,2,...,i maxa _Ï†_ _Î¾[Ëœ]k(st, a, Ï†) R[Ëœ]i(Ï†)_ // GPI optimal policy
_â†_ _âˆˆ{_ _}_

With probability Ïµ select a random action at, otherwise

P

_at â†_ argmaxa _Ï†_ _Î¾[Ëœ]c(st, a, Ï†) R[Ëœ]i(Ï†)_

Take action at and observe reward rt and next state st+1

P

**if** _R[Ëœ]i not provided then_

Update Î¸i[R] [using SGD(][Î±][R][) with][ L][R][ = (][r][t][ âˆ’] _R[Ëœ]i(Ï†(st, at, st+1))[2]_

**foreach Ï† âˆˆ** Î¦ do

**if Ï† = Ï†(st, at, st+1) then yÏ† â†** 1 else yÏ† â† 0

Update Î¸[p] using SGD(Î²) with Lp = _Ï†[(][y][Ï†][ âˆ’]_ _p[Ëœ](st, at, Ï†))[2]_

**if st+1 is a terminal state then**

new_episode true

[P]

_â†_
_Î³t_ 0
_â†_

**else**
_Î³t_ _Î³_
_â†_

// GPI optimal next action for task i
_aÂ¯t+1_ argmaxa argk 1,2,...,i _Ï†_ _Î¾[Ëœ]k(st+1, a, Ï†) R[Ëœ]i(Ï†)_
_â†_ _âˆˆ{_ _}_

**foreach Ï†** Î¦ do
_âˆˆ_ P

_yÏ† â†_ _pËœ(st, at, Ï†) + Î³tÎ¾[Ëœ]i(st+1, Â¯at+1, Ï†)_

Update Î¸i[Î¾] [using SGD(][Î±][) with][ L][Î¾][ =][ P]Ï†[(][y][Ï†][ âˆ’] _Î¾[Ëœ]i(st, at, Ï†))[2]_

**if c Ì¸= i then**

// optimal next action for task c
_aÂ¯t+1 â†_ argmaxa _Ï†_ _Î¾[Ëœ]c(st+1, a, Ï†) R[Ëœ]c(Ï†)_

**foreach Ï†** Î¦ do
_âˆˆ_ P

_yÏ† â†_ _pËœ(st, at, Ï†) + Î³tÎ¾[Ëœ]c(st+1, Â¯at+1, Ï†)_

Update Î¸c[Î¾] [using SGD(][Î±][) with][ L][Î¾] [=][ P]Ï†[(][y][Ï†][ âˆ’] _Î¾[Ëœ]c(st, at, Ï†))[2]_

_st_ _st+1_
_â†_


-----

D EXPERIMENTAL DETAILS: RACER ENVIRONMENT

This section extends the brief introduction to the racer environment (Fig. 2 - a) given in Section 4.2.

D.1 ENVIRONMENT

The environment is a continuous two-dimensional area in which the agent drives similar to a car. The
position of the agent is a point in the 2D space: p = (x, y) âˆˆ [0, 1][2]. Moreover, the agent has an
orientation which it faces: Î¸ âˆˆ [âˆ’Ï€, Ï€1]. The action space of the agent consists of three movement
directions: A = {right, straight, left }. Each action changes the position of the agent depending on
its current position and orientation. The action straight changes the agentâ€™s position by 0.075 towards
its orientation Î¸. The action right changes the orientation of the agent to Î¸ + 7[1] _[Ï€][ and its position]_

0.06 towards this new direction, whereas left the direction to Î¸ âˆ’ [1]7 _[Ï€][ changes. The environment is]_

stochastic by adding Gaussian noise with Ïƒ = 0.005 to the final position x, y, and orientation Î¸. If
the agent drives outside the area (x, y) âˆˆ [0, 1][2], then it reappears on the other opposite side. The
environment resembles therefore a torus (or donut). As a consequence, distances d(px, py) are also
measure in this space, so that the positions px = (0.1, 0.5) and py = (0.9, 0.5) have not a distance
of 0.8 but d(px, py) = 0.2. The environment has 3 markers at the positions m1 = (0.25, 0.75),
_m2 = (0.75, 0.25), and m3 = (0.75, 0.6). The features measure the distance of the agent to each_
marker:the beginning of an episode the agent is randomly placed and oriented in environment. An episode Ï† âˆˆ R[3] with Ï†k = d(p, mk). Each feature dimensions is normalized to be Ï†k âˆˆ [0, 1]. At
ends after 200 time steps.

The state space of the agents is similarly constructed as for the object collection environment. The
agentâ€™s position is encoded with adifference, that the distances are measure according to the torus shape. A similar radial basis function 10 Ã— 10 radial basis functions spos âˆˆ R[100] as defined in 34. In
approach is also used to encode the orientationgaussian centers in [âˆ’Ï€, Ï€] and Ïƒ = [1]5 _[Ï€][. Please note,] sori âˆˆ_ R[ Ï€][20][ and]of the agent using[ âˆ’][Ï€][ are also connected in this space,] 20 equally distributed

i.e. d(Ï€, âˆ’Ï€) = 0. The combination of the position and orientation of the agent is the final state:
_s = [s[âŠ¤]pos[, s][âŠ¤]ori[]][âŠ¤]_ _[âˆˆ]_ [R][120][.]

The reward functions define preferred positions in the environment based on the features, i.e. the
distance of the agent to the markers. A preference function rk exists for each distance. The functions
are composed of a maximization over m Gaussian components that evaluate the agents distance:


_m_

_._ (44)
_j=1_




3

_rk(Ï†k) with ri = [1]_ exp

3 [max] _âˆ’_ [(][Ï†][k][ âˆ’]Ïƒj[Âµ][j][)][2]

_k=1_  

X


_R(Ï†) =_


Reward functions are randomly generated by sampling the number of Gaussian components m
to be 1 or 2. The properties of each component are sampled according to Âµj (0.0, 0.7), and
_Ïƒj_ (0.001, 0.01). Fig. 2 - a illustrates one such randomly sampled reward function where dark âˆ¼U
areas represent locations with high rewards. âˆ¼U

D.2 ALGORITHMS

We evaluated Q-learning, SFQL (O), SFQL, and MF Î¾-learning (see Section C.2 for their full
description) in the racer environment. In difference to their implementation for the object collection
environment, they used a different neural network architecture to approximate their respective value
functions.

D.2.1 QL

Q-learning uses a fully connected feedforward network with bias and a ReLU activation for hidden
layers. It has 2 hidden layers with 20 neurons each.

D.2.2 SFQL

Q-learning uses a feedforward network with bias and a ReLU activation for hidden layers. It has for
each of the three feature dimensions a separate fully connected subnetwork. Each subnetwork has 2
hidden layers with 20 neurons each.


-----

D.2.3 CONTINUOUS MODEL-FREE Î¾-LEARNING

The racer environment has continuous features Ï† âˆˆ R[3]. Therefore, the MF Î¾-learning procedure
(Alg. 3) can not be directly applied as it is designed for discrete feature spaces. We introduce here a
MF Î¾-learning procedure for continuous feature spaces (CMF Î¾-learning). It is a feature dimension
independent, and discretized version of Î¾-learning. As the reward functions (44) are a sum over the
individual feature dimensions, the Q-value can be computed as:


_Q[Ï€](s, a) =_


_R(Ï†)Î¾[Ï€](s, a, Ï†)dÏ† =_


_rk(Ï†k)Î¾k[Ï€][(][s, a, Ï†][k][)][d][Ï†][k]_ _[,]_ (45)
Î¦k


where Î¦k is the feature space for each feature dimension which is Î¦k = [0, 1] in the racer environment.
_Î¾k[Ï€]_ [is a][ Î¾][-function for the feature dimension][ k][. (45) shows that the][ Î¾][-function can be independently]
represented over each individual feature dimension Ï†k, instead of over the full features Ï†. This
reduces the complexity of the approximation.

Moreover, we introduce a discretization of the Î¾-function that discretizes the space of each feature
dimension k in U = 11 bins with the centers:

_Xk =_ _Ï†[min]k_ + jâˆ†Ï†k : 0 < j < U _, with âˆ†Ï†k :=_ _[Ï†]k[max]U âˆ’âˆ’_ _Ï†1[min]k_ _,_


where âˆ†Ï†k is the distance between the centers, and Ï†[min]k = 0.0 is the lowest center, and Ï†[max]k = 1.0
the largest center. Given this discretization and the decomposition of the Q-function according to
(45), the Q-values can be computed by:


_Q[Ï€](s, a) =_


_R(x)Î¾[Ï€](s, a, x) ._
_xXâˆˆXk_


Alg. 5 lists the complete CMF Î¾-learning procedure with the update steps for the Î¾-functions. Similar
to the SFQL agent, the CMF Xi uses a feedforward network with bias and a ReLU activation for
hidden layers. It has for each of the three feature dimensions a separate fully connected subnetwork.
Each subnetwork has 2 hidden layers with 20 neurons each. The discretized outputs per feature
dimension share the last hidden layer per subnetwork.

The Î¾-function is updated according to the following procedure. Instead of providing a discrete
learning signal to the model, we encode the observed feature using continuous activation functions
around each bin center. Given the jâ€™th bin center of dimension k, xk,j, its value is encoded to be 1.0
if the feature value of this dimension aligns with the center (Ï†k = xk,j). Otherwise, the encoding for
the bin decreases linearily based on the distance between the bin center and the value ( _xk,j_ _Ï†k_ ) and
reaches 0 if the value is equal to a neighboring bin center, i.e. has a distance âˆ†Ï†k| . We represent âˆ’ _|_
_â‰¥_
this encoding for each feature dimension k by uk (0, 1)[U] with:
_âˆˆ_

_k_ 1,2,3 : 0<j<U : uk,j = max 0, [(1][ âˆ’|][x][k,j][ âˆ’] _[Ï†][k][|][)]_ _._
_âˆ€_ _âˆˆ{_ _}_ _âˆ€_ âˆ†Ï†k
 

To learn _Î¾[Ëœ] we update the parameters Î¸[Î¾]_ using stochastic gradient descent following the gradients
_Î¸Î¾_ _Î¾(Î¸[Î¾]) of the loss based on the Î¾-learning update (13):_
_âˆ‡_ _L_

3

1 2

_Ï†_ Î¦ : _Î¾(Î¸[Î¾]) = E_ **uk + Î³Î¾[Ëœ]k(st+1, Â¯at+1; Î¸[Â¯][Î¾])** _Î¾k(st, at; Î¸[Î¾])_
_âˆ€_ _âˆˆ_ _L_ ( _n_ _kX=1_  _âˆ’_ [Ëœ]  ) (46)

with Â¯at+1 = argmax _R(x)Î¾[Ëœ](st+1, a, Ï†; Î¸[Â¯][Î¾]),_
_a_

Xk _xXâˆˆXk_

where n = 3 is the number of feature dimensions and _Î¾[Ëœ]k is the vector of the U discretized Î¾-values_
for dimension k.

D.3 EXPERIMENTAL PROCEDURE

All agents were evaluated on 37 tasks. The agents experienced the tasks sequentially, each for
1000 episodes (200, 000 steps per task). The agents had knowledge when a task change happened.


-----

Each agent was evaluated for 10 repetitions to measure their average performance. Each repetition
used a different random seed that impacted the following elements: a) the sampling of the tasks,
b) the random initialization of function approximator parameters, c) the stochastic behavior of the
environments when taking steps, and d) the Ïµ-greedy action selection of the agents. The tasks, i.e.
the reward functions, were different between the repetitions of a particular agent, but identical to the
same repetition of a different agent. Thus, all algorithms were evaluated over the same tasks.

SFQL was evaluated under two conditions. First, by learning the reward weights online during the
training (indicated by (O) in figures). Second, the reward weights were trained with the iterative
gradient decent method in (43). The weights were trained for 10, 000 iterations with an learning rate
of 1.0. At each iteration, 50 random points in the task were sampled and their features and rewards
are used for the training step.

**Hyperparameters** A grid search over the learning rates of all algorithms was performed. Each
learning rate was evaluated for three different settings which are listed in Table 2. If algorithms had
several learning rates, then all possible combinations were evaluated. This resulted in a different
number of evaluations per algorithm and condition: QL - 4, SFQL (O) - 12, SFQL - 4, CMF Xi 4. In
total, 24 parameter combinations were evaluated. The reported performances in the figures are for
the parameter combination that resulted in the highest cumulative total reward averaged over all 10
repetitions in the respective environment. The probability for random actions of the Ïµ-Greedy action
selection was set to Ïµ = 0.15 and the discount rate to Î³ = 0.9. The initial weights and biases Î¸ for the
function approximators were initialized according to an uniform distribution with Î¸i ( _âˆšk,_ _âˆšk),_

where k = in_features1 [.] _âˆ¼U_ _âˆ’_

Table 2: Evaluated Learning Rates in the Racer Environment

Parameter Description Values

_Î±_ Learning rate of the Q, Ïˆ, and Î¾-function _{0.0025, 0.005, 0.025, 0.5}_
_Î±w_ Learning rate of the reward weights 0.025, 0.05, 0.075
_{_ _}_

**Computational Resources:** Experiments were conducted on the same cluster as for the object
collection environment experiments. The time for evaluating one repetition of a certain parameter
combination over the 37 tasks depended on the algorithm: QL â‰ˆ 9h, SFQL (O) â‰ˆ 70h, SFQL Î¾
_â‰ˆ_ 73h, and CMF Î¾ â‰ˆ 88h. Please note, the reported times do not represent well the computational
complexity of the algorithms, as the algorithms were not optimized for speed, and some use different
software packages (numpy or pytorch) for their individual computations.


-----

**Algorithm 5: Model-free Î¾-learning for Continuous Features (CMF Î¾)**
**Input : exploration rate: Ïµ**
learning rate for Î¾-functions: Î±
learning rate for reward models R: Î±R
features Ï† âˆˆ R[n]
components of reward functions for tasks: {R1 = {r1[1][, r]2[1][, ..., r]n[1] _[}][, R][2][, . . ., R][num_tasks][}]_
discretization parameters: X, âˆ†Ï†

**for i â†** 1 to num_tasks do

**if i = 1 then**

_âˆ€kâˆˆ{1,...,n}: initialize_ _Î¾[Ëœ]k[i]_ [:][ Î¸]i,k[Î¾] _[â†]_ [small random values]

**else**

_âˆ€kâˆˆ{1,...,n}: Î¸i,k[Î¾]_ _[â†]_ _[Î¸]i[Î¾]âˆ’1,k_

new_episode â† true
**for t â†** 1 to num_steps do

**if new_episode then**

new_episode â† false
_st_ initial state

_c â†_ argmax â† _jâˆˆ{1,2,...,i} maxa_ _nk=1_ _xâˆˆXk_ _Î¾[Ëœ]k[j]_ [(][s][t][, a, x][)][r]k[i] [(][x][)] // GPI policy

With probability Ïµ select a random action at, otherwise
_n_ P P
_at_ argmaxa _k=1_ _x_ _Xk_ _Î¾[Ëœ]k[j]_ [(][s][t][, a, x][)][r]k[i] [(][x][)]
Take action â† at and observe rewardâˆˆ _rt and next state st+1_
**if st+1 is a terminal stateP** **thenP**

new_episode â† true
_Î³t_ 0
_â†_

**else**
_Î³t_ _Î³_
_â†_

// GPI optimal next action for task i
_aÂ¯t+1 â†_ argmaxa argjâˆˆ{1,2,...,i} _nk=1_ _xâˆˆXk_ _Î¾[Ëœ]k[j]_ [(][s][t][, a, x][)][r]k[i] [(][x][)]

_Ï†t_ _Ï†(st, at, st+1)_
**for â† k â†** 1 to n do P P

**foreach x âˆˆ** _Xk do_

_yk,x_ max 0, 1 âˆ†Ï† + Î³tÎ¾[Ëœ]k[i] [(][s][t][+1][,][ Â¯]at+1, x)
_â†_ _âˆ’_ _[|][x][âˆ’][Ï†][t,k][|]_
 

Update Î¸i[Î¾] [using SGD(][Î±][) with][ L][Î¾][ =][ P]k[n]=1 _x_ _Xk_ [(][y][k,x][ âˆ’] _Î¾[Ëœ]k[i]_ [(][s][t][, a][t][, x][))][2]

_âˆˆ_

**if c** = i then
_Ì¸_ P

// optimal next action for task c
_aÂ¯t+1_ argmaxa _nk=1_ _x_ _Xk_ _Î¾[Ëœ]k[c][(][s][t][, a, x][)][r]k[c]_ [(][x][)]
_â†_ _âˆˆ_

**for k** 1 to n do
_â†_ P P

**foreach x âˆˆ** _Xk do_

_yk,x_ max 0, 1 âˆ†Ï† + Î³tÎ¾[Ëœ]k[c][(][s][t][+1][,][ Â¯]at+1, x)
_â†_ _âˆ’_ _[|][x][âˆ’][Ï†][t,k][|]_
 

Update Î¸c[Î¾] [using SGD(][Î±][) with][ L][Î¾] [=][ P][n]k=1 _x_ _Xk_ [(][y][k,x][ âˆ’] _Î¾[Ëœ]k[c][(][s][t][, a][t][, x][))][2]_
_âˆˆ_

_st_ _st+1_
_â†_ P


-----

E ADDITIONAL EXPERIMENTAL RESULTS

This section reports additional results and experiments:

1. Report of the total return and the statistical significance of differences between agents for
all experiments

2. Evaluation of the agents in the original object collection task by Barreto et al. (2017)

E.1 OBJECT COLLECTION TASK BY BARRETO ET AL. (2017)

We additionally evaluated all agents in the original object collection task by Barreto et al. (2017).


**Environment:** The environment differs to the modified object

G

collection task (Section. C) only in terms of the objects and features.
The environment has 3 object types: orange, blue, and pink (Fig. 3).
The feature encode if the agent has collected one of these object
types or if it reached the goal area. The first three dimensions of
the features Ï†(st, at, st+1) 0, 1 encode which object type is
_âˆˆ{_ _}[4]_
collected. The last dimension encodes if the goal area was reached.
In total |Î¦| = 5 possible features exists: Ï†1 = [0, 0, 0, 0][âŠ¤]- standard S
observation, Ï†2 = [1, 0, 0, 0][âŠ¤]- collected an orange object, Ï†3 =

[0, 1, 0, 0][âŠ¤]- collected a blue object, Ï†4 = [0, 0, 1, 0][âŠ¤]- collected a Figure 3: Object collecpink object, and Ï†5 = [0, 0, 0, 1][âŠ¤]- reached the goal area. tion environment from (Bar
reto et al., 2017) with 3 object

The rewards r = Ï†[âŠ¤]wi are defined by a linear combination of

types: orange, blue, pink.

discrete features Ï† âˆˆ N[4] and a weight vector w âˆˆ R[4]. The first
three dimensions in w define the reward that the agent receives for
collecting one of the object types. The final weight defines the reward for reaching the goal state
which is w4 = 1 for each task. All agents were trained in on 300 randomly generated linear reward
functions with the same experimental procedure as described in Section. C. For each task the reward
weights for the 3 objects are randomly sampled from a uniform distribution: wkâˆˆ{1,2,3} âˆ¼U(âˆ’1, 1).

**Results:** The results (Fig. 4) follow closely the results from the modified object collection task
(Fig. 1 - b, and 5 - a). MF Î¾ reaches the highest performance outperforming SFQL in terms of
learning speed and asymptotic performance. It is followed by MB Î¾ and SFQL which show no
statistical significant difference between each other in their final performance. Nonetheless, MB
Îž has a higher learning speed during the initial 40 tasks. The results for the agents that learn the
reward weights online (SFQL (O), MF Î¾ (O), and MB Î¾ (O)) follow the same trend with MF Î¾ (O)
outperforming SFQL (O) slightly. Nonetheless, the Î¾-agents have a much stronger learning speed
during the initial 70 tasks compared to SFQL (O), due to the errors in the approximation of the
weight vectors, especially at the beginning of a new task. All agents can clearly outperform standard
Q-learning.

E.2 TOTAL RETURN IN TRANSFER LEARNING EXPERIMENTS AND STATISTICAL SIGNIFICANT
DIFFERENCES

Fig. 5 shows for each of the transfer learning experiments in the object collection and the racer
environment the total return that each agent accumulated over all tasks. Each dot besides the boxplot
shows the total return for each of the 10 repetitions. The box ranges from the upper to the lower
quartile. The whiskers represent the upper and lower fence. The mean and standard deviation are
indicated by the dashed line and the median by the solid line. The tables in Fig. 5 report the p-value
of pairwise Mannâ€“Whitney U test. A significant different total return can be expected if p < 0.05.

For the object collection environment (Fig.5 - a; b), Î¾-learning outperforms SFQL in both conditions,
in tasks with linear and general reward functions. However, the effect is stronger in tasks with general
reward functions where SFQL has more problems to correctly approximate the reward function
with its linear approach. For the condition, where the agents learn a reward model online (O), the
difference between the algorithms in the general reward case is not as strong due the effect of their
poor approximated reward models for all agents.


-----

In the racer environment (Fig.5 - c) SFQL has a poor performance below standard Q-learning as
it can not appropriately approximate the reward functions with a linear model. In difference CMF
_Î¾-learning outperforms QL._

(a) Environment by Barreto et al. (2017) with Linear Reward Functions


QL SFQL (O) MF Xi (O) MB Xi (O) SFQL MF Xi MB Xi


800

600

400

200

0

250k

200k

150k

100k

50k


10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300

Tasks Trained


(b) Total Return and Statistical Significance Tests

QL SFQL (O) MF Xi (O) MB Xi (O) SFQL MF Xi MB Xi


**p-value** SFQL (O) MF Xi (O) MB Xi (O) SFQL MF Xi MB Xi

QL _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_
SFQL (O) 0.038 0.121 _< 0.001_ _< 0.001_ _< 0.001_
MF Xi (O) 0.623 _< 0.001_ _< 0.001_ _< 0.001_
MB Xi (O) _< 0.001_ _< 0.001_ _< 0.001_
SFQL _< 0.001_ 0.121
MF Xi _< 0.001_

Figure 4: MF Î¾-learning outperforms SFQL in the object collection environment by Barreto et al.
(2017), both in terms of asymptotic performance and learning speed. (a) The average over 10 runs
of the average reward per task per algorithm and the standard error of the mean are depicted. (b)
Total return over the 300 tasks in each evaluated condition. The table shows the p-values of pairwise
Mannâ€“Whitney U tests between the agents.


-----

(a) Object Collection Environment with Linear Reward Functions

QL SFQL (O) MF Xi (O) MB Xi (O) SFQL MF Xi MB Xi


200k

150k

100k

50k


**p-value** SFQL (O) MF Xi (O) MB Xi (O) SFQL MF Xi MB Xi

QL _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_
SFQL (O) _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_
MF Xi (O) 0.054 _< 0.001_ _< 0.001_ _< 0.001_
MB Xi (O) _< 0.001_ _< 0.001_ _< 0.001_
SFQL _< 0.001_ _< 0.001_
MF Xi 0.162


(b) Object Collection Environment with General Reward Functions

QL SFQL (O) MF Xi (O) MB Xi (O) SFQL MF Xi MB Xi


250k

200k

150k

100k

50k


**p-value** SFQL (O) MF Xi (O) MB Xi (O) SFQL MF Xi MB Xi

QL _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_ _< 0.001_
SFQL (O) 0.045 0.009 _< 0.001_ _< 0.001_ _< 0.001_
MF Xi (O) 0.345 _< 0.001_ _< 0.001_ _< 0.001_
MB Xi (O) _< 0.001_ _< 0.001_ _< 0.001_
SFQL _< 0.001_ _< 0.001_
MF Xi _< 0.001_

(c) Racer Environment with General Reward Functions


QL SFQL (O) SFQL CMF Xi


3M


**p-value** SFQL (O) SFQL CMF Xi

QL _< 0.001_ _< 0.001 < 0.001_
SFQL (O) 0.021 _< 0.001_
SFQL _< 0.001_


2.5M

2M


1.5M

Figure 5: Total return over all tasks in each evaluated condition. The tables show the p-values of
pairwise Mannâ€“Whitney U tests between the agents. See the text for more information.


-----

