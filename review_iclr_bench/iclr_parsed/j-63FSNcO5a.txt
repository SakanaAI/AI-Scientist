# LEARNING DISENTANGLED REPRESENTATION
## BY EXPLOITING PRETRAINED GENERATIVE MODELS:
# A CONTRASTIVE LEARNING VIEW

**Xuanchi Ren[1][∗], Yang Tao[2][∗], Yuwang Wang[3][†], Wenjun Zeng[4]**

1HKUST, 2Xi’an Jiaotong University, 3Microsoft Research Asia, 4EIT
1 xrenaa@connect.ust.hk 2 yt14212@stu.xjtu.edu.cn
3 yuwwan@microsoft.com 4 wenjunzeng@eias.ac.cn

ABSTRACT

From the intuitive notion of disentanglement, the image variations corresponding to different factors should be distinct from each other, and the disentangled
representation should reflect those variations with separate dimensions. To discover the factors and learn disentangled representation, previous methods typically
leverage an extra regularization term when learning to generate realistic images.
However, the term usually results in a trade-off between disentanglement and
generation quality. For the generative models pretrained without any disentanglement term, the generated images show semantically meaningful variations when
traversing along different directions in the latent space. Based on this observation, we argue that it is possible to mitigate the trade-off by (i) leveraging the
pretrained generative models with high generation quality, (ii) focusing on discovering the traversal directions as factors for disentangled representation learning.
To achieve this, we propose Disentaglement via Contrast (DisCo) as a framework to model the variations based on the target disentangled representations,
and contrast the variations to jointly discover disentangled directions and learn
disentangled representations. DisCo achieves the state-of-the-art disentangled
representation learning and distinct direction discovering, given pretrained nondisentangled generative models including GAN, VAE, and Flow. Source code is at
[https://github.com/xrenaa/DisCo.](https://github.com/xrenaa/DisCo)

1 INTRODUCTION

Disentangled representation learning aims to identify and decompose the underlying explanatory
factors hidden in the observed data, which is believed by many to be the only way to understand the
world for AI fundamentally (Bengio & LeCun, 2007). To achieve the goal, as shown in Figure 1 (a),
we need an encoder and a generator. The encoder to extract representations from images with each
dimension corresponds to one factor individually. The generator (decoder) decodes the changing of
each factor into different kinds of image variations.

With supervision, we can constrain each dimension of the representation only sensitive to one kind
of image variation caused by changing one factor respectively. However, this kind of exhaustive
supervision is often not available in real-world data. The typical unsupervised methods are based on a
generative model to build the above encoder and generator framework, e.g., VAE (Kingma & Welling,
2014) provides encoder and generator, and GAN (Goodfellow et al., 2014; Miyato et al., 2018; Karras
et al., 2019) provides generator. During the training process of the encoder and generator, to achieve
disentangled representation, the typical methods rely on an additional disentanglement regularization
term, e.g., the total correlation for VAE-based methods (Higgins et al., 2017; Burgess et al., 2018;
Kumar et al., 2017; Kim & Mnih, 2018; Chen et al., 2018) or mutual information for InfoGAN-based
methods (Chen et al., 2016; Lin et al., 2020).

_∗Equal contribution. Work done during internships at Microsoft Research Asia._
_†Corresponding author_


-----

Disentangled

Representation Space


Disentangled

Representation Space


Factors


Latent Space


Latent Space


Fixed

Generator


Encoder


Encoder


Generator


(a) (b)

Figure 1: (a) The encoder and generator framework for learning disentangled representation. (b) Our
alternative route to learn disentangle representation with fixed generator.

However, the extra terms usually result in a trade-off between disentanglement and generation
quality (Burgess et al., 2018; Khrulkov et al., 2021). Furthermore, those unsupervised methods
have been proved to have an infinite number of entangled solutions without introducing inductive
bias (Locatello et al., 2019). Recent works (Shen & Zhou, 2021; Khrulkov et al., 2021; Karras et al.,
2019; Härkönen et al., 2020; Voynov & Babenko, 2020) show that, for GANs purely trained for
image generation, traversing along different directions in the latent space causes different variations
of the generated image. This phenomenon indicates that there is some disentanglement property
embedded in the latent space of the pretrained GAN. The above observations indicate that training
the encoder and generator simultaneous may not be the best choice.

We provide an alternative route to learn disentangled representation: fix the pretrained generator,
jointly discover the factors in the latent space of the generator and train the encoder to extract
disentangled representation, as shown in Figure 1(b). From the intuitive notion of disentangled
representation, similar image variations should be caused by changing the same factor, and different
image variations should be caused by changing different factors. This provide a novel contrastive
learning view for disentangled representation learning and inspires us to propose a framework:
**Disentanglement via Contrast (DisCo) for disentangled representation learning.**

In DisCo, changing a factor is implemented by traversing one discovered direction in the latent
space. For discovering the factors, DisCo adopts a typical network module, Navigator, to provides
candidate traversal directions in the latent space (Voynov & Babenko, 2020; Jahanian et al., 2020;
Shen et al., 2020). For disentangled representation learning, to model the various image variations,
we propose a novel ∆-Contrastor to build a Variation Space where we apply the contrastive loss. In
addition to the above architecture innovations, we propose two key techniques for DisCo: (i) an
entropy-based domination loss to encourage the encoded representations to be more disentangled,
(ii) a hard negatives flipping strategy for better optimization of Contrastive Loss.

We evaluate DisCo on three major generative models (GAN, VAE, and Flow) on three popular
disentanglement datasets. DisCo achieves the state-of-the-art (SOTA) disentanglement performance
compared to all the previous discovering-based methods and typical (VAE/InfoGAN-based) methods.
Furthermore, we evaluate DisCo on the real-world dataset FFHQ (Karras et al., 2019) to demonstrate
that it can discover SOTA disentangled directions in the latent space of pretrained generative models.

Our main contributions can be summarized as: (i) To our best knowledge, DisCo is the first
unified framework for jointly learning disentangled representation and discovering the latent space
of pretrained generative models by contrasting the image variations. (ii) We propose a novel
∆-Contrastor to model image variations based on the disentangled representations for utilizing
Contrastive Learning. (iii) DisCo is an unsupervised and model-agnostic method that endows
non-disentangled VAE, GAN, or Flow models with the SOTA disentangled representation learning
and latent space discovering. (iv) We propose two key techniques for DisCo: an entropy-based
domination loss and a hard negatives flipping strategy.

2 RELATED WORK

**Typical unsupervised disentanglement. There have been a lot of studies on unsupervised disen-**
tangled representation learning based on VAE (Higgins et al., 2017; Burgess et al., 2018; Kumar
et al., 2017; Kim & Mnih, 2018; Chen et al., 2018) or InfoGAN (Chen et al., 2016; Lin et al.,
2020). These methods achieve disentanglement via an extra regularization, which often sacrifices the
generation quality (Burgess et al., 2018; Khrulkov et al., 2021). VAE-based methods disentangle the
variations by factorizing aggregated posterior, and InfoGAN-based methods maximize the mutual


-----

information between latent factors and related observations. VAE-based methods achieve relatively
good disentanglement performance but have low-quality generation. InfoGAN-based methods have a
relatively high quality of generation but poor disentanglement performance. Our method supplements
generative models pretrained without disentanglement regularization term with contrastive learning
in the Variation Space to achieve both high-fidelity image generation and SOTA disentanglement.

**Interpretable directions in the latent space. Recently, researchers have been interested in discover-**
ing the interpretable directions in the latent space of generative models without supervision, especially
for GAN (Goodfellow et al., 2014; Miyato et al., 2018; Karras et al., 2020). Based on the fact that the
GAN latent space often possesses semantically meaningful directions (Radford et al., 2015; Shen
et al., 2020; Jahanian et al., 2020), Voynov & Babenko (2020) propose a regression-based method
to explore interpretable directions in the latent space of a pretrained GAN. The subsequent works
focus on extracting the directions from a specific layer of GANs. Härkönen et al. (2020) search for
important and meaningful directions by performing PCA in the style space of StyleGAN (Karras et al.,
2019; 2020). Shen & Zhou (2021) propose to use the singular vectors of the first layer of a generator
as the interpretable directions, and Khrulkov et al. (2021) extend this method to the intermediate
layers by Jacobian matrix. All the above methods only discover the interpretable directions in the
latent space, except for Khrulkov et al. (2021) which also learns disentangled representation of
generated images by training an extra encoder in an extra stage. However, all these methods can
not outperform the typical disentanglement methods. Our method is the first to jointly learn the
disentangled representation and discover the directions in the latent spaces.

**Contrastive Learning. Contrastive Learning gains popularity due to its effectiveness in representation**
learning (He et al., 2020; Grill et al., 2020; van den Oord et al., 2018; Hénaff, 2020; Li et al., 2020;
Chen et al., 2020). Typically, contrastive approaches bring representations of different views of
the same image (positive pairs) closer, and push representations of views from different images
(negative pairs) apart using instance-level classification with Contrastive Loss. Recently, Contrastive
Learning is extended to various tasks, such as image translation (Liu et al., 2021; Park et al., 2020) and
controllable generation (Deng et al., 2020). In this work, we focus on the variations of representations
and achieve SOTA disentanglement with Contrastive Learning in the Variation Space. Contrastive
Learning is suitable for disentanglement due to: (i) the actual number of disentangled directions is
usually unknown, which is similar to Contrastive Learning for retrieval (Le-Khac et al., 2020), (ii) it
works in the representation space directly without any extra layers for classification or regression.

3 DISENTANGLEMENT VIA CONTRAST

3.1 OVERVIEW OF DISCO

From the contrastive view of the intuitive notion of disentangled representation learning, we propose a
DisCo to leverage pretrained generative models to jointly discover the factors embedded as directions
in the latent space of the generative models and learn to extract disentangled representation. The
benefits of leveraging a pretrained generative model are two-fold: (i) the pretrained models with
high-quality image generation are readily available, which is important for reflecting detailed image
variations and downstream tasks like controllable generation; (ii) the factors are embedded in the
pretrained model, severing as an inductive bias for unsupervised disentangled representation learning.

DisCo consists of a Navigator to provides candidate traversal directions in the latent space and a
∆-Contrastor to extract the representation of image variations and build a Variation Space based on
the target disentangled representations. More specifically, ∆-Contrastor is composed of two sharedweight Disentangling Encoders. The variation between two images is modeled as the difference of
their corresponding encoded representations extracted by the Disentangling Encoders.

In the Variation Space, by pulling together the variation samples resulted from traversing the same
direction and pushing away the ones resulted from traversing different directions, the Navigator
learns to discover disentangled directions as factors, and Disentangling Encoder learns to extract
disentangled representations from images. Thus, traversing along the discovered directions causes
distinct image variations, which causes separated dimensions of disentangled representations respond.

Different from VAE-based or InfoGAN-based methods, our disentangled representations and factors
are in two separate spaces, which actually does not affect the applications. Similar to the typical


-----

Δ-Contrastor


Latent Space


Variation Space


Navigator

Fixed

Generative

Model

Encoder

Encoder

Discovered Directions Disentangled Representations Samples from Variation Space Contrastive Loss


Figure 2: Overview of DisCo. DisCo consists of: (i) a Navigator exploring traversal directions
in the latent space of a given pretrained generative model, (ii) a ∆-Contrastor encoding traversed
images into the Variation Space, where we utilize Contrastive Learning. Samples in the Variation
_Space correspond to the image variations along the directions provided by the Navigator labeled with_
different colors, respectively. ∆-Contrastor includes two shared-weight Disentangling Encoders to
extract disentangled representations respectively, and outputs the difference between the disentangled
representations as variation representation. The Generative Model is fixed, and the Navigator and
Disentangling Encoders marked with grey color are learnable.

methods, the Disentangling Encoder can extract disentangled representations from images, and
the pretrained generative model with discovered factors can be applied to controllable generation.
Moreover, DisCo can be applied to different types of generative models.

Here we provide a detailed workflow of DisCo. As Figure 2 shows, given a pretrained generative
model G: Z →I, where Z ∈ R[L] denotes the latent space, and I denotes the image space, the
workflow is: 1) A Navigator A provides a total of D candidate traversal directions in the latent
space Z, e.g., in the linear case, A ∈ R[L][×][D] is a learnable matrix, and each column is regarded
as a candidate direction. 2) Image pairs G(z), G(z[′]) are generated. z is sampled from Z and
**_z[′]_** = z + A(d, ε), where d ∈{1, ..., D} and ε ∈ R, and A(d, ε) denotes the shift along the dth direction with ε scalar. 3) The ∆-Contrastor, composed of two shared-weight Disentangling
Encoders E, encodes the image pair to a sample v ∈V as

**_v(z, d, ε) = |E(G(z + A(d, ε))) −_** **_E(G(z))|,_** (1)

where V ∈ R[J]+ [denotes the][ Variation Space][. Then we apply Contrastive Learning in][ V][ to optimize]
the Disentangling Encoder E to extract disentangled representations and simultaneously enable
_Navigator A to find the disentangled directions in the latent space Z._

3.2 DESIGN OF DISCO


We present the design details of DisCo, which include: (i) the collection of query set Q = {qi}i[B]=1[,]
_Variation Spacepositive key set K V[+], (=ii {) the formulation of the Contrastive Loss.ki[+][}]i[N]=1_ [and negative key set][ K][−] [=][ {][k]i[−][}]i[M]=1[, which are three subsets of the]

According to our goal of contrasting the variations, the samples from Q and K[+] share the same
traversal direction and should be pulled together, while the samples from Q and K[−] have different
directions and should be pushed away. Recall that each sample v in V is determined as v(z, d, ε).
To achieve the contrastive learning process, we construct the query sample qi = v(zi, di, εi), the
key sample ki[+] [=][ v][(][z]i[+][, d]i[+][, ε]i[+][)][ and the negative sample][ k]i[−] [=][ v][(][z]i[−][, d]i[−][, ε]i[−][)][. Specifically, we]
randomly sample a direction index _d[ˆ] from a discrete uniform distribution U{1, D} for {di}i[B]=1_ [and]
_{d[+]i_ _[}]i[N]=1_ [to guarantee they are the same. We randomly sample][ {][d]i[−][}]i[M]=1 [from the set of the rest of the]
directions U{1, D} \ {d[ˆ]} individually and independently to cover the rest of directions in Navigator
**_A. Note that the discovered direction should be independent with the starting point and the scale_**
of variation, which is in line with the disentangled factors. Therefore, **_zi_** _i=1[,][ {][z]i[+]_ _i=1[,][ {][z]i[−]_ _i=1_
are all sampled from latent spacecontinuous uniform distribution U Z[−, andϵ, ϵ] individually and independently. We normalize each sample {εi}i[B]=1[,][ {][ε][+]i _[}]i[N]=1[,][ {][ε][−]i_ _[}]i[M]=1_ [are all sampled from a shared] { _}[B]_ _[}][N]_ _[}][M]_
in Q, K[+], and K[−] to a unit vector to eliminate the impact caused by different shift scalars.


-----

For the design of Contrastive Loss, a well-known form of Contrastive Loss is InfoNCE (van den
Oord et al., 2018):

_B_ _N_ exp(qi **_kj[+][/τ]_** [)]

_LNCE = −_ _|B[1]_ _|_ Xi=1 Xj=1 log _Ns=1+M_ exp( · **_qi · ks/τ_** ) _,_ (2)

where τ is a temperature hyper-parameter and **_kiPi=1_** = **_ki[+]_** _i=1_ **_ki−_** _i=1[. The InfoNCE]_
is originate from BCELoss (Gutmann & Hyvärinen, 2010). BCELoss has been used to achieve { _}[N]_ [+][M] _{_ _[}][N]_ _{_ _[}][M]_
contrastive learning (Wu et al., 2018; Le-Khac et al., 2020; Mnih & Kavukcuoglu, 2013; Mnih &S
Teh, 2012). We choose to follow them to use BCELoss Llogits for reducing computational cost:


_logits =_
_L_ _−_ _B[1]_

_|_ _|_


_li[−]_ [+][ l]i[+] _,_ (3)



_i=1_


_j=1_ log σ(qi · kj[+][/τ] [)][,] _li[−]_ [=]

X


_li[+]_ [=]


_m=1_ log(1 − _σ(qi · km[−]_ _[/τ]_ [))][,] (4)

X


where σ denotes the sigmoid function, li[+] [denotes the part for positive samples, and][ l]i[−] [denotes the]
part for the negative ones.Note that we use a shared positive set for B different queries to reduce the
computational cost.

3.3 KEY TECHNIQUES FOR DISCO

**Entropy-based domination loss. By optimizing the Contrastive Loss, Navigator A is optimized**
to find the disentangled directions in the latent space, and Disentangling Encoder E is optimized
to extract disentangled representations from images. To further make the encoded representations
more disentangled, i.e., when traversing along one disentangled direction, only one dimension of
the encoded representation should respond, we thus propose an entropy-based domination loss
to encourage the corresponding samples in the Variation Space to be one-hot. To implement the
entropy-based domination loss, we first get the mean c of Q and K[+] as


**_ki[+]_**
_i=1_

X


(5)


**_c =_** _B + N_ **_qi +_** **_ki[+]_** _._ (5)

_|_ _|_ _i=1_ _i=1_ !

X X

We then compute the probability as pi = exp c(i)/[P][J]j=1 [exp][ c][(][j][)][, where][ c][(][i][)][ is the][ i][-th element of]
**_c and J is the number of dimensions of c. The entropy-based domination loss Led is calculated as_**


**_c =_**


_ed =_
_L_ _−_ _J[1]_


_pj log(pj)._ (6)
_j=1_

X


**Hard negatives flipping. Since the latent space of the generative models is a high-dimension complex**
manifold, many different directions carry the same semantic meaning. These directions with the same
semantic meaning result in hard negatives during the optimization of Contrastive Loss. The hard
negatives here are different from the hard negatives in the works of self-supervised representation
learning (He et al., 2020; Coskun et al., 2018), where they have reliable annotations of the samples.
Here, our hard negatives are more likely to be “false” negatives, and we choose to flip these hard
negatives into positives. Specifically, we use a threshold T to identify the hard negative samples, and
use their similarity to the queries as the pseudo-labels for them:


ˆli[−] [=]


ˆli[−] [=] log(1 − _σ(αij)) +_ _αij log(σ(αij)),_ (7)

_αXij_ _<T_ _αXij_ _≥T_

where [ˆ]li[−] [denotes the modified][ l]i[−][, and][ α][ij][ =][ q][i][ ·][ k]j[−][/τ] [. Therefore, the modified final BCELoss is:]


log(1 _σ(αij)) +_
_−_
_αij_ _<T_

X


_li[+]_ [+ ˆ]li[−] _._ (8)



_logits_ _f =_
_L_ _−_


_|B|_


_i=1_


-----

Cars3D Shapes3D MPI3D
**Method**

MIG DCI MIG DCI MIG DCI

_Typical disentanglement baselines:_


InfoGAN-CRFactorVAEβ-TCVAE 000...142080011 ± ± ± 0 0 0...023023009 000...161140020 ± ± ± 0 0 0...019019011 000...434406297 ± ± ± 0 0 0...143175124 000...611613478 ± ± ± 0 0 0...101114055 000...099114163 ± ± ± 0 0 0...029042076 000...240237241 ± ± ± 0 0 0...051056075

_Methods on pretrained GAN:_

DisCoLDGSDSCF (ours) **00000.179....086083136118 ± ± ± ± ± 0 0 0 0 0.....029024006044037** **00000.271....216243209222 ± ± ± ± ± 0 0 0 0 0.....072048031044037** **00000.512....168307121356 ± ± ± ± ± 0 0 0 0 0.....056124048090068** **00000.708....380525284513 ± ± ± ± ± 0 0 0 0 0.....062078034075048** **00000.222....097183163093 ± ± ± ± ± 0 0 0 0 0.....057081065035027** **00000.318....292196229248 ± ± ± ± ± 0 0 0 0 0.....024038042038014**

_Methods on pretrained VAE:_

DisCoLD (ours) **00.103.030 ± ± 0 0..025028** **00.211.068 ± ± 0 0..030041** **00.331.040 ± ± 0 0..035161** **00.844.068 ± ± 0 0..075033** **00.068.024 ± ± 0 0..026030** **00.288.035 ± ± 0 0..014021**

_Methods on pretrained Flow:_

DisCoLD (ours) **00.060.015 ± ± 0 0..000000** **00.199.029 ± ± 0 0..000000** **00.150.067 ± ± 0 0..000000** **00.525.211 ± ± 0 0..000000** **00.076.025 ± ± 0 0..000000** **00.264.035 ± ± 0 0..000000**

Table 1: Comparisons of the MIG and DCI disentanglement metrics (mean ± std). A higher mean
indicates a better performance. DisCo can extract disentangled representations from all three
generative models, and DisCo on GAN achieves the highest score in almost all the cases, compared
to all the baselines. All the cells except for Flow are results over 25 runs.

DCI

MIG

Shapes3D Car3D MPI3D

Figure 3: Violin plots on three datasets (1: β-TCVAE, 2: FactorVAE, 3: InfoGAN-CR, 4: CF, 5:
LD, 6: GS, 7: DS, 8: DisCo (ours)). DisCo on pretrained GAN consistently achieves the best
performance. Each method has 25 runs, and the variance is due to randomness.


**Full objective. With the above two techniques, the full objective is:**

= _logits_ _f + λ_ _ed,_ (9)
_L_ _L_ _−_ _L_

where λ is the weighting hyper-parameter for entropy-based domination loss _ed._
_L_


4 EXPERIMENT

In this section, we first follow the well-accepted protocol (Locatello et al., 2019; Khrulkov et al., 2021)
to evaluate the learned disentangled representation, which also reflects the performance of discovered
directions implicitly (Lin et al., 2020) (Section 4.1). Secondly, we follow Li et al. (2021a) to directly
evaluate the discovered directions (Section 4.2). Finally, we conduct ablation study (Section 4.3).


4.1 EVALUATIONS ON DISENTANGLED REPRESENTATION

4.1.1 EXPERIMENTAL SETUP


**Datasets. We consider the following popular datasets in the disentanglement areas: Shapes3D (Kim**
& Mnih, 2018) with 6 ground truth factors, MPI3D (Gondal et al., 2019) with 7 ground truth factors,


-----

LD


CF

DisCo


StyleGAN2 FFHQ – Smile StyleGAN2 FFHQ – Bald


Figure 4: Comparison of discovered directions. DisCo can better manipulate desirable attributes
while keeping others intact. Please refer to Appendix C for more qualitative results.

_Concat_ _Variation_ _Contrast_ _Classification_ MIG DCI

✓ ✓ 0.023 0.225
✓ ✓ **0.562** **0.736**
✓ ✓ 0.012 0.138

Variation ✓ ✓ 0.002 0.452


(a) w/o _ed_ (b) w/ _ed_
_L_ _L_

Figure 5: Visualization of the variation of the
encoded disentangled representations caused by
the change of a single ground truth factor.


Table 2: Ablation on Contrast v.s. Classification
and Concatenation (Concat) v.s. Variation.


and Cars3D (Reed et al., 2015) with 3 ground truth factors. In the experiments of the above datasets,
images are resized to the 64x64 resolution.

**Pretrained generative models. For GAN, we use the StyleGAN2 model (Karras et al., 2020). For**
VAE, we use a common structure with convolutions (Locatello et al., 2019). For Flow, we use
Glow (Kingma & Dhariwal, 2018).

**Baseline. For the typical disentanglement baselines, we choose FactorVAE (Kim & Mnih, 2018),**
_β-TCVAE (Chen et al., 2018) and InfoGAN-CR (Lin et al., 2020). For discovering-based methods,_
we consider serveral recent methods: GANspace (GS) (Härkönen et al., 2020), LatentDiscovery
**(LD) (Voynov & Babenko, 2020), ClosedForm (CF) (Shen & Zhou, 2021) and DeepSpectral**
**(DS) (Khrulkov et al., 2021). For these methods, we follow Khrulkov et al. (2021) to train an**
additional encoder to extract disentangled representation. We are the first to extract disentangled
representations from pretrained VAE and Flow, so we extend LD to VAE and Flow as a baseline.

**Disentanglement metrics. We mainly consider two representative ones: the Mutual Information**
_Gap (MIG) (Chen et al., 2018) and the Disentanglement metric (DCI) (Eastwood & Williams, 2018)._
_MIG requires each factor to be only perturbed by changes of a single dimension of representation._
_DCI requires each dimension only to encode the information of a single dominant factor. We evaluate_
the disentanglement in terms of both representation and factors. We also provide results for β-VAE
_score (Higgins et al., 2017) and FactorVAE score (Kim & Mnih, 2018) in Appendix B.3._

**Randomness. We consider the randomness caused by random seeds and the strength of the regular-**
ization term (Locatello et al., 2019). For random seeds, we follow the same setting as the baselines.
Since DisCo does not have a regularization term, we consider the randomness of the pretrained
generative models. For all methods, we ensure there are 25 runs, except that Glow only has one run,
limited by GPU resources. More details are presented in Appendix A.

4.1.2 EXPERIMENTAL RESULTS


The quantitative results are summarized in Table 1 and Figure 3. More details about the experimental
settings and results are presented in Appendix A & C.

**DisCo vs. typical baselines. Our DisCo achieves the SOTA performance consistently in terms**
of MIG and DCI scores. The variance due to randomness of DisCo tends to be smaller than those
typical baselines. We demonstrate that the method, which extracts disentangled representation from
pretrained non-disentangled models, can outperform typical disentanglement baselines.


-----

**DisCo vs. discovering-based methods. Among the baselines based on discovering pretrained GAN,**
**CF achieves the best performance. DisCo outperforms CF in almost all the cases by a large margin.**
Besides, these baselines need an extra stage (Khrulkov et al., 2021) to get disentangled representation,
while our Disentangling Encoder can directly extract disentangled representation.

4.2 EVALUATIONS ON DISCOVERED DIRECTIONS

To evaluate the discovered directions, we com
**MDS on CelebAHQ-Attributes**

pare DisCo on StyleGAN2 with GS, LD, CF **Method** **Young** **Smile** **Bald** **Blonde Hair** **Overall**
and DS on the real-world dataset FFHQ (Karras DS 0.518 0.570 0.524 0.511 0.531
et al., 2019)[1]. and adopt the comprehensive Ma- CF 0.518 0.553 0.504 0.560 0.534
nipulation Disentanglement Score (MDS) (Li GS 0.502 0.534 0.494 0.538 0.517

LD **0.627** 0.531 0.524 0.514 0.549

et al., 2021a) as a metric. To calculate MDS, DisCo 0.516 **0.688** **0.568** **0.592** **0.591**
we use 40 CelebaHQ-Attributes predictors released by StyleGAN. Among them, we select Table 3: MDS comparison on facial attribute edit**Young, Smile, Bald and Blonde Hair, as they** ing. Our DisCo shows the best overall score for
are attributes with an available predictor and the latent discovering task on FFHQ dataset.
commonly found by all methods at the same
time. The results are summarized in Table 3. DisCo has shown better overall performance compared
to other baselines, which verifies our assumption that learning disentangled representation benefits
latent space discovering. We also provide qualitative comparisons in Figure 4.

Finally, we provide an intuitive analysis in Appendix D for why DisCo can find those disentangled
directions.

4.3 ABLATION STUDY

In this section, we perform ablation study of DisCo only on GAN, limited by the space. For the
experiments, we use the Shapes3D dataset, and the random seed is fixed.

**Choice of latent space. For style–based GANs (Karras et al., 2019; 2020), there is a style space W,**
which is the output of style network (MLP) whose input is a random latent space Z. As demonstrated
in Karras et al. (2019), W is more interpretable than Z. We conduct experiments on W and Z
respectively to see how the latent space influences the performance. As shown in Table 4, DisCo on
_W is better, indicating that the better the latent space is organized, the better disentanglement DisCo_
can achieve.


**Choices of A. Following the setting of Voynov**
& Babenko (2020), we mainly consider three
options of A: a linear operator with all matrix
columns having a unit length, a linear operator with orthonormal matrix columns, or a nonlinear operator of 3 fully-connected layers.


Method MIG DCI


options of A: a linear operator with all matrix _Z + Unit length matrix_ **0.242** **0.673**
columns having a unit length, a linear opera- _Z + Orthonormal matrix_ 0.183 0.578

+ 3 fully-connected layers 0.169 0.504

tor with orthonormal matrix columns, or a non- _Z_
linear operator of 3 fully-connected layers. _W + Unit length matrix_ 0.547 **0.730**

_W + Orthonormal matrix_ **0.551** 0.709

The results are shown in Table 4. For latent _W + 3 fully-connected layers_ 0.340 0.665
spaces and, A with unit-norm columns _logits_ 0.134 0.632
achieves nearly the best performance in terms W _Z_ _LLlogits + Led_ 0.296 0.627
of MIG and DCI scores. Compared to A with _logits_ _f_ 0.134 0.681

_L_ _−_

orthonormal matrix columns, using A with unit- _Llogits−f + Led_ **0.547** **0.730**
norm columns is more expressive with less constraints. Another possible reason is that A is Table 4: Ablation study of DisCo on the latent
global without conditioned on the latent code spaces, types of A, and our proposed techniques.
_z. A non-linear operator is more suitable for a_
local navigator A. For such a much more complex local and non-linear setting, more inductive bias
or supervision should be introduced.

**Entropy-based domination loss. Here, we verify the effectiveness of entropy-based domination**
loss Led for disentanglement. For a desirable disentangled representation, one semantic meaning
corresponds to one dimension. As shown in Table 4, Led can improve the performance by a large

1The above disentanglement metrics (DCI and MIG) are not available for FFHQ dataset.


-----

(a) Impact of N : M with a fixed sum (b) Impact of N + M with a fixed ratio 1 : 2

Figure 6: Study on numbers of positive (N) and negative samples (M). The balance between positive
and negative samples is crucial for DisCo.

margin. We also visualize the Variation Space to further demonstrate the effectiveness of our proposed
loss in Figure 5. Adding the domination loss makes the samples in the Variation Space to be one-hot,
which is desirable for disentanglement.

**Hard negatives flipping. We run our DisCo with or without the hard negatives flipping strategy to**
study its influence. As shown in Table 4, flipping hard negatives can improve the disentanglement
ability of DisCo. The reason is that the hard negatives have the same semantics as the positive
samples. In this case, treating them as the hard negatives does not make sense. Flipping them with
pseudo-labels can make the optimization of Contrastive Learning easier.

**Hyperparmeter N & M. We run DisCo with different ratios of N : M with a fixed sum of 96, and**
different sum of N + M with a fixed ratio 1 : 2 to study their impacts. As shown in Figure 6 (a), the
best ratio is N : M = 32 : 64 = 1 : 2, as the red line (MIG) and blue line (DCI) in the figure show
that larger or smaller ratios will hurt DisCo, which indicates DisCo requires a balance between
_N and M_ . As shown in Figure 6 (b), the sum of N + M has slight impact on DisCo. For other
hyperparameters, we set them empirically, and more details are presented in Appendix A.

**Contrast vs. Classification. To verify the effectiveness of Contrast, we substitute it with classification**
by adopting an additional linear layer to recover the corresponding direction index and the shift along
this direction. As Table 2 shows, Contrastive Learning outperforms Classification significantly.

**Concatenation vs. Variation. We further demonstrate that the Variation Space is crucial for**
DisCo. By replacing the difference operator with concatenation, the performance drops significantly
(Table 2), indicating that the encoded representation is not well disentangled. On the other hand, the
disentangled representations of images are achieved by Contrastive Learning in the Variation Space.

4.4 ANALYSIS OF DIFFERENT GENERATIVE MODELS

As shown in Table 1, DisCo can be well generalized to different generative models (GAN, VAE,
and Flow). DisCo on GAN and VAE can achieve relative good performance, while DisCo on Flow
is not as good. The possible reason is similar to the choice of latent space of GAN. We assume the
disentangled directions are global linear and thus use a linear navigator. In contrast to GAN and VAE,
we suspect that Flow may not conform to this assumption well. Furthermore, Flow has the problems
of high GPU cost and unstable training, which limit us to do further exploration.

5 CONCLUSION

In this paper, we present an unsupervised and model-agnostic method DisCo, which is a Contrastive
Learning framework to learn disentangled representation by exploiting pretrained generative models.
We propose an entropy-based domination loss and a hard negatives flipping strategy to achieve
better disentanglement. DisCo outperforms typical unsupervised disentanglement methods while
maintaining high image quality. We pinpoint a new direction that Contrastive Learning can be well
applied to extract disentangled representation from pretrained generative models. There may be
some specific complex generative models, for which the global linear assumption of disentangled
directions in the latent space could be a limitation. For future work, extending DisCo to the existing
VAE-based disentanglement framework is an exciting direction.


-----

REFERENCES

Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. In Large Scale Kernel
_Machines. MIT Press, 2007._

Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins,
and Alexander Lerchner. Understanding disentangling in beta-vae. arXiv:1804.03599, 2018.

Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of
disentanglement in variational autoencoders. In NeurPIS, 2018.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for
contrastive learning of visual representations. In ICML, 2020.

Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
interpretable representation learning by information maximizing generative adversarial nets. In
_NeurPIS, 2016._

Huseyin Coskun, David Joseph Tan, Sailesh Conjeti, Nassir Navab, and Federico Tombari. Human
motion analysis with deep metric learning. In ECCV, 2018.

Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin Tong. Disentangled and controllable face
image generation via 3d imitative-contrastive learning. In CVPR, 2020.

Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of
disentangled representations. In ICLR, 2018.

Muhammad Waleed Gondal, Manuel Wuthrich, Djordje Miladinovic, Francesco Locatello, Martin
Breidt, Valentin Volchkov, Joel Akpo, Olivier Bachem, Bernhard Schölkopf, and Stefan Bauer. On
the transfer of inductive bias from simulation to the real world: a new disentanglement dataset. In
_NeurIPS, 2019._

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. CoRR, abs/1406.2661,
[2014. URL http://arxiv.org/abs/1406.2661.](http://arxiv.org/abs/1406.2661)

Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Ávila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
Bilal Piot, Koray Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent - A
new approach to self-supervised learning. In NeurPIS, 2020.

Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In AISTATS. JMLR Workshop and Conference Proceedings,
2010.

Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering
interpretable GAN controls. In NeurIPS, 2020.

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In CVPR, 2020.

Olivier J. Hénaff. Data-efficient image recognition with contrastive predictive coding. In ICML,
2020.

Irina Higgins, Loïc Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In ICLR, 2017.

Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017.

Ali Jahanian, Lucy Chai, and Phillip Isola. On the "steerability" of generative adversarial networks.
In ICLR, 2020.


-----

Y Jin, J Zhang, M Li, Y Tian, and H Zhu. Towards the high-quality anime characters generation
with generative adversarial networks. In Proceedings of the Machine Learning for Creativity and
_Design Workshop at NeurIPS, 2017._

Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In CVPR, 2019.

Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing
and improving the image quality of stylegan. In CVPR, 2020.

Valentin Khrulkov, Leyla Mirvakhabova, Ivan V. Oseledets, and Artem Babenko. Disentangled
representations from non-disentangled models. CoRR, abs/2102.06204, 2021.

Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In ICML, 2018.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.

Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.

Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
_NeurIPS, 2018._

Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentangled
latent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848, 2017.

Phuc H Le-Khac, Graham Healy, and Alan F Smeaton. Contrastive representation learning: A
framework and review. IEEE Access, 2020.

Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
_Available: http://yann.lecun.com/exdb/mnist, 2, 2010._

Wonkwang Lee, Donggyun Kim, Seunghoon Hong, and Honglak Lee. High-fidelity synthesis with
disentangled representation. In ECCV, 2020.

Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven C. H. Hoi. Prototypical contrastive
[learning of unsupervised representations. CoRR, abs/2005.04966, 2020. URL https://arxiv.](https://arxiv.org/abs/2005.04966)
[org/abs/2005.04966.](https://arxiv.org/abs/2005.04966)

Minjun Li, Yanghua Jin, and Huachun Zhu. Surrogate gradient field for latent space manipulation. In
_CVPR, 2021a._

Suichan Li, Dongdong Chen, Yinpeng Chen, Lu Yuan, Lei Zhang, Qi Chu, Bin Liu, and Nenghai Yu.
Improve unsupervised pretraining for few-label transfer. ICCV, 2021b.

Zinan Lin, Kiran Thekumparampil, Giulia Fanti, and Sewoong Oh. Infogan-cr and modelcentrality:
Self-supervised model training and selection for disentangling gans. In ICML, 2020.

Rui Liu, Yixiao Ge, Ching Lam Choi, Xiaogang Wang, and Hongsheng Li. Divco: Diverse conditional
image synthesis via contrastive generative adversarial network. In CVPR, 2021.

Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rätsch, Sylvain Gelly, Bernhard Schölkopf,
and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In ICML, 2019.

Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In ICLR, 2018.

Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efficiently with noise-contrastive
estimation. In NeurIPS, 2013.

Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic
language models. ICML, 2012.

Taesung Park, Alexei A. Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired
image-to-image translation. In ECCV, 2020.


-----

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In ICLR, 2015.

Scott E. Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. In NeurIPS,
2015.

Yujun Shen and Bolei Zhou. Closed-form factorization of latent semantics in gans. In CVPR, 2021.

Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for
semantic face editing. In CVPR, 2020.

Akash Srivastava, Yamini Bansal, Yukun Ding, Cole L. Hurwitz, Kai Xu, Bernhard Egger, Prasanna
Sattigeri, Josh Tenenbaum, David D. Cox, and Dan Gutfreund. Improving the reconstruction of
disentangled representation learners via multi-stage modelling. CoRR, abs/2010.13187, 2020.

Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
[coding. CoRR, abs/1807.03748, 2018. URL http://arxiv.org/abs/1807.03748.](http://arxiv.org/abs/1807.03748)

Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the GAN
latent space. In ICML, 2020.

Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In ICML, pp. 9929–9939. PMLR, 2020.

Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via
non-parametric instance discrimination. In CVPR, 2018.


-----

A IMPLEMENTATION DETAILS

A.1 SETTING FOR DISCO

For the hyperparameters, we empirically set the temperature τ to 1, threshold T to 0.95, batch size B
to 32, the number of positives N to 32, the number of negatives K to 64, the loss weight λ for _ed_
_L_
to 1, the number of directions D to 64 and the dimension of the representation J to 32. We use an
Adam optimizer (Kingma & Ba, 2015) in the training process, as shown in Table 5. Besides N and
_M_, we empirically find that DisCo is not sensitive to threshold T ≥ 0.9 and other hyperparameters.

For the randomness, there is no regularization term for DisCo, thus the disentanglement performance
is mainly influenced by the pretrained generative models. We follow Khrulkov et al. (2021) to run
5 random seeds to pretrain the GAN and 5 random seeds for training DisCo. We have the same
setting for DisCo on GAN and VAE on all three datasets. For Flow, we only use one random seed
to pretrain the Glow and use one random seed for DisCo. Compare with the baselines, for DisCo
on StyleGAN, our modification happens globally on all layers in the W space without any manual
selection.

Parameter Values

Optimizer Adam
Adam: beta1 0.9
Adam: beta2 0.999
Adam: epsilon 1.00e-08
Adam: learning rate 0.00001
Iteration: 100,000

Table 5: Optimizer for DisCo.

_T = 0.7_ _T = 0.8_ _T = 0.9_ _T = 0.95_ _T = 0.98_

MIG 0.157 0.244 0.508 **0.547** 0.408
DCI 0.396 0.576 0.710 **0.730** 0.703

Table 6: Ablation study on hyperparameter T . DisCo is not sensitive to T when T ≥ 0.9. For
_T < 0.9, we may flip true hard negative and thus lead the optimization of Contrastive Loss collapse._


-----

A.2 SETTING FOR BASELINES

In this section, we introduce the implementation setting for the baselines (including randomness).

**VAE-based methods. We choose FactorVAE and β-TCVAE as the SOTA VAE-based methods,**
we follow Locatello et al. (2019) to use the same architecture of encoder and decoder. For the
**hyper-parameters, we use the the best settings by grid search. We set the latent dimension of**
representation to 10. For FactorVAE, we set the hyperparameter γ to 10. For β-TCVAE, we set the
hyperparameter β to 6. For the random seeds, considering our method has 25 run, we run 25 times
with different random seeds for each model to make the comparison fair.

**InfoGAN-based methods. We choose InfoGAN-CR as a baseline. We use the official implementa-**
tion [2] with the best hyperparameter settings by grid search. For the random seeds, we run 25 times
with different random seeds

**Discovering-based methods. We follow Khrulkov et al. (2021) to use the same settings for the**
following four baselines: LD (GAN), CF, GS, and DS. Similar to our method (DisCo), discoveringbased methods do not have a regularization term. Thus, for the randomness, we adopt the same
strategy with DisCo. We take the top-10 directions for 5 different random seeds for GAN and 5
different random seeds for the additional encoder to learn disentangled representations.

**LD (VAE) & LD (Flow). We follow LD (GAN) to use the same settings and substitute the GAN**
with VAE / Glow. The only exception is the randomness for LD (Flow). We only run one random
seed to pretrain the Glow and use one random seed for the encoder.

A.3 MANIPULATION DISENTANGLEMENT SCORE

As claimed in Li et al. (2021a), it is difficult to evaluate the performance on discovering the latent
space among different methods, which often use model-specific hyper-parameters to control the
editing strength. Thus, Li et al. (2021a) propose a comprehensive metric called Manipulation Disen**tanglement Score (MDS), which takes both the accuracy and the disentanglement of manipulation**
into consideration. For more details, please refer to Li et al. (2021a).

A.4 DOMAIN GAP PROBLEM

Please note that there exists a domain gap between the generated images of pretrained generative
models and the real images. However, the good performance on disentanglement metrics shows that
the domain gap has limited influence on DisCo.

[2https://github.com/fjxmlzn/InfoGAN-CR](https://github.com/fjxmlzn/InfoGAN-CR)


-----

A.5 ARCHITECTURE

Here, we provide the model architectures in our work. For the architecture of StyleGAN2, we follow
Khrulkov et al. (2021). For the architecture of Glow, we use the open-source implementation [3].

Conv 7 × 7 × 3 × 64, stride = 1
ReLu
Conv 4 × 4 × 64 × 128, stride = 2
ReLu
Conv 4 × 4 × 128 × 256, stride = 2
ReLu
Conv 4 × 4 × 256 × 256, stride = 2
ReLu
Conv 4 × 4 × 256 × 256, stride = 2
ReLu

FC 4096 × 256
ReLu
FC 256 × 256
ReLu
FC 256 × J

Table 7: Encoder E architecture used in DisCo. J is 32 for Shapes3D, MPI3D and Car3D.

FC J × 256
ReLu
FC 256 × 256
ReLu
FC 256 × 4096

ConvTranspose 4 × 4 × 256 × 256, stride = 2
ReLu
ConvTranspose 4 × 4 × 256 × 256, stride = 2
ReLu
ConvTranspose 4 × 4 × 256 × 128, stride = 2
ReLu
ConvTranspose 4 × 4 × 128 × 64, stride = 2
ReLu
ConvTranspose 7 × 7 × 64 × 3, stride = 1

Table 8: VAE’s decoder architecture. Its encoder is the same as the encoder in DisCo.

[3https://github.com/rosinality/glow-pytorch](https://github.com/rosinality/glow-pytorch)


-----

B MORE EXPERIMENTS

B.1 MORE QUALITATIVE COMPARISON

We provide some examples for qualitative comparison. We first demonstrate the trade-off problem of
the VAE-based methods. As shown in Figure 7, DisCo leverages the pretrained generative model
and does not have the trade-off between disentanglement and generation quality.

_β-TCVAE_

DisCo

Figure 7: Demonstration of the trade-off problem of the VAE-based method. β-TCVAE has bad
generation quality, especially on the real-world dataset. DisCo lerverages pretrained generative
model that can synthesize high-quality images.


-----

Furthermore, as shown in Figure 8 and Figure 9, VAE-based methods suffer from poor image quality.
When changing one attribute, the results of discovering-based methods tend to also change other
attributes.

LD

CF

GS

DS

FactorVAE

_β-TCVAE_

DisCo (GAN)

Figure 8: Comparison with baselines on Shapes3D dataset with Pose attribute.


-----

LD

CF

GS

DS

FactorVAE

_β-TCVAE_

DisCo (GAN)

Figure 9: Comparison with baselines on Shapes3D dataset with Wall Color attribute. VAE-based
methods suffer from poor image quality. Discovering-based methods tend to entangle Wall Color
with other attributes.


-----

We also provide qualitative comparisons between DisCo and InfoGAN-CR. Note that the latent
space of InfoGAN-CR is not aligned with the pretrained StyleGAN2. InfoGAN-CR also suffers from
the trade-off problem, and its disentanglement ability is worse than DisCo.

InfoGAN-CR

DisCo (GAN)

Figure 10: Comparison with baselines on Shapes3D dataset with Wall Color attribute. InfoGAN-CR
entangles Wall Color with Object Color and Pose.

InfoGAN-CR

DisCo (GAN)

Figure 11: Comparison with baselines on Shapes3D dataset with Floor Color attribute. InfoGAN-CR
entangles Floor Color with Object Color.


-----

We explain the comparison in the main paper and show more manipulation comparisons here.

GS

LD

CF

DisCo

StyleGAN2 FFHQ – Smile

Figure 12: Manipulation comparison with discovering-based pipeline with Smile attribute. We explain
the left column here. For GS, the manipulation also changes age. For LD, the manipulation also
changes pose and skin tone. For CF, the manipulation also change identity.

GS

LD

CF

DisCo

StyleGAN2 FFHQ – Bald

Figure 13: Manipulation comparison with discovering-based pipeline with Bald attribute. We explain
the left column here. For GS and LD, the manipulations also change age. For CF, the manipulation
also changes skin tone.

GS

CF

DisCo

StyleGAN2 FFHQ – Pose

Figure 14: Manipulation comparison with discovering-based pipeline with Pose attribute. LD does
not find the direction of pose attribute. GS, CF and DisCo can manipulate pose successfully.


-----

B.2 ANALYSIS OF THE LEARNED DISENTANGLED REPRESENTATIONS

We feed the images traversing the three most significant factors (wall color, floor color, and object
color) of Shapes3D into the Disentangling Encoders and plot the corresponding dimensions of the
encoded representations to visualize the learned disentangled space. The location of each point is the
disentangled representation of the corresponding image. An ideal result is that all the points form a
cube, and color variation is continuous. We consider three baselines that have relatively higher MIG
and DCI: CF, DS, LD. As the figures below show, the points in the latent space of CF and DS are
not well organized, and the latent space of all the three baselines are not well aligned with the axes,
especially for LD. DisCo learns a well-aligned and well-organized latent space, which signifies a
better disentanglement.

CF DS LD Ours

B.3 MORE QUANTITATIVE COMPARISON

We provide additional quantitative comparisons in terms of β-VAE score and FactorVAE score.
DisCo on pretrained GAN is comparable to discovering-based baselines in terms of β-VAE score
and FactorVAE score, suggesting that some disagreement between these two scores and MIG/ DCI.
However, note that the qualitative evaluation in Figure 8, Figure 9 and Section B.2 shows that the
disentanglement ability of DisCo is better than all the baselines on Shapes3D dataset.

Cars3D Shapes3D MPI3D
**Method**

_β-VAE score_ FactorVAE score _β-VAE score_ FactorVAE score _β-VAE score_ FactorVAE score

_Typical disentanglement baselines:_


InfoGAN-CRFactorVAEβ-TCVAE 0.09991.450.00 ± ± ± 1 0 0.0..e00022 − 4 000...906855411 ± ± ± 0 0 0...052082013 000...892978837 ± ± ± 0 0 0...064036039 000...840873587 ± ± ± 0 0 0...066074058 000...339348672 ± ± ± 0 0 0...029012101 000...152179439 ± ± ± 0 0 0...025017061

_Methods on pretrained GAN:_

DisCoLDGSDSCF (ours) 00..999999111...000000 ± ± ± ± ± 2 6.. 0 0 05486...000000ee − − 45 00000.....852855873932871 ± ± ± ± ± 0 0 0 0 0.....039074036018047 00000.....913987999944991 ± ± ± ± ± 0 0 0 0 0.....063028001044022 00000.....805877951788929 ± ± ± ± ± 0 0 0 0 0.....064031021091065 00000.....535530669605651 ± ± ± ± ± 0 0 0 0 0.....057015033061043 00000.....371391523465502 ± ± ± ± ± 0 0 0 0 0.....030039056036042

_Methods on pretrained VAE:_

DisCoLD (ours) 0.9990.951 ± ± 5. 042.074e − 5 00..711761 ± ± 0 0..085114 0.0999.602 ± ± 8 0.9.e196 − 4 00..437956 ± ± 0 0..188041 00..266411 ± ± 0 0..068034 00..391242 ± ± 0 0..075010

_Methods on pretrained Flow:_

DisCoLD (ours) 01..92200 ± ± 0 0..000000 00..633880 ± ± 0 0..000000 00..699860 ± ± 0 0..000000 00..597854 ± ± 0 0..000000 00..266538 ± ± 0 0..000000 00..486242 ± ± 0 0..000000

Table 9: Comparisons of the β-VAE and FactorVAE scores on the Shapes3D dataset (mean ±
variance). A higher mean indicates a better performance.

We also provide an additional experiment on Noisy-DSprites dataset. We compare DisCo with
_β-TCVAE (the best typical method) and CF (the best discovering-based method) in terms of MIG_
and DCI metrics.

**Method** _β-TCVAE_ CF DisCo (GAN)

DCI 0.088 ± 0.049 0.027 ± 0.016 **0.120 ± 0.059**
MIG 0.046 ± 0.031 0.020 ± 0.015 **0.104 ± 0.030**

Table 10: Comparisons on Noisy-DSprites.


-----

C LATENT TRAVERSALS

In this section, we visualize the disentangled directions of the latent space discovered by DisCo
on each dataset. For Cars3D, Shapes3D, Anime and MNIST, the iamge resolution is 64 × 64. For
FFHQ, LSUN cat and LSUN church, the image resolution is 256 × 256. Besides StyleGAN2, we
also provide results of Spectral Norm GAN (Miyato et al., 2018) [4] on MNIST (LeCun et al., 2010)
and Anime Face (Jin et al., 2017) to demonstrate that DisCo can be well generalized to other types
of GAN.

(a) StyleGAN2 Cars3D – Azimuth

(b) StyleGAN2 Cars3D – Yaw

(c) StyleGAN2 Cars3D – Type

Figure 15: Examples of disentangled directions for StyleGAN2 on Cars3D discovered by DisCo.

[4https://github.com/anvoynov/GANLatentDiscovery](https://github.com/anvoynov/GANLatentDiscovery)


-----

(a) StyleGAN2 Shapes3D – Wall Color

(b) StyleGAN2 Shapes3D – Floor Color

(c) StyleGAN2 Shapes3D – Object Color

(d) StyleGAN2 Shapes3D – Pose

Figure 16: Examples of disentangled directions for StyleGAN2 on Shapes3D discovered by DisCo.
As shown in (b), the latent space has local semantic.


-----

(a) StyleGAN2 LSUN Cat – Black

(b) StyleGAN2 LSUN Cat – Eye

(c) StyleGAN2 LSUN Cat – Hair

Figure 17: Examples of disentangled directions for StyleGAN2 on LSUN Cat discovered by DisCo.


-----

(a) StyleGAN2 LSUN Church – Hue

(b) StyleGAN2 LSUN Church – Backgroud Removal

(c) StyleGAN2 LSUN Church – Sky

Figure 18: Examples of disentangled directions for StyleGAN2 on LSUN Church discovered by
DisCo.


-----

(a) StyleGAN2 FFHQ – Oldness

(b) StyleGAN2 FFHQ – Hair

(c) StyleGAN2 FFHQ – Race

Figure 19: Examples of disentangled directions for StyleGAN2 on FFHQ discovered by DisCo.


-----

(d) StyleGAN2 FFHQ – Overexpose

(e) StyleGAN2 FFHQ – Pose

(f) StyleGAN2 FFHQ – Smile

Figure 20: Examples of disentangled directions for StyleGAN2 on FFHQ discovered by DisCo.


-----

(a) SNGAN Anime – Tone

(b) SNGAN Anime – Skin

(c) SNGAN Anime – Pose

Figure 21: Examples of disentangled directions for SNGAN on Anime discoverd by DisCo.


-----

(d) SNGAN Anime – Naturalness

(e) SNGAN Anime – Glass

(f) SNGAN Anime – Whiteness

Figure 22: Examples of disentangled directions for SNGAN on Anime discovered by DisCo.


-----

(a) SNGAN MNIST – Angle

(b) SNGAN MNIST – Width

(c) SNGAN MNIST – Thickness

Figure 23: Examples of disentangled directions for SNGAN on MNIST discovered by DisCo.


-----

(a) VAE Shapes3D – Wall Color

(b) VAE Shapes3D – Floor Color

(c) VAE Shapes3D – Object Color

Figure 24: Examples of disentangled directions for VAE on Shapes3D discovered by DisCo.


-----

(d) VAE Shapes3D – Pose

(e) VAE Shapes3D – Height

(f) VAE Shapes3D – Object Shape

Figure 25: Examples of disentangled directions for VAE on Shapes3D discovered by DisCo.


-----

(a) Glow Shapes3D – Wall Color

(b) Glow Shapes3D – Floor Color

(c) Glow Shapes3D – Object Color

Figure 26: Examples of disentangled directions for Glow on Shapes3D discovered by DisCo.


-----

D AN INTUITIVE ANALYSIS FOR DISCO

DisCo works by contrasting the variations resulted from traversing along the directions provided
by the Navigator. Is the method sufficient to converge to the disentangled solution? Note that it is
very challenging to answer this question. To our best knowledge, for unsupervised disentangled
representation learning, there is no sufficient theoretical constraint to guarantee the convergence to
a disentangled solution Locatello et al. (2019). Here we provide an intuitive analysis for DisCo
and try to provide our thoughts on how DisCo find the disentangled direction in the latent space,
which is supported by our observations on pretrained GAN both quantitatively and qualitatively. The
intuitive analysis consists of two part: (i) The directions that can be discovered by DisCo have
different variation patterns compared to random directions. (ii) DisCo hardly converges to the an
entangled solution.


D.1 WHAT KIND OF DIRECTIONS DI SCO CAN CONVERGE TO?

Discovered

Discovered

Random

Random

Traversal images Floor color Wall color

Figure 27: Visualization of the latent space of GAN on Shapes3D with discovered directions from
DisCo or random sampled directions. We traverse the latent space with a range of [−25, 25] and a
step of 0.5, which results in 10, 000 (100 × 100) samples.

First, we visualize the latent space and show that there are some variation patterns in the latent space
for disentangled factors. We design the following visualization method. Given a pretrained GAN
and two directions in the latent space, we traverse along the plane expanded by the two directions
to generate a grid of images. The range is large enough to include all values of these disentangled
factors, and the step is small enough to obtain a dense grid. Then, we input these images into an
encoder that trained with ground truth factors labels. We get a heatmap of each factor (the value is
the response value corresponding dimension of the factor). In this way, we can observe the variation
pattern that emerged in the latent space.


-----

We take the pretrained StyleGAN on Shapes3D (synthetic) and FFHQ (real-world). For Shapes3D,
we take background color and floor color as the two factors (since they refer to different areas in the
image, these two factors are disentangled). For FFHQ, we take smile (mouth) and bald (hair) as the
two factors (disentangled for referring to different areas). We then choose random directions and the
directions discovered by DisCo. The results are shown in Figure 27 and Figure 28.

We find a clear difference between random directions and directions discovered by DisCo. This is
because DisCo can learn the directions by separating the variations resulted from traversing along
with them. However, not all directions can be separated. For those directions in which the variations
are not able to be recognized or clustered by the encoder E, it is nearly impossible for DisCo
to converge to them. Conversely, for those directions that can be easily recognized and clustered,
DisCo will converge to them with a higher probability. From the following observations, we find
that the variation patterns resulting from the directions corresponding to disentangled factors are
easily recognized and clustered.

Discovered

Discovered

Random

Random

Traversal images Smile Bald

Figure 28: Visualization of the latent space of GAN on FFHQ with discovered directions from
DisCo or random sampled directions. We traverse the latent space with a range of [−15, 15] and a
step of 0.3, which results in 1, 0000 (100 × 100) samples. For better visualization, we only present
the traversal results with a step of 5 (10 × 10).

D.2 WHY DISCO HARDLY CONVERGES TO THE ENTANGLED CASES?


In the previous section, we show that DisCo can discover the directions with distinct variation
patterns and exclude random directions. Here we discuss why DisCo can hardly converge to the
following entangled case (trivial solution based on disentangled one). For example, suppose there is
an entangled direction of factors A and B (A and B change with the same rate when traversing along
with it) in the latent space of generative models, and DisCo can separate the variations resulting


-----

Figure 29: Sketch map of latent space of generative models.

from the direction of A and the entangled direction. In that case, DisCo has no additional bias to
update these directions to converge to disentangled ones.

In the following text, for ease of referring to, we denote the entangled direction of factors A and B
(A and B change with the same rate when traversing along with it) as A+B direction, and direction
of factor A (only A change when we traverse along with it). The reasons for why DisCo is hardly
converged to the case of A and A+B are two-fold:

(i) Our encoder is a lightweight network (5 CNN layers + 3 FC layers). It is nearly impossible for it
to separate the A and A+B directions.

(ii) In the latent space of the pretrained generative models, the disentangled directions (A, B) are
consistent at different locations. In contrast, the entangled directions (A+B) are not, as shown in
Figure 29.

We conduct the following experiments to verify them. For (i), we replace our encoder in DisCo
with a ResNet-50 and train DisCo from scratch on the Shapes3D dataset. The loss, MIG, and DCI
are presented in Table 11. The trivial solution is possible when the encoder is powerful enough to fit
the A and A+B directions to “become orthogonal”. With this consideration, in DisCo we adopt a
lightweight encoder to avoid this issue.

Our Encoder ResNet-50

Param 4M 25.5M
Loss (↓) 0.550 0.725
MIG (↑) 0.562 0.03
DCI (↑) 0.736 0.07

Table 11: Ablation study on encoder of DisCo.

For (ii), as the sketch Figure 29 demonstrates, the disentangled directions (”A“- blue color or “B”green color) are consistent, which is invariant to the location in the latent space, while the entangled
directions (”A+B“- red color) is not consistent on different locations. The fundamental reason is
that: the directions of the disentangled variations are invariant with the position in the latent space.
However, the “rate” of the variation is not. E.g., at any point in the latent space, going “up” constantly
changes the camera’s pose. However, at point a, going “up” with step 1 means rotating 10 degrees.
At point b, going “up” with step 1 means rotating 5 degrees. When the variation “rate” of “A” and
“B” are different, the “A+B” directions at different locations are not consistent.

Based on the different properties of disentangled and entangled directions in the latent space, DisCo
can discover the disentangled directions with contrastive loss. The contrastive loss can be understood
from the clustered view (Wang & Isola, 2020; Li et al., 2021b). The variations from the disentangled
directions are more consistent and can be better clustered compared to the variations from the


-----

entangled ones. Thus, DisCo can discover the disentangled directions in the latent space and learn
disentangled representations from images. We further provide the following experiments to support
our above analysis.

D.2.1 QUANTITATIVE EXPERIMENT

We compare the losses of three different settings:

-  A: For a navigator with disentangled directions, we fix the navigator and train the encoder
until convergence.

-  A + B: For a navigator with entangled directions (we use the linear combination of the
disentangled directions to initialize the navigator), we fix it and train the encoder until
convergence.

-  A + B → _A: After A+B is convergent, we update both the encoder and the navigator until_
convergence.

The Contrastive loss after convergence is presented in Table 12.

_A_ _A + B_ _A + B →_ _A_

Loss 0.5501 0.7252 0.5264

Table 12: Loss comparison on different settings.

The results show that: (i) The disentangled directions (A) can lead to lower loss and better performance than entangled directions (A+B), indicating no trivial solution. (ii) Even though the encoder
with A+B is converged, when we optimize the navigator, gradients will still backpropagate to the
navigator and converge to A.

D.2.2 QUALITATIVE EXPERIMENT

We visualize the latent space of GAN in Figure 30 to verify the variation “rate” in the following
way: in the latent space, we select two ground truth disentangled directions: floor color (A) and
background color (B) obtained by supervision with InterFaceGAN (Shen et al., 2020), we conduct
equally spaced sampling along the two disentangled directions: A (labeled with green color variation),
B (labeled with gradient blue color) and composite direction (A+B, labeled with gradient red color)
as shown in Figure 30 (a).

Then we generate the images (also include other images on the grid as shown in Figure 30 (b) ),
and feed the images in the bounding boxes into a “ground truth” encoder (trained with ground truth
disentangled factors) to regress the “ground truth” disentangled representations of the images.

Figure 30: Visualization of GAN latent space.

In Figure 30 (c), the points labeled with green color are well aligned with the x-axis indicating only
floor color change, points labeled with blue variation are well aligned with the y-axis indicating only


-----

background color change. However, the points labeled with red color are NOT aligned with any line,
which indicates the directions of A+B are not consistent. Further, the variation “rate” is relevant to
the latent space locations for the two disentangled directions. This observation well supports our idea
shown in Figure 29. The different properties between disentangled and entangled directions enable
DisCo to discover the disentangled directions in the latent space.

E EXTENSION: BRIDGE THE PRETRAINED VAE AND PRETRAINED GAN

Researchers are recently interested in improving image quality given the disentangled representation
generated by typical disentanglement methods. Lee et al.(Lee et al., 2020) propose a post-processing
stage using a GAN based on disentangled representations learned by VAE-based disentanglement
models. This method scarifies a little generation ability due to an additional constraint. Similarly,
Srivastava et al. (Srivastava et al., 2020) propose to use a deep generative model with AdaIN (Huang
& Belongie, 2017) as a post-processing stage to improve the reconstruction ability. Following this
setting, we can replace the encoder in DisCo (GAN) with an encoder pretrained by VAE-based
disentangled baselines. In this way, we can bridge the pretrained disentangled VAE and pretrained
GAN, as shown in Figure 31. Compared to previous methods, our method can fully utilize the
state-of-the-art GAN and the state-of-the-art VAE-based method and does not need to train a deep
generative model from scratch.

_β-TCVAE_

DisCo (with a pretrained encoder)

Figure 31: DisCo with a pretrained encoder allows synthesizing high-quality images by bridging
pretrained β-TCVAE and pretrained StyleGAN2.


-----

F DISCUSSION ON RELATION BETWEEN BCELOSS AND NCELOSS

We would like to present a deep discussion on the relation between the BCELoss Llogits and NCELoss
_NCE. This discussion is related to the NCE paper Gutmann & Hyvärinen (2010), and InfoNCE_
_L_
paper van den Oord et al. (2018). The discussion is as following: (i) we first provide a formulation
of a general problem and get two objectives, L1 and L2, and L1 is the upper bound of L2. (ii)
Following Gutmann & Hyvärinen (2010), we show that L1 is aligned with LBCE under the setting
of Gutmann & Hyvärinen (2010). (iii) Following van den Oord et al. (2018), we prove L2 is aligned
with LNCE under the setting of van den Oord et al. (2018). (iii) We discuss the relation between
these objectives and the loss in our paper.

**Part I.** Assume we have S observations {xi}i[S]=1 [from a data distribution][ p][(][x][)][, each with a label]
_Ci_ 0, 1 . The we denote the posterior probabilities as p[+](x) = p(x _C = 1) and p[−](x) =_
_p(x ∈{|C = 0)}._ _|_

We define two objectives as follow:


_Ci log P_ (Ci = 1|xi) + (1 − _Ci) log P_ (Ci = 0|xi), (10)
_i=1_

X


_L1 = −_


and


_Ci log P_ (Ci = 1|xi) (11)
_i=1_

X


_L2 = −_


Since − [P]i[S]=1[(1][ −] _[C][i][) log][ p][(][C][i][ = 0][|][x][i][)][ ≥]_ [0][, we have]

1 2. (12)
_L_ _≥L_

_L1 is the upper bound of L2._

This a general formulation of a binary classification problem. In the context of our paper, we
have a paired observation xi : (q, ki), with q as the query, and the key ki is either from a positive
key setM = S { −kj[+]N[}]. Andj[N]=1 [or as negative key set] Ci is assigned as: _[ {][k]m[−]_ _[}][M]m=1_ [(i.e.,][ {][k][i][}]i[N]=1[+][M] = {kj[+][}]j[N]=1 S{km− _[}][M]m=1[), where]_

1, _ki_ _kj[+]_ _j=1_

_Ci =_ _∈{_ _[}][N]_ (13)

( 0, _ki_ _km[−]_ _[}][M]m=1_

_∈{_

In our paper, we have h(x) = exp(q · k/τ ).

**Part II.** In this part, following Gutmann & Hyvärinen (2010), we show that L1 is aligned with
_Llogits (Equation 3 in the main paper) under the setting of Gutmann & Hyvärinen (2010). Following_
Gutmann & Hyvärinen (2010)), we assume the prior distribution P (C = 0) = P (C = 1) = 1/2,
according to the Bayes rule, we have


_p(x_ _C = 1)P_ (C = 1)
_P_ (C = 1 _x) =_ _|_
_|_ _p(x_ _C = 1)P_ (C = 1) + p(x _C = 0)P_ (C = 0) [=]

_|_ _|_

And P (C = 0|x) = 1 − _P_ (C = 1|x).

On the other hand, we have a general form of BCELoss, as


1

_._ (14)

1 + _[p]p[−][+]([(]x[x])[)]_


_Ci log σ(q · ki/τ_ ) + (1 − _Ci) log(1 −_ _σ(q · ki/τ_ )), (15)
_i=1_

X


_LBCE = −_


where σ(·) is the sigmoid function. We have


1 _,_ (16)

_h(x)_


_σ(q · k/τ_ ) =


1 + exp( _q_ _k/τ_ ) [=]
_−_ _·_


1 +


1 +


exp(q·k/τ )


-----

From Gutmann & Hyvärinen (2010) Theorem 1, we know that when LBCE is minimized, we have

_h(x) =_ _[p][+][(][x][)]_ (17)

_p[−](x)_ _[.]_

Thus, we know the BCELoss LBCE is a approximation of the objective L1.

**Part. III** Following van den Oord et al. (2018), we prove L2 is aligned with LNCE (Equation 2 in
the main paper) under the setting of van den Oord et al. (2018)

From the typical contrastive setting (one positive sample, others are negative samples, following
van den Oord et al. (2018)), we assume there is only one positive sample, others are negatives in
_{xi}i[S]=1[. Then, the probability of][ x][i][ sample from][ p][+][(][x][)][ rather then][ p][−][(][x][)][ is as follows,]_

_P_ (Ci = 1 _xi) =_ _Sp[+](xi)Πl≠_ _ip[−](xl)_ = _Spp[−][+]((xpx[+]ii))(xj_ ) (18)
_|_
_j=1_ _[p][+][(][x][j][)Π][l][̸][=][i][p][−][(][x][l][)]_ _j=1_ _p[−](xj_ )
P P

From van den Oord et al. (2018), we know that when minimize Equation 11, we have h(x) =
exp(q _k/τ_ ) _p_ (x) [. In this case, we get the form of][ L][NCE][ as]
_·_ _∝_ _[p]−[+][(][x][)]_


_S_

exp(q _ki/τ_ )
_Ci log_ _S_ _·_ (19)
_i=1_ _j=1_ [exp(][q][ ·][ k][j][/τ] [)]

X

P


_LNCE = −_

_LNCE is a approximate of L2._


**Part. IV** When generalize the contrastive loss into our setting (N positive samples, M negative
samples). The BCELoss (Equation 15) can be reformulated as

The BCELoss (Equation 15) can be reformulated as


log σ(q · kj[+][/τ] [)][ −]
_j=1_

X


log(1 − _σ(q · km[−]_ _[/τ]_ [))][.] (20)
_m=1_

X


ˆ
_LBCE = −_


Similarly, the NCEloss (Equation 19) can be reformulated as

ˆNCE = _N_ log _Mexp(+N_ _q · kj[+][/τ]_ [)] (21)
_L_ _−_ _j=1_ _s=1_ exp(q _ks/τ_ )

X _·_

P

_LˆBCE is aligned with Llogits (Equation 3 in our main paper), and_ _L[ˆ]NCE is aligned with LNCE_
(Equation 2 in the main paper).

Now we have L1 (approximated by LBCE) is the upper bound of L2 (approximated by LNCE).
However, as you may notice, the assumptions we made in Part II and Part III are different, one is
_P_ (C = 0) = P (C = 1), the other one is only one positive sample, others are negative. Also the
extent to our situation is more general case (N positives, others are negatives).

However, they have the same objective, which is by contrasting positives and negatives, we can use
_h(x) = exp(q · k/τ_ ) to estimate p[+]/p[−]. We can think the h(x) as a similarity score, i.e. if q and
_k are from a positive pair (they have the same direction in our paper), h(x) should be as large as_
possible (p[+]/p[−] _> 1) and vice versa. From this way, we can learn the representations (q, k) to reflect_
the image variation, i.e., similar variations have higher score h(x), while different kinds of variation
have lower score h(x). Then with this meaningful representation, in the latent space, can help to
discover the directions carrying different kinds of image variation. This is an understanding, from a
contrastive learning view, of how our method works.


-----

