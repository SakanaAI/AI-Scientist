# DECOUPLED KERNEL NEURAL PROCESSES: NEU## RAL NETWORK-PARAMETERIZED STOCHASTIC PRO- CESSES USING EXPLICIT DATA-DRIVEN KERNEL

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Neural Processes (NPs) are a class of stochastic processes parametrized by neural
networks. Unlike traditional stochastic processes (e.g., Gaussian processes), which
require specifying explicit kernel functions, NPs implicitly learn kernel functions
appropriate for a given task through observed data. While this data-driven learning
of stochastic processes has been shown to model various types of data, the current
NPsâ€™ implicit treatment of the mean and the covariance of the output variables
limits its full potential when the underlying distribution of the given data is highly
complex. To address this, we introduce a new class of neural stochastic processes,
Decoupled Kernel Neural Processes (DKNPs), which explicitly learn a separate
mean and kernel function to directly model the covariance between output variables in a data-driven manner. By estimating kernel functions with cross-attentive
neural networks, DKNPs demonstrate improved uncertainty estimation in terms of
conditional likelihood and diversity in generated samples in 1-D and 2-D regression tasks, compared to other concurrent NP variants. Also, maintaining explicit
kernel functions, a key component of stochastic processes, allows the model to
reveal a deeper understanding of underlying distributions.

1 INTRODUCTION

Neural processes (NPs) (Garnelo et al., 2018a;b) are a class of stochastic processes parametrized by
neural networks. By embracing statistical properties in stochastic processes, NPs can effectively estimate the uncertainty of underlying distributions of functions with a set of realizations and their data
points. Different from traditional stochastic processes (e.g., Gaussian processes (GP) (Rasmussen
& Williams, 2006)), NPs learn data-driven stochastic processes without a need to specify or keep
an explicit form of kernel functions. As a result of their simplicity and flexibility, there have been
numerous efforts to further develop improved variants of NPs (Kim et al., 2019; Lee et al., 2020;
Gordon et al., 2020) and apply them to various downstream tasks (Singh et al., 2019; Requeima
et al., 2019).

Though significant progress has been made in NPs, the current architectures of NPs either fails to
capture output dependencies as in Conditional NPs (Garnelo et al., 2018a; Gordon et al., 2020), or
indirectly capture the full stochasticity present in the traditional stochastic processes. For instance,
different from GPs, conventional NPs reserve stochasticity in a global latent variable and output
variables separately. The output variables estimate point-wise uncertainty, which corresponds to the
diagonal elements of a kernel matrix. Similarly, the global latent variable takes charge of the functional uncertainty and diversity, represented by the full covariance matrix in GPs. Due to this inductive bias of conventional NPs, the role of estimating functional stochasticity is mainly assigned to a
fixed-length vector (i.e. the global latent variable), and consequently, capturing the underlying distributions can be restricted in complex scenarios (e.g., variable relationships are periodic or abruptly
changing at a certain point). Although several approaches attempt to alleviate the problem by introducing attention (Kim et al., 2019) and bootstrapping (Lee et al., 2020) on top of conventional NPs,
the problem still exists as the architectural limitation (i.e. implicit modeling of the mean and covariance) has not been addressed directly. Besides this, as NPs implicitly learn the kernel functions
inside the model, the interpretability of kernels such as in GPs (Lloyd et al., 2014) is diminished.


-----

ANP Ours Ground Truth Ours

Prediction Covariance matrix


Figure 1: (Left) Comparison of predictions given context points (black dot) by the Attentive NP
(ANP) and DKNP (Ours) after learning samples generated from a periodic kernel with fixed hyperparameters. The mean (blue curve) and sigma (shaded blue area) predicted from the DKNP better
represent the data, including target points (red dot) compared to the ANP. (Right) Visualization of
kernel functions learned by the DKNP. As a result of kernel learning in the DKNP, the underlying
prior distribution of data can be inferred (periodic kernel in this case).

To address this concern, we propose Decoupled Kernel Neural Processes (DKNPs), a new class
of neural stochastic processes that explicitly learn a separate mean and kernel function to directly
model the covariance between output variables in a data-driven manner. Our experiments in 1-D and
2-D regression tasks reveal that the DKNP outperforms concurrent NP variants in terms of predictive
likelihood, better global coherence of generated samples, and improved interpretability via explicitly
learned kernels.

2 BACKGROUND

2.1 NEURAL PROCESS

Given a stochastic process sample consisting of n points, let us denote the input and output as
_X = {xi}i[n]=1_ [and][ Y][ =][ {][y][i][}]i[n]=1[, respectively, where][ x][i][ âˆˆ] [R][d][x][ and][ y][i][ âˆˆ] [R][d][y] [. For a set of target]
input XT = **_xi_** _i_ _T_ _X, NPs model the conditional distribution of target outputs YT conditioned_
on the context set { _} (âˆˆX âŠ‚C, YC) =_ (xi, yi) _i_ _C using a factorized Gaussian distribution:_
_{_ _}_ _âˆˆ_


log p(yi **_xi, XC, YC)._** (1)
_|_

XiâˆˆT


log p(YT |XT, XC, YC) =


For obtaining the predictive distribution log p(yi **_xi, XC, YC), NPs use an encoder-decoder architec-_**
_|_
ture that ensures the permutation invariance of the predictions of the target points given the context
set (XC, YC).

Following Kim et al. (2019) and Lee et al. (2020), we consider the NP encoder consisting of two
separate paths, namely the deterministic path and the latent path. For the deterministic path, fÎ¸ represents each context points in (xi, yi) _i_ _C as ri_ R[d][r], i.e., ri = fÎ¸(xi, yi). Then, we aggregate
the riâ€™s by averaging them across all context points, { _}_ _âˆˆ_ _âˆˆ rC =_ _n1c_ _i_ _C_ **_[r][i][ where][ n][c][ =][ |][C][|][. This vector]_**

_âˆˆ_
**_rC_** R[d][r] is the summarized representation of context points, and it is permutation invariant over
the order of âˆˆ (xi, yi) (XC, YC). P
_âˆˆ_

The latent path of the NP encoder operates in a similar fashion to the deterministic path, i.e.,
**_eC =_** _n1c_ _i_ _C_ **_[e][i][ where][ e][i][ =][ g][Ï•][([][x][i][;][ y][i][])][. Unlike the deterministic path, however, the la-]_**

_âˆˆ_
tent path uses stochastic layers for obtaining a distribution of the latent variable z R[d][z] ;

P _âˆˆ_

_q(z|eC) = N_ (z; Âµz, Ïƒz[2][)][ where][ Âµ][z][,][ Ïƒ][z] [are the output of the additional fully-connected layer]
applied to eC. Finally, by concatenating these aggregated vectors rC, z with the target inputs
**_xi âˆˆ_** _XT, the decoder hÏˆ produces the predictive distribution p(yt|xt, rC, z) = N_ (y; Âµy, Ïƒy[2][)]
where (Âµy, Ïƒy) = hÏˆ(xt, rC, z). Note that the Ïƒy is used for capturing the point-wise uncertainty.
During the training phase, the parameters are learned by maximizing the evidence lower bound of
log2014) as follows: p(YT |XT, XC, YC) via the reparametrization trick (Kingma & Welling, 2014; Rezende et al.,


-----

|Col1|ğ’‰ğ‘¡1 ğ’‰ğ‘1 ğ’‰ğ‘1 ğ’‰ğ‘¡2 ğ’‰ğ‘2 ğ’‰ğ‘2|
|---|---|


ğ’™ğ‘1 ğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–â„ğ‘’ğ‘ğ‘‘

ğ‘€ğ¿ğ‘ƒğœƒ ğ’†ğ‘1 ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ âˆ’ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›

ğ’šğ‘1 ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦ ğ¾ğ‘’ğ‘¦ ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ ğ‘€ğ¿ğ‘ƒğœ“ ğ’šà·ğ‘¡1 ğğ’š

ğ’™ğ‘2 ğ’‰ğ‘¡1 ğ’‰ğ‘1 ğ’†ğ‘1 ğ‘€ğ¿ğ‘ƒğœ“ ğ’šà·ğ‘¡2

ğ’šğ‘2 ğ‘€ğ¿ğ‘ƒğœƒ ğ’†ğ‘2 ğ’‰ğ‘¡2 ğ’‰ğ‘2 ğ’†ğ‘2

ğ’™ğ‘1 ğ‘€ğ¿ğ‘ƒğœ™ ğ’‰ğ‘1 ğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–â„ğ‘’ğ‘ğ‘‘

ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ âˆ’ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›

ğ’™ğ’™ğ‘ğ‘¡12 ğ‘€ğ¿ğ‘ƒğ‘€ğ¿ğ‘ƒğœ™ğœ™ ğ’‰ğ’‰ğ‘ğ‘¡12 ğ‘„ğ‘¢ğ‘’ğ‘Ÿğ‘¦ğ’‰ğ‘¡1 ğ¾ğ‘’ğ‘¦ğ’‰ğ‘1 ğ‘‰ğ‘ğ‘™ğ‘¢ğ‘’ğ’‰ğ‘1 ğ‘€ğ¿ğ‘ƒğ‘€ğ¿ğ‘ƒğœ”ğœ” ğ’˜ğ’˜ğ‘¡ğ‘¡12 ğ’˜ğ’˜ğ‘¡ğ‘¡12 ğ‘Šâ‹¯â‹¯ ğ‘Šğ‘Š[âŠ¤] Î£ğ’š

ğ’™ğ‘¡2 ğ‘€ğ¿ğ‘ƒğœ™ ğ’‰ğ‘¡2 ğ’‰ğ‘¡2 ğ’‰ğ‘2 ğ’‰ğ‘2 : Concat


Figure 2: Model architecture of DKNPs. DKNPs estimate the predictive distribution (y; Âµy, Î£y)
_N_
by using two attention-based deterministic paths using Multihead Cross-Attention (MCA) to model
the mean vector (upper path) and the full covariance matrix (lower path).

log p(YT |XT, XC, YC) â‰¥ Eq(z|XT,YT )[log p(YT |XT, z)] âˆ’ _KL(q(z|eT )âˆ¥q(z|eC))_ (2)

where q(z|eT ) = q(et|XT, YT ) is a distribution of latent variable by encoding the set of target
points (XT, YT ) = (xi, yi) _i_ _T . This objective function consists of two parts: 1) the reconstruc-_
_{_ _}_ _âˆˆ_
tion loss for the target points and 2) the KL divergence term, minimizing the divergence between
two distributionsduring the training phase. The KL divergence term encourages two distributions inferred by the q(z|XT, YT ) and q(z|XC, YC). Note that, in practice, we assume that XC âŠ‚ _XT_
context sets and target sets to be similar, which is reasonable because two sets are generated from
the same function. Therefore, during the inference phase, the distribution of q(z _XC, YC) captures_
_|_
the functional stochasticity which is demonstrated with the coherent sample generation. It can be
thought that z learns to capture the correlation of output variables of the stochastic processes.


2.2 ATTENTIVE NEURAL PROCESSES
**Multihead Attention** Given n key-value pairs of matrices K âˆˆ R[n][Ã—][d][model] and V âˆˆ R[n][Ã—][d][model],
and m queries Q âˆˆ R[m][Ã—][d][model], the scaled dot product attention is formulated as:

_QK_ _âŠ¤_
Attention(Q, K, V ) = Softmax _âˆšdmodel_ _V_ _âˆˆ_ R[m][Ã—][d][model], (3)
 

where K, V, and Q are projected by learnable linear maps Ws[K][,][ W][ V]s [, and][ W][ Q]t [from the source][ S]
and target T .

The attention mechanism can be calculated from multiple subspaces, namely, multihead attention
(MHA) (Vaswani et al., 2017). Denoting a single attention head as Headi = Attention(Qi, Ki, Vi),
the aggregate attention from multiple subspaces can be expressed as:

MHA(Q, K, V ) = Concat(Head1, ..., HeadH )W _[O]_ _âˆˆ_ R[m][Ã—][d][model], (4)

where W _[O]_ is the learnable linear map for aggretating the subspaces.

**Attentive NPs** Attentive NPs (ANPs) leverage attention to resolve the underfitting issue in NPs.
Unlike NPs that produce a single variable rC from the deterministic path, ANPs utilize a queryspecific variable ri[âˆ—] [by applying the attention score][ a][i][ for each][ r][i][ during the aggregation of the]
deterministic path, formulated as ri[âˆ—] [=][ P]i _C_ _[a][i][ Â·][ r][i][. The attention-based aggregation of ANPs]_

_âˆˆ_
resembles how the GPs utilize the correlation to estimate the predictive distribution of the context
and the target points.

3 DECOUPLED KERNEL NEURAL PROCESSES

Decoupled Kernel Neural Processes use attention to explicitly learn a separate mean and kernel
function so as to directly model covariances between output variables with related input variables
in a data-driven fashion, which is contrary to conventional NPs that implicitly model the mean and
kernel function through the latent variable z. As shown in Figure 2, DKNPs estimate the predictive


-----

distribution as multivariate Gaussian (y; Âµy, Î£y) by using two attention-based deterministic paths
_N_
using Multihead Cross-Attention (MCA) to model the mean vector (upper path) and the full covariance matrix (lower path). Here, attention modules are extensively utilized as Le et al. (2018); Kim
et al. (2019) have demonstrated attention was helpful in achieving low predictive uncertainty near
the context points. With the predictive distribution (y; Âµy, Î£y), DKNPs are trained and evaluated
_N_
based on the function likelihood.

The core design of DKNPs is motivated by the predictive posterior distribution of GPs, where
_XT, XC, YC are used for deriving the posterior mean,[1]_ but only XT, XC for deriving the posterior covariance as follows:

**GP:** _p(YT |XT, XC_ _, YC_ ) = N _YT ; Î£X[âŠ¤]C_ _,XT_ [Î£]X[âˆ’][1]C _,XC_ _[Y][C]_ _[,][ Î£][X]T_ _[,X]T_ _[âˆ’]_ [Î£]X[âŠ¤]C _,XT_ [Î£]X[âˆ’][1]C _,XC_ [Î£][X]C _[,X]T_ _,_
 (5)

**DKNP:** _p(YT |XT, XC_ _, YC_ ) = N _YT ; MLPÏˆ â—¦_ MCA MLPÎ¸(XC _, YC_ ), MLPÏ•(XT, XC ) _,_
   

MLPÏ‰ â—¦ MCA MLPÏ•(XT, XC ) _,_ (6)
  []

where, in contrast to GPs, DKNPsâ€™ covariance is learned via attention.[2] This decoupled process
allows DKNPs to explicitly learn the prior of the given dataset, and thus act as a true generative
process, ensuring the global consistency of all points in the given stochastic process samples. Unlike
DKNPs, NPs pack all information (XC, YC, XT ) into latent variables to derive both mean and covariance, thus inherently becoming a conditional process that requires a sufficient amount of context
points, unable to explicitly learn a prior.

Specifically, the DKNPs pass each context point (xi, yi) _i_ _C, the concatenation of xi and yi, to_
_{_ _}_ _âˆˆ_
MLPÎ¸ and represent it as ei. Similarly, we produce the representation vector of xi, hi, using another
MLPÏ• for all inputs **_xi_** _i_ _C_ _T . Then, ei and hi are passed to the MCA module to create the mean_
_{_ _}_ _âˆˆ_ _âˆª_
vector Âµy. All heads in the MCA module perform cross-attention, Q = **_hi_** _i_ _T, K =_ **_hi_** _i_ _C,_
_{_ _}_ _âˆˆ_ _{_ _}_ _âˆˆ_
and V = {ei}iâˆˆC. We adopted the architecture of MCA used in image transformers (Parmar et al.,
2018), where the original query vectors are added to the output from the MCA through the residual
path. This allows to do inference the output distributions without context points, which can be considered as prior distributions of DKNPsâ€”the learned prior of DKNP. After the MCA, the last MLPÏˆ
generates the predictive mean vector Âµy for each data point. Intuitively, this can be interpreted as
predicting the target mean based on the context and the correlation, which resembles the estimation
of the predictive distribution in GPs and ANPs.

Different from the NPs, DKNPs explicitly capture the correlation between the output variables using
another multihead cross-attention (MCA), where Q = **_hi_** _i_ _T, K =_ **_hi_** _i_ _C, and V =_ **_ei_** _i_ _C._
_{_ _}_ _âˆˆ_ _{_ _}_ _âˆˆ_ _{_ _}_ _âˆˆ_
Then MLPgenerate the covariance matrixÏ‰ produces the representation vector Î£ = WW _[âŠ¤], where wi âˆˆ Wi,R: =[d]w_ [for each position, which are combined to] wi and Î£ij = kernel(xi, xj) = wi[âŠ¤][w][j][.]
One might consider using self-attention to let the model learn the correlation between all data points.
However, the self-attention module on only X as inputs receives no indication of context and target
points and therefore fails to reduce the uncertainty near the points that have high confidence (e.g.,
context points). Also, the interaction between the target points through self-attention does not guarantee consistency under the marginal distribution of target points when the context points are given.
Lastly, it is also important to note that the representation h is shared when modeling both the mean
and the covariance. This motivation is drawn from Equation 5 that the calculation of the mean is
also based on the kernel matrices, Î£[âŠ¤]XC _,XT_ [and][ Î£]X[âˆ’][1]C _,XC_ [.]

To train the DKNPs, the obtained mean vector Âµ and the covariance matrix Î£ act as parameters of a
predictive distribution N (Y ; ÂµY, Î£Y ). Instead of maximizing the lower bound of the log-likelihood
as in most NP models, the training objective of DKNPs is to maximize the tractable log-likelihood
of the Gaussian as follows:

log p(YT |XT, XC, YC) = log N (YT ; ÂµY, Î£Y ) where Î£Y = WW _[âŠ¤]_ _._ (7)

1We follow the typical GP formulation where the mean function is set to zero.
2Note that DKNPs assume C âŠ‚ _T during training, which makes Eq. 5 and Eq. 6 technically different. This_
difference comes from DKNPs using all data points for better learning the kernels, unlike fixed-kernel GPs.
DKNPs and GPs, however, are different methods and superior empirical performance led to the current design
choice.


-----

Table 1: Results (log-likelihood) of the NP variants and DKNP in multiple 1D regression tasks

Methods RBF Periodic MatÂ´ern 5/2


CNP 0.695 (Â±0.010) -0.328 (Â±0.032) 0.558 (Â±0.006)
NP 0.577 (Â±0.015) -0.619 (Â±0.005) 0.417 (Â±0.009)
BNP 0.754 (Â±0.004) -0.018 (Â±0.042) 0.617 (Â±0.006)
ANP 1.086 (Â±0.001) 0.831 (Â±0.011) 1.020 (Â±0.000)
BANP 1.084 (Â±0.001) 0.821 (Â±0.018) 1.018 (Â±0.001)

Ours **1.109 (Â±0.001)** **0.941 (Â±0.006)** **1.039 (Â±0.004)**

RBF Periodic MatÃ©rn

ANP

Ours

Figure 3: Comparison of predictions by ANP and DKNP. The dark blue line indicates the mean, and
the light blue shade indicates one standard deviation range.


Although the proposed objective function is equivalent to CNPâ€™s (Garnelo et al., 2018a), modeling the correlation between output variables for capturing functional stochasticity shares the same
motivation of NPs, thus DKNP being one of NP variants.

4 EXPERIMENTS


We compare DKNP with diverse NP variants such as Conditional NP (CNP)(Garnelo et al., 2018a),
NP, Bootstrapping NP (BNP)(Lee et al., 2020), ANP, and Bootstrapping ANP (BANP) on both 1D
and 2D regression tasks. For a fair comparison, all NP variants use two paths for context encoding.
CNP, BNP, and BANP have two deterministic paths, and NP and ANP have one deterministic and
one stochastic path. Also, ANP and BANP have additional self-attention in both stochastic and deterministic paths. Following Lee et al. (2020), BNP and BANP were trained with 50 bootstrap context
samples and tested with 4 samples. Note that we mostly followed the same hyperparameter setup
used in Lee et al. (2020) and the details are described in Appendix A. Unlike the NP variants (except
or its slight modification, DKNP can directly evaluate it as well as maximize it during training.CNP) which are trained with the lower bound of conditional log-likelihood log p(YT |XT, XC, YC)
All results are also reported with the conditional log-likelihood to evaluate the methodsâ€™ ability to
model the given stochastic processes. Following (Le et al., 2018; Lee et al., 2020), we use importance weighting estimation (Burda et al., 2016) with 50 samples to evaluate the performance of NP
variants that utilize z.

4.1 1D REGRESSION WITH GAUSSIAN PROCESSES DATA


We first conduct basic evaluation of DKNP in comparison to NP variants by testing its ability to
model 1D stochastic process samples generated from Gaussian Processes of diverse kernels, namely
RBF, Periodic and MatÂ´ern 5/2. In this task, we randomly sampled input x from [-2, 2] and generated y from GP kernels where the hyperparameters were also sampled both during training and
testing. The number of context points nc is sampled from Unif (3, 25) and the target points is


-----

RBF Periodic MatÃ©rn

Ground Truth Ours Ground Truth Ours Ground Truth Ours


Figure 4: Visualization of the correlation matrices learned from the DKNP compared to the ground truths. We
take a Monte Carlo estimate (averaging 1,000 kernel functions of the same type) to visualize the ground truth
kernels for multiple hyperparameters at once.

Periodic - Periodic RBF - Periodic RBF - RBF

ANP

Ours

Figure 5: Comparison of predictions by ANP and DKNP. The dark blue line indicates the mean, and
the light blue shade indicates one standard deviation range.

_nc + Unif_ (3, 50 âˆ’ _nc). Details about the data generation process (kernel hyperparameters), the_
training process, and the evaluation process are in Appendix C.1.

The results in Table 1 show that DKNP can better model the given stochastic processes compared to
various NP variants in terms of the log-likelihood. Note that the attention mechanism dramatically
increases all methodsâ€™ ability to model GP samples, as can be seen by the clear divide between
CNP, NP, BNP and the rest. The true merit of DKNP, however, is its capability to better estimate the
uncertainty of the stochastic processes thanks to its explicit modeling of the full covariance matrix.

Figure 3, for example, clearly demonstrates the distinguishing feature of DKNP compared to previous NP variants, in this case ANP, which recorded the second highest likelihood in Table 1. In all
kernel types, ANP fails to contain the target points within one standard deviation range while DKNP
successfully captures the uncertainty in all unobserved points. Specifically, ANP is over-confident
when modeling kernel types RBF and MatÂ´ern 5/2, where only a local structure exists. For periodic
kernels, where both local and global structure exists, ANP fails to capture the underlying periodicity, due to its architecture that tries to estimate functional stochasticity with a fixed-size global latent
variable. Unlike ANP, however, DKNP can correctly model the whole stochastic process thanks to
its ability to explicitly model the correlations between all output variable.

Employing decoupled paths for modeling mean and covariance as in Figure 2, another advantage of
DKNP is improved interpretability, as we can explicitly check the learned prior by visualizing the
covariance matrix as described in Section 3. Such example is depicted in Figure 4, where we can
readily compare the learned prior with the ground truth prior for all three kernel types. Note that
DKNP not only model allows us to visually check the learned prior, but also demonstrate its ability
to accurately learn the ground truth kernels for all kernel types.


4.2 1D REGRESSION WITH CHANGE POINT

Next we test all methods in a more challenging setup, where the underlying dynamics of the stochastic process changes at midpoint. Specifically, we employ three different GP configurations: Periodic

-----

Kernel Generated Samples Kernel Generated Samples

Periodic - Periodic

RBF - Periodic

Gaussian Process Ours


Figure 6: Visualization of the learned correlation matrices and the sampled data for Periodic-Periodic and
RBF-Periodic compared to the ground truths.

Table 2: Results (log-likelihood) of the NP variants and DKNP for three different change point configurations.

Methods Periodic - Periodic Periodic - RBF RBF - RBF

CNP -0.423 (Â±0.016) 0.051 (Â±0.028) 0.966 (Â±0.006)
NP -0.531 (Â±0.022) -0.007 (Â±0.011) 0.872 (Â±0.017)
BNP -0.358 (Â±0.022) 0.105 (Â±0.023) 0.984 (Â±0.011)
ANP 0.602 (Â±0.017) 0.883 (Â±0.005) 1.142 (Â±0.001)
BANP 0.605 (Â±0.026) 0.877 (Â±0.010) 1.145 (Â±0.001)

Ours **0.892 (Â±0.003)** **0.955 (Â±0.013)** **1.260 (Â±0.000)**

Periodic, Periodic-RBF, and RBF-RBF. Each configuration uses one kernel type (with a fixed hyperparamter setup) up to point 0, then another kernel type afterwards. As the correlations between
output variables are more complex in this setup, we expect DKNP to demonstrate even more distinguishing performance than NP variants. The training and evaluation processes are the same as the
previous experiments in Section 4.1. See Appendix C.1 for data generation details.

The results in Table 2, as expected, show wider gaps between DKNP and NP variants for all three
configurations in terms of log-likelihood. Interestingly, unlike the the previous results (Table 1)
where DKNP marginally outperformed NP variants for RBF kernels (but beyond one standard deviation margin), the performance gap is more distinguished even for RBF-RBF configuration in this
setup. This observation indicates DKNPâ€™s ability to better capture complex correlations derived from
a combination of rather simpler kernels than NP variants.

Figure 5 demonstrates predictions by ANP and DKNP for all three change point configurations.
Compared to ANP which fails to correctly capture the changing dynamics of the given processes
(especially for configurations including periodic kernels), DKNP shows its ability to correctly learn
the underlying dynamics before and after the change point. In Figure 6, we compare the learned
priors as well as the generated samples from them to the ground truths priors and samples for two
change point configurations, Periodic-Periodic and RBF-Periodic. Note that DKNP can accurately
learn two distinct kernels in both sides of the midpoint, (i.e. low and high frequency in the PeriodicPeriodic case, smooth and periodic in the RBF-Periodic case) thus able to generate samples that are
practically equivalent to the ground truth samples.

4.3 2D IMAGE COMPLETION
The purpose of the 2D image completion task is to test how well each model shows its functional
flexibility to learn non-trivial kernels. Assuming that image data is generated by a stochastic process,
the task is to regress pixels that are missing based on the provided pixels (i.e. context pixels) in the
image. Specifically, we use the CelebA dataset (Liu et al., 2015), which consists of 202,599 number
of facial images of celebrities of diverse ethnicities, gender, and age.


-----

(Single) Context Ours BANP ANP NP

DKNP Prior


Figure 7: (Left) The prior learned by DKNP. (Right) Predicted means of various models given single context
point shown in first column.

Figure 8: Examples of the image completion task. Based on the original image, roughly 10% of pixels (100
pixels) were given as the context. The filled images generated from DKNP (Ours) are much more diverse and
clearer, compared to other baselines.

For both training and evaluation, the number of context points nc is sampled from Unif(3, 200) and
the target points is nc + Unif(3, 200 - nc). Details about the data generation process, the training
process, and the evaluation process are in Appendix C.2.

In Table 3, consistent with the previous results, DKNP shows superior ability in modeling even twodimensional stochastic processes compared to all NP variants. This is somewhat predictable based
on two previous results (Table 1 and Table 2) where the performance gap between DKNP and NP
variants was more visible for stochastic processes with complex underlying dynamics (where both
local and global structure exist), and the fact that the facial images are likely to follow non-trivial
dynamics. For example, the dynamics to generate the hair or the background, which are typically
low frequency signals, would be quite different from the dynamics to generate the details of the face
where eyes, nose, and mouth altogether compose higher frequency signals. And unlike the previous
change point data where the boundary between two different dynamics (i.e. kernels) is a point in a
1D line, the boundary between different kernels in facial images is a (curved) line in a 2D plane (e.g.
hairline). Therefore we can expect DKNP to significantly outperform all NP variants in estimating
the correlations between output variables, as shown in Table 3.

Figure 7 illustrates the mean of the prior distribution learned by DKNP on the CelebA dataset and
the predictive mean of the NP variants and DKNP when a single context point is given in the background. The posterior mean of DKNP demonstrates distinct behavior for the face and background,


-----

such that the given context uniformly affects the background while keeping the face practically
intact. This shows that DKNP accurately captures the correlations between output variables by successfully learning kernel functions that behave differently for face and background. BANP, ANP,
and NP, on the other hand, demonstrate unpredictable behaviors where the context point in the background also affects the hair or the clothes, or the background is only partially affected by the given
context point. This behavior reveals that NPs, which implicitly learn the kernel function, have a
harder time modeling the accurate correlation between output variables.


We conclude this section with qualitative examples in
Figure 8 that highlight the DKNPâ€™s capability to generate diverse samples thanks to its explicit modeling of the
covariance. Given 10% of the data points as context, ANP
samples are generated by sampling the latent variable
**_z, BANP samples are generated via bootstrapping, and_**
DKNP samples are generated with the predicted mean
and covariance. While ANP, BANP and DKNP all produce reasonable predictive means, ANPâ€™s generated samples heavily resemble one another, whereas DKNPâ€™s generated samples demonstrate considerable diversity such
as different genders, facial expressions and hair styles.
BANP also demonstrates more varied samples than ANP
thanks to its bootstrapping process, but they sometimes
contain noisy pixels.

5 RELATED WORK


Table 3: Results (log-likelihood) of NP
variants and our model in the 2D image
completion task.

Methods Celeb A

CNP 2.280 (Â±0.010)
NP 2.374 (Â±0.024)
BNP 2.876 (Â±0.014)
ANP 3.471 (Â±0.005)
BANP 3.627 (Â±0.001)

Ours **3.951 (Â±0.021)**


Many advances in NPs are made with the advent of neural network-based stochastic processes such
as NPs (Garnelo et al., 2018b) and CNPs (Garnelo et al., 2018a). ANP (Kim et al., 2019) show a dramatic performance gain by leveraging attention mechanisms in the aggregating operation. Also, Le
et al. (2018) confirm through extensive empirical evaluation on the design of NPs such as architecture and objective functions that combining attention and NPs improve the predictive log-likelihood
marginally. Lee et al. (2020) extends NPs using the bootstrap technique for estimating functional
uncertainty without maintaining a latent variable in the NP architecture. This notion of removing the
latent variable part is inline with our motivation.

For modeling stationary stochastic processes, Gordon et al. (2020) introduced Convolutional CNP
(ConvCNP), where translation equivariance in the data is explicitly encoded in the model. Convolutional NPs (ConvNP) Foong et al. (2020) are a natural extension of ConvCNPs where a global
latent variable was introduced to model dependencies of the predictive distribution. Gaussian Neural Processes (GNP) (Bruinsma et al., 2021) further generalize these classes of models with translation equivariance by leveraging convolutional neural networks to capture the predictive correlations.
While GNPs share a similar motivation to our work, the way covariance matrix is parameterized is
quite different (e.g., 1-D, 2-D convolution versus attention) from DKNPs, as their focus is mainly
on translation equivariance. Specifically, the covariance matrix of GNPs depends on YT, while for
DKNPs, motivated by posterior of GPs, the covariance only depends on the inputs XC, XT (see
equation 6).

6 CONCLUSION

We propose a new neural stochastic processes, Decoupled Kernel Neural Processes (DKNPs), that
learn an explicit kernel function to better capture the correlation between output variables. By leveraging cross- and mixed attention mechanisms to model an explicit kernel function, DKNPs outperform the concurrent NP variants in terms of predictive likelihood and better global coherence of
generated samples. By the novel model architecture of DKNPs, the learned prior can be accessible,
which provides a deeper understanding of the underlying distributions of data. As future work, one
could consider developing a method to manipulate a learned kernel or to impose a constraint on the
kernel learning process of DKNPs with prior knowledge.


-----

[Reproducibility Statement] We utilized the datasets and the baselines that are all available on Github
(https://github.com/juho-lee/bnp). As for our proposed model and code, we will make a comment
directed to the reviewer and area chairs once the discussion forums are open.

REFERENCES

Wessel Bruinsma, James Requeima, Andrew Y. K. Foong, Jonathan Gordon, and Richard E Turner.
The gaussian neural process. In Symposium on Advances in Approximate Bayesian Inference
_(AABI), 2021._

Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In
_International Conference on Learning Representations (ICLR), 2016._

Andrew Y. K. Foong, Wessel Bruinsma, Jonathan Gordon, Yann Dubois, James Requeima, and
Richard E. Turner. Meta-learning stationary stochastic process prediction with convolutional
neural processes. In Advances in Neural Information Processing Systems (NeurIPS), 2020.

Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray
Shanahan, Yee Whye Teh, Danilo Rezende, and S. M. Ali Eslami. Conditional neural processes.
In International Conference on Machine Learning (ICML), 2018a.

Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami,
and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018b.

Jonathan Gordon, Wessel P. Bruinsma, Andrew Y. K. Foong, James Requeima, Yann Dubois, and
Richard E. Turner. Convolutional conditional neural processes. In International Conference on
_Learning Representations (ICLR), 2020._

Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol
Vinyals, and Yee Whye Teh. Attentive neural processes. In International Conference on Learning
_Representations (ICLR), 2019._

Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
_on Learning Representations (ICLR), 2014._

Tuan Anh Le, Hyunjik Kim, Marta Garnelo, Dan Rosenbaum, Jonathan Schwarz, and Yee Whye
Teh. Empirical evaluation of neural process objectives. In NeurIPS workshop on Bayesian Deep
_Learning, 2018._

Juho Lee, Yoonho Lee, Jungtaek Kim, Eunho Yang, Sung Ju Hwang, and Yee Whye Teh. Bootstrapping neural processes. In Advances in Neural Information Processing Systems (NeurIPS),
2020.

Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of the IEEE international conference on computer vision, 2015.

James Robert Lloyd, David Duvenaud, Roger Grosse, Joshua B. Tenenbaum, and Zoubin Ghahramani. Automatic construction and natural-language description of nonparametric regression models. Association for the Advancement of Artificial Intelligence (AAAI), 2014.

Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. In International Conference on Machine Learning (ICML),
2018.

Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning.
MIT Press, 2006.

James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and Richard E Turner. Fast
and flexible multi-task classification using conditional neural adaptive processes. In Advances in
_Neural Information Processing Systems (NeurIPS). 2019._

Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning
_(ICML), 2014._


-----

Gautam Singh, Jaesik Yoon, Youngsung Son, and Sungjin Ahn. Sequential neural processes. In
_Advances in Neural Information Processing Systems (NeurIPS), 2019._

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Å ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor_mation Processing Systems (NeurIPS), 2017._


-----

A ARCHITECTURES DETAILS

We mostly followed the same model architectures in the paper (Lee et al., 2020). num det path
and num det path indicate the number of deterministic and stochastic paths.

A.1 1D REGRESSION TASK

CNP NP BNP ANP BANP DKNP

dim x 1 1 1 1 1 1
dim y 1 1 1 1 1 1
dim hid 128 128 128 128 128 128
dim lat -  128 -  128 -  - 
num det path 2 1 2 1 2 2
num stoch path 0 1 0 1 0 0
enc v depth -  -  -  4 4 4
enc qk depth -  -  -  2 2 2
enc pre depth 4 4 4 4 4 4
enc post depth 2 2 2 2 2 2
dec depth 3 3 3 3 3 3

Table 4: Hyperparameter setting for the 1D regression task

A.2 IMAGE COMPLETION TASK

CNP NP BNP ANP BANP DKNP

dim x 2 2 2 2 2 2
dim y 3 3 3 3 3 3
dim hid 128 128 128 128 128 128
dim lat -  128 -  128 -  - 
num det path 2 1 2 1 2 2
num stoch path 0 1 0 1 0 0
enc v depth -  -  -  6 6 6
enc qk depth -  -  -  3 3 3
enc pre depth 6 6 6 6 6 6
enc post depth 3 3 3 3 3 3
dec depth 5 5 5 5 5 5

Table 5: Hyperparameter setting for the image completion task

B COMPUTATIONAL COMPLEXITY

GP NP ANP Ours

Complexity _O((n + m)[3])_ _O(n + m)_ _O(n(n + m))_ _O(n(n + m))_

Table 6: The complexity comparison between GP, NP, ANP, and DKNP

DKNP has attention mechanisms like ANP, therefore, it costs O(n(n+m)), which is still computationally much more efficient than GP even with the full covariance matrix.

C EXPERIMENTAL DETAILS

C.1 1D REGRESSION

**Data generation from a single kernel** We constructed training and test data generated from GPs
with multiple kernels including RBF, periodic kernels, and MatÂ´ern 5/2. For each task, we generated


-----

_x âˆ¼_ Unif(âˆ’2, 2) and y from GP kernels where the GP hyperparameters are sampled from uniform
distributions. For RBF kernel, k(x, x[â€²]) = s[2] exp(âˆ’||x âˆ’ _x[â€²]||[2]/2l[2]), we sampled s âˆ¼_ Unif(0.1, 1.0),
_l âˆ¼_ Unif(0.1, 0.6), and output additive noise N (0, 10[âˆ’][2]). Given n, the size of the context C was
drawn from Unif(3, 47) and the size of the target is sampled from Unif(3, 50 _nc). For MatÂ´ern 5/2_
_âˆ’_
kernel k(x, x[â€²]) = s[2](1 + _âˆš5d/l + 5d[2]/(3l[2])) Â· exp(âˆ’âˆš5d/l), (d = ||x âˆ’_ _x[â€²]||), we sampled from_

_s âˆ¼_ Unif(0.1, 1.0) and l âˆ¼ Unif(0.1, 0.6). For periodic kernel, k(x, x[â€²]) = s[2] exp(âˆ’2sin[2](Ï€||x âˆ’
_x[â€²]||[2]/p)/l[2]), we sampled from s âˆ¼_ Unif(0.3, 1.0), l âˆ¼ Unif(0.6, 1.0), and p âˆ¼ Unif(0.8, 1.0). We
trained all models identically for 100,000 steps with training batch size of 100. We used Adam
optimizer with initial learning rate of 5 Â· 10[âˆ’][4] and decayed using cosine annealing scheme.

**Data generation from two kernels** Similar to a single kernel case, data points are sampled from a
GP but with two types of kernels, RBF and periodic. The half of the points ranging from x = âˆ’2 and
_x = 0 were sampled from one kernel and the rest from x = 0 to x = 2 were from another kernel._
We generated samples from three scenarios: Periodic - Periodic, RBF - Periodic, and RBF - RBF.
Note that we set the correlation between two kernels being zero. The training details are identical to
the single kernel case.

C.2 IMAGE COMPLETION

**CelebA32** Similar to 1-D regression tasks, we randomly sampled pixels of a given image at training as targets, and treat a subset of the points as contexts. The size of the contexts and targets is
sampled from Unif(3, 200) and nc+Unif(0, 200 _nc). For preprocessing, x is rescaled to [-1, 1] and_
_âˆ’_
_y is rescaled to [-0.5, 0.5]. We trained all models identically for 200 steps with training batch size of_
100. We used Adam optimizer with initial learning rate of 5 Â· 10[âˆ’][4] with cosine annealing scheme.


-----

