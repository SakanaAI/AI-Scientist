# MANDERA: MALICIOUS NODE DETECTION IN FEDERATED LEARNING VIA RANKING

ABSTRACT

Federated Learning is a distributed learning paradigm which seeks to preserve the
privacy of each participating nodeâ€™s data. However, federated learning is vulnerable to attacks, specifically to our interest, model integrity attacks. In this paper, we propose a novel method for malicious node detection called MANDERA.
By transferring the original message matrix into a ranking matrix whose column
shows the relative rankings of all local nodes along different parameter dimensions, our approach seeks to distinguish the malicious nodes from the benign ones
with high efficiency based on key characteristics of the rank domain. We have
proved, under mild conditions, that MANDERA is guaranteed to detect all malicious nodes under typical Byzantine attacks with no prior knowledge or history
about the participating nodes. The effectiveness of the proposed approach is further confirmed by experiments on three classic datasets, CIFAR-10, FASHIONMNIST and MNIST. Compared to the state-of-art methods in the literature for
defending Byzantine attacks, MANDERA is unique in its way to identify the malicious nodes by ranking and its robustness to effectively defense a wide range of
attacks.

1 INTRODUCTION

Federated learning (FL) has observed a steady rise in use across a plethora of applications. FL
departs from conventional centralized learning by allowing multiple participating nodes to learn
on a local collection of training data, before each respective nodeâ€™s updates are sent to a global
coordinator for aggregation. The global model collectively learns from each of these individual
nodes before relaying the updated global update back to the participating nodes. With an aggregation
of multiple nodes, the resulting model observes greater performance than if each node was to learn
on their local subset only. FL presents two key advantages, increased privacy for the contributing
node as local data is not communicated to the global coordinator, and a reduction in computation by
the global node as the computation is offloaded to contributing nodes.

However, the presence of malicious actors in the collaborative process may seek to poison the performance of the global model, to reduce the output performance of the model (Chen et al., 2017;
Fang et al., 2020; Tolpegin et al., 2020b), or to embed hidden back-doors within the model (Bagdasaryan et al., 2020). Byzantine attack aims to devastate the performance of the global model by
manipulating the gradient values of malicious nodes in a certain fashion. As these attacks emerged,
researchers seek to defend FL from the negative impacts of these attacks.

In the literature, there are two typical defense strategies: malicious node detection and robust learning. Malicious node detection defenses by detecting malicious nodes and removing them from the
aggregation (Blanchard et al., 2017; Guerraoui et al., 2018; Li et al., 2020; So et al., 2021). Robust
learning (Blanchard et al., 2017; Yin et al., 2018; Guerraoui et al., 2018; Fang et al., 2020; Cao et al.,
2020), however, withstands a proportion of malicious nodes and defenses by reducing the negative
impacts of the malicious nodes via various robust learning methods (Wu et al., 2020b; Xie et al.,
2019; 2020; Cao et al., 2021).

In this paper, we focus on defensing Byzantine attacks via malicious node detection. In the literature,
there have been a collection of efforts along this research line. Blanchard et al. (2017) propose a
defense referred to as Krum that treats local nodes whose update vector is too far away from the
aggregated barycenter as malicious nodes and precludes them from the downstream aggregation.
Guerraoui et al. (2018) propose Bulyan, a process that performs aggregation on subsets of node


-----

updates (by iteratively leaving each node out) to find a set of nodes with the most aligned updates
given an aggregation rule. Xie et al. (2019) compute a Stochastic Descendant Score (SDS) based
on the estimated descendant of the loss function, and the magnitude of the update submitted to the
global node, and only include a predefined number of nodes with the highest SDS in the aggregation.
On the other hand, Chen et al. (2021) propose a zero-knowledge approach to detect and remove
malicious nodes by solving a weighted clustering problem. The resulting clusters update the model
individually and accuracy against a validation set are checked. All nodes in a cluster with significant
negative accuracy impact are rejected and removed from the aggregation step.

Although the aforementioned methods try to detect malicious nodes in different ways, they all share
a common nature: the detection is based on the gradient updates directly. However, it is usually
the case that different dimensions of the gradients remain quite different in the range of values and
follow very different distributions. This phenomena makes it very challenging to precisely detect
malicious nodes directly based on the node updates, as a few dimensions often dominate the final
result. Although the weighted clustering method proposed by Chen et al. (2021) could avoid this
problem partially by re-weighting different update dimensions, it is often not trivial to determine the
weights in a principled way.

In this paper, we propose to resolve this critical problem from a novel perspective. Instead of working on the node updates directly, we propose to extract information about malicious nodes indirectly
by transforming the node updates from numeric gradient values to the rank domain. Compared to the
original numeric gradient values, whose distribution is difficult to model, the ranks are much easier
to handle both theoretically and practically. Moreover, as ranks are scale-free, we no longer need to
worry about the scale difference across different dimensions. We proved under mild conditions that
the first two moments of the transformed rank vectors carry key information to detect the malicious
nodes under a wide range of Byzantine attacks. Based on these theoretical results, a highly efficient
method called MANDERA is proposed to separate the malicious nodes from the benign ones by
clustering all local nodes into two groups based on the moments of their rank vectors. With the
assumption that malicious nodes are the minority in the node pool, we can simply treat all nodes in
the smaller cluster as malicious nodes and remove them from the aggregation.

|Reject Malicious Update MANDERA|Rank Node Parameters Compute Find and reject nodes ğœ½ğŸ 0.03, 0.12, 0.06, 0.2, 0.9 1, 3, 2, 4, 5 Rank Mean in malicious cluster ğœ½ğŸ 0.72, 0.90, 0.69, 0.7, 0.1 4, 5, 2, 3, 1 & SD SD ğœ½ğŸ‘ 0.48, 0.42, 0.43, 0.5, 0.8 3, 1, 2, 4, 5 Aggregate and then, Update Global Model ğœ½ğ’‘ 0.18, 0.2, 0.16, 0.3, 0.00 2, 3, 1, 4, 1 Cluster Mean with benign cluster|
|---|---|



Figure 1: An Overview of MANDERA

The contributions of this work are as follows. (1) We propose the first algorithm leveraging the
rank domain of model updates to detect malicious nodes (Figure 1). (2) We provide theoretical
guarantee for the detection of malicious nodes based on the rank domain under Byzantine attacks.
**(3) Our method does not assume knowledge on the number of malicious nodes, which is required**
in the learning process of prior methods. (4) We experimentally demonstrate the effectiveness and
robustness of our defense on Byzantine attacks, including Gaussian attack, Sign Flipping attack and
Zero Gradient attack, in addition to a more subtle Label Flipping data poisoning attack. (5) An
experimental comparison between MANDERA and a collection of robust aggregation techniques
are provided. The computation times are also compared, demonstrating gains of MANDERA by
operating in the rank domain.

2 DEFENSE FORMALIZATION

2.1 NOTATIONS
Suppose there are n local nodes in the federated learning framework, where n1 nodes are benign
nodes whose indices are denoted by Ib and the other n0 = n âˆ’ _n1 nodes are malicious nodes_
whose indices are denoted by Im. The training model is denoted by f (Î¸, D), where Î¸ âˆˆ R[p][Ã—][1]
is a p-dimensional parameter vector and D is a data matrix. Denote the message matrix received
from all local nodes by the central server as M âˆˆ R[n][Ã—][p], where Mi,: denotes the message received
from node i. For a benign node i, let Di be the data matrix on it with Ni as the sample size, we
have Mi,: = _[âˆ‚f]_ [(]âˆ‚[Î¸]Î¸[,][D][i][)] . A malicious node j _m, however, tends to attack the learning system by_

_âˆˆI_


-----

manipulating Mj,: in some way. Hereinafter, we denote N _[âˆ—]_ = min({Ni}iâˆˆIb ) to be the minimal
sample size of the benign nodes.

Given a vector of real numbers a âˆˆ R[p][Ã—][1], define its ranking vector as b = Rank(a) âˆˆ
_perm{1, Â· Â· Â·, p}, where the ranking operator Rank maps the vector a to its permutation_
space perm{1, Â· Â· Â·, p} which is the set of all the permutations of {1, Â· Â· Â·, p}. For example,
_Rank(1.1, âˆ’2, 3.2) = (2, 3, 1). We adopt average ranking, when there are ties. With the Rank_
operator, we can transfer the message matrix M to a ranking matrix R by replacing its column
**_M:,j by the corresponding ranking vector R:,j = Rank(M:,j). Further define_**


_ei â‰œ_ [1]

_p_


**_Ri,j_** and _vi â‰œ_ [1]

_p_

_j=1_

X


(Ri,j _ei)[2]_
_j=1_ _âˆ’_

X


to be the mean and variance of Ri,:, respectively. As it is shown in later subsections, we can judge
whether node i is a malicious node based on (ei, vi) under various attack types. In the following, we
will highlight the behaviour of the benign nodes first, and then discuss the behaviour of malicious
nodes and their interactions with the benign nodes under various Byzantine attacks respectively.

2.2 BEHAVIOUR OF BENIGN NODES
As the behaviour of benign nodes does not depend on the type of Byzantine attack, we can study the
statistical properties of (ei, vi) for a benign node i âˆˆIb before the specification of a concrete attack
type. For any benign node i, the message generated for j[th] parameter is


_Ni_

_l=1_

X


**_Mi,j = [1]_**

_Ni_


_âˆ‚f_ (Î¸, Di,l)

_,_ (1)
_âˆ‚Î¸j_


where Di,l denotes the l[th] sample on it. Throughout this paper, we always assume that Di,ls are
independent and identically distributed (IID) samples drawn from a data distribution D. Under the
independent data assumption, since Equation 1 tells us that Mi,j is the sample mean of IID random
variables, i.e., _[âˆ‚f]_ [(]âˆ‚[Î¸][,]Î¸[D]j _[i,l][)]_ _l=1[, directly applying the Strong Law of Large Numbers (SLLN) and]_
_{_ _}[N][i]_

Central Limit Theorem (CLT) leads to the lemma below immediately.

**Lemma 1. Under the independent data assumption, further denote Âµj = E(** _[âˆ‚f]_ [(]âˆ‚[Î¸][,]Î¸[D]j _[i,l][)]_ ) and Ïƒj[2] [=]

Var( _[âˆ‚f]_ [(]âˆ‚[Î¸][,]Î¸[D]j _[i,l][)]_ ) < _, with Ni going to infinity we have for_ _j_ 1, _, p_

_âˆ_ _âˆ€_ _âˆˆ{_ _Â· Â· Â·_ _}_

**_Mi,j_** _Âµj a.s._ _and_ **_Mi,j_** _d_ _Âµj, Ïƒj[2][/N][i]_ _._ (2)
_â†’_ _â†’ N_
  

2.3 BEHAVIOUR OF MALICIOUS NODE UNDER THE GAUSSIAN ATTACK

**Definition 1 (Gaussian attack). In a Gaussian attack, the attacker manipulates malicious nodes**
_random samples from Gaussian distributionto send Gaussian random messages to the global coordinator, i.e., MVN_ (mb,:, Î£), where { mMb,i,:: =}iâˆˆInm11 are independentiâˆˆIb **_[M][i,][:][ and]_**

Î£ is the covariance matrix determined by the attacker.

P

Considering thatLemma 1, it is straightforward to see that Mi,j â†’ _Âµj almost surely (a.s.) with limN âˆ—â†’âˆ_ **_mb,j N =i going to infinity for all Âµj a.s., and the distribution of i âˆˆIb based on Mi,j for_**
each i âˆˆIm converges to the Gaussian distribution centered at Âµj. Lemma 2 provides the details.

**Lemma 2. Under the same assumption as in Lemma 1, with N** _[âˆ—]_ _going to infinity, we have for each_
_malicious node i âˆˆIm under the Gaussian attack that_

**_Mi,j_** _d_ (Âµj, Î£j,j), 1 _j_ _p._ (3)
_â†’ N_ _â‰¤_ _â‰¤_

Lemma 1 and Lemma 2 tell us that for each parameter dimension j, {Mi,j}i[n]=1 [are independent]
Gaussian random variables with the same mean (i.e, Âµj) but different variances (i.e., Ïƒj[2][/N][i][ or][ Î£][j,j][)]
under the Gaussian attack. Due to the symmetry of Gaussian distribution, it is straightforward to see


E(Ri,j) = _[n][ + 1]_ _, 1_ _i_ _n, 1_ _j_ _p._

2 _â‰¤_ _â‰¤_ _â‰¤_ _â‰¤_


-----

Moreover, the exchangeability of benign nodes and the exchangeability of malicious nodes when
_N_ _[âˆ—]_ is reasonably large tell us: for each parameter dimension j, there exist two positive constants
_s[2]b,j_ [and][ s]m,j[2] [such that]

Var(Ri,j) = s[2]b,j[,][ âˆ€] _[i][ âˆˆI][b][,]_ and Var(Ri,j) = s[2]m,j[,][ âˆ€] _[i][ âˆˆI][m][,]_

where both s[2]b,j [and][ s]m,j[2] [are complex functions of][ Ïƒ]j[2][,][ Î£][j,j][ and][ {][N][i][}][i][âˆˆI]b [. Further assume that]
**_Ri,jâ€™s are independent of each other, thus ei =_** _p1_ _pj=1_ **_[R][i,j][ is the sum of independent random]_**
variables with a common mean. Thus, according to the Kolmogorov Strong Law of Large Numbers
P
(KSLLN), we know that ei converges to a constant almost surely, which in turn indicates that vi also
converge some constant almost surely. The Theorem 1 summarizes the results formally, with the
detailed proof provided in Appendix C.
**Theorem 1. Assuming** **_R:,j_** 1 _j_ _p are independent of each other, under the Gaussian attack, we_
_{_ _}_ _â‰¤_ _â‰¤_
_have for each local node i that_

lim _a.s.,_ (4)
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_ _[e][i][ =][ n][ + 1]2_

lim _vi_ _sÂ¯[2]b_ _s[2]m_ = 0 a.s., (5)
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_ _âˆ’_ _[Â·][ I][(][i][ âˆˆI][b][)][ âˆ’]_ [Â¯] _[Â·][ I][(][i][ âˆˆI][m][)]_

_p_ _p_

_where I(Â·) stands for the indicator function, _ Â¯s[2]b [â‰œ] _p[1]_ _j=1_ _[s]b,j[2]_ _[and][ Â¯]s[2]m_ [â‰œ] _p[1]_ _j=1_ _[s]m,j[2]_ _[.]_

P P

Considering that Â¯s[2]b [= Â¯]s[2]m [if and only if][ Î£][j,j][â€™s fall into a lower dimensional manifold whose mea-]
surement is zero under the Lebesgue measure, we have P (Â¯s[2]b [= Â¯]s[2]m[) = 0][ if the attacker specifies]
the Gaussian variance Î£j,jâ€™s arbitrarily in the Gaussian attack. Thus, Theorem 1 in fact suggests
that the benign nodes and the malicious nodes are different on the value of vi, and therefore provides a guideline to detect the malicious nodes. Although the we do need N _[âˆ—]_ and p to go to infinity
for getting the theoretical results in Theorem 1, in practice the malicious node detection algorithm
based on the theorem typically works very well when N and p are reasonably large and Niâ€™s are

_[âˆ—]_
not dramatically far away from each other.

The independent rank assumption in Theorem 1, which assumes that **_R:,j_** 1 _j_ _p are independent_
_{_ _}_ _â‰¤_ _â‰¤_
of each other, may look restrictive. However, in fact it is a mild condition that can be easily satisfied in practice due to the following reasons. First, for a benign node i âˆˆIb, Mi,j and Mi,k are
often nearly independent, as the correlation between two model parameters Î¸j and Î¸k is often very
weak in a larger deep neural network with a huge number of parameters. To verify the statement,
we implemented independence tests for 100,000 column pairs randomly chosen from the message
matrix M generated from the FASHION-MNIST data. Distribution of the p-values of these tests
are demonstrated in Figure 2 via a histogram, which is very close to a uniform distribution, indicating that Mi,j and Mi,k are indeed nearly independent in practice. Second, even some M:,j and
**_M:,k shows strong correlation, magnitude of the correlation would be reduced greatly during the_**
transformation from M to R, as the final ranking Ri,j also depends on many other factors. Actually, the independent rank assumption could be relaxed to be uncorrelated rank assumption which
assumes the ranks are uncorrelated with each other. Adopting the weaker assumption will result in a
change of convergence type of our theorems from the â€œalmost surely convergenceâ€ to â€œconvergence
in probabilityâ€, but with no essential influence to the our algorithm below.

Figure 2: Independence tests for 100,000 column pairs randomly chosen from message matrix M

6000

count 40002000

0

0.00 0.25 0.50 0.75 1.00

p.value

generated from FASHION-MNIST data supports the independence assumption made in Theorem 1.

2.4 MALICIOUS NODE DETECTION FOR SIGN FLIPPING ATTACK

**Definition 2 (Sign flipping attack). Sign flipping attack aims to generate the gradient values of**
_malicious nodes by flipping the sign of the average of all the benign nodesâ€™ gradient at each epoch,_
_i.e., specifying Mi,: = âˆ’rmb,: for any i âˆˆIm, where r > 0, mb =_ _n11_ _kâˆˆIb_ **_[M][k,][:][.]_**

P


-----

Based on the above definition, the update message of a malicious node i under the sign flipping
attack is
**_Mi,: =_** _rmb,: =_ **_Mk,:._** (6)
_âˆ’_ _âˆ’_ _n[r]1_

_kXâˆˆIb_

For fixed **_Mk,:_** _k_ _b_, Mi,: is also a fixed vector without randomness, as it is a deterministic function
_{_ _}_ _âˆˆI_
of **_Mk,:_** _k_ _b_ . On the other hand, however, we can also treat Mi,: as a random vector, since the
_{_ _}_ _âˆˆI_
randomness ofany parameter dimension {Mk,:}kâˆˆI jb, considering that can be transferred to Mk,j Mi,d: via the link function in equation 6. In fact, forÂµj, Ïƒj[2][/N][k] for any k _b according to_
_â†’ N_ _âˆˆI_
Lemma 1, it is straightforward to see that Mi,j = âˆ’ _n[r]1_  kâˆˆIb **_[M][k,j][ can also be well approximated]_**

by a Gaussian distribution. The lemma 3 summarizes the result formally.

P

**Lemma 3. Under the sign flipping attack, for each malicious node i âˆˆIm and any parameter**
_dimension j, we have Mi,j =_ _n1_ _k_ _b_ **_[M][k,j][ is a deterministic function of][ {][M][k,j][}][k][âˆˆI]b_** _[, whose]_
_âˆ’_ _[r]_ _âˆˆI_

_limiting distribution when N_ _[âˆ—]_ _goes to infinity is_

P

**_Mi,j_** _d_ _Âµj(r), Ïƒj[2][(][r][)]_ _, 1_ _j_ _p,_ (7)
_â†’ N_ _â‰¤_ _â‰¤_

_where Âµj(r) = âˆ’rÂµj, Ïƒj[2][(][r][) =]_ _nr1[2]Â·Â·NÏƒ[Â¯]j[2]b_ _[, and] [ Â¯]Nb =_ _kâˆˆIn1b_ _Nk1_ _is the harmonic mean of {Nk}kâˆˆIb_ _._

P

Lemma 1 and Lemma 3 tell us that for each parameter dimension j, the distribution of {Mi,j}i[n]=1 [is]
a mixture of Gaussian components {N _Âµj, Ïƒj[2][/N][i]_ _}iâˆˆIb centered at Âµj plus a point mass located_
at Âµj(r) = _rÂµj. If Niâ€™s are reasonably large, variances Ïƒj[2][/N][i][â€™s would be very close to zero,]_
_âˆ’_   
and the probability mass of the mixture distribution would concentrate to two local centers Âµj and
_Âµj(r) =_ _rÂµj, one for the benign nodes and the other one for the malicious nodes. This intuition_
_âˆ’_
provides us the guidance to identify the malicious nodes in this attack pattern. Transforming to
the rank domain, the above intuition leads to different behavior patterns of the benign nodes and
the malicious nodes in the rank matrix R, which in turn result in different limiting behavior of
(ei, vi) for the benign and malicious nodes. The theorem 2 summarizes the results formally, with
the detailed proof provided in Appendix D.
**Theorem 2. With the same independent rank assumption as posed in Theorem 1, under the sign**
_flipping attack, we have for each local node i that_

lim = _ÂµÂ¯b_ I(i _b) + Â¯Âµm_ I(i _m) a.s.,_ (8)
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_ _[e][i]_ _Â·_ _âˆˆI_ _Â·_ _âˆˆI_

lim = _sÂ¯[2]b_ _s[2]m_ (9)
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_ _[v][i]_ _[Â·][ I][(][i][ âˆˆI][b][) + Â¯]_ _[Â·][ I][(][i][ âˆˆI][m][)][ a.s.,]_

_p_

_where Â¯Âµb =_ _[n][+][n]2[0][+1]_ _âˆ’_ _n0Ï, Â¯Âµm = n1Ï +_ _[n][0]2[+1]_ _, Ï = limpâ†’âˆ_ Pj=1 [I]p[(][Âµ][j] _[>][0)]_ _, Â¯s[2]m_ _[and][ Â¯]s[2]b_ _[are both]_

_quadratic functions of Ï whose concrete form also depends on n0 and n1._

Considering that Â¯Âµb = Â¯Âµm if and only if Ï = 2[1] [, and][ Â¯]s[2]b [= Â¯]s[2]m [if and only if][ Ï][ is the solution of a]

quadratic function, the probability of (Â¯Âµb, Â¯s[2]b[) = (Â¯]Âµm, Â¯s[2]m[)][ is zero as][ p][ â†’âˆ][. Such a phenomenon]
suggests that we can detect the malicious nodes based on the moments (ei, vi) to defense the sign
flipping attack as well. Noticeably, we note that the limit behaviour of ei and vi does not dependent
on the specification of r, which defines the sign flipping attack. Although such a fact looks a bit
abnormal at the first glance, it is totally understandable once we realize that with the variance of
**_Mi,j shrinks to zero with Ni goes to infinity for each benign node i, any different between Âµj and_**
_Âµj(r) would result in the same rank vector R:,j in the rank domain._

2.5 MALICIOUS NODE DETECTION FOR ZERO GRADIENT ATTACK

**Definition 3 (Zero gradient attack). Zero gradient attack aims to make the aggregated message to**
_be zero, i.e.,_ _i=1_ **_[M][i,][:][ = 0][, at each epoch, by specifying][ M][i,][:][ =][ âˆ’]_** _[n]n[1]0_ **_[m][b,][:][ for all][ i][ âˆˆI][m][.]_**

Apparently, the zero gradient attack defined above is a special case of sign flipping attack by speci
[P][n]

fying r = _n[n]0[1]_ [. Since the conclusions of Theorem 2 keep unchanged for different specifications of][ r]

as we have discussed, we have the following corollary for zero gradient attack.
**Corollary 1. Under the zero gradient attack, eiâ€™s and viâ€™s follow exactly the same limiting be-**
_haviours as described in Theorem 2._


-----

2.6 MANDERA
Theorem 1, 2 and Corollary 1 imply that, under these three attacks (Gaussian attack, zero gradient
attack and sign flipping attack), the first two moments of Ri,:, i.e., (ei, vi), converge to two different
limits for the benign nodes and the malicious nodes, respectively. Thus, for a real dataset where
_Niâ€™s and p are all finite but reasonably large numbers, the scatter plot of_ (ei, vi) 1 _i_ _n would_
_{_ _}_ _â‰¤_ _â‰¤_
demonstrate a clustering structure: one cluster for the benign nodes and the other cluster for the
malicious nodes. Figure 3 illustrates such a scatter plot for the 100 local nodes in a typical epoch
of training the FASHION-MNIST dataset under different FL settings (to keep the two dimensions
of the scatter plot to the same scale, we replaced vi by its square root si = _vi instead). Clearly, a_

_[âˆš]_
simple clustering procedure would detect the malicious nodes from the scatter plot. Based on this
intuition, we propose MAlicious Node DEtection via RAnking (MANDERA) to detect the malicious
nodes, whose workflow is detailed in Algorithm 1.

**Algorithm 1 Malicious node detection via ranking (MANDERA)**
**Input: The message matrix M** .

1: Convert the message matrix M to the ranking matrix R by applying Rank operator.
2: Compute mean and standard deviation of rows in R, i.e., {(ei, si)}1â‰¤iâ‰¤n.
3: Run the clustering algorithm K-means to (ei, si) 1 _i_ _n with K = 2, and predict the set of_
_{_ _}_ _â‰¤_ _â‰¤_
benign nodes with the lager cluster denoted by [Ë†]b.
_I_

**Output: The predicted benign node set** _I[Ë†]b._

**Remark. MANDERA can be applied to either a single epoch or multiple epochs. For a single-epoch**
_mode, the input data M is the message matrix received from a single epoch. For multiple-epoch_
_mode, the data M is the column-concatenation of the message matrices from multiple epochs. By_
_default, the experiments below all use single epoch to detect the malicious nodes._

**_mThe predicted benign nodesË†_** _b,: =_ #(1I[Ë†]b) _iâˆˆI[Ë†]b_ **_[M][i,][:][. The theorem 3 shows that]I[Ë†]b obtained by MANDERA naturally leads to an aggregated message[ Ë†]Ib and Ë†mb lead to consistent estimations of_**

_b and mb respectively, indicating that MANDERA enjoys robustness guarantee (Steinhardt, 2018)_
_I_ P
for typical Byzantine attacks.
**Theorem 3. Under the three typical Byzantine attacks, i.e., Gaussian attack, sign flipping attack**
_and zero gradient attack, we have:_

lim _b =_ _b) = 1_ _and_ lim **_mb,:_** **_mb,:_** 2 = 0. (10)
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_ [P][(Ë†]I _I_ _N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_ [E][||][ Ë†] _âˆ’_ _||_

The proof of Theorem 3 can be found in Appendix E.

3 EXPERIMENTS

We evaluate the efficacy in detecting malicious nodes within the federated learning framework with
the use of three Datasets. The first is the FASHION-MNIST dataset (Xiao et al., 2017), a dataset of
60,000 and 10,000 training and testing samples respectively divided into 10 classes of apparel. The
second is CIFAR-10 (Krizhevsky et al., 2009), a dataset of 60,000 small object images also containing 10 object classes. The third is MNIST (Deng, 2012) dataset which appears in Appendix H.
In these experiments we mainly adopt implementations of Byzantine attacks released by (Wu et al.,
2020b;a) and the label flipping attack (Tolpegin et al., 2020b;a). The label flipping attack is a data
poisoning attack that alters one or more labels of training data to an attackerâ€™s pre-determined target
label. For example, in CIFAR-10â€™s object labels, an attacker may change the labels of their local
_cat images to be labelled as dogs. We use the Label Flipping attack as a comparative poisoning_
attack that achieves its objective in a more subtle manner. In our experiments, we set Î£ = 30I for
the Gaussian attack and r = 3 for the sign flipping attack, where I is the identity matrix. For all
experiments we fix n = 100 participating nodes, of which a variable number of nodes are poisoned
_n0_ 5, 10, 15, 20, 25, 30 . The training process is run until 25 epochs have elapsed. We have
_|_ _| âˆˆ{_ _}_
described the structure of these networks in Appendix A.

3.1 ILLUSTRATION OF THE AVERAGE RANKING AND STANDARD DEVIATION OF RANKING
Section 2 speculated that the distribution of parameter ranks differ sufficiently for the detection of
malicious and benign nodes. We validate this hypothesis in Figure 3 by illustrating the difference


-----

Figure 3: The scatter plots of (ei, si) for the 100 nodes under four types of attack as illustrative ex
Node.type Benign Malicious

5 10 15 20 25 30

40

GA

30

20

45
40
35 ZG
30

SD 25

40

35 SF

30

25

32
31
30 LF
29
28
27

48 49 50 51 52 53 48 49 50 51 52 53 48 49 50 51 52 53 48 49 50 51 52 53 48 49 50 51 52 53 48 49 50 51 52 53

Mean

amples demonstrating ranking mean and variance from the 1st epoch of training for the FASHIONMNIST dataset.

Metric Accuracy Recall Precision F1

GA ZG SF LF

1.00

0.75

0.50

0.25 Accuracy

0.00

1.00

0.75

0.50

Recall

0.25

0.00

Score 1.00

0.75

0.50

0.25 Precision

0.00

1.00

0.75

0.50 F1

0.25

0.00

5 1015202530 5 1015202530 5 1015202530 5 1015202530

Number of malicious nodes

Metric Accuracy Recall Precision F1

GA ZG SF LF

1.00

0.75

0.50

0.25 Accuracy

0.00

1.00

0.75

0.50

Recall

0.25

0.00

Score 1.00

0.75

0.50

0.25 Precision

0.00

1.00

0.75

0.50 F1

0.25

0.00

5 1015202530 5 1015202530 5 1015202530 5 1015202530

Number of malicious nodes


(a) CIFAR-10 (b) FASHION-MNIST

Figure 4: Classification performance of our proposed approach MANDERA (Algorithm 1) under
four types of attack for CIFAR-10 and FASHION-MNIST data. Gaussian Attack (GA); ZeroGradient (ZG); Sign-Flipping (SF); and Label-Flipping (LF). The boxplot bounds the 25th (Q1)
and 75th (Q3) percentile, with the central line representing the 50th quantile (median). The end
points of the whisker represent the Q1-1.5(Q3-Q1) and Q3+1.5(Q3-Q1) respectively.

between the benign nodes and malicious nodes in terms of the mean of gradientsâ€™ rankings and the
standard deviation of gradientsâ€™ ranking.

It can be observed from Figure 3 that, under Gaussian and Label flipping attacks, the average rankings of malicious nodes are of a similar distribution to benign nodes. It is problematic for distinguishing between the two types of nodes, if only average ranking information is used. On the other
hand, Figure 3 displays a larger separation of distributions for the standard deviation of ranking. It is
noted that all 4 attacks observe a convergence of the distributions as the number of malicious nodes
increase, increasing the difficulty of defense for both MANDERA and all other defenses. However, the likelihood of an attacker controlling increasingly large numbers of malicious nodes also
decrease.


-----

3.2 MALICIOUS NODE DETECTION BY MANDERA
We test the performance of MANDERA on the update gradients of a model under attacks. In this
section, MANDERA acts as an observer without intervening in the learning process to identify
malicious nodes with a set of gradients from a single epoch. Each configuration of 25 training
epochs, with a given number of malicious nodes was repeated 20 times. Figure 4 demonstrates the
classification performance (Metrics defined in Appendix B) of MANDERA with different settings
of participating malicious nodes and the four poisoning attacks of Guassian Attack (GA), Zero
Gradient attack (ZG), Sign Flipping attack (SF) and the Label Flipping attack (LF).

While we have formally demonstrated the efficacy of MANDERA in accurately detecting potentially malicious nodes participating in the federated learning process. In practice, to leverage an
unsupervised K-means clustering algorithm, we must also identify the correct group of nodes as the
malicious group. Our strategy is to identify the group with the most exact gradients, or otherwise
the smaller group (we regard a system with over 50% of their nodes compromised as having larger
issues than just poisoning attacks) [1]. We also test other clustering algorithms, such as hierarchical
clustering and Gaussian mixture models (Fraley & Raftery, 2002). It turns out that the performance
of MANDERA is quite robust with different choices of clustering methods. Detailed results can be
found in Appendix F.

From Figure 4, it is immediately evident that the recall of the malicious nodes for the Byzantine attacks is exceptional. However, occasionally benign nodes have also been misclassified as malicious
under a SF, and to a lesser extent the ZG attack for both datasets. On all attacks, in the presence of
more malicious nodes, the recall of malicious nodes trends down. As for the data poisoning attack
of LF, it is consistently more difficult to detect, however we note that the LF attack has a more subtle
influence on the model in contrast to the impact of Byzantine attacks.

3.3 MANDERA FOR DEFENDING AGAINST POISONING ATTACKS
In this section, we encapsulate MANDERA into a module prior to the the aggregation step, MANDERA has the sole objective of identifying malicious nodes, and excluding their updates from the
global aggregation step. Each configuration of 25 training epochs, a given poisoning attack, defense
method, and a given number of malicious nodes was repeated 10 times. We compare MANDERA
against 5 other robust aggregation defense methods, Krum (Blanchard et al., 2017), Bulyan (Guerraoui et al., 2018), Trimmed Mean (Yin et al., 2018), Median (Yin et al., 2018) and FLTrust (Cao
et al., 2020). Of which the first 2 requires an assumed number of malicious nodes, and the latter 3
only aggregate robustly.

|Krum Bulyan Trimâˆ’mean FLTrust Defence NOâˆ’attack Median MANDERA 5 10 15 20 25 30 50 40 30 GA 20 10 50 40 ZG 30 Accuracy 20 50 40 SF 30 20 50 40 LF 30 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 Number of Epoch|Krum Bulyan Trimâˆ’mean FLTrust Defence NOâˆ’attack Median MANDERA 5 10 15 20 25 30 75 50 GA 25 90 80 70 60 ZG 50 Accuracy 40 90 80 70 SF 60 50 85 80 LF 75 70 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 Number of Epoch|
|---|---|



(a) CIFAR-10 Dataset (b) FASHION-MNIST dataset

Figure 5: Model Accuracy at each epoch of training, each line of the curve represents a different
defense against the poisoning attacks.

From Figure 5, it is observed that MANDERA performs about the same as the best performing
defense mechanisms, close to the performance of a model not under attack. MANDERAâ€™s accuracy

1More informed approaches to selecting the malicious cluster can be tested in future work. E.g. Figure 3
displays less variation of rank variance in malicious cluster compared to benign nodes. This could robust
selection of the malicious group, and enabling selection of malicious groups larger than 50%.


-----

is observed to vary slightly under the LF attack on fashion data with 30 malicious nodes, this is
consistent with the larger accuracy ranges previously observed in Figure 4b. Interestingly, FLTrust
as a standalone defense is weak in protecting against the most extreme Byzantine attacks. However,
we highlight that FLtrust is a robust aggregation method against targeted attacks that may thwart
defences like Krum, Trimmed mean. We see FLTrust as a complementary defence that relies on a
base method of defence against Byzantine attacks, but expands the protection coverage of the FL
system against adaptive attacks.

3.4 COMPUTATIONAL EFFICIENCY
We have previously been able to observe that MANDERA can perform at par with the current
highest performing poisoning attack defenses. Another benefit arises with the simplification of
the mitigation strategy with the introduction of ranking at the core of the algorithm. Sorting and
Ranking algorithms are fast. Additionally, we only apply clustering on the two dimensions of rank
mean and standard deviation, in contrast to other works that seek to cluster on the entire node
update (Chen et al., 2021). The times in Table 1 for MANDERA, Krum and Bulyan do not include
the parameter/gradient aggregation step. These times were computed on 1 core of a Dual Xeon
14-core E5-2690, with 8 Gb of system RAM and a single Nvidia Tesla P100. Table 1 demonstrates
that MANDERA is able to achieve a faster speed than that of single Krum [2] (by more than half) and
Bulyan (by an order of magnitude).
Table 1: Mean and standard deviation of computational times for defense function given the same
set of gradients from 100 nodes, of which 30 were malicious. Each function was repeated 100 times.

|Defense (Detection)|Mean Â± SD (ms)|Defense (Aggregation)|Mean Â± SD (ms)|
|---|---|---|---|
|MANDERA|643 Â± 8.646|Trimmed Mean|3.96 Â± 0.41|
|Krum (Single)|1352 Â± 10.09|Median|9.81 Â± 3.88|
|Bulyan|27209 Â± 233.4|FLTrust|361 Â± 4.07|


Defense (Detection) Mean Â± SD (ms) Defense (Aggregation) Mean Â± SD (ms)

**_MANDERA_** 643 Â± 8.646 Trimmed Mean 3.96 Â± 0.41

Krum (Single) 1352 Â± 10.09 Median 9.81 Â± 3.88

Bulyan 27209 Â± 233.4 FLTrust 361 Â± 4.07


4 DISCUSSION AND CONCLUSION

If attackers create more adaptive attacks unlike Definition 1, 2 and 3, they may evade MANDERA
and achieve model poisoning. In this work, we have configured our Federated Learner to use all 100
nodes in the learning process at every round, we acknowledge FL framework may learn the global
model only using subset of nodes at each round. In these settings MANDERA would still function,
as we would rank and cluster on the parameters of the participating nodes, without assuming any
number of poisoned nodes. In Algorithm 1, performance could be improved by incorporating higher
order moments. MANDERA is unable to function when gradients are securely aggregated in its
current form. However, malicious nodes can be identified and excluded from the secure aggregation
step, while still protecting the privacy of participating nodes by performing MANDERA through
secure ranking (Zhang et al., 2013; Lin & Tzeng, 2005) (recall that MANDERA only requires the
ranking matrix to detect poisoned nodes). It remains to be seen the effectiveness of MANDERA on
more advanced poisoning techniques like adversarial poisoning or Evasion attacks.

In conclusion, we have provided theoretical guarantees and experimentally shown efficacy in the
use of ranking algorithms for the detection of malicious nodes performing poisoning attacks against
federated learning. Our proposed method MANDERA, is able to achieve high detection accuracy
and maintain a model accuracy on par with other seminal, high performing defense mechanisms, but
with three notable advantages. First, provable guarantees for the use of ranking to detect Gaussian,
Zero Gradient and Sign Flipping attacks. Next, faster detection with the use of ranking algorithms.
Finally, the MANDERA defense does not need a prior estimation of the number of poisoned nodes.
In this work we demonstrate how the rank domain can be useful in applications to defend against
malicious actors.

**Ethics Statement**

The core objective of our research is to provide an additional means of defense against poisoning
nodes that target Federated Learning. To test our defense we have implemented different attacks
against the Federated Learning framework. Attackers may adopt our defense strategy to design
new poisoning attacks. Fortunately, these poisoning attacks can not be leveraged to leak private
information from Federated learning models, instead only impact its performance.

2The use of multi-krum would have yielded better protection (c.f. Section 3) at the behest of speed.


-----

**Reproducibility Statement**

To ensure reproducible research, we have supplemented our proposal for MANDERA, by supplying
both R and Python implementations of MANDERA used in this paper, uploaded with the remainder
of the experiment code. The three datasets featured in this paper is CIFAR-10 (Krizhevsky et al.,
2009), Fasion-MNIST (Xiao et al., 2017), and MNIST (Deng, 2012); we have used each of these
dataset unaltered from their respective sources. We have stated the assumptions in our theorems
and their proofs can be found in the Appendix. But to explain our assumptions in simple terms, (1)
The data samples on each local node are independently drawn from the same distribution. (2) The
gradient value for each parameter is independent to each other.

REFERENCES

Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to
backdoor federated learning. In International Conference on Artificial Intelligence and Statistics,
pp. 2938â€“2948. PMLR, 2020.

Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso[ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/](https://proceedings.neurips.cc/paper/2017/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf)
[f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf.](https://proceedings.neurips.cc/paper/2017/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf)

Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. Fltrust: Byzantine-robust federated learning via trust bootstrapping. arXiv preprint arXiv:2012.13995, 2020.

Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. Provably secure federated learning against
malicious clients. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,
pp. 6885â€“6893, 2021.

Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial settings: Byzantine gradient descent. Proc. ACM Meas. Anal. Comput. Syst., 1(2), December 2017.
[doi: 10.1145/3154503. URL https://doi.org/10.1145/3154503.](https://doi.org/10.1145/3154503)

Zheyi Chen, Pu Tian, Weixian Liao, and Wei Yu. Zero knowledge clustering based adversarial
mitigation in heterogeneous federated learning. IEEE Transactions on Network Science and En_gineering, 8(2):1070â€“1083, 2021. doi: 10.1109/TNSE.2020.3002796._

Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE
_Signal Processing Magazine, 29(6):141â€“142, 2012._

Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to
byzantine-robust federated learning. In 29th {USENIX} Security Symposium ({USENIX} Se_curity 20), pp. 1605â€“1622, 2020._

Chris Fraley and Adrian E Raftery. Model-based clustering, discriminant analysis, and density
estimation. Journal of the American statistical Association, 97(458):611â€“631, 2002.

Rachid Guerraoui, SÂ´ebastien Rouault, et al. The hidden vulnerability of distributed learning in
byzantium. In International Conference on Machine Learning, pp. 3521â€“3530. PMLR, 2018.

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.

Suyi Li, Yong Cheng, Wei Wang, Yang Liu, and Tianjian Chen. Learning to detect malicious clients
for robust federated learning. arXiv preprint arXiv:2002.00211, 2020.

Hsiao-Ying Lin and Wen-Guey Tzeng. An efficient solution to the millionairesâ€™ problem based on
homomorphic encryption. In International Conference on Applied Cryptography and Network
_Security, pp. 456â€“466. Springer, 2005._


-----

Jinhyun So, BasÂ¸ak GÂ¨uler, and A. Salman Avestimehr. Byzantine-resilient secure federated learning.
_IEEE Journal on Selected Areas in Communications, 39(7):2168â€“2181, 2021. doi: 10.1109/_
JSAC.2020.3041404.

Jacob Steinhardt. Robust learning: Information theory and algorithms. PhD thesis, Stanford University, 2018.

Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. Data poisoning attacks against
federated learning systems - github. https://github.com/git-disl/DataPoisoning FL, 2020a.

Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. Data poisoning attacks against
federated learning systems. In European Symposium on Research in Computer Security, pp. 480â€“
501. Springer, 2020b.

Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B Giannakis. Byrd-saga - github.
https://github.com/MrFive5555/Byrd-SAGA, 2020a.

Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B Giannakis. Federated variance-reduced
stochastic gradient descent with robustness to byzantine attacks. IEEE Transactions on Signal
_Processing, 68:4583â€“4596, 2020b._

Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. 2017.

Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno: Distributed stochastic gradient descent with
suspicion-based fault-tolerance. In International Conference on Machine Learning, pp. 6893â€“
6901. PMLR, 2019.

Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno++: Robust fully asynchronous sgd. In Interna_tional Conference on Machine Learning, pp. 10495â€“10503. PMLR, 2020._

Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In International Conference on Machine Learning,
pp. 5650â€“5659. PMLR, 2018.

Lan Zhang, Xiang-Yang Li, Yunhao Liu, and Taeho Jung. Verifiable private multi-party computation: ranging and ranking. In 2013 Proceedings IEEE INFOCOM, pp. 605â€“609. IEEE, 2013.

A NEURAL NETWORK CONFIGURATIONS

We train these models with a batch size of 10, an SGD optimizer operates with a learning rate of
0.01, and 0.5 momentum for 25 epochs. The accuracy of the model is evaluated on a holdout set of
1000 samples.

A.1 FASHION-MNIST AND MNIST

-  Layer 1: 1 âˆ— 16 âˆ— 5, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.

-  Layer 2: 16âˆ—32âˆ—5, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.

-  Output: 10 Classes, Linear.


A.2 CIFAR-10

-  Layer 1: 1 âˆ— 32 âˆ— 3, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.

-  Layer 2: 32âˆ—32âˆ—3, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.

-  Output: 10 Classes, Linear.


-----

B METRICS

The metrics observed in Section 3 to evaluate the performance of the defense mechanisms are defined as follows:

TP
Precision =

TP+FP _[,]_

TP+TN
Accuracy =

TP+FP+FN+TN _[,]_

TP
Recall =

TP+FN _[,]_

F1 = 2 [Precision][ Ã—][ Recall]
_Ã—_ Precision+Recall _[.]_


C PROOF OF THEOREM 1

_Proof. Because_ **_Ri,j_** 1 _j_ _p are independent random variables with a finite upper bound (since n_
_{_ _}_ _â‰¤_ _â‰¤_
is fixes) as assumed, direct application of KSLLN leads to


**_Ri,j âˆ’_** E(Ri,j) = 0 a.s., (11)



lim
_pâ†’âˆ_


_j=1_


_p_

1 2
lim **_Ri,j_** E(Ri,j) Var(Ri,j) = 0 a.s.. (12)
_pâ†’âˆ_ _p_ _j=1_ _âˆ’_ _âˆ’_

X h   i

To prove Theorem 1 based on Equation 11 and 12, we need to derive the concrete form of E(Ri,j)
and Var(Ri,j).

Fortunately, because Mi,j _d_ _Âµj, Î£j,j_ for _i_ _m and Mi,j_ _d_ _Âµj, Ïƒj[2][/N][i]_ for _i_ _b_
when N _[âˆ—]_ _â†’âˆ, it is straightforward to see due to the symmetry of Gaussian distribution that â†’ N_    _âˆ€_ _âˆˆI_ _â†’ N_    _âˆ€_ _âˆˆI_

lim _, 1_ _i_ _n, 1_ _j_ _p._ (13)
_N_ _[âˆ—]â†’âˆ_ [E][(][R][i,j][) =][ n][ + 1]2 _â‰¤_ _â‰¤_ _â‰¤_ _â‰¤_

Moreover, assuming that the sample sizes of different benign nodes approach to each other with N _[âˆ—]_
going to infinity, i.e.,

1
lim (14)
_N_ _[âˆ—]â†’âˆ_ _N_ _[âˆ—]_ _i,k[max]âˆˆIb_ _[|][N][i][ âˆ’]_ _[N][k][|][ = 0][,]_

for each parameter dimension j, {Mi,j}iâˆˆIb would converge to the same Gaussian distribution
_N_ (Âµj, Ïƒj[2][/N][ âˆ—][)][ with the increase of][ N][ âˆ—][.] Thus, due to the exchangeability of {Mi,j}iâˆˆIb and
_{Mi,j}iâˆˆIm, it is easy to see that there exist two positive constants s[2]b_ [and][ s]m[2] [, such that]

lim _b_ _m,j_ (15)
_N_ _[âˆ—]â†’âˆ_ [Var(][R][i,j][) =][ s][2] _[Â·][ I][(][i][ âˆˆI][b][) +][ s][2]_ _[Â·][ I][(][i][ âˆˆI][m][)][,]_

where s[2]b,j [and][ s]m,j[2] [are both complex functions of][ n][0][,][ n][1][,][ Ïƒ]j[2][,][ Î£][j,j][ and][ N][ âˆ—][, and][ s]b,j[2] [=][ s]m,j[2] [if and]
only if Ïƒj[2][/N][ âˆ—] [= Î£][j,j][.]


lim
_pâ†’âˆ_


_j=1_


Combining Equation 11 and 13, we have

lim lim
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_ _[e][i][ =]_ _N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_


**_Ri,j =_** _[n][ + 1]_

2

_j=1_

X


_a.s.,_


i.e., Equation 4, which further indicates that ei and E(Ri,j) share the same limit when both p and
_N_ _[âˆ—]_ go to infinity. Thus, we have


2
**_Ri,j_** _ei_
_âˆ’_


2
**_Ri,j âˆ’_** E(Ri,j) _a.s.._ (16)



lim = lim
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_ _[v][i]_ _N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_

= lim
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_


_j=1_

_p_

_j=1_

X


-----

Combining Equation 12, 15, and 16, we have

lim _vi_ _sÂ¯[2]b_ _s[2]m_ = 0 a.s.,
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_ _âˆ’_ _[Â·][ I][(][i][ âˆˆI][b][)][ âˆ’]_ [Â¯] _[Â·][ I][(][i][ âˆˆI][m][)]_
  

i.e., Equation 5. Thus, the proof is complete.

D PROOF OF THEOREM 2

_Proof. It is straightforward to see that equation 11 also holds for sign flipping attack under the_
assumptions of Theorem 2. But, we need to re-calculate E(Ri,j) for benign and malicious nodes
under the new setting.

Under the sign flipping attack, because Mi,j â†’d _N_ _Âµj(r), Ïƒj[2][(][r][)]_ for âˆ€ _i âˆˆIm and Mi,j â†’d_

_N_ _Âµj, Ïƒj[2][/N][i]_ for âˆ€ _i âˆˆIb when N_ _[âˆ—]_ _â†’âˆ, and_   
   lim _j_ _[/N][i][) =]_ lim _j_ [(][r][) = 0][,]

_N_ _[âˆ—]â†’âˆ[(][Ïƒ][2]_ _N_ _[âˆ—]â†’âˆ_ _[Ïƒ][2]_

it is straightforward to see that

lim
_N_ _[âˆ—]â†’âˆ_ _[P]_ [(][M][i,j][ >][ M][k,j][) =][ I][(][Âµ][j][ >][ 0)][,][ âˆ€] _[i][ âˆˆI][b][,][ âˆ€]_ _[k][ âˆˆI][m][,]_

which further indicates that


lim
_N_ _[âˆ—]â†’âˆ_ [E][(][R][i,j][) =][ n][1][ + 1]2

lim
_N_ _[âˆ—]â†’âˆ_ [E][(][R][i,j][) =][ n][0][ + 1]2


I(i _b) +_ _[n][ +][ n][1][ + 1]_ I(i _m) if Âµj > 0,_

_Â·_ _âˆˆI_ 2 _Â·_ _âˆˆI_

(17)

I(i _m) +_ _[n][ +][ n][0][ + 1]_ I(i _b) if Âµj < 0;_

_Â·_ _âˆˆI_ 2 _Â·_ _âˆˆI_


_Nlim[âˆ—]â†’âˆ_ [E][(][R]i,j[2] [) =][ S][1[2] _,n1]_ _[Â·][ I][(][i][ âˆˆI][b][) +][ S][[2]n1+1,n]_ _[Â·][ I][(][i][ âˆˆI][m][)][ if Âµ][j][ >][ 0][,]_ (18)

_Nlim[âˆ—]â†’âˆ_ [E][(][R]i,j[2] [) =][ S][1[2] _,n0]_ _[Â·][ I][(][i][ âˆˆI][m][) +][ S][[2]n0+1,n]_ _[Â·][ I][(][i][ âˆˆI][b][)][ if Âµ][j][ <][ 0][,]_

where S[[2]a,b] [=] _b_ _a1+1_ _bk=a_ _[k][2][.]_

_âˆ’_

Combining Equation 11 and 17, we haveP


_Ï_ _[n][+][n]2[1][+1]_ + (1 _Ï)_ _[n][0]2[+1]_
_Â·_ _âˆ’_ _Â·_

_Ï_ _[n][1]2[+1]_ + (1 _Ï)_ _[n][+][n]2[0][+1]_
_Â·_ _âˆ’_ _Â·_

, i.e., Equation 8.


= Â¯Âµm a.s., if i âˆˆIm,

= Â¯Âµb a.s., if i âˆˆIb,


lim
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_ _[e][i][ =]_


_p_
_j=1_ [I][(][Âµ][j] _[>][0)]_

_p_

P


where Ï = limpâ†’âˆ


Define Â¯Âµi = Â¯Âµm Â· I(i âˆˆIm) + Â¯Âµb Â· I(i âˆˆIb). Based on KSLLN, we have:

_p_

1
lim (Ri,j _ÂµÂ¯i)[2]_ E(Ri,j _ÂµÂ¯i)[2][i]_ = 0 a.s..
_pâ†’âˆ_ _p_ _j=1_ _âˆ’_ _âˆ’_ _âˆ’_

X h

As we have proved in Equation 8 that

lim _Âµi a.s.,_
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_ _[e][i][ = Â¯]_


we have

which implies that


(Ri,j âˆ’ _ei)[2]_ _âˆ’_ E(Ri,j âˆ’ _ÂµÂ¯i)[2][i]_ = 0 a.s.,
h


lim
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_


_j=1_


(Ri,j _ei)[2]_ = lim
_j=1_ _âˆ’_ _N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_

X


_j=1_ E(Ri,j âˆ’ _ÂµÂ¯i)[2]_ _a.s.._

X


lim lim
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_ _[v][i][ =]_ _N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_


-----

Considering that


_j=1_ E(Ri,j âˆ’ _ÂµÂ¯i)[2]_

X


lim
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_


_p_

1
= lim lim E(Ri,j[2] [)][ âˆ’] [2Â¯]ÂµiE(Ri,j) + (Â¯Âµi)[2][]
_pâ†’âˆ_ _N_ _[âˆ—]â†’âˆ_ _p_ _j=1_

X 

= _Ï„Â¯m âˆ’_ (Â¯Âµm)[2][] _Â· I(i âˆˆIm) +_ _Ï„Â¯b âˆ’_ (Â¯Âµb)[2][] _Â· I(i âˆˆIb),_
where
 
_Ï„Â¯b = Ï Â· S[1[2]_ _,n1]_ [+ (1][ âˆ’] _[Ï][)][ Â·][ S][[2]n0+1,n][,]_

_Ï„Â¯m = Ï Â· S[[2]n1+1,n]_ [+ (1][ âˆ’] _[Ï][)][ Â·][ S][1[2]_ _,n0][.]_
we have
lim _Ï„Â¯m_ (Â¯Âµm)[2][] I(i _m) +_ _Ï„Â¯b_ (Â¯Âµb)[2][] I(i _b)._
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_ _[v][i][ =]_ _âˆ’_ _Â·_ _âˆˆI_ _âˆ’_ _Â·_ _âˆˆI_

It completes the proof of Equation 9 by specifying Â¯s[2]b [= Â¯]Ï„b âˆ’(Â¯Âµb)[2] and Â¯s[2]m [= Â¯]Ï„m âˆ’ (Â¯Âµm)[2].

E PROOF OF THEOREM 3


_Proof. According to Theorem 1, 2 and Corollary 1, when both N_ _[âˆ—]_ and p are large enough, with
probability 1 there exist (eb, vb), (em, vm) and Î´ > 0 such that ||(eb, vb) âˆ’ (em, vm)||2 > Î´, and

(ei, vi) (eb, vb) 2 and (ei, vi) (em, vm) 2
_||_ _âˆ’_ _||_ _â‰¤_ 2[Î´] [for][ âˆ€] _[i][ âˆˆI][b]_ _||_ _âˆ’_ _||_ _â‰¤_ 2[Î´] [for][ âˆ€] _[i][ âˆˆI][m][.]_

Therefore, with a reasonable clustering algorithm such as K-mean with K = 2, we would expect
Ë†
_Ib = Ib with probability 1._

Because we can always find a âˆ† _> 0 such that_ **_Mi,:_** **_Mj,:_** 2 âˆ† for any node pair (i, j) in a
_||_ _âˆ’_ _||_ _â‰¤_
fixed dataset with a finite number of nodes, and Ë†mb,: = mb,: when _I[Ë†]b = Ib, we have_

E|| Ë†mb,: âˆ’ **_mb,:||2 â‰¤_** âˆ† _Â· P(I[Ë†]b Ì¸= Ib),_
and thus


lim **_mb,:_** **_mb,:_** 2 = 0.
_N_ _[âˆ—]â†’âˆ_ _p[lim]â†’âˆ_ [E][||][ Ë†] _âˆ’_ _||_


It completes the proof.


F MANDERA PERFORMANCE WITH DIFFERENT CLUSTERING ALGORITHMS

In this section, Figure 6 demonstrate that the discriminating performance of MANDERA when
hierarchical clustering and Gaussian mixture models are used in-place of K-means for FASHIONMNIST data set remain robust.

G MODEL LOSSES ON CIFAR-10 AND FASHION-MNIST DATA

Figure 7 presents the model loss to accompany the model prediction performance of Figure 5 previously seen in Section 3.

H MODEL PERFORMANCE ON MNIST DATA

In this section we replicate experiments that were previously performed in Section 3 on the
MNIST (Deng, 2012) dataset. The MNIST dataset is a dataset of 60,000 and 10,000 training and
testing samples respectively divided into 10 classes of handwritten digits from multiple authors.
Figure 8 contains the performance characteristics of MANDERAâ€™s defense against the four attacks,
whilst Figure 9 contains the comparative accuracy and loss when the different defenses are applied.
Generally speaking, the observations previously observed continue to hold for this dataset.


-----

Metric Accuracy Recall Precision F1

GA ZG SF LF

1.00

0.75

0.50

0.25 Accuracy

0.00

1.00

0.75

0.50

Recall

0.25

0.00

Score 1.00

0.75

0.50

0.25 Precision

0.00

1.00

0.75

0.50 F1

0.25

0.00

5 1015202530 5 1015202530 5 1015202530 5 1015202530

Number of malicious nodes


(a) Gaussian mixture model.

Metric Accuracy Recall Precision F1

GA ZG SF LF

1.00

0.75

0.50

0.25 Accuracy

0.00

1.00

0.75

0.50

Recall

0.25

0.00

Score 1.00

0.75

0.50

0.25 Precision

0.00

1.00

0.75

0.50 F1

0.25

0.00

5 1015202530 5 1015202530 5 1015202530 5 1015202530

Number of malicious nodes


(b) Hierarchical clustering.

Figure 6: Classification performance of our proposed approach MANDERA (Algorithm 1) with
other clustering algorithms under four types of attack for FASHION-MNIST data. GA: Gaussian
attack; ZG: Zero-gradient attack; SF: Sign-flipping; and LF: Label-flipping. The boxplot bounds the
25th (Q1) and 75th (Q3) percentile, with the central line representing the 50th quantile (median).
The end points of the whisker represent the Q1-1.5(Q3-Q1) and Q3+1.5(Q3-Q1) respectively.


-----

Krum Bulyan Trimâˆ’mean FLTrust

Defence

NOâˆ’attack Median MANDERA

5 10 15 20 25 30

8

6

GA

4

4.5

4.0

3.5 ZG

3.0

2.5

3.2

log(Loss)

3.0

SF

2.8

2.6

3.1

2.9 LF

2.7

2.5

0 5 10 15 20 250 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25

Number of Epoch


(a) CIFAR-10

Krum Bulyan Trimâˆ’mean FLTrust

Defence

NOâˆ’attack Median MANDERA

5 10 15 20 25 30

7.5

5.0 GA

2.5

6
5
4 ZG
3
2
1

log(Loss) 2.5

2.0 SF

1.5

2.25
2.00
1.75 LF
1.50
1.25

0 5 10 15 20 250 5 10 15 20 250 5 10 15 20 250 5 10 15 20 250 5 10 15 20 250 5 10 15 20 25

Number of Epoch


(b) FASHION-MNIST

Figure 7: Model Loss at each epoch of training, each line of the curve represents a different defense
against the attacks (GA: Gaussian attack; ZG: Zero-gradient attack; SF: Sign-flipping; and LF:
Label-flipping).


-----

Metric Accuracy Recall Precision F1

GA ZG SF LF

1.00

0.75

0.50

0.25 Accuracy

0.00

1.00

0.75

0.50

Recall

0.25

0.00

Score 1.00

0.75

0.50

0.25 Precision

0.00

1.00

0.75

0.50 F1

0.25

0.00

5 1015202530 5 1015202530 5 1015202530 5 1015202530

Number of malicious nodes


Figure 8: Classification performance of our proposed approach MANDERA (Algorithm 1) under
four types of attack for MNIST data. GA: Gaussian attack; ZG: Zero-gradient attack; SF: Signflipping; and LF: Label-flipping. The boxplot bounds the 25th (Q1) and 75th (Q3) percentile, with
the central line representing the 50th quantile (median). The end points of the whisker represent the
Q1-1.5(Q3-Q1) and Q3+1.5(Q3-Q1) respectively.


-----

Krum Bulyan Trimâˆ’mean FLTrust

Defence

NOâˆ’attack Median MANDERA

5 10 15 20 25 30

100

75

50 GA

25

100

90

80 ZG

70

60

100

Accuracy 90

80 SF

70

60

96

92 LF

88

84

0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25

Number of Epoch


(a) Model prediction accuracy after defense

Krum Bulyan Trimâˆ’mean FLTrust

Defence

NOâˆ’attack Median MANDERA

5 10 15 20 25 30

7.5

5.0

GA

2.5

0.0

4
3
2 ZG
1
0

3

log(Loss) 2

1 SF

0

1.5
1.0
0.5 LF
0.0

âˆ’0.5

0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25

Number of Epoch


(b) Model prediction loss after defense

Figure 9: Model Accuracy and Loss for MNIST data at each epoch of training, each line of the curve
represents a different defense against the attacks (GA: Gaussian attack; ZG: Zero-gradient attack;
SF: Sign-flipping; and LF: Label-flipping).


-----

