# EQUIVARIANT GRAPH MECHANICS NETWORKS WITH CONSTRAINTS

**Wenbing Huang[âˆ—]** **[1], Jiaqi Han[âˆ—][2][â€ ], Yu Rong[B][3], Tingyang Xu[3], Fuchun Sun[B][2], Junzhou Huang[4]**

1 Institute for AI Industry Research (AIR), Tsinghua University
2 Beijing National Research Center for Information Science and Technology (BNRist),
Department of Computer Science and Technology, Tsinghua University
3 Tencent AI Lab
4 Department of Computer Science and Engineering, University of Texas at Arlington
hwenbing@126.com, hanjq21@mails.tsinghua.edu.cn, yu.rong@hotmail.com
tingyangxu@tencent.com, fcsun@mail.tsinghua.edu.cn, jzhuang@uta.edu

ABSTRACT

Learning to reason about relations and dynamics over multiple interacting objects
is a challenging topic in machine learning. The challenges mainly stem from that
the interacting systems are exponentially-compositional, symmetrical, and commonly geometrically-constrained. Current methods, particularly the ones based on
equivariant Graph Neural Networks (GNNs), have targeted on the first two challenges but remain immature for constrained systems. In this paper, we propose
Graph Mechanics Network (GMN) which is combinatorially efficient, equivariant
and constraint-aware. The core of GMN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward kinematics. Moreover, to allow equivariant message
passing in GMN, we have developed a general form of orthogonality-equivariant
functions, given that the dynamics of constrained systems are more complicated
than the unconstrained counterparts. Theoretically, the proposed equivariant formulation is proved to be universally expressive under certain conditions. Extensive experiments support the advantages of GMN compared to the state-of-the-art
GNNs in terms of prediction accuracy, constraint satisfaction and data efficiency
on the simulated systems consisting of particles, sticks and hinges, as well as two
real-world datasets for molecular dynamics prediction and human motion capture.

1 INTRODUCTION


Representing and reasoning about the relations and dynamics of a group of interacting
objects is among the core aspects of human
intelligence (Tenenbaum et al., 2011; Ding Stick Hinge
et al., 2021; Gan et al., 2021). As a motivating example, we consider the N-body system (Kipf et al., 2018) where the movement
of a single charged particle is attracted or
repelled by other charged particles. Physi
(a) N-body System (b) Constrained N-body System

Stick Hinge

cists have revealed that this process can be

(a) N-body System

Figure 1: N-body vs. constrained N-body (red/blue

modeled by Newtonâ€™s laws along with the

balls denote positive/negative charges).

Coulomb force. One may wonder, however,
if we can teach a machine to rediscover the underlying physics by solely observing the particlesâ€™
states. This thinking has inspired the study of learning to model interacting systems, which now is a
prevailing topic in machine learning (Battaglia et al., 2016; Thomas et al., 2018; KÂ¨ohler et al., 2019;
Sanchez-Gonzalez et al., 2019; Fuchs et al., 2020; Martinkus et al., 2021; Satorras et al., 2021).

_âˆ—Equal contributions: Wenbing Huang and Jiaqi Han; B Corresponding authors: Yu Rong and Fuchun Sun._
_â€ This work is done when Jiaqi Han works as an intern in Tencent AI Lab._


-----

Learning to model interacting systems is challenging. First, the systems are combinatorially
complex, on account of that objects can be composed in combinatorially many possible arrangements (Battaglia et al., 2016). This challenge, to some extent, can be addressed by making use of
Graph Neural Networks (GNNs) (Wu et al., 2020). By regarding objects as nodes and interactions
as edges, GNNs extract information via message passing, which is able to characterize arbitrarily
ordered objects and combinatorial relations. The second challenge is related to an important symmetry in physics: the model we use should be equivariant to any Euclidean transformation (translation/reflection/rotation) of the input. This complies with the fact that physics rules keep unchanged
regardless of the reference coordinate system. Several works (Fuchs et al., 2020; Satorras et al.,
2021) have investigated equivariance upon GNNs and exhibited remarkable benefits on N-body.

Another challenge, though less explored, is that the systems could be geometrically constrained.
Geometric constraints arise in common practical systems, for example, in robotics where the joints
of mechanical arms should be linked one by one, or in biochemistry where the atoms of molecules
are connected by chemical bonds. When modeling these constrained systems, it is crucial to enforce the model to output legal predictions. For instance, the lengths/angles of chemical bonds in a
molecular closely determine its chemical property which will be changed dramatically if the structural constraints are broken. As mentioned above, equivariant GNNs (Fuchs et al., 2020; Satorras
et al., 2021) have achieved desirable performance on the N-body systemâ€”this system, nevertheless,
lacks of constraint. Considering constraints is not easy, as the dynamics of all elements within a
constrained system evolve in a joint and complex manner. Unfortunately, to the best of our knowledge, there is no research (particularly among equivariant GNN methods) that learns to model the
dynamical systems of multiple interacting objects under geometrical constraints.

In this paper, we propose Graph Mechanics Network (GMN) that can tackle the above three challenges simultaneously: I. To cope with the combinatorial complexity, GMN takes advantage of
graph models to encode the states of and interactions between objects, analogous to previous approaches. II. For the constraint satisfaction, GMN resorts to generalized coordinates, a well known
notion in conventional mechanics to model the dynamics of structural objects. Here, a structural
object (such as the stick and hinge in Fig. 1) is defined as a set of multiple rigidly connected particles. In GMN, the constraints are implicitly encoded in the forward kinematics that describes the
Cartesian states as the function of generalized coordinates. This strategy can inherently maintain
the constraints and requires no external regulation. III. GMN is equivariant. In GMN, we need to
compute the interaction forces in the Cartesian space (see Eq. 5), and infer the accelerations of the
generalized coordinates based on the inverse dynamics (see Eq. 6), both of which are learned by a
general form of equivariant functions (see Eq. 11). Notably, the proposed equivariant formulation is
more efficient to compute compared to the previous versions based on spherical harmonics (Thomas
et al., 2018; Fuchs et al., 2020), or more general than the one developed by EGNN (Satorras et al.,
2021). More importantly, we have theoretically discussed when and how our formulation can universally approximate any equivariant function.

To evaluate the advantages of GMN, we have constructed a simulated dataset composed of three
types of objects: particles, sticks and hinges, which is a complex case of the N-body system (Kipf
et al., 2018) and can be used as building blocks for common systems. Under various scenarios
in terms of different ratios of object types and different numbers of training data, we empirically
verify the superiority of GMN compared to state-of-the-art models in prediction accuracy, constraint
satisfaction and data efficiency. In addition, we also test the effectiveness of GMN on two real-world
datasets: MD17 (Chmiela et al., 2017) and CMU Motion Capture (CMU, 2003).

2 RELATED WORK

Learning to simulate complex physical systems has been shown to greatly benefit from using GNNs.
Interaction Network (IN) proposed by Battaglia et al. (2016) is the first attempt for this purpose, and
it can be deemed as a special kind of GNNs to learn how the system interacts and how the states
of particles evolve. Later researches have extended IN in different aspects: HRN (Mrowca et al.,
2018) utilizes hierarchical graph convolution for tackling objects of various geometrical shapes and
materials, NRI (Kipf et al., 2018) further explicitly infers the interactions with the help of a variational auto-encoder, and Hamiltonian graph networks (Sanchez-Gonzalez et al., 2019) equip GNNs
with ordinary differential equations and Hamiltonian mechanics for energy conservation. However,


-----

all above approaches have ignored the symmetry in physics and the GNN models they use are not
Euclidean equivariant. There is a subset of models (Ummenhofer et al., 2019; Sanchez-Gonzalez
et al., 2020; Pfaff et al., 2020) that partially implement symmetries, but they only enforce translation
equivariance but not rotation equivariance. On the other hand, it is nontrivial to enforce rotation
equivariance. Tensor-Field networks (Thomas et al., 2018) uses filters built from spherical harmonics to allow 3D rotation equivariance, and this idea has been developed by SE(3) Transformer (Fuchs
et al., 2020) that further takes the attention mechanism into account. Anther class of works (Finzi
et al., 2020a; Hutchinson et al., 2021) resorts to the Lie convolution for the equivariance on any Lie
group, based on lifting and sampling. Recently, EGNN (Satorras et al., 2021) has proposed a simple
yet effective form of equivariant message passing on graphs, which does not require computationally
expensive higher-order representations while still achieving better performance on N-body.

As interpreted before, ensuring geometrical constraints is crucial for many practical systems, which,
nevertheless, is seldom investigated in aforementioned works. Although several attempts (Yang
et al., 2020; Finzi et al., 2020b) have been proposed for learning to enforce constraints, they explicitly augment the training loss with soft Lagrangian regulation and thus are completely data-driven
and have no guarantee of generalization for limited training data. DeLaN (Lutter et al., 2019) also
employs generalized coordinates to describe the kinematics of the rigid object. Nevertheless, it only
target on the physical process of a single object, other than the complex systems with multiple rigid
and structural objects that are the focus of this paper. In DPI-Net (Li et al., 2018), the BoxBath task
does share the similar strategy to us by first predicting the canonical coordinates of the box and then
using the forward kinematic model to obtain the Cartesian positions. However, the passing messages
in DPI-Net are scalars other than directional vectors (positions, velocities, and accelerations) used
in our work. After all, both DeLaN and DPI-Net never study equivariant models.

3 GRAPH MECHANICS NEURAL NETWORK

We begin with introducing the N-body system (Kipf et al., 2018), as illustrated in Fig. 1 (a). This
system consists of N interacting particles {Pi}i[N]=1 [of the same mass, and the kinematics states of]
respectively. There could be certain non-vector information of each particle (such as charge), whicheach particle are defined as Si = (xi, vi), where xi, vi âˆˆ R[3] are the position and velocity vectors,
is represented by athe dynamics is driven by the interaction force c-channel feature hi âˆˆ R[c][ 1]. Suppose the system we study is conservative and fij âˆˆ R[3] between any pair of particles i and j.
According to Newtonâ€™s second law, the acceleration of particleaggregated force from other particles _j=i_ **_[f][ij][. All symbols will be specified with a superscript] i, ai âˆˆ_** R[3] is proportional to the[ t]

_Ì¸_
for temporal denotations, e.g., x[t]i [indicating the position of particle][ i][ at time][ t][. In this paper, we are]
mainly concerned with the prediction task: we need to seek out a function[P] _Ï†({(Si[0][, h]i[0][)][}]i[N]=1[)][ given]_
the initial states {Si[0][}]i[N]=1 [and features][ {][h]i[0][}]i[N]=1 [to forecast the future states][ {][S]i[T] _[}]i[N]=1_ [at time][ T] [.]

As presented in Introduction, two kinds of inductive biases have been explored previously. The first
one is to apply the graph structure to capture the distribution of particle states and their interactions (Battaglia et al., 2016; Kipf et al., 2018), where, particularly, the particle states Si[0] [(along with]
_h[0]i_ [) are as node features and the interaction forces][ f][ 0]ij [as edge messages. In this way, the transition]
function Ï† boils down to a GNN model. The second inductive bias is that Ï† should be equivariant to
any translation/reflection/rotation of the input states. By saying equivariance, we imply

_Ï†({(g Â· Si[0][, h]i[0][)][}]i[N]=1[) =][ g][ Â·][ Ï†][(][{][(][S]i[0][, h]i[0][)][}]i[N]=1[)][,][ âˆ€][g][ âˆˆO][(3)][,][ âˆ€][S]i[0][,][ âˆ€][h]i[0][.]_ (1)


Here, O(3) defines the 3D orthogonal group (Fuchs et al., 2020) that consists of translation, reflection and rotation transformations; g Â· Si[0] [denotes to perform transformation][ g][ on the states][ S]i[0][, and]
it is instantiated as R(x[0]i [+][ b][)][ for the position and][ Rv]i[0] [for the velocity, where][ R][ âˆˆ] [R][3][Ã—][3][ is the]
orthogonal matrix and b âˆˆ R[3] is the translation vector.

Several works (Thomas et al., 2018; KÂ¨ohler et al., 2019; Fuchs et al., 2020; Satorras et al., 2021)
have investigated both inductive biases, among which EGNN (Satorras et al., 2021) has achieved
promising performance on N-body. The typical process in EGNN iterates the following steps:

1Although hi could be of multiple dimensions, we still call it as a non-vector value since the dimension of
_hi is distinct from that of the kinematics vector, for example xi, the latter of which has intrinsic geometry and_
should obey translation/rotation equivariance. This is also why we do not denote hi in bold.


-----

where the superscript l denotes the l-th layer;

**_a[l]i[, h][l]i_** [=] _Ï•egnn(x[l]ji[âˆ’][1][, h]i[l][âˆ’][1], h[l]j[âˆ’][1], eji),_ (2) the acceleration a[l]i [returned by the message ag-]

_j_ gregation of Ï•egnn in Eq. 2 is adopted for the

X

**_vi[l]_** [=][ Ïˆ][(][h][l]i[âˆ’][1])vi[l][âˆ’][1] + a[l]i[,] (3) update of the velocity vi[l] [in Eq. 3 (multiplied]

by a scalar Ïˆ(h[l]i[âˆ’][1]) R), followed by the ren
**_x[l]i_** [=][ x]i[l][âˆ’][1] + vi[l][,] (4) ovation of the position âˆˆ **_x[l]i_** [in Eq. 4. The formu-]

lation of Ï•egnn is physically reasonable, since the interaction (actually the Coulomb force) between
particles i and j truly depends on their relative position x[l]ji[âˆ’][1] = x[l]i[âˆ’][1] **_x[l]j[âˆ’][1], node features h[l]i[âˆ’][1]_**
_âˆ’_
and h[l]j[âˆ’][1], and edge feature eji. To enable equivariant message passing, EGNN has developed a
specific form of Ï•egnn to let a[l]i [be equivariant and][ h]i[l] [be invariant in terms of the input][ x]ji[l][âˆ’][1][, which]
will be presented in Eq. 10. Notice that the computations for fi[l] [and][ h]i[l] [are actually by two different]
functions, and we have abbreviated them into one in Eq. 2 (and also Eq. 5 later), since they share the
same inputs; besides, their parameters are shared following EGNN.

3.1 OUR GENERAL ARCHITECTURE

Despite the desirable performance on N-body, existing methods are incapable of maintaining the
geometric constraint. In this section, we design a general architecture that intrinsically meets the
requirement of geometry constraints by making use of the generalized coordinates.

We define Ok = {Pi}i[n]=1[k] [a structural object composed of][ n][k][ rigidly connected particles. Fig. 1 (b)]
illustrates two examples of the structural object, the stick with 2 connected particles and the hinge
with 3 particles. To preserve the distance, the dynamics of the two particles on a stick should be
updated in a joint way, rather than fulfilling the independent process in EGNN (Eq. 3 and 4). Besides,
the force on each particle within Ok will indirectly influence the dynamics of others through the
physical connections. This requires us to analyze the dynamics of the particles in Ok as a whole,
which is implemented by generalized coordinates. There could be multiple generalized coordinates
for each _k, some located in the Cartesian space but some in the angle space. For instance, the states_
_O_
of a stick can be decoupled by two independent sets of generalized coordinates: the state of particle 1
as the Cartesian coordinates and the relative rotation angles of particle 2 to 1 as the angle coordinates.
For conciseness, this section only focus on the Cartesian part which essentially determines the local
coordinates in _k, with providing full examples in Â§ 3.3. We denote the position, velocity and_
_O_
acceleration of the generalized Cartesian coordinates as qk, Ë™qk and Â¨qk, respectively.

We now detail how to update the states of _k._ In Fig. 2, we first compute the in_O_
teraction force between each particle and others, and aggregate information of all particles within each structural object to infer the acceleration of the generalized coordinates (which is termed as the generalized acceleration henceforth) by inverse dynamics.

Then, the dynamical updates are carried out in the
space of the generalized coordinates. Finally, the updated generalized coordinates will be projected backto the particlesâ€™ states via the forward kinematics. **Forward KinematicsCartesian Coordinates ğ’™ğ‘–, ğ’—ğ‘–**


**_fi[l][, h][l]i_** [=] _Ï•1(x[l]ji[âˆ’][1][, h]i[l][âˆ’][1], hj[l][âˆ’][1], eji),_ (5) Generalized Coordinates ğ’’ğ‘™ğ‘˜, áˆ¶ğ’’ğ‘™ğ‘˜
Xj Generalized Acceleration áˆ·ğ’’ğ‘™ğ‘˜

**_qÂ¨k[l]_** [=] _Ï•2(fi[l][,][ x][l]ki[âˆ’][1][,][ v]ki[l][âˆ’][1][)][,]_ (6) **Inverse Dynamics**

_iXâˆˆOk_ Interaction Force ğ’‡ğ‘™ğ‘–

**_qË™k[l]_** [=][ Ïˆ][(]iXâˆˆOk _h[l]i[âˆ’][1]) Ë™qk[l][âˆ’][1]_ + Â¨qk[l] _[,]_ (7) Cartesian Coordinates ğ’™ğ‘™âˆ’1ğ‘–, ğ’—ğ‘™âˆ’1ğ‘–

**_qk[l]_** [=][ q]k[l][âˆ’][1] + Ë™qk[l] _[,]_ (8) Generalized Coordinates ğ’’ğ‘™âˆ’1ğ‘˜, áˆ¶ğ’’ğ‘™âˆ’1ğ‘˜

**_x[l]i[,][ v]i[l]_** [=][ FK][(][q]k[l] _[,][ Ë™]qk[l]_ [)][,][ âˆ€][i][ âˆˆO][k][,] (9)

Cartesian Coordinates ğ’™ğ‘™ğ‘–, ğ’—ğ‘™ğ‘–

**Forward Kinematics**

Generalized Coordinates ğ’’ğ‘™ğ‘˜, áˆ¶ğ’’ğ‘™ğ‘˜

Generalized Acceleration áˆ·ğ’’ğ‘™ğ‘˜

**Inverse Dynamics**

Interaction Force ğ’‡ğ‘™ğ‘–

Cartesian Coordinates ğ’™ğ‘™âˆ’1ğ‘–, ğ’—ğ‘™âˆ’1ğ‘–

**Forward Kinematics**

Generalized Coordinates ğ’’ğ‘™âˆ’1ğ‘˜, áˆ¶ğ’’ğ‘™âˆ’1ğ‘˜

Figure 2: The flowchart of our GMN.

where, the elements of the system can be described
in two views, {Ok}k[K]=1 [as the object-level view and][ {][P][i][}]i[N]=1 [as the particle-level view; for distinc-]
tion, we index the structural object with the subscript k and particles with i or j. We explain each
equation separately.
**Interaction force (Eq. 5).** The interaction force fi[l] [is computed analogous to Eq. 2.] EGNN


-----

straightly regards the interaction force to be the acceleration of each particle in Eq. 2. But here,
given the constraints between particles, we record the force as an intermediate variable that will
contribute to the inference of the generalized acceleration in the next step.
**Inverse dynamics (Eq. 6). This step is the core of our methodology. The generalized accel-**
eration Â¨qk[l] [is dependent to the forces][ f][ l]i [on all particles within][ O][k][, and their relative positions]
**_xki[l][âˆ’][1]_** = x[l]i[âˆ’][1] **_qk[l][âˆ’][1]_** and relative velocities vki[l][âˆ’][1] = vi[l][âˆ’][1] **_qË™k[l][âˆ’][1]_** with regard to the generalized
_âˆ’_ _âˆ’_
coordinates. The formulation of Eq. 6 is physics-inspired. In Appendix (Eq. 26), we have analytically derived the dynamics of hinges, where the acceleration of the hinge is indeed related to the
forces, relative positions and relative velocities of all particles in each hinge. This is reasonable in
mechanics, since the cross product x[l]ki[âˆ’][1] _Ã— fi[l]_ [yields the torque, and the relative velocity][ v]ki[l][âˆ’][1] is
related to the centrifugal force of particle i around qk, both of which influence the acceleration Â¨qk.
Different from the analytical form which is always complex and hard to compute in practice, we will
employ a learnable and equivariant function with universal expressivity. The details are in Â§ 3.2.
**Generalized update (Eq. 7-8). The updates of the generalized coordinates are akin to Eq. 3 and 4**
in EGNN, as the dimensions of the generalized coordinates have been made independent. Notice
that in Eq. 7 the scalar factor for Ë™qk[l][âˆ’][1] takes as input the summation of all hidden features, which is
a generalized form of Eq. 3 for multiple particles in _k._
_O_
**Forward kinematics (Eq. 9). Once the generalized coordinates have been refreshed, we can derive**
the states of all particles in Ok by proceeding the forward kinematics. Different system of Ok could
have different type of the forward kinematics. Here we denote it as the function FK(Â·) in general,
while providing the specifications in Â§ 3.3.

Our method will reduce to EGNN if setting the generalized coordinates as the states of each particle
and utilize the identify map in Eq. 6 and 9. Alg. 1 has summarized the updates for all objects.

3.2 EQUIVARIANT MESSAGE PASSING

As shown in Eq. 1, the Euclidean equivariance on the estimation function Ï† is necessary for ensuring
the physical symmetry. When considering this property in our case, we demand the interaction force
(Eq. 5) and generalized acceleration (Eq. 6) to be equivariant with respect to orthogonal transformations, while other equations are already equivariant[2]. EGNN (Satorras et al., 2021) has developed a
particular orthogonality-equivariant form for the acceleration output in Eq. 2:
_Ï•egnn(x, h) := xÏƒw(âˆ¥xâˆ¥2[2][, h][)][,]_ (10)
where Ïƒw( ) is an arbitrary Multi-Layer Perceptron (MLP) with parameter w, and we have abbrevi
_Â·_
ated other non-vector terms in Eq. 2 as h. This formulation does satisfy the rotation equivariance,
but it is unknown if it can be generalized to functions (such as Eq. 6) with multiple input vectors, and
more importantly, its representation completeness is never explored rigorously. In this section, we
propose a general form of orthogonality-equivariant functions with necessary theoretical guarantees.

Without loss of generality, the target function we would like to enforce equivariance is denoted as
_Ï•(Z, h) : R[d][Ã—][m]_ _Ã— R[c]_ _â†’_ R[d][Ã—][m][â€²] . We define the below formulation,

_Ï•(Z, h) := ZÏƒw(Z_ _[âŠ¤]Z, h)._ (11)

It is easy to justify the function Ï•(Z, h) in Eq. 11 is equivariant to any orthogonal matrix i.e.,
_Ï•(OZ, h) = OÏ•(Z, h), âˆ€O âˆˆ_ R[d][Ã—][d], O[âŠ¤]O = I. Apparently, Eq. 11 reduces to Eq. 10 by setting
the number of vectors in Z as 1, namely, m = m[â€²] = 1. We immediately have the following theory.
**Theorem 1. If m â‰¥** _d and the row rank of Z is full, i.e. rank(Z) = d, then for any continuous_
_orthogonality-equivariant function Ë†Ï•(Z, h), there must exist an MLP Ïƒw satisfying âˆ¥Ï•(Z, h) âˆ’_
_Ï•Ë†(Z, h)âˆ¥_ _< Ïµ for arbitrarily small error Ïµ._

The proof employs the universality of MLP (Cybenko, 1989; Hornik, 1991), with the entire details
deferred in Appendix. Theorem 1 is nutritive, as it characterizes the rich expressivity of formulating
the equivariant function via Eq. 11. The condition holds in general for the function like Eq. 6 whose
input Z = (fi, xki, vki) âˆˆ R[3][Ã—][3] is of full rank when the force, position and velocity expand the
whole space. Indeed, this condition holds with probability 1, stated by the following corollary.

2The translation equivariance holds naturally as we use relative positions in Eq. 5, making the force, acceleration, velocity to be invariant with respect to translation. After the addition with the previous position in
Eq. 8, the updated position qk[l] [is translation equivariant.]


-----

**Corollary 1. Assume m â‰¥** _d, and also the entries of Z are drawn independently from a distribution_
_that is absolutely continuous with respect to the Lebesgue measure in R. Then, almost surely, the_
_conclusion of Theorem 1 holds._

When the condition m â‰¥ _d is invalid, the universal approximation still maintains if restricted in the_
linear subspace expanded by the columns of Z.
**Corollary 2. For any continuous orthogonality-equivariant function Ë†Ï•(Z, h) located in the linear**
_subspace expanded by the columns of Z, the conclusion of Theorem 1 holds universally._

Corollary 2 tells that the message passing Ï•egnn in Eq. 10 is not universally expressive since m < d,
and can only fit the vectors parallel to x (i.e. the relative position). Yet, Ï•egnn is still physically
complete for the Coulomb force that is oriented by the relative position between two particles.

In our experiments, we find that more stable performance is delivered by further adding the normalization term specifically when m > 1:
_Ï•(Z, h) := ZÏƒw(Z_ _[âŠ¤]Z/âˆ¥Z_ _[âŠ¤]Zâˆ¥F, h),_ (12)
where âˆ¥Â· âˆ¥F computes the Frobenius norm. Notice that adding normalization does not change the
conclusions in Theorem 1 and its two corollaries, since the norm is also a function of Z _[âŠ¤]Z that_
can be approximated by MLP. We implement Ï•1 in Eq. 5 and Ï•2 in Eq. 6 by using the general
formulation Eq. 12, where Z = xji[l][âˆ’][1] and Z = (fi[l][,][ x][l]ki[âˆ’][1][,][ v]ki[l][âˆ’][1][)][, respectively. Note that for the]
update of hidden feature h[l]i [in Eq. 5, we do not need equivariance but invariance, hence we set]
_h[l]i_ [=][ Ïƒ][w][(][Z] _[âŠ¤][Z][/][âˆ¥][Z]_ _[âŠ¤][Z][âˆ¥][F][, h][l]i[âˆ’][1]) by just keeping the invariant part in Eq. 12._

3.3 IMPLEMENTATIONS OF FORWARD KINEMATICS

**Implementation of hinges. A hinge, as displayed in Fig.3 (a), consists of three particles 0, 1,**
2, and two sticks 01 and 02. The freedom degrees of this system can be explained in this way:
particle 0 moves freely, and particles 1 and 2 can only rotate round particle 0 owing to the length
constraint by the two sticks. Hence, the generalized coordinates include the states of particle 0
denoted asAs q0 are Cartesian, the dynamical update fol- q0 âˆˆ R[3] and the rotation Euler angles of stick 01 as Î¸01 âˆˆ R[3] and 02 as Î¸02 âˆˆ R[3].

lows directly Eq. 5-8. With the forces **_fi[l][}][3]i=0_**
_{_
estimated in Eq. 5 and acceleration Â¨q0[l] [by Eq. 6,]
the angle acceleration of Î¸01 is calculated as
ing to rigid mechanics, where the numerator isÎ¸Â¨01[l] [= (][x]01[l][âˆ’][1] _Ã— (f1[l]_ _[âˆ’]_ **_q[Â¨]0[l]_** [))][/][âˆ¥][x][l]01[âˆ’][1][âˆ¥][2][ accord-]
the relative torque, the denominator is the moment of inertia under unit mass, and Ã— means
the cross product. Then the angle velocity is
**_Î¸Ë™01[l]_** [=][ Ïˆ][â€²][(][h]0[l][âˆ’][1] + _h[l]1[âˆ’][1]_ + _h[l]2[âˆ’][1]) Î¸[Ë™]01[l][âˆ’][1]_ [+ Â¨]Î¸01[l] [simi-]

Figure 3: Illustrations of hinges and sticks.

lar to Eq. 7. We do not need to record Î¸01, since
the forward kinematics can be conducted without it. In detail, the forward kinematics in Eq. 9 is:
**_x[l]1_** [=][ q]0[l] [+][ rot][( Ë™]Î¸01[l] [)][x][l]01[âˆ’][1][,] **_v1[l]_** [= Ë™]q0[l] [+ Ë™]Î¸01[l] _[Ã—][ x]01[l]_ _[,]_ (13)
where rot( Î¸[Ë™]01[l] [)][ indicates the rotation matrix around the direction of][ Ë™]Î¸01[l] [by absolute angle][ âˆ¥]Î¸[Ë™]01[l] _[âˆ¥][.]_
The dynamic updates for particle 2 is similar. We put the whole details into Alg. 1 in Appendix.

**Implementation of sticks. For a stick with particles 1 and 2 in Fig 3 (b), we choose the center as the**
generalized Cartesian coordinate q0, and the rotation of particle 1 Î¸01 and particle 2 Î¸02 (Î¸01 = Î¸02)
as the generalized angle coordinates. The dynamics propagation of q0 is given by Eq. 5-8. There
are two choices to compute Â¨q0[l] [, one using the general form in Eq. 6 and the other one leveraging a]
simplified version as Â¨q0[l] [=][ P]i _k_ _[Ï•][(][f][ l]i_ [)][ by explicitly omitting the relative position and velocity.]

_âˆˆO_
The physical motivation of introducing the simplified version is that the acceleration of a stick center
is only affected by the forces according to the theorem of the motion of the center of mass. The angle
**_Î¸accelerations areË™01[l]_** [=][ Ïˆ][â€²][(][h]1[l][âˆ’][1] +Î¸h[Â¨]01[l][l]2[âˆ’][1][= (]) Î¸[Ë™]01[l][x][âˆ’]01[l][1][âˆ’][+ Â¨][1] **_Î¸[Ã—]01[l][ f][. The states of particles 1 and 2 are renewed as the same as Eq. 13.][ l]1_** [+][ x]02[l][âˆ’][1] _[Ã—][ f][ l]2[)][/][(][âˆ¥][x][l]01[âˆ’][1][âˆ¥][2][ +][ âˆ¥][x]02[l][âˆ’][1][âˆ¥][2][)][, and the velocity becomes]_

**Learnable FK. Besides the above hand-crafted FK, we propose a learnable variant by replacing**
Eq. (7-9) with the update vi[l] [=][ Ï†][(][h]i[l][âˆ’][1])vi[l][âˆ’][1] + Ï(Â¨qk[l] _[,][ x][l]ki[âˆ’][1][,][ f][ l]i_ [)][,][ x][l]i [=][ x]i[l][âˆ’][1] + vi[l][, where][ Ï][ is the]
equivariant function via Eq. 12. Full details are provided in Appendix I.


-----

Table 1: Prediction error (Ã—10[âˆ’][2]) on various types of systems. The header of each column â€œp, s, hâ€

|Ã— denotes the scenario with p isolated particles, s sticks an|nd h hinges. Results averaged across 3 runs.|
|---|---|
||Train| = 500 1,2,0 2,0,1 3,2,1 0,10,0 5,3,3||Train| = 1500 1,2,0 2,0,1 3,2,1 0,10,0 5,3,3|
|Linear 8.23 Â±0.00 7.55 Â±0.00 9.76 Â±0.00 11.36 Â±0.00 11.62 Â±0.00 GNN 5.33 Â±0.07 5.01 Â±0.08 7.58 Â±0.08 9.83 Â±0.04 9.77 Â±0.02 TFN 11.54 Â±0.38 9.87 Â±0.27 11.66 Â±0.08 13.43 Â±0.31 12.23 Â±0.12 SE(3)-Tr. 5.54 Â±0.06 5.14 Â±0.03 8.95 Â±0.04 11.42 Â±0.01 11.59 Â±0.01 RF 3.50 Â±0.17 3.07 Â±0.24 5.25 Â±0.44 7.59 Â±0.25 7.73 Â±0.39 EGNN 2.81 Â±0.12 2.27 Â±0.04 4.67 Â±0.07 4.75 Â±0.05 4.59 Â±0.07 EGNNReg 2.94 Â±0.01 2.66 Â±0.06 7.01 Â±0.34 5.03 Â±0.08 6.31 Â±0.04|8.22 Â±0.00 7.55 Â±0.00 9.76 Â±0.00 11.36 Â±0.00 11.62 Â±0.00 3.61 Â±0.13 3.23 Â±0.07 4.73 Â±0.11 7.97 Â±0.44 7.91 Â±0.31 5.86 Â±0.35 4.97 Â±0.23 8.51 Â±0.14 11.21 Â±0.21 10.75 Â±0.08 5.02 Â±0.03 4.68 Â±0.05 8.39 Â±0.02 10.82 Â±0.03 10.85 Â±0.02 2.97 Â±0.15 2.19 Â±0.11 3.80 Â±0.25 5.71 Â±0.31 5.66 Â±0.27 2.59 Â±0.10 1.86 Â±0.02 2.54 Â±0.01 2.79 Â±0.04 3.25 Â±0.07 2.74 Â±0.08 1.58 Â±0.03 2.62 Â±0.05 3.03 Â±0.07 3.07 Â±0.04|
|GMN 1.84 Â±0.02 2.02 Â±0.02 2.48 Â±0.04 2.92 Â±0.04 4.08 Â±0.03|1.68 Â±0.04 1.47 Â±0.03 2.10 Â±0.04 2.32 Â±0.02 2.86 Â±0.01|



4 EXPERIMENTS

4.1 SIMULATION DATASET: CONSTRAINED N-BODY

**Datasets. We inherit the 3D extension of Fuchs et al. (2020) based on the N-body simulation**
introduced in Kipf et al. (2018). For each trajectory, we provide the initial states of the system
_Tconnected by sticks or hinges. The task is to predict the final positions{S = 1000i[0][}]i[N]=1[, the particlesâ€™ charges]. The validation and testing sets contain 2000 trajectories. We evaluate the prediction error[ {][c][i][ âˆˆ]_ [(][âˆ’][1][,][ 1)][}][N]i=1 [and a configuration indicating which particles are] {x[T]i _[}]i[N]=1_ [of the particles when]
by the MSE metric. Compared with the simulation conducted in Fuchs et al. (2020); Satorras et al.
(2021), our dataset is more challenging in three senses: 1. We consider systems with multiple scales,
including 5, 10, and 20 particles in total, respectively. 2. We introduce to the system the dynamics
of hinges and sticks (depicted in Appendix B), and construct various combinations between these
objects. 3. We investigate the performance of each model across different scales of training set, e.g.,
500 and 1500, to see how the models perform with scarce or relatively abundant training data. The
system consisting of p isolated particles, s sticks and h hinges, is abbreviated as (p, s, h).

**Implementation details. Following Satorras et al. (2021), we use a linear mapping of the scale of**
of the product of chargesinitial velocity ||vi[0][||][2][ as the input node feature] cicj and an edge type indicator[ h]i[0][. The edge feature is provided by a concatenation] Iij, where Iij is valued as 0 if node i and
_j are disconnected, 1 if connected by a stick, and 2 if connected by a hinge. Note that this edge_
type indicator is an augmentation over the original setting in EGNN, designed to enforce EGNN and
other baselines the ability to distinguish different types of edges, namely, with or without constraints.
Other settings including the hyper-parameters are introduced in Appendix E.

**Comparison with SOTAs. Table 1 reports the performance of GMN and various compared models:**
EGNN (Satorras et al., 2021) and its regulated version EGNNReg, SE(3)-Transformer (Fuchs et al.,
2020), Radial-Field (RF) (KÂ¨ohler et al., 2019), Tensor-Field-Network (TFN) (Thomas et al., 2018),
and other two baselines, GNN and the Linear prediction (Satorras et al., 2021). Regarding EGNN
and EGNNReg, they share the same backbones (i.e. Ï•1 and Ïˆ ) and training hyper-parameters
(learning rates, layer number, etc) with our GMN for a fair comparison. For EGNNReg, we explicitly involve a regularization term during training by enforcing the geometrical constraints, namely
preserving the lengths between two particles on sticks and hinges; the regularization factor is ranged
from 0.01 to 0.1, where the value giving the best performance is selected. The default settings of
SE(3)-Transformer and TFN perform poorly on our experiments, hence we have tried our best to
tune their hyper-parameters by validation. From Table 1, we have these observations:

**1. GMN achieves the best performance in all scenarios, suggesting its general superiority. 2. GMN**
is more robust when the complexity of the system increases. On |Train| = 500, for example, the
performance of GMN degenerates slightly by increasing the number of particles and hinges (e.g.
from (1,2,0) to (3,2,1)), while other methods (such as EGNN) drops significantly. 3. Reducing the
training size will hinder the performance of all compared methods remarkably. On the contrary,
GMN still performs promisingly in general. For instance, on (3,2,1), EGNNReg becomes much
worse from 2.62 to 7.01 when the volume of training data is decreased from 1500 to 500, whereas
the change of GMN is smaller (2.10 v.s. 2.48). This is reasonable as GMN has explicitly encoded
the constraints as opposed to EGNN and EGNNReg that learn to remember constraints by training.


-----

Table 2: Generalization across different systems. All models are trained in the (3,2,1) scenario.

||Train| = 500 3,2,1 2,4,0 1,0,3 Average||Train| = 1500 3,2,1 2,4,0 1,0,3 Average|
|---|---|


|GNN 7.58 8.06 8.37 8.00 EGNN 4.67 3.42 4.40 4.16 EGNNReg 7.01 4.49 6.62 6.04|4.73 4.98 5.58 5.10 2.54 2.75 3.49 2.93 2.62 2.62 4.29 3.18|
|---|---|


|GMN 2.48 2.53 3.28 2.76|2.10 2.18 2.65 2.31|
|---|---|


**Data efficiency. Fig. 4 depicts the prediction errors on (3,2,1)** GMN
when the training size varies. It is observed that GMN acts EGNNReg
steadily, justifying its benefit in data efficiency. Once again,
both EGNN and EGNNReg deliver much worse performance 5.0
when the training size is small, and they approach GMN when 4.0

Prediction Error (1e-2)3.0

the training dataset is enlarged sufficiently. In physics, it is
important to ensure the physics rules that are discovered by a 2.0

|Col1|Col2|Col3|Col4|Col5|Col6|GMN|
|---|---|---|---|---|---|---|
|||||||EGNN EGNNReg|
||||||||
|||||||GNN|
||||||||
||||||||
||||||||
||||||||
||||||||


500 1000 1500 2000 2500 3000

relatively small number of experiments to be general enough # of Samples
for explaining universal phenomena. Hence, data efficiency, Figure 4: Data efficiency.
as a key advantage of GMN, comes as an important requirement for learning to model physical systems. Besides, by the comparison between GNN and other
equivariant models, it is seen that equivariance is a crucial point for performance guarantee.

**The generalization capability across different systems. It is interesting to check how the models**
perform when trained on one system but tested on others. Hence, Table 2 tests the generalization
from (3,2,1) to (2,4,0) and (1,0,3). Interestingly, the performances of all models on new systems are
comparable with the original environment. We conjecture that this ability could be attributed to the
usage of GNN in capturing the combination diversity of the objects. As before, GMN performs best.

**Ablation studies.** **1.** The func- Table 3: Ablations. â€œO.F.â€ denotes numerical over-flow.

||Train| = 500 3,2,1 5,3,3||Train| = 1500 3,2,1 5,3,3|
|---|---|


of the same size, and report the er
_Ï•2(fi[l][,][ x][l]ki[âˆ’][1][,][ v]ki[l][âˆ’][1][)][ for hinges. Here]_

|GMN 2.48 4.08 GMN-L 3.19 4.34 w/o Equivariance 3.74 4.41 Ï• with (f il, xl kâˆ’ 1, v klâˆ’ 1) 2.86 4.15 2 i i Ï• with (f il, xl kâˆ’ 1, v klâˆ’ 1), shared 3.86 4.25 2 i i Ï• with only f il 3.10 4.39 2 Ï• with only f il, shared 2.91 4.94 2 w/o Normalization 3.15 O.F.|2.10 2.86 2.28 3.03 2.46 3.29 2.20 3.00 2.30 3.10 2.34 4.19 2.39 3.45 O.F. O.F.|
|---|---|

we investigate different cases by assigning the identical form of Ï•2 for sticks and hinges with shared
or unshared parameters. Applying identical Ï•2(fi[l][,][ x][l]ki[âˆ’][1][,][ v]ki[l][âˆ’][1][)][ mostly outperforms the case by us-]
ing Ï•2(fi[l][)][, probably owing to the better expressivity of the former version. Yet, both cases are]
worse than our design, implying that the dynamics of sticks and hinges should be modeled distinctly. 3. We have introduced a normalization term in Eq. 12. Table 3 demonstrates that eliminating
this term leads to divergence during training, possibly owing to the numerical instability in the forward/backward propagation. 4. We have also implemented GMN-L that replaces the hand-crafted
FK in GMN with a learnable black-box equivariant function. GMN-L outperforms EGNN in various settings, verifying the validity of using our proposed equivariant message passing layer and
the object-level message Â¨qk in Eq. 6. Yet, GMN-L still yields minor gap with GMN, implying the
benefit of involving domain knowledge. The full results are deferred to Appendix I.


4.2 APPLICATIONS ON REAL-WORLD DATASETS

This subsection introduces how to apply GMN to practical applications including MD17 (Chmiela
et al., 2017) and CMU Motion Capture (CMU, 2003). It is not required to manually derive the entire
kinematics for these complex systems; instead, each input system is decomposed as a set of particles
and sticks (e.g., the circles in Fig. 5), which can be directly processed by our current formulation of
GMN without any modification. The core is modeling partial length-constraints of the input system
with disjoint sticks. The full details of the kinematics decomposition trick are in Appendix C.


-----

Table 4: Prediction error (Ã—10[âˆ’][2]) on MD17 dataset. Results averaged across 3 runs.

Aspirin Benzene Ethanol Malonaldehyde Naphthalene Salicylic Toluene Uracil

RF 10.94Â±0.01 103.72Â±1.29 4.64Â±0.01 13.93Â±0.03 0.50Â±0.01 1.23Â±0.01 10.93Â±0.04 0.64Â±0.01
TFN 12.37Â±0.18 58.48Â±1.98 4.81Â±0.04 13.62Â±0.08 0.49Â±0.01 1.03Â±0.02 10.89Â±0.01 0.84Â±0.02
SE(3)-Tr. 11.12Â±0.06 68.11Â±0.67 4.74Â±0.13 13.89Â±0.02 0.52Â±0.01 1.13Â±0.02 10.88Â±0.06 0.79Â±0.02
EGNN 14.41Â±0.15 62.40Â±0.53 4.64Â±0.01 13.64Â±0.01 0.47Â±0.02 1.02Â±0.02 11.78Â±0.07 0.64Â±0.01
EGNNReg 13.82Â±0.19 61.68Â±0.37 6.06Â±0.01 13.49Â±0.06 0.63Â±0.01 1.68Â±0.01 11.05Â±0.01 0.66Â±0.01

GMN 10.14Â±0.03 48.12Â±0.40 4.83Â±0.01 13.11Â±0.03 **0.40Â±0.01** 0.91Â±0.01 10.22Â±0.08 0.59Â±0.01
GMN-L **9.76Â±0.11** 54.17Â±0.69 4.63Â±0.01 **12.82Â±0.03** 0.41Â±0.01 **0.88Â±0.01 10.45Â±0.04 0.59Â±0.01**


**MD17. We adopt MD17 (Chmiela et al., 2017) which**
involves the trajectories of eight molecules generated via
molecular dynamics simulation. Our goal here is to predict the future positions of the atoms given the current **Aspirin** **Benzene** **Ethanol** **Malonaldehyde**
system state. We observe that the lengths of chemical
bonds remain very stable during the simulation, making
it reasonable to model the bonds as sticks. The complete
implementation details are deferred to Appendix E. As **Naphthalene** **Salicylic** **Toluene** **Uracil**
presented in Table 4, GMN outperforms other competi- Figure 5: Molecules in MD17.

**Aspirin** **Benzene** **Ethanol** **Malonaldehyde**

**Naphthalene** **Salicylic** **Toluene** **Uracil**

tive equivariant models on 7 of the 8 molecules. Particularly, on molecules with complex structures (e.g., Aspirin, Benzene, and Salicylic), the improvement
of GMN is more significant, showcasing the benefit of constraint modeling on the bonds. Yet, we
also observe that the constraint-aware models (GMN and EGNNReg) perform worse than others
on Ethanol, possibly because Ethanol is a relatively small molecule with simple structure, where
considering the bond constraints possibly makes less benefit but instead hinders the learning. Surprisingly, GMN-L showcases very competitive performance on this dataset. It surpasses GMN on 4
of the 8 molecules, exhibiting that learnable FK works in practice even it does not involve domain
knowledge of the constraint into kinematics modeling.

**CMU Motion Capture. We use the motion data from the CMU Motion Capture Database (CMU,**
2003), which contains the trajectory of human motion in various scenarios. Different parts of the human body could be treated as hard rigid-body constraints. We focus on the walking motion of single
object (subject #35) containing 23 trials, similar to Kipf et al. (2018). As depicted in Table 5, GMN
outperforms other models by a large margin, verifying the efficacy of our equivariant constraint
module on modeling complex rigid bodies. We further provide a visualization in Fig. 6, where
GMN predicts the motion accurately while EGNN yields larger error. Again, GMN-L, although is
inferior to GMN, is better than other methods remarkably.

Table 5: Prediction error (Ã—10[âˆ’][2]) on motion capture. Results averaged across 3 runs.

GNN TFN SE(3)-Tr. RF EGNN EGNNReg GMN GMN-L

67.3Â±1.1 66.9Â±2.7 60.9Â±0.9 197.0Â±1.0 59.1Â±2.1 59.5Â±2.2 **43.9Â±1.1** 50.9Â±0.7





  

  

  

  

  

  

  

  

     

     

  

                                         


Figure 6: Left to Right: initial position, GMN, EGNN (all in blue). Ground truths are in red.

5 CONCLUSION

In this paper, we propose Graph Mechanics Networks (GMN) that are capable of characterising
constrained systems of interacting objects. The core is making use of generalized coordinates, by
which the constraints are implicitly and exactly encapsulated in the forward kinematics. To enable
Euclidean symmetry, we have developed a general form of equivariant functions to simulate the
interaction forces and backward dynamics, whose expressivity is theoretically justified. For the
simulated systems with particles, sticks and hinges, GMN outperforms existing methods regarding
prediction error, constraint satisfaction and data efficiency. Moreover, the evaluations on two realworld datasets support the generalization ability of GMN towards complex systems.


-----

6 REPRODUCIBILITY STATEMENT

The complete proof of the theorems is provided in Appendix A. The hyper-parameters and other
experimental details are provided in Appendix E.

[Our code is available at: https://github.com/hanjq17/GMN.](https://github.com/hanjq17/GMN)

7 ETHICS STATEMENT

The research in this paper does NOT involve any human subject, and our dataset is not related to any
issue of privacy and can be used publicly. All authors of this paper follow the ICLR Code of Ethics
(https://iclr.cc/public/CodeOfEthics).

ACKNOWLEDGMENTS

This work was jointly supported by the following projects: the Scientific Innovation 2030 Major
Project for New Generation of AI under Grant NO. 2020AAA0107300, Ministry of Science and
Technology of the Peopleâ€™s Republic of China; the National Natural Science Foundation of China
(No.62006137); Tencent AI Lab Rhino-Bird Visiting Scholars Program (VS2022TEG001); Beijing
Academy of Artificial Intelligence (BAAI).

REFERENCES

Peter W Battaglia, Razvan Pascanu, Matthew Lai, Danilo Rezende, and Koray Kavukcuoglu.
Interaction networks for learning about objects, relations and physics. _arXiv preprint_
_arXiv:1612.00222, 2016._

Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T SchÂ¨utt, and
Klaus-Robert MÂ¨uller. Machine learning of accurate energy-conserving molecular force fields.
_Science advances, 3(5):e1603015, 2017._

[CMU. Carnegie-mellon motion capture database. 2003. URL http://mocap.cs.cmu.edu.](http://mocap.cs.cmu.edu)

George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
_signals and systems, 2(4):303â€“314, 1989._

Mingyu Ding, Zhenfang Chen, Tao Du, Ping Luo, Josh Tenenbaum, and Chuang Gan. Dynamic
visual reasoning by learning differentiable physics models from video and language. NeurIPS,
34, 2021.

Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data. In International
_Conference on Machine Learning, pp. 3165â€“3176. PMLR, 2020a._

Marc Finzi, Ke Alexander Wang, and Andrew Gordon Wilson. Simplifying hamiltonian and lagrangian neural networks via explicit constraints. arXiv preprint arXiv:2010.13581, 2020b.

Fabian B Fuchs, Daniel E Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d rototranslation equivariant attention networks. arXiv preprint arXiv:2006.10503, 2020.

Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer, Julian De Freitas, Jonas
Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, et al. Threedworld: A platform for
interactive multi-modal physical simulation. NeurIPS, 2021.

Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4
(2):251â€“257, 1991.

Michael J Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont, Yee Whye Teh, and
Hyunjik Kim. Lietransformer: equivariant self-attention for lie groups. In International Confer_ence on Machine Learning, pp. 4533â€“4543. PMLR, 2021._


-----

Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational
inference for interacting systems. arXiv preprint arXiv:1802.04687, 2018.

Jonas KÂ¨ohler, Leon Klein, and Frank NoÂ´e. Equivariant flows: sampling configurations for multibody systems with symmetric energies. arXiv preprint arXiv:1910.00753, 2019.

Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B Tenenbaum, and Antonio Torralba. Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids. _arXiv preprint_
_arXiv:1810.01566, 2018._

Michael Lutter, Christian Ritter, and Jan Peters. Deep lagrangian networks: Using physics as model
prior for deep learning. arXiv preprint arXiv:1907.04490, 2019.

Karolis Martinkus, Aurelien Lucchi, and NathanaÂ¨el Perraudin. Scalable graph networks for particle
simulations. AAAI, 2021.

RV Mises and Hilda Pollaczek-Geiringer. Praktische verfahren der gleichungsauflÂ¨osung. ZAMM_Journal of Applied Mathematics and Mechanics/Zeitschrift fÂ¨ur Angewandte Mathematik und_
_Mechanik, 9(1):58â€“77, 1929._

Damian Mrowca, Chengxu Zhuang, Elias Wang, Nick Haber, Li Fei-Fei, Joshua B Tenenbaum,
and Daniel LK Yamins. Flexible neural representation for physics prediction. arXiv preprint
_arXiv:1806.08047, 2018._

Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. Learning meshbased simulation with graph networks. arXiv preprint arXiv:2010.03409, 2020.

Alvaro Sanchez-Gonzalez, Victor Bapst, Kyle Cranmer, and Peter Battaglia. Hamiltonian graph
networks with ode integrators. arXiv preprint arXiv:1909.12790, 2019.

Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter
Battaglia. Learning to simulate complex physics with graph networks. In International Confer_ence on Machine Learning, pp. 8459â€“8468. PMLR, 2020._

Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. arXiv preprint arXiv:2102.09844, 2021.

Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular conformation generation. arXiv preprint arXiv:2105.03902, 2021.

Joshua B Tenenbaum, Charles Kemp, Thomas L Griffiths, and Noah D Goodman. How to grow a
mind: Statistics, structure, and abstraction. science, 331(6022):1279â€“1285, 2011.

Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick
Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point
clouds. arXiv preprint arXiv:1802.08219, 2018.

Benjamin Ummenhofer, Lukas Prantl, Nils Thuerey, and Vladlen Koltun. Lagrangian fluid simulation with continuous convolutions. In International Conference on Learning Representations,
2019.

Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
comprehensive survey on graph neural networks. IEEE transactions on neural networks and
_learning systems, 32(1):4â€“24, 2020._

Shuqi Yang, Xingzhe He, and Bo Zhu. Learning physical constraints with neural projections. arXiv
_preprint arXiv:2006.12745, 2020._


-----

A THE PROOF OF THEOREM 1

In the following, we define the orthogonal group as O(d) = {O âˆˆ R[d][Ã—][d] _| O[âŠ¤]O = OO[âŠ¤]_ = Id}
Prior to providing the proof for Theorem 1, we first list two necessary lemmas below.
**Lemma 1.Z2 â‡”** **_Z1[âŠ¤] For any two matrices[Z][1]_** [=][ Z]2[âŠ¤][Z][2][.] **_Z1, Z2 âˆˆ_** R[d][Ã—][m], we have this equivalence: âˆƒO âˆˆO(d), OZ1 =

_Proof. We only need to prove the â€œâ‡â€ direction, as â€œâ‡’â€ holds clearly. Suppose the SVD decom-_
onal matrixposition of Z S11 as ZR1[d] =[Ã—][m] U, and the right-singular matrix1S1V1[âŠ¤] with the left-singular matrix V1 **_U(m1 âˆˆO). Since(d) Z, the singular diag-1[âŠ¤][Z][1]_** [=][ Z]2[âŠ¤][Z][2][,]
then there must exists a certain SVD decomposition of âˆˆ **_Z2 that shares the same singular matrix âˆˆO_**
and right-singular matrix withZ2 = U2S1V1[âŠ¤] = U2U1[âŠ¤][U][1][S] Z[1][V]1[ âŠ¤]1, implying that= U2U1[âŠ¤][Z][1] Z[, which concludes the proof owing to the orthog-]2 = U2S1V1[âŠ¤][, where][ U][2] _[âˆˆO][(][d][)][. Hence,]_
onality of U2U1[âŠ¤][.]

**Lemma 2. The function f : R[d][Ã—][m]** _â†’_ R[m][â€²] _is invariant on O(d), namely, f_ (OZ) = f (Z), âˆ€O âˆˆ
_O(d), âˆ€Z âˆˆ_ R[d][Ã—][m], if and only if there exists function g : R[m][Ã—][m] _â†’_ R[m][â€²] _satisfying f_ (Z) =
_g(Z_ _[âŠ¤]Z)._

_Proof. The sufficiency is obvious. We now prove the necessity. We define the equivalence class_

[Z0] = **_Z_** **_O_** (d), OZ = Z0 . Since f is invariant to the orthogonal transformation,
_{_ _| âˆƒ_ _âˆˆO_ _}_
it means f is actually a function on the equivalence class, i.e., f (Z) = f ([Z]). On the other
hand, according to Lemma 1, we have [Z1] = [Z2] â‡” **_Z1[âŠ¤][Z][1]_** [=][ Z]2[âŠ¤][Z][2][, which implies the one-]
to-one correspondence between [Z] and Z _[âŠ¤]Z; hence, there must exist a function f_ _[â€²]_ leading to

[Z] = f _[â€²](Z_ _[âŠ¤]Z), and f_ _[â€²]_ is continuous in terms of any invariant metric such as the norm âˆ¥Z _[âŠ¤]Zâˆ¥._
Overall, f (Z) = f ([Z]) = f (f _[â€²](Z_ _[âŠ¤]Z)) := g(Z_ _[âŠ¤]Z)._

We are now ready to prove Theorem 1 that is copied below for better readability.
**Theorem 1. If m â‰¥** _d and the row rank of Z is full, i.e. rank(Z) = d, then for any continuous_
_orthogonality-equivariant function Ë†Ï•(Z, h), there must exist an MLP Ïƒw satisfying âˆ¥Ï•(Z, h) âˆ’_
_Ï•Ë†(Z, h)âˆ¥_ _< Ïµ for arbitrarily small error Ïµ._

_Proof. Without loss of generality, we will omit the non-vector term h, which does not change the_
story but let our proof more concise. Because Z is of full row-rank, the columns of an arbitrary
function Ë†Ï•(Z) can be represented as a linear combination of the columns of Z; in other words,
there must exist a function Ï€ : R[d][Ã—][m] _â†’_ R[m][Ã—][m][â€²] giving rise to Ë†Ï•(Z) = ZÏ€(Z). Considering the
orthogonality-equivariance, we derive the property of Ï€(Z) as:
_Ï•Ë†(OZ) = O Ë†Ï•(Z),_
_â‡’_ **_OZÏ€(OZ) = OZÏ€(Z),_**
_â‡’_ **_ZÏ€(OZ) = ZÏ€(Z)._** (14)
Since d â‰¤ _m and the row-rank of Z is full, we perform the compact SVD decomposition on Z,_
namely, Z = UZSZVZ[âŠ¤][, where][ U][Z][ âˆˆO][(][d][)][,][ R][d][Ã—][d][ âˆ‹] **_[S][Z][ >][ 0][, and][ V][Z][ âˆˆ]_** [R][m][Ã—][d][ satisfying]
**_VZ[âŠ¤][V][Z][ =][ I][d][. Eq. 14 becomes:]_**
**_ZÏ€(OZ) = ZÏ€(Z),_**

_â‡’_ **_UZSZVZ[âŠ¤][Ï€][(][OZ][) =][ U][Z][S][Z][V][ âŠ¤]Z_** _[Ï€][(][Z][)][,]_

_â‡’_ **_SZVZ[âŠ¤][Ï€][(][OZ][) =][ S][Z][V][ âŠ¤]Z_** _[Ï€][(][Z][)][.]_ (15)
Given that SZ = Eigen(Z _[âŠ¤]Z) and VZ = EigenVector(Z_ _[âŠ¤]Z) are respectively the eigenvalues_
and eigenvectors of Z _[âŠ¤]Z and are clearly invariant to the orthogonal transformation of Z. Their_
values can be numerically approximated by iterative programs, such as the power method (Mises &
Pollaczek-Geiringer, 1929), thus can be treated as the continuous functions of Z _[âŠ¤]Z. Let us define_
_g[â€²](Z) := SZVZ[âŠ¤][Ï€][(][Z][)][. Then, we keep deriving Eq. 15 by:]_
**_SZVZ[âŠ¤][Ï€][(][OZ][) =][ S][Z][V][ âŠ¤]Z_** _[Ï€][(][Z][)][,]_

_â‡’_ **_SOZVOZ[âŠ¤]_** _[Ï€][(][OZ][) =][ S][Z][V][ âŠ¤]Z_ _[Ï€][(][Z][)][,]_

_â‡’_ _g[â€²](OZ) = g[â€²](Z)._ (16)


-----

According to Lemma 2, the function g satisfies Eq. 16 if and only if it is written as g[â€²](Z) = g(Z _[âŠ¤]Z)_
for a certain function g. By checking the formulation of Ë†Ï•(Z) as demonstrated before, we arrive at

_Ï•Ë†(Z) = ZÏ€(Z),_

= UZSZVZ[âŠ¤][Ï€][(][Z][)][,]

= UZg[â€²](Z),

= UZg(Z _[âŠ¤]Z),_

= UZSZVZ[âŠ¤][V][Z][S]Z[âˆ’][1][g][(][Z] _[âŠ¤][Z][)][,]_

= ZVZSZ[âˆ’][1][g][(][Z] _[âŠ¤][Z][)][,]_

:= ZÎ·(Z _[âŠ¤]Z)._ (17)

Here the function Î· can be approximated by MLP whose universality has been justified by (Cybenko,
1989; Hornik, 1991). The conclusion of Theorem 1 is proved.

**Corollary 1. Assume m â‰¥** _d, and also the entries of Z are drawn independently from a distribution_
_that is absolutely continuous with respect to the Lebesgue measure in R. Then, almost surely, the_
_conclusion of Theorem 1 holds._

_Proof. This is straightforward. When rank(Z) < d, the columns of Z are located in a subspace of_
R[d] (for example a line or a plane in the 3D space), whose measure is zero. Therefore, the probability
for making rank(Z) = d is 1, and we almost surely have the same conclusion as Theorem 1.

**Corollary 2. For any continuous orthogonality-equivariant function Ë†Ï•(Z, h) whose output is lo-**
_cated in the linear subspace expanded by the columns of Z, the conclusion of Theorem 1 holds_
_universally._

_Proof. According to the definition of Ë†Ï•(Z), we still obtain Ë†Ï•(Z) = ZÏ€(Z). Let us assume d > m_
(otherwise we can directly obtain Theorem 1), then the full SVD decomposition of Z is Z =
**_UZSZVZ[âŠ¤][, with][ U][Z][ âˆˆO][(][d][)][,][ S][Z][ âˆˆ]_** [R][d][Ã—][m][, and][ V][Z][ âˆˆO][(][m][)][. But here,][ S][Z][ is not strictly positive.]

Suppose SZ = **_S0+_**, where S+ > 0. We retain that SZVZ[âŠ¤][Ï€][(][Z][) =][ g][(][Z] _[âŠ¤][Z][)][ by imitating the]_
 

proof in Eq. 14-16. Analogous to Eq. 17, we derive,

_Ï•Ë†(Z) = ZÏ€(Z),_

= UZg(Z _[âŠ¤]Z),_

= UZSZVZ[âŠ¤][V][Z] **_S+[âˆ’][1][,][ 0]_** _g(Z_ _[âŠ¤]Z),_

= ZVZ **_S+[âˆ’][1][,][ 0]_**  g(Z _[âŠ¤]Z),_

:= ZÎ·(Z [âŠ¤]Z),  (18)

which concludes the proof.

B THE DYNAMICS ANALYSES FOR STICKS AND HINGES

The analytical forms of the dynamics for sticks and hinges can be found from a mechanics book.
Here we derive the formulas on our own to make our paper more self-contained. For simplicity, we
consider all particles to be of the equal mass and the sticks of no mass.

**Dynamics analysis of sticks.** In Fig. 7 (Left), we assume the forces acting on particles 1 and 2 are
separately f1 and f2. By following the theorem of the motion of the center of mass, the acceleration
of the center is given by


**_qÂ¨0 =_** **_[f][1][ +][ f][2]_**


(19)


-----

Figure 7: Left: Dynamics of sticks. Right: Dynamics of hinges.

The rotation accelerations of particles 1 and 2 around the center are calculated by

**_Î¸Â¨01 = Î¸[Â¨]02 =_** **_[M]_** (20)

_J_ [=][ x]m[01]x[ Ã—]01[ f][1][ +]+ m[ x][02]x[ Ã—]02[ f][2]

_âˆ¥_ _âˆ¥[2]_ _âˆ¥_ _âˆ¥[2][,]_

where M defines the total torque and J is the moments of inertia.

**Dynamics analysis of hinges.** The analysis for hinges is more complicated than sticks. In Fig. 7
(Right), the forces on particles 0, 1 and 2 are f0, f1 and f2. By using Newtonâ€™s second law,

**_a0 + a1 + a2 =_** **_[f]_** (21)

_m_ _[,]_

where the aggregated force is f = f0 + f1 + f2. In addition, the kinematics relations between the
three particles show that

**_a1 = a0 + Î¸[Â¨]01 Ã— x01 + Î¸[Ë™]01 Ã— Î½01,_** (22)

**_a2 = a0 + Î¸[Â¨]02 Ã— x02 + Î¸[Ë™]02 Ã— Î½02,_** (23)

where **_Î¸[Ë™]01 and_** **_Î¸[Â¨]01 denote the speed and acceleration of the rotation angle of particle 1 around 0,_**
and Î½01 is the corresponding linear velocity; the symbols **_Î¸[Ë™]02,_** **_Î¸[Â¨]02 and Î½02 are defined similarly._**
Moreover, the relative rotation acceleration of particle 1 to 0 is caused by the external force f1 and
the inertia force âˆ’ma0 acting on 1, which derives that

**_Î¸Â¨01 =_** **_[M][01]_** = **_[x][01][ Ã—][ (][f][1][ âˆ’]_** _[m][a][0][)]_ _._ (24)

_J01_ _m_ **_x01_**

_âˆ¥_ _âˆ¥[2]_

Analogously,

**_Î¸Â¨02 =_** **_[M][02]_** = **_[x][02][ Ã—][ (][f][2][ âˆ’]_** _[m][a][0][)]_ _._ (25)

_J02_ _m_ **_x02_**

_âˆ¥_ _âˆ¥[2]_

After substituting Eq. 24 into Eq. 22, and Eq. 25 into Eq. 23, and then rearranging Eq. 21, we have


**_a0 = (I + e01e[âŠ¤]01_** [+][ e][02][e]02[âŠ¤] [)][âˆ’][1][a][,] (26)

where a = _mf_ **_Î¸01_** **_Î½01_** **_Î¸02_** **_Î½02_** (I **_e01e[âŠ¤]01[)][ f]m[1]_** 02[)][ f]m[2] [with][ e][01][ and][ e][02]

represent the unit vectors along[âˆ’] [Ë™] _Ã—_ _âˆ’_ **_x[Ë™]_** 01 Ã— and x âˆ’02, respectively. We have applied a trick to simplify the âˆ’ _[âˆ’]_ [(][I][ âˆ’] **_[e][02][e][âŠ¤]_**
derivation by observing **_[x][01][Ã—]x[f]01[1][Ã—][x][01]_** = (I **_e01e[âŠ¤]01[)][f][1][. Besides,][ I][ +][ e][01][e][âŠ¤]01_** [+][ e][02][e]02[âŠ¤] [is invertible,]

_âˆ¥_ _âˆ¥[2]_ _âˆ’_
hence Eq. 26 is always meaningful.

Substituting Eq. 26 back into Eq. 24 and Eq. 25 derives the values of **_Î¸[Â¨]01 and_** **_Î¸[Â¨]02. Note that in Â§ 3.3,_**
we employ the denotation of generalized coordinates by q0, Ë™q0 and Â¨q0 which indeed share the same
meaning with the Cartesian coordinates x0, v0 and a0 of particle 0.

C KINEMATICS DECOMPOSITION

We manually build the forward kinematics (Eq. 9) of the stick (and hinge), which relies on the
domain knowledge of the underlying physics. However, for complex systems, we no longer require


-----

Table 6: Prediction error (Ã—10[âˆ’][2]) on various types of systems. The first column â€œp, s, hâ€ denotes
the scenario with p isolated particles, s sticks and h hinges. Models are trained with 500 samples.

GMN EGNN EGNNReg GNN TFN SE(3)-Tr. RF Linear

1,2,0 1.84 2.81 2.94 5.33 11.54 5.54 3.50 8.23
2,0,1 2.02 2.27 2.66 5.01 9.87 5.14 3.07 7.55

2,4,0 2.34 3.59 3.87 8.05 11.30 9.22 5.37 10.10
0,5,0 2.54 4.13 4.29 8.63 11.92 9.83 5.94 10.48
7,0,1 2.39 2.66 3.41 7.05 10.67 8.38 4.66 9.71
1,0,3 3.21 4.56 5.14 8.32 11.62 9.57 5.91 9.90
3,2,1 2.48 4.67 7.01 7.58 11.66 8.95 5.25 9.76

4,8,0 3.69 4.79 7.09 9.65 12.05 11.21 7.59 11.45
0,10,0 2.92 4.75 5.03 9.83 13.43 11.42 7.59 11.36
8,0,4 3.37 4.17 5.32 9.49 11.72 11.12 7.51 11.44
2,0,6 4.06 5.06 5.58 10.13 12.13 11.74 8.15 11.61
5,3,3 4.08 4.59 6.31 9.77 12.23 11.59 7.73 11.62

to derive the kinematics of the entire system. Instead, we propose kinematics decomposition, a
simple yet effective trick that can decompose each input system (an arbitrary graph) into particles
and sticks. Taking the MD17 dataset as an example, for each molecule, we select certain bonds as
sticks (the circles in Fig. 5) and the remaining atoms as isolated particles; in this way, we obtain
a set of particles and sticks. Note that different sticks are not allowed to intersect; otherwise, it
will generate two values for the intersecting particle of two sticks and cause ambiguity if these
two values are distinct. Although this kind of kinematics decomposition will only maintain partial
constraints, it greatly enlarges the application scope of our current formulation Eq. (5-9) without
any revision. More importantly, our experiments verify that GMN by this formulation is sufficient
to surpass other methods on complex systems like molecules. On CMU Motion Capture, since the
motion graph contains no circle and is of the tree-like structure, it is tractable to derive the exact
forward kinematics by recursive kinematics computation from the root node. Yet, we still encourage
the usage of the above kinematics decomposition for its easy implementation and compatibility with
our GMN.

D FULL ALGORITHMIC DETAILS

In the main body of the paper, for better readability, we first introduce the general pipeline of our
method in Â§ 3.1 and then present the implementation details by taking the angels into account in
Â§ 3.3. Here, we combine them into one singe algorithmic flowchart in Alg. 1.

E MORE EXPERIMENTAL DETAILS AND RESULTS

**Hyper-parameters and baselines. For GNN, RF, EGNN, EGNNReg, and GMN, we empirically**
find that the following hyper-parameters generally work well, and use them across all experimental
evaluations: batch size 200, Adam optimizer with learning rate 0.0005, hidden dim 64, and weight
decay 1 Ã— 10[âˆ’][10]. All models are evaluated with four layers. SE(3)-Transformer and TFN do not
perform well on our datasets, potentially due to the challenge of highly complex and constrained
systems. Consequently, we tune the hyper-parameters and adopt the following configuration: batch
size 100, learning rate 0.001, hidden dim 64, representation degrees 3 and weight decay 1 Ã— 10[âˆ’][8].
Models are trained for 600 epochs on the simulation dataset, and 500 epochs on the real-world
datasets. EGNNReg is a variant of EGNN that explicitly adds the constraint error into its training
loss by a regularization factor of Î». In our experiments, we also treat Î» as a hyper-parameter, and
choose Î» that yields the best performance within the range [0.01, 0.1].

**Detailed experimental setup on MD17. We randomly split the dataset into train/validation/test**
sets containing 500/2000/2000 frame pairs respectively. We choose T = 5000 as the span between
the input and prediction frames, and the difference in positions as the input velocity. The hyper

-----

**Algorithm 1 Graph Mechanics Networks (GMNs)**

ant functionsInput: Initial states of all particles Ï•1, Ï•2, Ïˆ, Ïˆ[â€²]; Layer number {Si[0] [= (] L[x]. _i[0][,][ v]i[0][)][}]i[N]=1_ [and features][ {][h]i[0][}]i[N] [; Learnable equivari-]
Compute the generalized coordinates of all structural objects {(qk[0][,][ Ë™]qk[0][,][ {][ Ë™]Î¸ki[0] _[}][i][âˆˆO]k_ [)][}][K]k=1[.]

**for layer l = 1 to L do**

**for particle i = 1 to N do**

Calculate the interaction force fi[l] [and feature][ h]i[l] [for each particle by:]


_Ï•1(x[l]ji[âˆ’][1][, h]i[l][âˆ’][1], h[l]j[âˆ’][1], eji)._ (27)
_j=1_

X


**_fi[l][, h][l]i_** [=]


**end for**
**for object k = 1 to K do**

Inference the generalized Cartesian acceleration Â¨qk[l] [via:]


**_qÂ¨k[l]_** [=]

**_qÂ¨k[l]_** [=]


_Ï•2(fi[l][)][,]_ (for sticks) (28)
_iXâˆˆOk_

_Ï•2(fi[l][,][ x][l]ki[âˆ’][1][,][ v]ki[l][âˆ’][1][)][.]_ (for hinges) (29)
_iXâˆˆOk_


Derive the generalized angle acceleration:

**_Î¸Â¨ki =_** _iâˆˆOk_ **_[x][ki][ Ã—][ f][i]_** (for sticks) (30)

P _iâˆˆOk_

_[âˆ¥][x][ki][âˆ¥][2][,][ âˆ€][i][ âˆˆO][k][,]_

**_qk)_**
**_Î¸Â¨ki =_** **_[x]P[ki][ Ã—][ (][f][i][ âˆ’]_** [Â¨] _,_ _i_ _k._ (for hinges) (31)

**_xki_** _âˆ€_ _âˆˆO_
_âˆ¥_ _âˆ¥[2]_


Update the positions and velocities as follows:

**_qË™k[l]_** [=][ Ïˆ][(] _h[l]i[âˆ’][1]) Ë™qk[l][âˆ’][1]_ + Â¨qk[l] _[,]_ (32)

_iXâˆˆOk_

**_qk[l]_** [=][ q]k[l][âˆ’][1] + Ë™qk[l] _[,]_ (33)

**_Î¸Ë™ki[l]_** [=][ Ïˆ][â€²][(] _h[l]i[âˆ’][1]) Î¸[Ë™]ki[l][âˆ’][1]_ + Î¸[Â¨]ki[l] _[,][ âˆ€][i][ âˆˆO][k][.]_ (34)

_iXâˆˆOk_


Perform the forward kinematics for each particle in _k:_
_O_

**_x[l]i_** [=][ q]k[l] [+][ rot][( Ë™]Î¸ki[l] [)][x]ki[l][âˆ’][1][,][ âˆ€][i][ âˆˆO][k][,] (35)

**_vi[l]_** [= Ë™]qk[l] [+ Ë™]Î¸ki[l] _[Ã—][ x]ki[l]_ _[,][ âˆ€][i][ âˆˆO][k][.]_ (36)

**end for**

**end for**
**Output: The predicted states of all particles {Si[L][}]i[N]=1[.]**

parameters of all models are kept the same as the synthetic dataset. We masked out the hydrogen
atoms, focusing on the prediction of large atoms. We further augment the original molecular graph
with 2-hop neighbors, and concatenate the hop index with atom numbers of the connected atoms as
well as the edge type indicator as the edge feature, similar to Shi et al. (2021). We use the norm of
velocity concatenated with the atom number as the node feature. We randomly select bonds without
commonly-connected atoms as sticks, and the rest of atoms as isolated particles. Although by this
means not all the bond lengths are preserved, our experiment does illustrate that this is a simple but
effective strategy of applying GMN on complex systems like molecules.

**Detailed experimental setup on CMU Motion Capture. We first split the data into train/val/test**
sets containing 11/6/6 trials respectively. We then sample from the trials to get 200/600/600 frame


-----

tion(s) of EGNN (in blue). Ground truth is marked in red. Better viewed by zooming in.


Figure 8: Left: initial position(s). Middle: the prediction(s) of GMN (in blue). Right: the predic




     

     

  

  

  

     

     

  

  

  

  

                                                     


Figure 9: Left: initial position (in blue). Middle: the prediction of GMN (in blue). Right: the
prediction of EGNN (in blue). Ground truths are marked in red. Better viewed by zooming in.

pairs with T = 30. We use the norm of velocity as node feature. We also augment the edges with
2-hop neighbors, and use the edge type indicator as edge feature. As for GMN, we sample 6 key
bones of human body (as specified in Appendix C) as sticks, resulting in a system with 19 isolated
particles and 6 sticks. Empirically, we select the edges connecting nodes (0, 11), (2, 3), (7, 8), (12,
13), (17, 18), and (24, 25) as sticks on the motion capture dataset, which indeed are the key parts of
human body like the arms and legs.

**More visualizations. We visualize the prediction outcomes by GMN and EGNN in Fig. 8 on**
the simulation dataset. GMN is found to be able to track the ground-truth trajectories accurately,
whereas EGNN yields clear position errors and particularly breaks the constraints for sticks and
hinges. These results are consistent with the performance in Table 1. Fig. 9 provides extra visualization on Motion Capture. Similarly, GMN yields more accurate prediction than EGNN.

**More experimental results. In Table 6, we provide a comprehensive performance comparison of**
different models under more scenarios with 500 training samples. Clearly, GMN outperforms other
models on all object combinations involved. Besides, we provide the learning curve for GMN,
EGNN, and EGNNReg on the simulation dataset (3,2,1) in Fig. 10. GMN yields lower training loss
and testing loss than EGNN and EGNNReg, benefiting from its constraint modeling.


-----

0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.00

|Col1|Col2|Col3|Col4|Col5|GMN (Tr|ain)|
|---|---|---|---|---|---|---|
||||||||
||||||GMN (Te EGNN (T|st) rain)|
||||||EGNN (T|est)|
||||||||
||||||EGNNR EGNNR|eg (Train) eg (Test)|
||||||||
||||||||
||||||||
||||||||


GMN (Train)
GMN (Test)
EGNN (Train)
EGNN (Test)
EGNNReg (Train)
EGNNReg (Test)


100 200 300 400 500 600

Epoch


Figure 10: Learning curves on (3,2,1) with 500 training samples.

MORE ABLATIONS


**Hinge treated as two sticks. We treat 0-1, 0-2 as two sticks, and apply the stick FK respectively.**
Afterwards, we translate the two sticks such that the 0s coincide at the midpoint of their predicted
positions. From Table 7, we find that this strategy performs worse than the hinge-modeled GMN,
since splitting the hinge into two sticks overlooks the kinematics at the connected point. Yet and
still, it performs better than EGNN, again verifying the benefit of our proposed constraint modeling.

Table 7: Ablation on hinge FK.


|Train = 500 | | 3,2,1 2,0,6 5,3,3|Train = 1500 | | 3,2,1 2,0,6 5,3,3|
|---|---|

|EGNN 4.67 5.06 4.59 EGNNReg 7.01 5.58 6.31 GMN (Stick only) 3.02 4.32 4.21 GMN 2.48 4.06 4.08|2.54 3.50 3.42 2.62 3.61 3.07 2.37 3.30 2.88 2.10 3.22 2.86|
|---|---|


**Charges as node or edge feature. In the experiment we by default assign eij = cicj as the edge**
feature (denoted as â€œEdge + Câ€). Here we instead concatenate ci to the node feature of particle i,
and set all eij = 0 (denoted as â€œNode + Câ€). In Table 8 we observe that these alternatives on charges
make very limited difference on performance, and indeed the models can learn the interaction of
charges from node features, which is truly the case as depicted in Eq. 2 and 5.

Table 8: Comparison of charge-assigning strategies.


|Node + C 1,2,0 2,0,1 3,2,1 0,10,0 5,3,3 1,|Edge + C 2,0 2,0,1 3,2,1 0,10,0 5,3,3|
|---|---|

|EGNN 2.89 2.28 4.25 4.80 4.50 2 EGNNReg 3.17 2.74 8.20 5.01 6.64 2 GMN 1.89 2.01 2.63 3.07 4.02 1|.81 2.27 4.67 4.75 4.59 .94 2.66 7.01 5.03 6.31 .84 2.02 2.48 2.92 4.08|
|---|---|


G CONSTRAINT SATISFACTION

**Constraint error. The constraint error is computed as the total change in the lengths of sticks and**
hinges between the input and output, averaged per trajectory. Specifically, for hinges, the two edges
are both considered.

**Results. One vital feature of GMN is that it maintains the geometrical constraints exactly and**
inherently. To show this, Table 9 records the corresponding constraint errors of several typical
models. The results do verify our claim that GMN always outputs near-zero errors (all below 1e-4).
Although EGNNReg that augments EGNN with regulation helps in reducing the constraint errors,
it is data-driven and limited by the number of training samples; further, the constraints are pursued
softly, making it defective for the applications where hard constraints are indispensable.


-----

Table 9: Constraint error on various types of systems.

|Train = 500 | | 1,2,0 2,0,1 3,2,1 0,10,0 5,3,3|Train = 1500 | | 1,2,0 2,0,1 3,2,1 0,10,0 5,3,3|
|---|---|


|GNN 0.200 0.386 0.492 0.154 0.468 EGNN 0.220 0.370 0.714 0.248 0.760 EGNNReg 0.172 0.146 0.232 0.198 0.241|0.225 0.426 0.779 0.251 0.772 0.217 0.317 0.521 0.139 0.596 0.159 0.053 0.091 0.097 0.075|
|---|---|


|GMN 0.000 0.000 0.000 0.000 0.000|0.000 0.000 0.000 0.000 0.000|
|---|---|



H MORE DISCUSSIONS ON GENERALIZATION

In Table 2 we compare the generalization capability of GMN with other methods. Here, we denote
GMN trained on (3,2,1) and tested across different scenarios as GMN-Transfer, and GMN trained
and tested on the same dataset as GMN-Original. It is observed from Table 10 that GMN has strong
generalization capability, since the transfer performance is very close to the original setting.

Table 10: Comparison of GMN in the transfer and original settings.

|Train = 500 | | 3,2,1 2,4,0 1,0,3 Average|Train = 1500 | | 3,2,1 2,4,0 1,0,3 Average|
|---|---|


|GMN-Transfer 2.48 2.53 3.28 2.76 GMN-Original 2.48 2.34 3.21 2.68|2.10 2.18 2.65 2.31 2.10 2.01 2.44 2.18|
|---|---|



I LEARNABLE FK

It is indeed instrumental to discuss whether a learnable black-box function, which requires less domain knowledge, could also yield competitive performance, and if our hand-crafted FK still shows
advantage over the learnable counterpart. To answer these questions, we replace the hand-crafted
part (Eq. (7-9) as well as the Euler angle computations in Sec. 3.1) with the following equations:
**_vi[l]_** [=][ Ï†][(][h]i[l][âˆ’][1])vi[l][âˆ’][1] + Ï(Â¨qk[l] _[,][ x][l]ki[âˆ’][1][,][ f][ l]i_ [)][,][ x][l]i [=][ x]i[l][âˆ’][1] + vi[l][, where][ Ï][ is the equivariant message passing]
layer we propose in Sec. 3.2. By this design, the parameterized FK preserves its equivariant property
(and the theoretical universality), and compared to EGNN, it additionally leverages the information
from the object-level generalized coordinates Â¨qk[l] [. We denote this variant of GMN as GMN-L. More-]
over, since the parameterized FK inevitably loses the constraint-preserving property compared with
the exact FK, therefore we also augment it with explicit constraint regularization, akin to what we
did to EGNNReg. We hence denote this variant as GMN-LReg.

We evaluate the performance of GMN-L, GMN-LReg and compare them with GMN with exact
FK as well as EGNN and EGNNReg in Table 11. We interestingly find that GMN-L consistently
outperforms EGNN in various settings (as well as the regularized version), which again verifies both
the validity of our proposed equivariant message passing layer and the efficacy of leveraging objectlevel message (i.e., Â¨qk[l] [) for the inference of FK. At the same time, GMN-L and GMN-LReg yield a]
minor gap with GMN, showing the evidence that hard-coding the constraints replaces a nontrivial
amount of learning complexity.

Table 11: Comparison with GMN-L and GMN-LReg.

|Train = 500 | | 1,2,0 2,0,1 3,2,1 0,10,0 5,3,3|Train = 1500 | | 1,2,0 2,0,1 3,2,1 0,10,0 5,3,3|
|---|---|

|EGNN 2.81 2.27 4.67 4.75 4.59 EGNNReg 2.94 2.66 7.01 5.03 6.31|2.59 1.86 2.54 2.79 3.25 2.74 1.58 2.62 3.03 3.07|
|---|---|

|GMN-L 2.32 2.09 3.19 3.88 4.34 GMN-LReg 2.52 2.23 3.34 3.67 4.31|1.93 1.56 2.28 2.72 3.03 1.91 1.88 2.49 2.61 3.00|
|---|---|

|GMN 1.84 2.02 2.48 2.92 4.08|1.68 1.47 2.10 2.32 2.86|
|---|---|


-----

J ROBUSTNESS TO ERRORS IN THE PHYSICAL PRIOR OF CONSTRAINTS

It is interesting to test the robustness of our model w.r.t. the noisy constraints. This scenario would
sometimes arise in real-world scenarios where we might not be certain about the exact connectivity
of the rigid body, and thus would involve slight errors in domain expertise. Since our paper focuses
on the distance constraint other than the angle constraint, the following investigations will only
involve noise into the stick connectivity. We design three random perturbation operations on the
input rigid body prior: 1. (Join) randomly selecting 2 isolated particles and joining them as if there
is a stick connecting; 2. (Split) randomly selecting an existing stick and splitting it as two isolated
particles; 3. (Join + Split) conducting operation 1 and 2 at the same time; and 4. (Change in Length)
randomly adding Gaussian noise N (0, 0.1L) to the length of a stick, where L is its original length.
Note that the operation is conducted independently for every training sample each time it is fed into
the network.

We summarize the results in Table 12. We observe that these perturbations, although somehow
hinder the performance, in general do not jeopardize the performance too much (difference in MSE
_â‰¤_ 0.30), indicating that GMN is not sensitive to slight errors of the input physical prior of the
constraints and it is still able to learn to some degree of given the wrong constraints.

Table 12: Robustness test in various scenarios.

|Train = 500 | | 3,2,1 5,3,3 8,6,0|Train = 1500 | | 3,2,1 5,3,3 8,6,0|
|---|---|


|GMN 2.48 4.08 2.84 GMN w/ Join 2.59 4.27 2.98 GMN w/ Split 2.57 4.11 2.95 GMN w/ Join + Split 2.63 4.16 3.01 GMN w/ Change in Length 2.75 4.36 3.11|2.10 2.86 2.22 2.22 3.16 2.37 2.16 3.13 2.26 2.31 3.01 2.34 2.15 3.15 2.41|
|---|---|



K MORE DISCUSSIONS ON THE DECOMPOSITION

It is possible to decompose into bigger objects rather than just sticks. As a comparison, we further
adopt the hinge-wise decomposition (i.e., decompose the system into particles and hinges), and
investigate the performance on both MD17 and Motion Capture. We denote this model as GMN-LH,
where H stands for hinges. The results are depicted in Table 13 and Table 14. On MD17, GMNLH yields a little bit worse performance than GMN-L on several molecules, while giving desirable
results on Ethanol and Benzene. On Motion Capture, GMN-LH outperforms GMN-L by a small
gap, while is still worse than GMN. By default, we still encourage to perform the decomposition via
sticks as sticks are actually the basic building blocks of hinges and other larger rigid objects.

Table 13: Prediction error (Ã—10[âˆ’][2]) on MD17 dataset. Results averaged across 3 runs.

Aspirin Benzene Ethanol Malonaldehyde Naphthalene Salicylic Toluene Uracil

EGNN 14.41Â±0.15 62.40Â±0.53 4.64Â±0.01 13.64Â±0.01 0.47Â±0.02 1.02Â±0.02 11.78Â±0.07 0.64Â±0.01
GMN 10.14Â±0.03 48.12Â±0.40 4.83Â±0.01 13.11Â±0.03 **0.40Â±0.01** 0.91Â±0.01 10.22Â±0.08 0.59Â±0.01
GMN-L **9.76Â±0.11 54.17Â±0.69 4.63Â±0.01** **12.82Â±0.03** 0.41Â±0.01 **0.88Â±0.01 10.45Â±0.04 0.59Â±0.01**
GMN-LH 10.25Â±0.06 52.02Â±0.97 4.62Â±0.01 12.83Â±0.03 0.41Â±0.01 1.03Â±0.01 10.81Â±0.14 0.59Â±0.01


Table 14: Prediction error (Ã—10[âˆ’][2]) on motion capture. Results averaged across 3 runs.

EGNN GMN GMN-L GMN-LH

59.1Â±2.1 **43.9Â±1.1** 50.9Â±0.7 48.7Â±1.1


-----

