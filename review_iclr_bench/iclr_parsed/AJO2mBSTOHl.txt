Under review as a conference paper at ICLR 2022
ANALYTICALLY TRACTABLE BAYESIAN DEEP
Q-LEARNING
Anonymous authors
Paper under double-blind review
ABSTRACT
Reinforcement learning (RL) has gained increasing interest since the demonstration
it was able to reach human performance on video game benchmarks using deep
Q-learning (DQN). The current consensus of DQN for training neural networks
(NNs) on such complex environments is to rely on gradient-descent optimization
(GD). This consensus ignores the uncertainty of the NN’s parameters which is a
key aspect for the selection of an optimal action given a state. Although alternative
Bayesian deep learning methods exist, most of them still rely on GD and numerical
approximations, and they typically do not scale on complex benchmarks such
as the Atari game environment. In this paper, we present how we can adapt the
temporal difference Q-learning framework to make it compatible with the tractable
approximate Gaussian inference (TAGI) which allows estimating the posterior
distribution of NN’s parameters using a closed-form analytical method. Throughout
the experiments with on- and off-policy reinforcement learning approaches, we
demonstrate that TAGI can reach a performance comparable to backpropagation-
trained networks while using only half the number of hyperparameters, and without
relying on GD or numerical approximations.
1
INTRODUCTION
Reinforcement learning (RL) has gained increasing interest since the demonstration it was able
to reach human performance on video game benchmarks using deep Q-learning (DQN) (Mnih
et al., 2015; Van Hasselt et al., 2016). Deep RL methods typically require an explicit deﬁnition of
an exploration-exploitation function in order to compromise between using the current policy and
exploring the potential of new actions. Such an issue can be mitigated by opting for a Bayesian
approach where we estimate the uncertainty of the neural network’s parameters using Bayes’ theorem,
and select the optimal action given a state according to the parameter uncertainty using Thompson
sampling technique (Strens, 2000). Bayesian deep learning (BDL) methods based on variational
inference (Kingma et al., 2015; Hernández-Lobato & Adams, 2015; Blundell et al., 2015; Louizos
& Welling, 2016; Osawa et al., 2019; Wu et al., 2019), Monte-Carlo dropout (Gal & Ghahramani,
2016), or Hamiltonian Monte-Carlo sampling (Neal, 1995) have shown to perform well on regression
and classiﬁcation benchmarks, despite being generally computationally more demanding than their
deterministic counterparts. Note that none of these approaches allow performing the analytical infer-
ence for the weights and biases deﬁning the neural network. Goulet et al. (2021) recently proposed
the tractable approximate Gaussian inference (TAGI) method that allows estimating the posterior
of the neural network’s parameters using a closed-form analytical method. More speciﬁcally, TAGI
leverages Gaussian conditional equations to analytically infer this posterior without the need of
numerical approximations (i.e., sampling techniques or Monte Carlo integration) or optimization on
which existing BDL methods rely. In addition, TAGI is able to maintain the same computational
complexity as deterministic neural networks based on the gradient backpropagation. For convolu-
tional architectures applied on classiﬁcation benchmarks, this approach was shown to exceed the
performance of other Bayesian and deterministic approaches based on gradient backpropagation, and
to do so while requiring a smaller number of training epochs (Nguyen & Goulet, 2021).
In this paper, we propose a new perspective on probabilistic inference for Bayesian deep reinforcement
learning which, up to now has been relying on gradient-based methods. More speciﬁcally, we present
how can we adapt the temporal difference Q-learning framework (Sutton, 1988; Watkins & Dayan,
1
Under review as a conference paper at ICLR 2022
1992) to make it compatible with TAGI. Section 2 ﬁrst reviews the theory behind TAGI and the
expected value formulation through the Bellman’s Equation. Then, we present how the action-
value function can be learned using TAGI. Section 3 presents the related work associated with
Bayesian reinforcement learning, and Section 4 compares the performance of the TAGI-DQN with
the deterministic DQN on on- and off-policy RL approaches.
2
TAGI-DQN FORMULATION
This section presents how to adapt the DQN frameworks in order to make them compatible with
analytical inference. First, Section 2.1 reviews the fundamental theory behind TAGI, and Section 2.1
reviews the concept of long-term expected value through the Bellman’s equation (Sutton & Barto,
2018). Then, Section 2.3 presents how to make the Q-learning formulation (Watkins & Dayan, 1992)
compatible with TAGI.
2.1
TRACTABLE APPROXIMATE GAUSSIAN INFERENCE
Bayesian deep learning aims to estimate the posterior of neural network’s parameters θ conditional
on a given training dataset D = {(xi, yi) , ∀i ∈1 : D} using Bayes’ theorem so that
f(θ|D) = f(θ)f(D|θ)
f(D)
,
(1)
where f(θ) is the prior Probability Density Function (PDF) of parameters, f(D|θ) is the likelihood
and f(D) is the marginal likelihood or evidence. Until recently, the posterior f(θ|D) in Equation 1
has been considered to be intractable (Goodfellow et al., 2016; Goan & Fookes, 2020). TAGI has
addressed this intractability issue by leveraging the Gaussian conditional equations; For a random
vector of parameter θ and observations Y for which the joint PDF satisﬁes
f

θ
y

= N

θ
y

;

µθ
µY

,

Σθ
Σ⊺
Yθ
ΣYθ
ΣY

,
(2)
the Gaussian conditional equation describing the PDF of θ conditional on y is deﬁned following
f(θ|y)
=
N(θ; µθ|y, Σθ|y)
µθ|y
=
µθ + Σ⊺
YθΣ−1
Y (y −µY )
Σθ|y
=
Σθ −Σ⊺
YθΣ−1
Y ΣYθ.
(3)
In its simple form, the intractable computational requirements for directly applying Equation 3
limits it to trivial neural networks. In order to overcome this challenge, TAGI leverages the inherent
conditional independence structure of hidden layers Z(j−1) ⊥
⊥Z(j+1)|z(j) under the assumptions
that parameters θ are independent between layers. This conditional independence structure allows
breaking down the computations for Equation 3 into a layer-wise two-step procedure; forward
uncertainty propagation and backward update.
The ﬁrst forward uncertainty propagation step is intended to build the joint prior between the hidden
states of a layer j, Z(j), and the parameters θ(j) directly connecting into it. This operation is made
by propagating the uncertainty from the parameters and the input layer through the neural network.
In order to maintain the analytical tractability of the forward step, we must address two challenges:
The ﬁrst challenge is related to the product of the weights W and activation units A involved in the
calculation of the hidden states Z so that
Z(j+1)
i
=
A
X
k=1
W (j)
i,k A(j)
k
+ B(j)
i
,
(4)
where B is the bias parameter, (j) refers to the jth layer, i refers to a node in the current layer, k
refers to a node from the previous layer, and A is the number of hidden units from the previous layer.
The second challenge is to propagate uncertainty through the non-linearity when activating hidden
states with an activation function φ(.),
A(j+1)
i
= φ

Z(j+1)
i

.
(5)
In order to tackle these issues, TAGI made the following assumptions
2
Under review as a conference paper at ICLR 2022
A1. The prior for the parameters and variables in the input layer are Gaussians.
A2. The product of a weight and an activation unit in Equation 4 is approximated by a Gaussian
random variable, W
A ≈N(µW
A, σ2
W
A), whose moments are computed analytically using
Gaussian multiplicative approximation (GMA).
A3. The activation function φ(.) in Equation 5 is locally linearized using a ﬁrst-order Taylor
expansion evaluated at the expected value of the hidden unit being activated.
Note that the linearization procedure in the assumption A3 may seems to be a crude approximation,
yet it has been shown to match or exceeds the state-of-the-art performance on fully-connected
neural networks (FNN) (Goulet et al., 2021), as well as convolutional neural networks (CNN) and
generative adversarial networks (Nguyen & Goulet, 2021). In order to maintain a linear computational
complexity with respect to the number of weights for the forward steps, TAGI relies on the following
assumptions
A4. The covariance for all parameters in the network and for all the hidden units within a same
layer are diagonal.
A5. The hidden states on a given layer are independent from the parameters connecting into the
subsequent layer, i.e., Z(j) ⊥
⊥θ(j).
A6. The hidden states at layer j + 1 depend only on the hidden states and parameters directly
connecting into it.
The empirical validity of those assumptions as well as the GMA formulations are provided by Goulet
et al. (2021).
The second backward update-step consists in performing layer-wise recursive Bayesian inference
going from hidden-layer to hidden-layer, and from hidden-layer to the parameters connecting into it
(see Figure 1). The assumptions A1-A6 allow performing analytical inference while still maintaining
a linear computational complexity with respect to the number of weight parameters in the network
where only the posterior of hidden states for the output layer are computed using Equation 3. For the
remaining layers, those quantities are obtained using the Rauch-Tung-Striebel recursive procedure
that was developed in the context of state-space models (Rauch et al., 1965). For simplicity, we
deﬁne the short-hand notation {θ+, Z+} ≡{θ(j+1), Z(j+1)} and {θ, Z} ≡{θ(j), Z(j)} so that the
posterior for the parameters and hidden states are computed following
f(Z|y)
=
N(z; µZ|y, ΣZ|y)
µZ|y
=
µZ + JZ
 µZ+|y −µZ+
ΣZ|y
=
ΣZ + JZ
 ΣZ+|y −ΣZ+
J⊺
Z
JZ
=
ΣZZ+Σ−1
Z+,
(6)
f(θ|y)
=
N(θ; µθ|y, Σθ|y)
µθ|y
=
µθ + Jθ
 µZ+|y −µZ+
Σθ|y
=
Σθ + Jθ
 ΣZ+|y −ΣZ+
J⊺
θ
Jθ
=
ΣθZ+Σ−1
Z+.
(7)
Figure 1 presents a directed acyclic graph (DAG) describing the interconnectivity in such a neural
network.
TAGI allows inferring the diagonal posterior knowledge for weights and bias parameters, either using
one observation at a time, or using mini-batches of data. As we will show in the next sections, this
online learning capacity is best suited for RL problems where we experience episodes sequentially
and where we need to deﬁne a tradeoff between exploration and exploitation, as a function of our
knowledge of the expected value associated with being in a state and taking an action.
2.2
EXPECTED VALUE AND BELLMAN’S EQUATION
We deﬁne r(s, a, s′) as the reward for being in a state s ∈RS, taking an action a ∈A =
{a1, a2, · · · aA}, and ending in a state s′ ∈RS. For simplicity, we use the short-form notation
3
Under review as a conference paper at ICLR 2022
x
z(1)
· · ·
z(L)
z(O)
y
f(θ(j)|y), f(Z(j)|y)
θ(0)
θ(1)
θ(L-1)
θ(L)
Figure 1: Compact representation of the variable nomenclature and the dependencies associated with
a feedforward neural network. The red node denotes the input vector, green nodes are vectors of
hidden units z, and purple node denote the observation vector. The gray arrows represent the weights
and bias θ connecting the different hidden layers and magenta arrows outline the ﬂow of information
that takes place during the inference step.
for the reward r(s, a, s′) ≡r(s) in order to deﬁne the value as the inﬁnite sum of discounted rewards
v(s) =
∞
X
k=0
γkr(st+k).
(8)
As we do not know what will be the future states st+k for k > 0, we need to consider them as random
variables (St+k), so that the value V (st) becomes a random variable as well,
V (st) = r(st) +
∞
X
k=1
γkr(St+k).
(9)
Rational decisions regarding which action to take among the set A is based the maximization of the
expected value as deﬁned by the action-value function
q(st, at) = µV ≡E[V (st, at, π)] = r(st) + E
" ∞
X
k=1
γkr(St+k)
#
,
(10)
where it is assumed that at each time t, the agent takes the action deﬁned in the policy π. In the case
of episode-based learning where the agent interacts with the environment, we assume we know the
tuple of states st and st+1, so that we can redeﬁne the value as
V (st, at)
=
r(st) + γ
 
r(st+1) +
∞
X
k=1
γkr(St+1+k)
!
=
r(st) + γV (st+1, at+1).
(11)
Assuming that the value V ∼N(v; µV , σ2
V ) in Equations 9 and 11 is described by Gaussian random
variables, we can reparameterize these equations as the sum of the expected value q(s, a) and a
zero-mean Gaussian random variable E ∼N(ϵ; 0, 1), so that
V (s, a) = q(s, a) + σV E,
(12)
where the variance σ2
V and E are assumed here to be independent of s and a. Although in a more
general framework this assumption could be relaxed, such an heteroscedastic variance term is outside
from the scope of this paper. Using this reparameterization, we can write Equation 11 as the
discounted difference between the expected values of two subsequent states
q(st, at)
=
r(st) + γq(st+1, at+1) −σVtEt + γσVt+1Et+1
=
r(st) + γq(st+1, at+1) + σV E.
(13)
Note that in Equation 13, σVt and γσVt+1 can be combined in a single standard deviation parameters
σV with the assumption that Ei ⊥
⊥Ej, ∀i ̸= j.
In the case where at a time t, we want to update the Q-values encoded in the neural net only after
observing n-step returns (Mnih et al., 2016), we can reformulate the observation equation so that
q(st, at) =
n−t−1
X
i=0
γir(st+i) + γn−tq(sn, an) + σV Et, ∀t = {1, 2, · · · , n −1}.
(14)
4
Under review as a conference paper at ICLR 2022
Note that in the application of Equation 14, we employ the simplifying assumption that Et ⊥
⊥
Et+i, ∀i ̸= 0, as Equation 13 already makes simplifying assumptions for the independence of σ2
V and
E. Note that in a more general framework, this assumption could be relaxed. An example of n-step
returns is presented in the the algorithm displayed in §1 from Appendix A.
The following subsections will present, for the case of categorical actions, how to model the deter-
ministic action-value function q(s, a) using a neural network.
2.3
TAGI DEEP Q-LEARNING FOR CATEGORICAL ACTIONS
Suppose we represent the environment’s state at a time t and t + 1 by {s, s′}, and the expected value
for each of the A possible actions a ∈A by the vector q ∈RA. In that context, the role of the neural
network is to model the relationships between {s, a} and q. Figure 2a presents a directed acyclic
graph (DAG) describing the interconnectivity in such a neural network, where red nodes denote state
variables, green nodes are vectors of hidden units z, the blue box is a compact representation for the
structure of a convolutional neural network, and where gray arrows represent the weights and bias θ
connecting the different hidden layers. Note that unlike other gray arrows, the magenta ones in (b)
are not directed arcs representing dependencies, but they simply outline the ﬂow of information that
takes place during the inference step. For simpliﬁcation purposes, the convolutional operations are
omitted and all regrouped under the CNN box. In order to learn the parameters θ of such a network,
s
CNN
z
(1)
z
(2)
q
θ(c0)
θ(0)
θ(1)
θ(q)
(a) Neural network DAG for modelling the action-value function q
s
CNN
z
(1)
z
(2)
s′
z
(1)
z
(2)
q
q′
σV ϵ
r
θ(c0)
θ(0)
θ(1)
θ(q)
θ(q)
h′
(b) DAG for the temporal-difference Q-learning conﬁguration
Figure 2: Graphical representation of a neural network structure for temporal-difference Q-learning
with categorical actions. The red nodes denote state variables, green nodes are vectors of hidden units
z, and the blue box is a compact representation for the structure of a convolutional neural network.
The gray arrows represent the weights and bias θ connecting the different hidden layers and magenta
arrows outline the ﬂow of information that takes place during the inference step.
we need to expand the graph from Figure 2a to include the reward r, the error term σV ϵ, and q′,
the q-values of the time step t + 1. This conﬁguration is presented in Figure 2b where the nodes
that have been doubled represent the states s and s′ which are both evaluated in a network sharing
the same parameters. Note that the value of q′ is computed using the same network presented in
Figure 2a in which this network acts as a predictor taking input as s′. When applying Equation 13,
q-values corresponding to a speciﬁc action can be selected using a vector hi ∈{0, 1}A having a
single non-zero value for the i-th component identifying which action was taken at a time t so that
qi = [q]i = h⊺
i q.
(15)
In the context TAGI-DQN, we employ Thompson sampling (Strens, 2000) for the selection of an
action given a state. More speciﬁcally, the vector h′
i ∈{0, 1}A is deﬁned such that the i-th non-zero
value corresponds to the index of the maximal value among q′, a vector of realizations from the
neural network’s posterior predictive output Q ∼N(q′; µQ|D, ΣQ|D). Because of the Gaussian
assumptions in TAGI, this posterior predictive is readily available from the forward uncertainty
propagation step, as outlined in §2.1.
The red arrows in Figure 2b outline the ﬂow of information during the inference procedure. The ﬁrst
step consists in inferring q using the relationships deﬁned in either Equation 13 or 14. As this is a
linear equation involving Gaussian random variables, the inference is analytically tractable. From
there, one can follow the same layer-wise recursive procedure presented in §2.1 in order to learn
5
Under review as a conference paper at ICLR 2022
the weights and biases in θ. With the exclusion of the standard hyperparameters related to network
architecture, batch size, buffer size or the discount factor, this TAGI-DQN framework only involves a
single hyperparameter, σV , the standard deviation for the value function. Note that when using CNNs
with TAGI, Nguyen & Goulet (2021) recommended using a decay function for the standard deviation
of the observation noise so that after seing e batches of n-steps,
σe
V = max(σmin
V
, η · σV )e−1,
(16)
because it allows putting more weight on the prior rather than on the likelihood during the inference
step at the early stage. This effect then diminishes with time. The model in Equation 16 has three
hyperparameters, the minimal noise parameter σmin
V
, the decay factor η and the initial noise parameter
σV . As it was shown by Nguyen & Goulet (2021) for CNNs and how we show in §4 for RL problems,
TAGI’s performance is robust towards the selection of these hyperparameters.
A comparison of implementation between TAGI and backpropagation on deep Q-network with
experience replay (Mnih et al., 2015) is shown in Figure 3. A practical implementation of n-step
TAGI deep Q-learning is presented in Algorithm 1 from Appendix A
Algorithm 1: TAGI-DQN with Experience Replay
1 Initialize replay memory R to capacity N; ΣV ;
2 Initialize parameters θ;
3 Discount factor γ;
4 for episode = 1 : E do
5
Reset environment s0;
6
for t = 1 : T do
7
q(st, a) : Q(st, a) ∼N(µQ
θ (st, a), ΣQ
θ (st, a));
8
at = arg max
a∈A
q(st, a);
9
st+1, rt = enviroment(at);
10
Store {st, at, rt, st+1} in R;
11
Sample random batch of {sj, aj, rj, sj+1};
12
q(sj+1, a′) : Q(sj+1, a′) ∼N(µQ
θ (sj+1, a′), ΣQ
θ (sj+1, a′));
13
a′
j+1 = arg max
a′∈A
q(sj+1, a′);
14
µYj = rj + γµQ
θ (sj+1, a′
j+1);
15
ΣYj = γ2ΣQ
θ (sj+1, a′
j+1) + ΣV ;
16
Update θ using TAGI on f(θ|y) (Equation 7);
Algorithm 2: DQN with Experience Replay
1 Initialize replay memory R to capacity N ;
2 Initialize parameters θ;
3 Discount factor γ;
4 Deﬁne ϵ (epsilon-greedy function);
5 for episode = 1 : E do
6
Reset environment s0;
7
for t = 1 : T do
8
u : U ∼U(0, 1);
9
at =
( randi(A)
u < ϵ;
arg max
a∈A
Qθ(st, a)
u ≥ϵ;
10
st+1, rt = enviroment(at);
11
Store {st, at, rt, st+1} in R;
12
Sample random batch of {sj, aj, rj, sj+1};
13
yj = rj + γ max
a′∈A Qθ(sj+1, a′);
14
Update θ using gradient descent on
15
L = 0.5 [yj −Qθ(sj, aj)]2;
Figure 3: Comparison of TAGI with backpropagation on deep Q-network with experience replay. L:
loss function; U: uniform distribution; randi: uniformly distributed pseudorandom integers.
3
RELATED WORKS
Over the last decades, several approximate methods have been proposed in order to allow for Bayesian
neural networks (Neal, 1995; Kingma et al., 2015; Hernández-Lobato & Adams, 2015; Blundell
et al., 2015; Louizos & Welling, 2016; Osawa et al., 2019; Wu et al., 2019; Gal & Ghahramani, 2016)
with various degree of approximations. Although some these methods have shown to be capable of
tackling classiﬁcation tasks on datasets such ImageNet (Osawa et al., 2019), none of them have been
applied on large-scale RL benchmark problems such as the Atari games environement. The key idea
behind using Bayesian methods for reinforcement learning is to consider the uncertainty associated
with Q-functions in order to identify a tradeoff between exploring the performance of possible actions
and exploiting the current optimal policy (Sutton & Barto, 2018). This typically takes the form of
performing Thompson sampling (Strens, 2000) rather than relying on heuristics such as ϵ-greedy.
For instance, MC dropout (Gal & Ghahramani, 2016) was introduced has a method intrinsically
suited for reinforcement learning. Nevertheless, ﬁve years after its inception, the approach has not
yet been reliably scaled to more advanced benchmarks such as the Atari game environment. The
6
Under review as a conference paper at ICLR 2022
same applies to Bayes-by-backprop (Blundell et al., 2015) which was recently applied to simple RL
problems (Lipton et al., 2018), and which has not yet been applied to more challenging environments
requiring convolutional networks. On the other hand, Bayesian neural networks relying on sampling
methods such as Hamiltonian Monte-Carlo (Neal, 1995) are typically computationally demanding to
be scaled to RL problems involving such a complex environment.
Although mainstream methods related to Bayesian neural networks have seldom been applied to
complex RL problems, several research teams have worked on alternative approaches in order to allow
performing Thompson sampling. For instance, Azizzadenesheli et al. (2018) have employed a deep
Q-network where the output layer relies on Bayesian linear regression. This approach was shown to
be outperforming its deterministic counterparts on Atari games. Another approach by Osband et al.
(2016) employs bootstrapped deep Q-networks with multiple network heads in order to represent
the uncertainty in the Q-functions. This approach was also shown to scale to Atari games while
presenting an improved performance in comparison with deterministic deep Q-networks. Finally,
Wang & Zhou (2020) have tackled the same problem, but this time by modelling the variability in the
Q-functions through a latent space learned using variational inference. Despite its good performance
on the benchmarks tested, it did not allowed to be scaled to the Atari game environment.
The TAGI deep Q-network presented in this paper is the ﬁrst demonstration that an analytically
tractable inference approach for Bayesian neural networks can be scaled to a problem as challenging
as the Atari game environment.
4
BENCHMARKS
This section compares the performance of TAGI with backpropagation-based standard implementa-
tions on off- and on-policy deep RL. For the off-policy RL, both TAGI-based and backpropagation-
based RL approaches are applied to deep Q-learning with experience replay (see Algorithm 1 & 2)
for the lunar lander and cart-pole environments. For the on-policy RL, TAGI is applied to the n-step
Q-learning algorithm (Mnih et al., 2016) and is compared with its backpropagation-based counterpart.
For this purpose, we perform the comparison on ﬁve Atari games including Beamrider, Breakout,
Pong, Qbert, and Space Invaders. Note that these ﬁve games are commonly selected for tuning
hyperparameters for the entire Atari games (Mnih et al., 2016; 2013) because they represent the
features sharing with the remaining Atari games. All benchmark environments are taken from the
OpenAI Gym (Brockman et al., 2016).
4.1
EXPERIMENTAL SETUP
In the ﬁrst experiments with off-policy RL, we use a fully-connected multilayer perceptron (MLP)
with two hidden layers of 256 units for the lunar lander environment, and with one hidden layer of 64
units for the cart pole environment. In these experiments, there is no need for input processing nor for
reward normalization. Note that unlike for the deterministic deep Q-network with experience replay,
TAGI does not separate the Q-network with the target network that is used for ensuring the stability
during training and allows eliminating the hyperparameter related to the target update frequency.
For the deep Q-network trained with backpropagation, we employ the pre-tuned implementation of
OpenAI baselines (Dhariwal et al., 2017) with all hyperparameters set to the default values.
For the Atari experiments with on-policy RL, we use the same input processing and model architecture
as Mnih et al. (2016). More speciﬁcally, the Q-network uses two convolutional layers (16-32 ﬁlters)
and a full-connected MLP of 256 units. The n-step Q-learning only uses a single network to represent
the value function for each action, and relies on a single learning agent in this experiment. The reason
behind this choice is that TAGI current main library is only available on Matlab which does not
support running a Python multiprocessing module such as the OpenAI gym. In the context of TAGI,
we use an horizon of 128 steps. As recommended by Andrychowicz et al. (2021) and following
practical implementation details (Pytorch, 2019; 2020), each return in n-step Q-learning algorithm
is normalized by subtracting the average return from the current n-steps and then dividing by the
empirical standard deviation from the set of n returns.
The standard deviation for the value function, (σV ), is initialized at 2. σV is decayed each 128
steps with a factor η = 0.9999. The minimal standard deviation for the value function σmin
V
= 0.3.
These hyperparameters values were not grid-searched but simply adapted to the scale of the problems
7
Under review as a conference paper at ICLR 2022
and are kept constant for both experiments. The complete details of the network architecture and
hyperparameters are provided in Appendix B and C.
4.2
RESULTS
For the ﬁrst set of experiments using off-policy RL, Figure 4 presents the average reward over
100 episodes for three runs for the lunar lander and cart pole environment. The TAGI-based deep
Q-learning with experience replay shows a faster and more stable learning than the one relying on
backpropagation.
0
0.5
1
−300
−150
0
150
300
Number of steps (M)
Average reward
TAGI
Backpropagation
(a) LunarLander-v2
0
0.5
1
0
50
100
150
200
Number of steps (M)
Average reward
(b) CartPole-v0
Figure 4: Illustration of average rewards over 100 episodes of ﬁve runs for one million time steps for
the TAGI-based and backpropagation-based deep Q-learning.
Table 1 shows that the average reward over the last 100 episodes obtained using TAGI are greater than
the one obtained using backpropagation. Figure 5 compares the average reward over 100 episodes
Table 1: Average reward over the last 100 episodes for the lunar lander and cart pole experiments.
TAGI: Tractable Approximate Gaussian Inference.
Method
Lunar lander
Cart pole
TAGI
279.9 ± 6.1
199.3 ± 1.0
Backpropagation 197.2 ± 84.3 137.7 ± 16.5
for three runs obtained for TAGI, with the results from Mnih et al. (2016) for the second set of
experiments on Atari games. Note that all results presented were obtained for a single agent, and that
the results for the backpropagation-trained networks are only reported at the end of each epoch.
Results show that TAGI outperforms the results from the original n-step Q-learning algorithm trained
with backpropagation (Mnih et al., 2016) on Breakout, Pong, and Qbert, while underperforming
on Beam Rider and Space Invaders. The average training time of TAGI for an Atari game is
approximately 13 hours on GPU calculations benchmarked on a 4-core-intel desktop of 32 GB of
RAM with a NVIDIA GTX 1080 Ti GPU. The training speed of TAGI for the experiment of the off-
policy deep RL is approximately three times slower on CPU calculations than the backpropagation-
trained counterpart. The reason behind this slower training time is because of its intrinsically
different inference engine making TAGI’s implementation incompatible with existing libraries such
as TensorFlow or Pytorch. TAGI’s library development is still ongoing and it is not yet fully optimized
for computational efﬁciency. Overall, these results for on- and off policy RL approaches conﬁrm that
TAGI can be applied to large scale problems such as deep Q-learning.
5
DISCUSSION
Although the performance of TAGI does not systematically outperform its backpropagation-based
counterpart, it requires fewer hyperparameters (see Appendix C). This advantage is one of the key
8
Under review as a conference paper at ICLR 2022
0
10
20
30
40
50
0
2,000
4,000
6,000
8,000
Number of epochs
Average reward
TAGI
Backpropagation
(a) Beam Rider
0
10
20
30
40
50
0
45
90
135
180
Number of epochs
Average reward
(b) Breakout
0
10
20
30
40
50
−21
−10
0
10
21
Number of epochs
Average reward
(c) Pong
0
10
20
30
40
50
0
3,000
6,000
9,000
12,000
Number of epochs
Average reward
(d) Qbert
0
10
20
30
40
50
160
320
480
640
800
Number of epochs
Average reward
(e) Space Invaders
Figure 5: Illustration of average reward over 100 episodes of three runs for ﬁve Atari games. The
number of epochs is used here for the comparison of TAGI and backpropagation-trained counterpart
obtained by Mnih et al. (2016). Each epoch corresponds to four million frames. The environment
identity are {Atari Game}NoFrameSkip-v4.
aspects for improving the generalization and reducing the computational cost of the hyperparameter
tuning process which are the key challenges in current state of deep RL (Irpan, 2018; Henderson et al.,
2018). For instance, in this paper, the TAGI’s hyperparameters relating to the standard deviation of
value function (σV ) are kept constant across all experiments. Moreover, since these hyperparameters
were not subject to grid-search in order to optimize the performance, the results obtained here
are representative of what a user should obtain by simply adapting the hyperparameters to ﬁt the
speciﬁcities and scale of the environment at hand.
More advanced RL approaches such as advanced actor critic (A2C) (Mnih et al., 2016) and proximal
policy optimization (PPO) (Schulman et al., 2017) employ two-networks architectures in which one
network is used to approximate a value function and other is employed to encode the policy. The
current TAGI-RL framework is not yet able to handle such architectures because training a policy
network involves an optimization problem for the selection of the optimal action. Backpropagation-
based approach currently rely on gradient optimization to perform this task, while TAGI will require
developing alternative approaches in order to maintain the analytical tractability without relying on
gradient-based optimization.
6
CONCLUSION
This paper proposes a Bayesian reinforcement learning framework that combines TAGI with deep
Q-learning. The proposed framework allows estimating the uncertainty of neural network’s param-
eters and taking an action given a state according to the parameter uncertainty using Thompson
sampling. Throughout the experiments with on- and off-policy reinforcement learning approaches,
we demonstrate that TAGI can reach a performance comparable to backpropagation-trained networks
while using only half the number of hyperparameters. These results challenge the common belief that
for large scale problems such as the Atari environment, neural networks can only be trained using
the gradient backpropagation. We have shown here that this current paradigm is no longer the only
alternative as TAGI has a linear computational complexity and can be used to learn the parameters of
complex networks in an analytically tractable manner, without relying on gradient-based optimization.
9
Under review as a conference paper at ICLR 2022
REFERENCES
M. Andrychowicz, A. Raichuk, P. Sta´
nczyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot, M. Geist,
O. Pietquin, M. Michalski, S. Gelly, and O. Bachem. What matters for on-policy deep actor-critic
methods? a large-scale study. In International Conference on Learning Representations, 2021.
URL https://openreview.net/forum?id=nIAxjsniDzg.
K. Azizzadenesheli, E. Brunskill, and A. Anandkumar. Efﬁcient exploration through Bayesian deep
q-networks. In IEEE Information Theory and Applications Workshop, pp. 1–9, 2018.
C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural networks.
arXiv preprint arXiv:1505.05424, 2015.
G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai
gym. arXiv preprint arXiv:1606.01540, 2016.
P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, Y. Wu,
and P. Zhokhov. Openai baselines. https://github.com/openai/baselines, 2017.
Y. Gal and Z. Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in
deep learning. In ICML proceedings, pp. 1050–1059, 2016.
E. Goan and C. Fookes. Bayesian neural networks: An introduction and survey. In Case Studies in
Applied Bayesian Data Science, pp. 45–87. Springer, 2020.
I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. MIT Press, 2016.
J-A. Goulet, L.H. Nguyen, and S. Amiri. Tractable approximate Gaussian inference for Bayesian
neural networks. Journal of Machine Learning Research (accepted for publication), 22:1–23,
September 2021.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artiﬁcial
intelligence, volume 32, 2018.
J. M. Hernández-Lobato and R. Adams. Probabilistic backpropagation for scalable learning of
bayesian neural networks. In International Conference on Machine Learning, pp. 1861–1869,
2015.
A. Irpan. Deep reinforcement learning doesn’t work yet. https://www.alexirpan.com/
2018/02/14/rl-hard.html, 2018.
D. P. Kingma, T. Salimans, and M. Welling. Variational dropout and the local reparameterization
trick. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural
Information Processing Systems, volume 28, 2015.
Z. Lipton, X. Li, J. Gao, L. Li, F. Ahmed, and L. Deng. Bbq-networks: Efﬁcient exploration in deep
reinforcement learning for task-oriented dialogue systems. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, volume 32, 2018.
C. Louizos and M. Welling. Structured and efﬁcient variational deep learning with matrix Gaussian
posteriors. In ICML proceedings, pp. 1708–1716, 2016.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.
Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, December 2013.
V. Mnih, K. Kavukcuoglu, D. Silver, A.A. Rusu, J. Veness, M.G. Bellemare, A. Graves, M. Riedmiller,
A.K. Fidjeland, and G Ostrovski. Human-level control through deep reinforcement learning. nature,
518(7540):529–533, 2015.
V. Mnih, Adria P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In ICML proceedings, pp. 1928–1937.
PMLR, 2016.
R. M. Neal. Bayesian learning for neural networks. PhD thesis, University of Toronto, 1995.
10
Under review as a conference paper at ICLR 2022
Luong-Ha Nguyen and James-A Goulet. Analytically tractable inference in deep neural networks.
4th Workshop on Tractable Probabilistic Modeling (UAI), 2021.
K. Osawa, S. Swaroop, A. Jain, R. Eschenhagen, R. E. Turner, R. Yokota, and M. E. Khan. Practical
deep learning with Bayesian principles. In Advances in Neural Information Processing Systems
proceedings, 2019.
I. Osband, C. Blundell, A. Pritzel, and Benjamin V. Roy. Deep exploration via bootstrapped dqn. In
NEURIPS proceedings, pp. 4033–4041, 2016.
Pytorch.
Pytorch examples for reinforce algorithm.
https://github.com/pytorch/
examples/blob/master/reinforcement_learning/reinforce.py, 2019.
Pytorch.
Pytorch examples for actor crtic algorithm.
https://github.com/pytorch/
examples/blob/master/reinforcement_learning/actor_critic.py, 2020.
H. E. Rauch, C. T. Striebel, and F. Tung. Maximum likelihood estimates of linear dynamic systems.
AIAA journal, 3(8):1445–1450, 1965.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.
M. Strens. A Bayesian framework for reinforcement learning. In ICML proceedings, pp. 943–950,
2000.
R. S. Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3(1):
9–44, 1988.
R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT Press, 2nd edition,
2018.
H. Van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double q-learning. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 30, 2016.
Z. Wang and M. Zhou. Thompson sampling via local uncertainty. In ICML proceedings, volume
119, pp. 10115–10125, 13–18 Jul 2020. URL http://proceedings.mlr.press/v119/
wang20ab.html.
C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.
A. Wu, S. Nowozin, E. Meeds, R. E. Turner, J. M. Hernández-Lobato, and A. L. Gaunt. Deterministic
variational inference for robust Bayesian neural networks. In ICLR proceedings, 2019.
11
Under review as a conference paper at ICLR 2022
A
ALGORITHM
This section presents the n-steps Q-learning algorithm with Tractable Approximate Gaussian Infer-
ence (TAGI).
Algorithm 3: n-step Q-learning with TAGI
1 Initialize θ ; ΣV ; number of steps (N)
2 Initialize memory R to capacity N;
3 steps = 0;
4 for episode = 1 : E do
5
Reset environment s0;
6
for t = 1 : T do
7
steps = steps + 1;
8
q(st, a) : Q(st, a) ∼N(µQ
θ (st, a), ΣQ
θ (st, a));
9
at = arg max
a∈A
q(st, a);
10
st+1, rt = enviroment(at);
11
Store {st, at, rr} in R;
12
if steps mod N == 0 then
13
q(st+1, a′) : Q(st+1, a′) ∼N(µQ
θ (st+1, a′), ΣQ
θ (st+1, a′));
14
a′
t+1 = arg max
a∈A
q(st+1, a′);
15
Take N samples of {sj, aj, rj} from R;
16
µy
N = µQ
θ (st+1, a′
t+1); Σy
N = ΣQ
θ (st+1, a′
t+1);
17
for j = N −1 : 1 do
18
µy
j = rj + γµy
j+1; Σy
j = γ2Σy
j+1 + ΣV ;
19
Update θ using TAGI;
20
Initialize memory R to capacity N;
B
MODEL ARCHITECTURE
This appendix contains the speciﬁcations for each model architecture in the experiment section. D
refers to a layer depth; W refers to a layer width; H refers to the layer height in case of convolutional
or pooling layers; K refers to the kernel size; P refers to the convolutional kernel padding; S refers
to the convolution stride; σ refers to the activation function type; ReLU refers to rectiﬁed linear unit;
Na refers to the number of actions.
Table 2: Model Architecture for Cart pole
Layer
D × W × H K × K P
S
σ
Input
4 × 1 × 1
-
-
-
-
Full connected 64 × 1 × 1
-
-
-
ReLU
Output
2 × 1 × 1
-
-
-
-
C
HYPERPARAMETER
This appendix details the hyperparameters for each model architecture in the experiment section
12
Under review as a conference paper at ICLR 2022
Table 3: Model Architecture for Lunar lander
Layer
D × W × H K × K P
S
σ
Input
8 × 1 × 1
-
-
-
-
Full connected 256 × 1 × 1
-
-
-
ReLU
Full connected 256 × 1 × 1
-
-
-
ReLU
Output
4 × 1 × 1
-
-
-
-
Table 4: Model Architecture for Atari domain
Layer
D × W × H
K × K P
S
σ
Input
4 × 84 × 84
-
-
-
-
Convolutional
16 × 20 × 20 8 × 8
0
4 ReLU
Convolutional
32 × 9 × 9
4 × 4
0
2 ReLU
Full connected 256 × 1 × 1
-
-
-
ReLU
Output
Na × 1 × 1
-
-
-
Table 5: Hyperparameters for Cart pole and Lunar lander
Method
#
Hyperparameter
Value
TAGI
1
Initial standard deviation for the value function (σV )
2
2
Decay factor (η)
0.9999
3
Minimal standard deviation for the value function (σmin
V
) 0.3
4
Buffer size
50 000
5
Batch size
10
6
Discount (γ)
0.99
Backprop
1
Learning rate
5 × 10−4
2
Adam epsilon
10−5
3
Adam β1
0.9
4
Adam β2
0.999
5
Buffer size
50 000
6
Exploration fraction
0.1
7
Final value of random action probability
0.02
8
Batch Size
32
9
Discount (γ)
0.99
10 Target update frequency
500
11 Gradient norm clipping coefﬁcient
10
13
Under review as a conference paper at ICLR 2022
Table 6: Hyperparameters for Atari domain
Method
#
Hyperparameter
Value
TAGI
1
Horizon
128
2
Initial standard deviation for the value function (σV )
2
3
Decay factor (η)
0.9999
4
Minimal standard deviation for the value function (σmin
V
) 0.3
5
Batch size
32
6
Discount (γ)
0.99
7
Number of actor-learners
1
Backprop
1
Horizon
5
2
Initial learning rate
LogUniform(10−4, 10−2)
3
Learning rate schedule
LinearAnneal(1, 0)
4
RMSProp decay parameter
0.99
5
Exploration rate 1 (ϵ1)
0.1
6
Exploration rate 2 (ϵ2)
0.01
7
Exploration rate 3 (ϵ3)
0.5
8
Probability of exploration rate 1
0.4
9
Probability of exploration rate 2
0.3
10 Probability of exploration rate 3
0.3
11 Exploration rate schedule (ﬁrst four million frames)
Anneal from 1 to ϵ1, ϵ2, ϵ3
12 Batch size
5
13 Discount (γ)
0.99
14 Number of actor-learners
1
14
