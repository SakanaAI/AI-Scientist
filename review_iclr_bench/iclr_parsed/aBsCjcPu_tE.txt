# SDEDIT: GUIDED IMAGE SYNTHESIS AND EDITING
## WITH STOCHASTIC DIFFERENTIAL EQUATIONS

**Chenlin Meng[1]** **Yutong He[1]** **Yang Song[1]** **Jiaming Song[1]**

**Jiajun Wu[1]** **Jun-Yan Zhu[2]** **Stefano Ermon[1]**

1Stanford University 2Carnegie Mellon University


ABSTRACT

Guided image synthesis enables everyday users to create and edit photo-realistic
images with minimum effort. The key challenge is balancing faithfulness to the
user inputs (e.g., hand-drawn colored strokes) and realism of the synthesized images. Existing GAN-based methods attempt to achieve such balance using either
conditional GANs or GAN inversions, which are challenging and often require
additional training data or loss functions for individual applications. To address
these issues, we introduce a new image synthesis and editing method, Stochastic Differential Editing (SDEdit), based on a diffusion model generative prior,
which synthesizes realistic images by iteratively denoising through a stochastic
differential equation (SDE). Given an input image with user guide in a form of
manipulating RGB pixels, SDEdit first adds noise to the input, then subsequently
denoises the resulting image through the SDE prior to increase its realism. SDEdit
does not require task-specific training or inversions and can naturally achieve the
balance between realism and faithfulness. SDEdit outperforms state-of-the-art
GAN-based methods by up to 98.09% on realism and 91.72% on overall satisfaction scores, according to a human perception study, on multiple tasks, including
stroke-based image synthesis and editing as well as image compositing.

|Col1|Stroke Painting to Image|Col3|
|---|---|---|
||||
||||
||||
||||


Stroke Painting to Image Stroke-based Editing

Input (guide) Output

Image Compositing

Source Input (guide) Output Source Input (guide) Output Source Input (guide) Output


Figure 1: Stochastic Differential Editing (SDEdit) is a unified image synthesis and editing framework based on stochastic differential equations. SDEdit allows stroke painting to image, image
compositing, and stroke-based editing without task-specific model training and loss functions.

1 INTRODUCTION

Modern generative models can create photo-realistic images from random noise (Karras et al., 2019;
Song et al., 2021), serving as an important tool for visual content creation. Of particular interest is
guided image synthesis and editing, where a user specifies a general guide (such as coarse colored
strokes) and the generative model learns to fill in the details (see Fig. 1). There are two natural


-----

desiderata for guided image synthesis: the synthesized image should appear realistic as well as be
_faithful to the user-guided input, thus enabling people with or without artistic expertise to produce_
photo-realistic images from different levels of details.

Existing methods often attempt to achieve such balance via two approaches. The first category
leverages conditional GANs (Isola et al., 2017; Zhu et al., 2017), which learn a direct mapping from
original images to edited ones. Unfortunately, for each new editing task, these methods require
data collection and model re-training, both of which could be expensive and time-consuming. The
second category leverages GAN inversions (Zhu et al., 2016; Brock et al., 2017; Abdal et al., 2019;
Gu et al., 2020; Wu et al., 2021; Abdal et al., 2020), where a pre-trained GAN is used to invert an
input image to a latent representation, which is subsequently modified to generate the edited image.
This procedure involves manually designing loss functions and optimization procedures for different
image editing tasks. Besides, it may sometimes fail to find a latent code that faithfully represents
the input (Bau et al., 2019b).

To balance realism and faithfulness while avoiding the previously mentioned challenges, we introduce SDEdit, a guided image synthesis and editing framework leveraging generative stochastic differential equations (SDEs; Song et al., 2021). Similar to the closely related diffusion models (SohlDickstein et al., 2015; Ho et al., 2020), SDE-based generative models smoothly convert an initial
Gaussian noise vector to a realistic image sample through iterative denoising, and have achieved
unconditional image synthesis performance comparable to or better than that of GANs (Dhariwal
& Nichol, 2021). The key intuition of SDEdit is to ‚Äúhijack‚Äù the generative process of SDE-based
generative models, as illustrated in Fig. 2. Given an input image with user guidance input, such as a
stroke painting or an image with stroke edits, we can add a suitable amount of noise to smooth out
undesirable artifacts and distortions (e.g., unnatural details at stroke pixels), while still preserving
the overall structure of the input user guide. We then initialize the SDE with this noisy input, and
progressively remove the noise to obtain a denoised result that is both realistic and faithful to the
user guidance input (see Fig. 2).

Unlike conditional GANs, SDEdit does not require collecting training images or user annotations
for each new task; unlike GAN inversions, SDEdit does not require the design of additional training
or task-specific loss functions. SDEdit only uses a single pretrained SDE-based generative model
trained on unlabeled data: given a user guide in a form of manipulating RGB pixels, SDEdit adds
Gaussian noise to the guide and then run the reverse SDE to synthesize images. SDEdit naturally
finds a trade-off between realism and faithfulness: when we add more Gaussian noise and run the
SDE for longer, the synthesized images are more realistic but less faithful. We can use this observation to find the right balance between realism and faithfulness.

We demonstrate SDEdit on three applications: stroke-based image synthesis, stroke-based image
editing, and image compositing. We show that SDEdit can produce realistic and faithful images
from guides with various levels of fidelity. On stroke-based image synthesis experiments, SDEdit
outperforms state-of-the-art GAN-based approaches by up to 98.09% on realism score and 91.72%
on overall satisfaction score (measuring both realism and faithfulness) according to human judgements. On image compositing experiments, SDEdit achieves a better faithfulness score and outperforms the baselines by up to 83.73% on overall satisfaction score in user studies. Our code and
models will be available upon publication.

2 BACKGROUND: IMAGE SYNTHESIS WITH STOCHASTIC DIFFERENTIAL
EQUATIONS (SDES)

Stochastic differential equations (SDEs) generalize ordinary differential equations (ODEs) by injecting random noise into the dynamics. The solution of an SDE is a time-varying random variable
(i.e., stochastic process), which we denote as x(t) ‚àà R[d], where t ‚àà [0, 1] indexes time. In image
synthesis (Song et al., 2021), we suppose that x(0) ‚àº _p0 = pdata represents a sample from the data_
distribution and that a forward SDE produces x(t) for t ‚àà (0, 1] via a Gaussian diffusion. Given
**x(0), x(t) is distributed as a Gaussian distribution:**

**x(t) = Œ±(t)x(0) + œÉ(t)z,** **z ‚àºN** (0, I), (1)


-----

Stroke

Image


Perturb with SDE Reverse SDE

Stroke

Image


Input Output

Figure 2: Synthesizing images from strokes with SDEdit. The blue dots illustrate the editing process of our method. The green and blue contour plots represent the distributions of images and
stroke paintings, respectively. Given a stroke painting, we first perturb it with Gaussian noise and
progressively remove the noise by simulating the reverse SDE. This process gradually projects an
unrealistic stroke painting to the manifold of natural images.

where œÉ(t) : [0, 1] ‚Üí [0, ‚àû) is a scalar function that describes the magnitude of the noise z, and
_Œ±(t) : [0, 1] ‚Üí_ [0, 1] is a scalar function that denotes the magnitude of the data x(0). The probability
density function of x(t) is denoted as pt.

Two types of SDE are usually considered: the Variance Exploding SDE (VE-SDE) has Œ±(t) = 1
for all t and œÉ(1) being a large constant so that p1 is close to N (0, œÉ[2](1)I); whereas the Variance
Preserving (VP) SDE satisfies Œ±[2](t) + œÉ[2](t) = 1 for all t with Œ±(t) ‚Üí 0 as t ‚Üí 1 so that p1 equals
to N (0, I). Both VE and VP SDE transform the data distribution to random Gaussian noise as t goes
from 0 to 1. For brevity, we discuss the details based on VE-SDE for the remainder of the main text,
and discuss the VP-SDE procedure in Appendix C. Though possessing slightly different forms and
performing differently depending on the image domain, they share the same mathematical intuition.

**Image synthesis with VE-SDE.** Under these definitions, we can pose the image synthesis problem
as gradually removing noise from a noisy observation x(t) to recover x(0). This can be performed
via a reverse SDE (Anderson, 1982; Song et al., 2021) that travels from t = 1 to t = 0, based on
the knowledge about the noise-perturbed score function ‚àáx log pt(x). For example, the sampling
procedure for VE-SDE is defined by the following (reverse) SDE:


dx(t) = **x log pt(x)** dt +
_‚àí_ [d[][œÉ]d[2]t[(][t][)]] _‚àá_
 


d[œÉ[2](t)]

d ¬Øw, (2)
dt


where ¬Øw is a Wiener process when time flows backwards from t = 1 to t = 0. If we set the initial
conditions x(1) ‚àº _p1 = N_ (0, œÉ[2](1)I), then the solution to x(0) will be distributed as pdata. In
practice, the noise-perturbed score function can be learned through denoising score matching (Vincent, 2011). Denote the learned score model as sŒ∏(x(t), t), the learning objective for time t is:

_Lt = Ex(0)_ _pdata,z_ (0,I)[ _œÉtsŒ∏(x(t), t)_ **z** 2[]][,] (3)
_‚àº_ _‚àºN_ _‚à•_ _‚àí_ _‚à•[2]_

where pdata is the data distribution and x(t) is defined as in Equation 1. The overall training objective is a weighted sum over t of each individual learning objective Lt, and various weighting
procedures have been discussed in Ho et al. (2020); Song et al. (2020; 2021).

With a parametrized score model sŒ∏(x(t), t) to approximate ‚àáx log pt(x), the SDE solution can be
approximated with the Euler-Maruyama method; an update rule from (t + ‚àÜt) to t is

**x(t) = x(t + ‚àÜt) + (œÉ[2](t)** _œÉ[2](t + ‚àÜt))sŒ∏(x(t), t) +_ _œÉ[2](t)_ _œÉ[2](t + ‚àÜt)z._ (4)
_‚àí_ _‚àí_

where z ‚àºN (0, I). We can select a particular discretization of the time interval fromp 1 to 0,
initialize x(0) ‚àºN (0, œÉ[2](1)I) and iterate via Equation 4 to produce an image x(0).

3 GUIDED IMAGE SYNTHESIS AND EDITING WITH SDEDIT

In this section, we introduce SDEdit and describe how we can perform guided image synthesis and
editing through an SDE model pretrained on unlabeled images.


-----

Faithful Realistic

Sweet spot


(a) KID and L2 norm squared
plot with respect to t0.


More faithful More realistic
Less realistic Less faithful

**SDEdit**

**Faithful**

**Realistic**

ùë°! = 0 ùë°! = 0.2 ùë°! = 0.4 ùë°! = 0.5 t! = 0.6 ùë°! = 0.7 ùë°! = 0.8 ùë°! = 0.9 ùë°! = 1

(b) We illustrate synthesized images of SDEdit with various t0 initializations.

_t0 = 0 indicates the guide itself, whereas t0 = 1 indicates a random sample._


Figure 3: Trade-off between faithfulness and realism for stroke-based generation on LSUN. As t0
increases, the generated images become more realistic while less faithful. Given an input, SDEdit
aims at generating an image that is both faithful and realistic, which means that we should choose
_t0 appropriately (t0 ‚àà_ [0.3, 0.6] in this example).

**Setup.** The user provides a full resolution image x[(][g][)] in a form of manipulating RGB pixels, which
we call a ‚Äúguide‚Äù. The guide may contain different levels of details; a high-level guide contains only
coarse colored strokes, a mid-level guide contains colored strokes on a real image, and a low-level
guide contains image patches on a target image. We illustrate these guides in Fig. 1, which can be
easily provided by non-experts. Our goal is to produce full resolution images with two desiderata:

**Realism. The image should appear realistic (e.g., measured by humans or neural networks).**

**Faithfulness. The image should be similar to the guide x[(][g][)]** (e.g., measured by L2 distance).

We note that realism and faithfulness are not positively correlated, since there can be realistic images
that are not faithful (e.g., a random realistic image) and faithful images that are not realistic (e.g., the
guide itself). Unlike regular inverse problems, we do not assume knowledge about the measurement
function (i.e., the mapping from real images to user-created guides in RBG pixels is unknown), so
techniques for solving inverse problems with score-based models (Dhariwal & Nichol, 2021; Kawar
et al., 2021) and methods requiring paired datasets (Isola et al., 2017; Zhu et al., 2017) do not apply
here.

**Procedure.** Our method, SDEdit, uses the fact that the reverse SDE can be solved not only from
_tSDE-based generative models. We need to find a proper initialization from our guides from which0 = 1, but also from any intermediate time t0 ‚àà_ (0, 1) ‚Äì an approach not studied by previous
we can solve the reverse SDE to obtain desirable, realistic, and faithful images. For any given guide
**x[(][g][)], we define the SDEdit procedure as follows:**

Sample x[(][g][)](t0) (x[(][g][)]; œÉ[2](t0)I), then produce x(0) by iterating Equation 4.
_‚àºN_

We use SDEdit(x[(][g][)]; t0, Œ∏) to denote the above procedure. Essentially, SDEdit selects a particular
time t0, add Gaussian noise of standard deviation œÉ[2](t0) to the guide x[(][g][)] and then solves the
corresponding reverse SDE at t = 0 to produce the synthesized x(0).

Apart from the discretization steps taken by the SDE solver, the key hyperparameter for SDEdit is t0,
the time from which we begin the image synthesis procedure in the reverse SDE. In the following,
we describe a realism-faithfulness trade-off that allows us to select reasonable values of t0.

**Realism-faithfulness trade-off.** We note that for properly trained SDE models, there is a realismfaithfulness trade-off when choosing different values of t0. To illustrate this, we focus on the LSUN
dataset, and use high-level stroke paintings as guides to perform stroke-based image generation. We
provide experimental details in Appendix D.2. We consider different choices of t0 [0, 1] for the
same input. To quantify realism, we adopt neural methods for comparing image distributions, such ‚àà
as the Kernel Inception Score (KID; Bi¬¥nkowski et al., 2018). If the KID between synthesized images
and real images are low, then the synthesized images are realistic. For faithfulness, we measure the
squared L2 distance between the synthesized images and the guides x[(][g][)]. From Fig. 3, we observe
increased realism but decreased faithfulness as t0 increases.

The realism-faithfulness trade-off can be interpreted from another angle. If the guide is far from
any realistic images, then we must tolerate at least a certain level of deviation from the guide (non

-----

Input (guide) StyleGAN2-ADA In-domain 1 In-domain 2 e4e SDEdit

User

created

Algorithm simulated

Figure 4: SDEdit generates more realistic and faithful images than state-of-the-art GAN-based models on stroke-based generation (LSUN bedroom). The guide in the first two rows are created by
human and the ones in the last two rows are simulated by algorithm.

faithfulness) in order to produce a realistic image. This is illustrated in the following proposition.

**Proposition 1. Assume that** _sŒ∏(x, t)_ 2
_with probability at least (1 ‚àí ‚à•Œ¥),_ _‚à•[2]_ _[‚â§]_ _[C][ for all][ x][ ‚ààX][ and][ t][ ‚àà]_ [[0][,][ 1]][. Then for all][ Œ¥][ ‚àà] [(0][,][ 1)]
2
**x[(][g][)]** SDEdit(x[(][g][)]; t0, Œ∏) _d_ log Œ¥ 2 log Œ¥) (5)
_‚àí_ 2 _[‚â§]_ _[œÉ][2][(][t][0][)(][CœÉ][2][(][t][0][) +][ d][ + 2]_ _‚àí_ _¬∑_ _‚àí_

p

_where d is the number of dimensions of x[(][g][)]._

We provide the proof in Appendix A. On a high-level, the difference from the guides and the synthesized images can be decomposed into the outputs of the score and random Gaussian noise; both
would increase as t0 increases, and thus the difference becomes greater. The above proposition suggests that for the image to be realistic with high probability, we must have a large enough t0. On the
flip side, if t0 is too large, then the faithfulness to the guide deteriorates, and SDEdit will produce
random realistic images (with the extreme case being unconditional image synthesis).

**Choice of t0.** We note that the quality of the guide may affect the overall quality of the synthesized
image. For reasonable guides, we find that t0 [0.3, 0.6] works well. However, if the guide is an
image with only white pixels, then even the closest ‚Äúrealistic‚Äù samples from the model distribution ‚àà
can be quite far, and we must sacrifice faithfulness for better realism by choosing a large t0. In
interactive settings (where user draws a sketch-based guide), we can initialize t0 [0.3, 0.6], synthesize a candidate with SDEdit, and ask the user whether the sample should be more faithful or ‚àà
more realistic; from the responses, we can obtain a reasonable t0 via binary search. In large-scale
non-interactive settings (where we are given a set of produced guides), we can perform a similar
binary search on a randomly selected image to obtain t0 and subsequently fix t0 for all guides in
the same task. Although different guides could potentially have different optimal t0, we empirically
observe that the shared t0 works well for all reasonable guides in the same task.

**Detailed algorithm and extensions.** We present the general algorithm for VE-SDE in Algorithm 1. Due to space limit, we describe our detailed algorithm for VP-SDE in Appendix C. Essentially, the algorithm is an Euler-Maruyama method for solving SDEdit(x[(][g][)]; t0, Œ∏). For cases
where we wish to keep certain parts of the synthesized images to be identical to that of the guides,
we can also introduce an additional channel that masks out parts of the image we do not wish to edit.
This is a slight modification to the SDEdit procedure mentioned in the main text, and we discuss the
details in Appendix C.2.


-----

**Algorithm 1 Guided image synthesis and editing with SDEdit (VE-SDE)**

**Require: x[(][g][)]** (guide), t0 (SDE hyper-parameter), N (total denoising steps)

‚àÜt _N_
_‚Üê_ _[t][0]_

**z ‚àºN** (0, I)
**x** **x + œÉ(t0)z**
_‚Üê_
**for n ‚Üê** _N to 1 do_

_t_ _t0_ _N[n]_
_‚Üê_

**z ‚àºN** (0, I)
_œµ ‚Üê_ _œÉ[2](t) ‚àí_ _œÉ[2](t ‚àí_ ‚àÜt)

**x** **x + œµ[2]sŒ∏(x, t) + œµz**
_‚Üêp_

**end for**
**Return x**


4 RELATED WORK

**Conditional GANs.** Conditional GANs for image editing (Isola et al., 2017; Zhu et al., 2017; Jo
& Park, 2019; Liu et al., 2021) learn to directly generate an image based on a user input, and have
demonstrated success on a variety of tasks including image synthesis and editing (Portenier et al.,
2018; Chen & Koltun, 2017; Dekel et al., 2018; Wang et al., 2018; Park et al., 2019; Zhu et al.,
2020b; Jo & Park, 2019; Liu et al., 2021), inpainting (Pathak et al., 2016; Iizuka et al., 2017; Yang
et al., 2017; Liu et al., 2018), photo colorization (Zhang et al., 2016; Larsson et al., 2016; Zhang
et al., 2017; He et al., 2018), semantic image texture and geometry synthesis (Zhou et al., 2018;
Gu¬¥erin et al., 2017; Xian et al., 2018). They have also achieved strong performance on image editing using user sketch or color (Jo & Park, 2019; Liu et al., 2021; Sangkloy et al., 2017). However,
conditional models have to be trained on both original and edited images, thus requiring data collection and model re-training for new editing tasks. Thus, applying such methods to on-the-fly image
manipulation is still challenging since a new model needs to be trained for each new application.
Unlike conditional GANs, SDEdit only requires training on the original image. As such, it can be
directly applied to various editing tasks at test time as illustrated in Fig. 1.

**GANs inversion and editing.** Another mainstream approach to image editing involves GAN inversion (Zhu et al., 2016; Brock et al., 2017), where the input is first projected into the latent space
of an unconditional GAN before synthesizing a new image from the modified latent code. Several methods have been proposed in this direction, including fine-tuning network weights for each
image (Bau et al., 2019a; Pan et al., 2020; Roich et al., 2021), choosing better or multiple layers
to project and edit (Abdal et al., 2019; 2020; Gu et al., 2020; Wu et al., 2021), designing better
encoders (Richardson et al., 2021; Tov et al., 2021), modeling image corruption and transformations (Anirudh et al., 2020; Huh et al., 2020), and discovering meaningful latent directions (Shen
et al., 2020; Goetschalckx et al., 2019; Jahanian et al., 2020; H¬®ark¬®onen et al., 2020). However, these
methods need to define different loss functions for different tasks. They also require GAN inversion,
which can be inefficient and inaccurate for various datasets (Huh et al., 2020; Karras et al., 2020b;
Bau et al., 2019b; Xu et al., 2021).

**Other generative models.** Recent advances in training non-normalized probabilistic models, such
as score-based generative models (Song & Ermon, 2019; 2020; Song et al., 2021; Ho et al., 2020;
Song et al., 2020; Jolicoeur-Martineau et al., 2021) and energy-based models (Ackley et al., 1985;
Gao et al., 2017; Du & Mordatch, 2019; Xie et al., 2018; 2016; Song & Kingma, 2021), have
achieved comparable image sample quality as GANs. However, most of the prior works in this
direction have focused on unconditional image generation and density estimation, and state-of-theart techniques for image editing and synthesis are still dominated by GAN-based methods. In this
work, we focus on the recently emerged generative modeling with stochastic differential equations
(SDE), and study its application to controllable image editing and synthesis tasks. A concurrent
work (Choi et al., 2021) performs conditional image synthesis with diffusion models, where the
conditions can be represented as the known function of the underlying true image.


-----

|LSUN bedroom|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|||||||||||
|||||||||||
||||||||||CelebA|
||||||LSUN church|||||


Figure 5: SDEdit can generate realistic, faithful and diverse images for a given stroke input drawn
by human.


EXPERIMENTS


In this section, we show that SDEdit is able to outperform state-of-the-art GAN-based models on
stroke-based image synthesis and editing as well as image compositing. Both SDEdit and the baselines use publicly available pre-trained checkpoints. Based on the availability of open-sourced SDE
checkpoints, we use VP-SDE for experiments on LSUN datasets, and VE-SDE for experiments on
CelebA-HQ.

Baselines Faithfulness score (L2) ‚Üì SDEdit is more realistic (MTurk) ‚Üë SDEdit is more satisfactory (Mturk) ‚Üë

In-domain GAN-1 101.18 94.96% 89.48%
In-domain GAN-2 57.11 97.87% 89.51%
StyleGAN2-ADA 68.12 98.09% 91.72%
e4e 53.76 80.34% 75.43%
SDEdit **32.55** ‚Äì ‚Äì


Table 1: SDEdit outperforms all the GAN baselines on stroke-based generation on LSUN (bedroom). The input strokes are created by human users. The rightmost two columns stand for the
percentage of MTurk workers that prefer SDEdit to the baseline for pairwise comparison.

**Evaluation metrics.** We evaluate the editing results based on realism and faithfulness. To quantify realism, we use Kernel Inception Score (KID) between the generated images and the target
realistic image dataset (details in Appendix D.2), and pairwise human evaluation between different
approaches with Amazon Mechanical Turk (MTurk). To quantify faithfulness, we report the L2
distance summed over all pixels between the guide and the edited output image normalized to [0,1].
We also consider LPIPS (Zhang et al., 2018) and MTurk human evaluation for certain experiments.
To quantify the overall human satisfaction score (realism + faithfulness), we leverage MTurk human
evaluation to perform pairwise comparsion between the baselines and SDEdit (see Appendix F).

5.1 STROKE-BASED IMAGE SYNTHESIS


Given an input stroke painting, our goal is to generate a realistic and faithful image when no paired
_data is available. We consider stroke painting guides created by human users (see Fig. 5). At the_
same time, we also propose an algorithm to automatically simulate user stroke paintings based on
a source image (see Fig. 4), allowing us to perform large scale quantitative evaluations for SDEdit.
We provide more details in Appendix D.2.

**Baselines.** For comparison, we choose three state-of-the-art GAN-based image editing and synthesis methods as our baselines. Our first baseline is the image projection method used in StyleGAN2ADA[1] (Karras et al., 2020a), where inversion is done in the W [+] space of StyleGANs by minimizing
the perceptual loss. Our second baseline is in-domain GAN[2] (Zhu et al., 2020a), where inversion
is accomplished by running optimization steps on top of an encoder. Specifically, we consider two
versions of the in-domain GAN inversion techniques: the first one (denoted as In-domain GAN-1)

[1https://github.com/NVlabs/stylegan2-ada](https://github.com/NVlabs/stylegan2-ada)
[2https://github.com/genforce/idinvert_pytorch](https://github.com/genforce/idinvert_pytorch)


-----

Figure 6: Stroke-based image editing with SDEdit on LSUN bedroom, CelebA-HQ, and LSUN
church datasets. For comparison, we show the results of GAN baselines, where results for LSUN
bedroom and CelebA-HQ are obtained by in-domain GAN (the leftmost 5 panels), and results for
LSUN church are from StyleGAN2-ADA (the rightmost 3 panels). We observe that SDEdit is able

only uses the encoder to maximize the inversion speed, whereas the second (denoted as In-domain
GAN-2) runs additional optimization steps to maximize the inversion accuracy. Our third baseline is
e4e[3] (Tov et al., 2021), whose encoder objective is explicitly designed to balance between perceptual
quality and editability by encouraging to invert images close to W space of a pretrained StyleGAN
model.

LSUN Bedroom LSUN Church

lines struggle to generate realistic images based Methods _L2_ KID _L2_ KID
on stroke painting inputs whereas SDEdit suc- _‚Üì_ _‚Üì_ _‚Üì_ _‚Üì_

In-domain GAN-1 105.23 0.1147 -  - 
In-domain GAN-2 76.11 0.2070 -  - 

serve semantics of the input stroke painting. StyleGAN2-ADA 74.03 0.1750 72.41 0.1544
As shown in Fig. 5, SDEdit can also syn- e4e 52.40 0.0464 68.53 0.0354
thesize diverse images for the same input. SDEdit (ours) **36.76** **0.0030** **37.67** **0.0156**

to produce more faithful and realistic editing compared to the baselines.

Tov et al., 2021
quality and editability by encouraging to invert images close to

We present qualitative comparison
4. We observe that all baselines struggle to generate realistic images based
on stroke painting inputs whereas SDEdit successfully generates realistic images that preserve semantics of the input stroke painting.
5, SDEdit can also synthesize diverse images for the same input.

We present quantitative comparison results using user-created stroke guides in Table 1 and Table 2: SDEdit outperforms all the GAN basealgorithm-simulated stroke guides in Table 2. lines on both faithfulness and realism for strokeWe report the L2 distance for faithfulness com- based image generation. The input strokes are
parison, and leverage MTurk (see Appendix F) generated with the stroke-simulation algorithm.
or KID scores for realism comparison. To quan- KID is computed using the generated images
tify the overall human satisfaction score (faith- and the corresponding validation sets (see Apfulness + realism), we ask a different set of pendix D.2).
MTurk workers to perform another 3000 pairwise comparisons between the baselines and SDEdit based on both faithfulness and realism. We
observe that SDEdit outperforms GAN baselines on all the evaluation metrics, beating the baselines
by more than 80% on realism scores and 75% on overall satisfaction scores. We provide more
experimental details in Appendix C and more results in Appendix E.


5.2 FLEXIBLE IMAGE EDITING

In this section, we show that SDEdit is able to outperform existing GAN-based models on image
editing tasks. We focus on LSUN (bedroom, church) and CelebA-HQ datasets, and provide more
details on the experimental setup in the Appendix D.


**Stroke-based image editing.** Given an image with stroke edits, we want to generate a realistic
and faithful image based on the user edit. We consider the same GAN-based baselines (Zhu et al.,
2020a; Karras et al., 2020a; Tov et al., 2021) as our previous experiment. As shown in Fig. 6,

[3https://github.com/omertov/encoder4editing](https://github.com/omertov/encoder4editing)


-----

Source Poisson Blending Laplacian Blending Input (guide) In-domain GAN StyleGAN2-ADA e4e SDEdit (ours)

Traditional Blending GAN baselines

Figure 7: SDEdit is able to achieve realistic while more faithful editing results compared to traditional blending and recent GAN-based approaches for image compositing on CelebA-HQ. Quantitative results are reported in Table 3.

_L2_ SDEdit more realistic SDEdit more satisfactory LPIPS
Methods (faithfulness) ‚Üì (Mturk) ‚Üë (Mturk) ‚Üë (masked) ‚Üì

Laplacian Blending 68.45 75.27% 83.73% 0.09
Poisson Blending 63.04 75.60% 82.18% 0.05
In-domain GAN 36.67 53.08% 73.53% 0.23
StyleGAN2-ADA 69.38 74.12% 83.43% 0.21
e4e 53.90 43.67% 66.00% 0.33
SDEdit (ours) **21.70** ‚Äì ‚Äì **0.03**

Table 3: Image compositing experiments on CelebA-HQ. The middle two columns indicate the percentage of MTurk workers that prefer SDEdit. We also report the masked LPIPS distance between
edited and unchanged images to quantify undesired changes outside the masks. We observe that
SDEdit is able to achieve realistic editing while being more faithful than the baselines, beating the
baseline by up to 83.73% on overall satisfaction score by human evaluators.

results generated by the baselines tend to introduce undesired modifications, occasionally making
the region outside the stroke blurry. In contrast, SDEdit generates image edits that are both realistic
and faithful to the input, while avoiding making undesired modifications. We provide extra results
in Appendix E.

**Image compositing.** We focus on compositing images on the CelebA-HQ dataset (Karras et al.,

2017). Given an image randomly sampled from the dataset, we ask users to specify how they want
the edited image to look like using pixel patches copied from other reference images as well as the
pixels they want to perform modifications (see Fig. 7). We compare our method with traditional
blending algorithms (Burt & Adelson, 1987; P¬¥erez et al., 2003) and the same GAN baselines considered previously. We perform qualitative comparison in Fig. 7. For quantitative comparison, we
report the L2 distance to quantify faithfulness. To quantify realism, we ask MTurk workers to perform 1500 pairwise comparisons between the baselines and SDEdit. To quantify user satisfaction
score (faithfulness + realism), we ask different workers to perform another 1500 pairwise comparisons against SDEdit. To quantify undesired changes (e.g. change of identity), we follow Bau et al.
(2020) to compute masked LPIPS (Zhang et al., 2018). As evidenced in Table 3, we observe that
SDEdit is able to generate both faithful and realistic images with much better LPIPS scores than the
baselines, outperforming the baselines by up to 83.73% on overall satisfaction score and 75.60%
on realism. Although our realism score is marginally lower than e4e, images generated by SDEdit
are more faithful and more satisfying overall. We present more experiment details in Appendix D.

6 CONCLUSION

We propose Stochastic Differential Editing (SDEdit), a guided image editing and synthesis method
via generative modeling of images with stochastic differential equations (SDEs) allowing for balanced realism and faithfulness. Unlike image editing techniques via GAN inversion, our method
does not require task-specific optimization algorithms for reconstructing inputs, and is particularly
suitable for datasets or tasks where GAN inversion losses are hard to design or optimize. Unlike
conditional GANs, our method does not require collecting new datasets for the ‚Äúguide‚Äù images or
re-training models, both of which could be expensive or time-consuming. We demonstrate that
SDEdit outperforms existing GAN-based methods on a variety of image synthesis and editing tasks.


-----

**Acknowledgments.** The authors would like to thank Kristy Choi for proofreading. This research was in part supported by NSF (#1651565, #1522054, #1733686), ONR (N00014-19-1-2145),
AFOSR (FA9550-19-1-0024), ARO (W911NF-15-1-0479), Autodesk, Google, Bosch, Stanford Institute for Human-Centered AI (HAI), Stanford Center for Integrated Facility Engineering (CIFE),
Amazon Research Award (ARA), and Amazon AWS. Yang Song is supported by the Apple PhD
Fellowship in AI/ML. J.-Y. Zhu is partly supported by Naver Corporation.

ETHICS STATEMENT

In this work, we propose SDEdit, which is a new image synthesis and editing methods based on
generative stochastic differential equations (SDEs). In our experiments, all the considered datasets
are open-sourced and publicly available, being used under permission. Similar to commonly seen
deep-learning based image synthesis and editing algorithms, our method has both positive and negative societal impacts depending on the applications and usages. On the positive side, SDEdit enables
everyday users with or without artistic expertise to create and edit photo-realistic images with minimum effort, lowering the barrier to entry for visual content creation. On the other hand, SDEdit
can be used to generate high-quality edited images that are hard to be distinguished from real ones
by humans, which could be used in malicious ways to deceive humans and spread misinformation.
Similar to commonly seen deep-learning models (such as GAN-based methods for face-editing),
SDEdit might be exploited by malicious users with potential negative impacts. In our code release,
we will explicitly specify allowable uses of our system with appropriate licenses.

We also notice that forensic methods for detecting fake machine-generated images mostly focus on
distinguishing samples generated by GAN-based approaches. Due to the different underlying nature
between GANs and generative SDEs, we observe that state-of-the-art approaches for detecting fake
images generated by GANs (Wang et al., 2020) struggle to distinguish fake samples generated by
SDE-based models. For instance, on the LSUN bedroom dataset, it only successfully detects less
than 3% of SDEdit-generated images whereas being able to distinguish up to 93% on GAN-based
generation. Based on these observations, we believe developing forensic methods for SDE-based
models is also critical as SDE-based methods become more prevalent.

For human evaluation experiments, we leveraged Amazon Mechanical Turk (MTurk). For each
worker, the evaluation HIT contains 15 pairwise comparison questions for comparing edited images.
The reward per task is kept as 0.2$. Since each task takes around 1 minute, the wage is around
12$ per hour. We provide more details on Human evaluation experiments in Appendix F. We also
note that the bias of human evaluators (MTurk workers) and the bias of users (through the input
‚Äúguidance‚Äù) could potentially affect the evaluation metrics and results used to track the progress
towards guided image synthesis and editing.

REPRODUCIBILITY STATEMENT

[1. Our code is released at https://github.com/ermongroup/SDEdit.](https://github.com/ermongroup/SDEdit)

2. We use open source datasets and SDE checkpoints on the corresponding datasets. We did
not train any SDE models.

3. Proofs are provided in Appendix A.

4. Extra details on SDEdit and pseudocode are provided in Appendix C.

5. Details on experimental settings are provided in Appendix D.

6. Extra experimental results are provided in Appendix E.

7. Details on human evaluation are provided in Appendix F.


-----

REFERENCES

Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent
space? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4432‚Äì4441,
2019.

Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images? In
_Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8296‚Äì8305,_
2020.

David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for boltzmann machines.
_Cognitive science, 9(1):147‚Äì169, 1985._

Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12
(3):313‚Äì326, 1982.

Rushil Anirudh, Jayaraman J Thiagarajan, Bhavya Kailkhura, and Peer-Timo Bremer. Mimicgan: Robust
projection onto image manifolds with corruption mimicking. International Journal of Computer Vision, pp.
1‚Äì19, 2020.

David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, and Antonio Torralba.
Semantic photo manipulation with a generative image prior. ACM SIGGRAPH, 38(4):1‚Äì11, 2019a.

David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, and Antonio Torralba.
Seeing what a gan cannot generate. In Proceedings of the IEEE/CVF International Conference on Computer
_Vision, pp. 4502‚Äì4511, 2019b._

David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, and Antonio Torralba. Rewriting a deep generative
model. In European Conference on Computer Vision (ECCV), 2020.

Miko≈Çaj Bi¬¥nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv
_preprint arXiv:1801.01401, 2018._

Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with introspective
adversarial networks. In International Conference on Learning Representations (ICLR), 2017.

Peter J Burt and Edward H Adelson. The laplacian pyramid as a compact image code. In Readings in computer
_vision, pp. 671‚Äì679. Elsevier, 1987._

Qifeng Chen and Vladlen Koltun. Photographic image synthesis with cascaded refinement networks. In IEEE
_International Conference on Computer Vision (ICCV), 2017._

Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning
method for denoising diffusion probabilistic models. arXiv preprint arXiv:2108.02938, 2021.

Tali Dekel, Chuang Gan, Dilip Krishnan, Ce Liu, and William T Freeman. Sparse, smart contours to represent
and edit images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. _arXiv preprint_
_arXiv:2105.05233, 2021._

Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv preprint
_arXiv:1903.08689, 2019._

Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning energy-based models as
generative convnets via multi-grid modeling and sampling. arXiv e-prints, pp. arXiv‚Äì1709, 2017.

Lore Goetschalckx, Alex Andonian, Aude Oliva, and Phillip Isola. Ganalyze: Toward visual definitions of
cognitive image properties. In IEEE International Conference on Computer Vision (ICCV), 2019.

Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code gan prior. In IEEE Conference on
_Computer Vision and Pattern Recognition (CVPR), 2020._

Eric Gu¬¥¬¥ erin, Julie Digne, Eric Galin, Adrien Peytavie, Christian Wolf, Bedrich Benes, and Benoundefinedt[¬¥]
Martinez. Interactive example-based terrain authoring with conditional generative adversarial networks.
_ACM Transactions on Graphics (TOG), 36(6), 2017._

Erik H¬®ark¬®onen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering interpretable
gan controls. In Advances in Neural Information Processing Systems, 2020.


-----

Mingming He, Dongdong Chen, Jing Liao, Pedro V Sander, and Lu Yuan. Deep exemplar-based colorization.
_ACM Transactions on Graphics (TOG), 37(4):1‚Äì16, 2018._

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _arXiv preprint_
_arXiv:2006.11239, 2020._

Minyoung Huh, Richard Zhang, Jun-Yan Zhu, Sylvain Paris, and Aaron Hertzmann. Transforming and projecting images into class-conditional generative networks. In European Conference on Computer Vision
_(ECCV), 2020._

Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Globally and locally consistent image completion.
_ACM Transactions on Graphics (TOG), 36(4):107, 2017._

Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional
adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.

Ali Jahanian, Lucy Chai, and Phillip Isola. On the‚Äùsteerability‚Äù of generative adversarial networks. In Interna_tional Conference on Learning Representations (ICLR), 2020._

Youngjoo Jo and Jongyoul Park. Sc-fegan: Face editing generative adversarial network with user‚Äôs sketch and
color. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1745‚Äì1753, 2019.

Alexia Jolicoeur-Martineau, R¬¥emi Pich¬¥e-Taillefer, Ioannis Mitliagkas, and Remi Tachet des Combes. Adversarial score matching and improved sampling for image generation. In International Conference on Learning
_Representations, 2021._

Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality,
stability, and variation. arXiv preprint arXiv:1710.10196, 2017.

Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial
networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.

Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative
adversarial networks with limited data. arXiv preprint arXiv:2006.06676, 2020a.

Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and
improving the image quality of stylegan. In IEEE Conference on Computer Vision and Pattern Recognition
_(CVPR), 2020b._

Bahjat Kawar, Gregory Vaksman, and Michael Elad. Snips: Solving noisy inverse problems stochastically.
_arXiv preprint arXiv:2105.14951, 2021._

Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for automatic colorization. In European Conference on Computer Vision (ECCV), 2016.

Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. Annals
_of Statistics, pp. 1302‚Äì1338, 2000._

Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolutions. In European Conference on Computer Vision (ECCV),
2018.

Hongyu Liu, Ziyu Wan, Wei Huang, Yibing Song, Xintong Han, Jing Liao, Bin Jiang, and Wei Liu. Deflocnet: Deep image editing via flexible low-level controls. In Proceedings of the IEEE/CVF Conference on
_Computer Vision and Pattern Recognition, pp. 10765‚Äì10774, 2021._

Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep generative
prior for versatile image restoration and manipulation. In European Conference on Computer Vision, 2020.

Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatiallyadaptive normalization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.

Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders:
Feature learning by inpainting. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2016.

Patrick P¬¥erez, Michel Gangnet, and Andrew Blake. Poisson image editing. In ACM SIGGRAPH, pp. 313‚Äì318,
2003.


-----

Tiziano Portenier, Qiyang Hu, Attila Szab¬¥o, Siavash Arjomand Bigdeli, Paolo Favaro, and Matthias Zwicker.
Faceshop: Deep sketch-based face image editing. ACM Transactions on Graphics (TOG), 37(4), 2018.

Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or.
Encoding in style: a stylegan encoder for image-to-image translation. In Proceedings of the IEEE/CVF
_Conference on Computer Vision and Pattern Recognition, 2021._

Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based editing of
real images. arXiv preprint arXiv:2106.05744, 2021.

Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, and James Hays. Scribbler: Controlling deep image
synthesis with sketch and color. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2017.

Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for semantic face
editing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.

Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning
using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015.

Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint_
_arXiv:2010.02502, 2020._

Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In
_Advances in Neural Information Processing Systems (NeurIPS), 2019._

Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. arXiv preprint
_arXiv:2006.09011, 2020._

Yang Song and Diederik P Kingma. How to train your energy-based models. arXiv preprint arXiv:2101.03288,
2021.

Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-based generative modeling through stochastic differential equations. In International Conference on
_Learning Representations (ICLR), 2021._

Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. Designing an encoder for stylegan
image manipulation. ACM Transactions on Graphics (TOG), 40(4):1‚Äì14, 2021.

Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23
(7):1661‚Äì1674, 2011.

Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images are
surprisingly easy to spot...for now. In CVPR, 2020.

Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution
image synthesis and semantic manipulation with conditional gans. In IEEE Conference on Computer Vision
_and Pattern Recognition (CVPR), 2018._

Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespace analysis: Disentangled controls for stylegan image
generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.

Wenqi Xian, Patsorn Sangkloy, Varun Agrawal, Amit Raj, Jingwan Lu, Chen Fang, Fisher Yu, and James Hays.
Texturegan: Controlling deep image synthesis with texture patches. In IEEE Conference on Computer Vision
_and Pattern Recognition (CVPR), 2018._

Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In International
_Conference on Machine Learning, pp. 2635‚Äì2644. PMLR, 2016._

Jianwen Xie, Yang Lu, Ruiqi Gao, and Ying Nian Wu. Cooperative learning of energy-based model and
latent variable model via mcmc teaching. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 32, 2018.

Yinghao Xu, Yujun Shen, Jiapeng Zhu, Ceyuan Yang, and Bolei Zhou. Generative hierarchical features from
synthesizing images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog_nition, pp. 4432‚Äì4442, 2021._

Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang, and Hao Li. High-resolution image inpainting
using multi-scale neural patch synthesis. In IEEE Conference on Computer Vision and Pattern Recognition
_(CVPR), 2017._


-----

Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European Conference on
_Computer Vision (ECCV), 2016._

Richard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng, Angela S Lin, Tianhe Yu, and Alexei A Efros.
Real-time user-guided image colorization with learned deep priors. ACM Transactions on Graphics (TOG),
9(4), 2017.

Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness
of deep features as a perceptual metric. In CVPR, 2018.

Yang Zhou, Zhen Zhu, Xiang Bai, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Non-stationary texture
synthesis by adversarial expansion. ACM Transactions on Graphics (TOG), 37(4), 2018.

Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing. In
_European Conference on Computer Vision (ECCV), 2020a._

Jun-Yan Zhu, Philipp Kr¬®ahenb¬®uhl, Eli Shechtman, and Alexei A Efros. Generative visual manipulation on the
natural image manifold. In European Conference on Computer Vision (ECCV), 2016.

Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using
cycle-consistent adversarial networks. In IEEE International Conference on Computer Vision (ICCV), 2017.

Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. Sean: Image synthesis with semantic regionadaptive normalization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020b.


-----

A PROOFS

**Proposition 1. Assume that** _sŒ∏(x, t)_ 2
_with probability at least (1 ‚àí ‚à•Œ¥),_ _‚à•[2]_ _[‚â§]_ _[C][ for all][ x][ ‚ààX][ and][ t][ ‚àà]_ [[0][,][ 1]][. Then for all][ Œ¥][ ‚àà] [(0][,][ 1)]
2
**x[(][g][)]** SDEdit(x[(][g][)]; t0, Œ∏) _d_ log Œ¥ 2 log Œ¥) (5)
_‚àí_ 2 _[‚â§]_ _[œÉ][2][(][t][0][)(][CœÉ][2][(][t][0][) +][ d][ + 2]_ _‚àí_ _¬∑_ _‚àí_

p

_where d is the number of dimensions of x[(][g][)]._


_Proof. Denote x[(][g][)](0) = SDEdit(x[(][g][)]; t, Œ∏), then_


0

_t0_

Z

0

_t0_

Z

0

_t0_

Z


2

dx[(][g][)](t)

dt
dt 2

_sŒ∏(x, t; Œ∏)_ dt +

_‚àí_ [d[][œÉ]d[2]t[(][t][)]]
 


**x[(][g][)](t0)** **x[(][g][)](0)**
_‚àí_


(6)

(7)

(8)


2 [=]


d[œÉ[2](t)]

d ¬Øw
dt


0

_t0_

Z


_sŒ∏(x, t; Œ∏)_ dt

_‚àí_ [d[][œÉ]d[2]t[(][t][)]]




d[œÉ[2](t)]

d ¬Øw
dt


From the assumption over sŒ∏(x, t; Œ∏), the first term is not greater than

0 2

_C_ dt = CœÉ[4](t0),

_t0_ _‚àí_ [d[][œÉ]d[2]t[(][t][)]] 2

Z  

where equality could only happen when each score output has a squared L2 norm of C and they are
linearly dependent to one other. The second term is independent to the first term as it only concerns
random noise; this is equal to the squared L2 norm of a random variable from a Wiener process
at time t = 0, with marginal distribution being œµ (0, œÉ[2](t0)I) (this marginal does not depend
_‚àºN_
on the discretization steps in Euler-Maruyama). The squared L2 norm of œµ divided by œÉ[2](t0) is a
_œá[2]-distribution with d-degrees of freedom. From Laurent & Massart (2000), Lemma 1, we have the_
following one-sided tail bound:

Pr(‚à•œµ‚à•2[2] _[/œÉ][2][(][t][0][)][ ‚â•]_ _[d][ + 2]_ _d ¬∑ ‚àí_ log Œ¥ ‚àí 2 log Œ¥) ‚â§ exp(log Œ¥) = Œ¥. (9)

Therefore, with probability at least (1 ‚àí _Œ¥p), we have that:_
2
**x[(][g][)](t0)** **x[(][g][)](0)** _d_ log Œ¥ 2 log Œ¥), (10)
_‚àí_ 2 _[‚â§]_ _[œÉ][2][(][t][0][)(][CœÉ][2][(][t][0][) +][ d][ + 2]_ _‚àí_ _¬∑_ _‚àí_

p

completing the proof.

B EXTRA ABLATION STUDIES

In this section, we perform extra ablation studies and analysis for SDEdit.

B.1 ANALYSIS ON THE QUALITY OF USER GUIDE

As discussed in Section 3, if the guide is far from any realistic images (e.g., random noise or has an
unreasonable composition), then we must tolerate at least a certain level of deviation from the guide
(non-faithfulness) in order to produce a realistic image.

For practical applications, we perform extra ablation studies on how the quality of guided stroke
would affect the results in Fig. 8, Fig. 9 and Table 4. Specifically, in Fig. 8 we consider stroke
input of 1) a human face with limited detail for a CelebA-HQ model, 2) a human face with spikes
for a CelebA-HQ model, 3) a building with limited detail for a LSUN-church model, 4) a horse
for a LSUN-church model. We observe that SDEdit is in general tolerant to different kinds of
user inputs. In Table 4, we quantitatively analyze the effect of user guide quality using simulated
stroke paintings as input. Described in Appendix D.2, the human-stroke-simulation algorithm uses


-----

different numbers of colors to generate stroke guides with different levels of detail. We compare
SDEdit with baselines qualitatively in Fig. 9 and quantitatively in Table 4. Similarly, we observe
that SDEdit has a high tolerance to input guides and consistently outperforms the baselines across
all setups in this experiment.

Input (guide)

Output

CelebA-HQ LSUN-Church


Figure 8: Analysis on the quality of user guide for stoke-based image synthesis. We observe that
SDEdit is in general tolerant to different kinds of user inputs.

# of colors = 3 # of colors = 6 # of colors = 16 # of colors = 30 # of colors = 50


Figure 9: Analysis on the quality of user guide for stoke-based image synthesis. We observe that
SDEdit is in general tolerant to different kinds of user inputs.


-----

StyleGAN2-ADA e4e SDEdit (Ours)
# of colors

KID _L2_ KID _L2_ KID _L2_
_‚Üì_ _‚Üì_ _‚Üì_ _‚Üì_ _‚Üì_ _‚Üì_

3 0.1588 67.22 0.0379 70.73 **0.0233** **36.00**
6 0.1544 72.41 0.0354 68.53 **0.0156** **37.67**
16 0.0923 69.52 0.0319 68.20 **0.0135** **37.70**
30 0.0911 67.11 0.0304 68.66 **0.0128** **37.42**
50 0.0922 65.28 0.0307 68.80 **0.0126** **37.40**

Table 4: We compare SDEdit with baselines quantitatively on LSUN-church dataset on strokebased generation. ‚Äú# of colors‚Äù denotes the number of colors used to generate the synthetic stroke
paintings, with fewer colors corresponding to a less accurate and less detailed input guide (see
Fig. 9). We observe that SDEdit consistently achieves more realistic and more faithful outputs and
outperforms the baselines across all setups.

B.2 FLEXIBLE IMAGE EDITING WITH SDEDIT

In this section, we perform extra image editing experiments including editing closing eyes Fig. 10,
opening mouth, and changing lip color Fig. 11. We observe that SDEdit can still achieve reasonable
editing results, which shows that SDEdit is capable of flexible image editing tasks.

Source Input (guide) Output

Figure 10: Flexible image editing on closing eyes with SDEdit.

**Changing lip color** **Opening mouth** **Smile with teeth**


Input (guide)

Output


Input (guide) Input (guide)

Output Output


Source


Figure 11: Flexible image editing on mouth with SDEdit.

B.3 ANALYSIS ON t0

In this section, we provide extra analysis on the effect of t0 (see Fig. 12). As illustrated in Fig. 3,
we can tune t0 to tradeoff between faithfulness and realism‚Äîwith a smaller t0 corresponding to a
more faithful but less realistic generated image. If we want to keep the brown stroke in Fig. 12, we
can reduce t0 to increase its faithfulness which could potentially decrease its realism. Additional
analysis can be found in Appendix D.2.


-----

t0=0 t0=0.1 t0=0.2 t0=0.3 t0=0.35 t0=0.4

Input (guide)

t0=0.45 t0=0.5 t0=0.6 t0=0.7 t0=0.8 t0=0.9

Figure 12: Extra analysis on t0. As t0 increases, the generated images become more realistic while
less faithful.

B.4 EXTRA COMPARISON WITH OTHER BASELINES


We perform extra comparison with SC-FEGAN (Jo & Park, 2019) in Fig. 13. We observe that
SDEdit is able to have more realistic results than SC-FEGAN (Jo & Park, 2019) when using the
same stroke input guide. We also present results for SC-FEGAN (Jo & Park, 2019) where we use
extra sketch together with stroke as the input guide (see Fig. 14). We observe that SDEdit is still
able to outperform SC-FEGAN in terms of realism even when SC-FEGAN is using both sketch and
stroke as the input guide.

Input (guide) SC-FEGAN SDEdit (ours) Input (guide) SC-FEGAN SDEdit (ours)

Figure 13: Comparison with SC-FEGAN (Jo & Park, 2019) on stroke-based image synthesis and
editing. We observe that SDEdit is able to generate more realistic results than SC-FEGAN.

Sketch Stroke Output

Input (guide)

Figure 14: Stroke-based editing for SC-FEGAN (Jo & Park, 2019) using both stroke and extra sketch
as the input guide. We observe that SDEdit still outperforms SC-FEGAN using only stroke as the
input guide.

B.5 COMPARISON WITH SONG ET AL. (2021)

Methods proposed by Song et al. (2021) introduce an extra noise-conditioned classifier for conditional generation and the performance of the classifier is critical to the conditional generation performance. Their settings are more similar to regular inverse problems where the measurement function
is known, which is discussed in Section 3. Since we do not have a known ‚Äúmeasurement‚Äù function


-----

for user-generated guides, their approach cannot be directly applied to user-guided image synthesis or editing in the form of manipulating pixel RGB values. To deal with this limitation, SDEdit
initializes the reverse SDE based on user input and modifies t0 accordingly‚Äîan approach different
from Song et al. (2021) (which always have the same initialization). This technique allows SDEdit
to achieve faithful and realistic image editing or generation results without extra task-specific model
learning (e.g., an additional classifier in Song et al. (2021)).

For practical applications, we compare with Song et al. (2021) on stroke-based image synthesis and
editing where we do not learn an extra noise-conditioned classifier (see Fig. 15). In fact, we are
also unable to learn the noise-conditioned classifier since we do not have a known ‚Äúmeasurement‚Äù
function for user-generated guides and we only have one random user input guide instead of a dataset
of input guide. We observe that this application of Song et al. (2021) fails to generate faithful results
by performing random inpainting (see Fig. 15). SDEdit, on the other hand, generates both realistic
and faithful images without learning extra task-specific models (e.g., an additional classifier) and can
be directly applied to pretrained SDE-based generative models, allowing for guided image synthesis
and editing using SDE-based models. We believe this shows the novelty and contribution of SDEdit.

Input (guide) (Song et. al.) SDEdit (ours) Input (guide) (Song et. al.) SDEdit (ours)

Figure 15: Comparison with Song et al. (2021) on stroke-based image synthesis and editing. We
observe that SDEdit is able to generate more faithful results than Song et al. (2021) without training
an extra task-specific model (e.g., an additional classifier).

C DETAILS ON SDEDIT

C.1 DETAILS ON THE VP AND VE SDES

We follow the definitions of VE and VP SDEs in Song et al. (2021), and adopt the same settings
therein.

**VE-SDE** In particular, for the VE SDE, we choose

0, _t = 0_

_œÉ(t) =_ (œÉmin _œÉœÉmaxmin_ _t,_ _t > 0_

where œÉmin = 0.01 and œÉmax = 380, 378, 348, 1348 for LSUN churches, bedroom, FFHQ/CelebA-
HQ 256 √ó 256, and FFHQ 1024 √ó 1024 datasets respectively.

**VP-SDE** For the VP SDE, it takes the form of

dx(t) = _Œ≤(t)dw(t),_ (11)

where Œ≤(t) is a positive function. In experiments, we follow ‚àí 2[1] _[Œ≤][(][t][)][x][(][t][)d][t][ +]_ p Song et al. (2021); Ho et al. (2020);
Dhariwal & Nichol (2021) and set
_Œ≤(t) = Œ≤min + t(Œ≤max ‚àí_ _Œ≤min),_
For SDE trained by Song et al. (2021); Ho et al. (2020) we use Œ≤min = 0.1 and Œ≤max = 20; for SDE
trained by Dhariwal & Nichol (2021), the model learns to rescale the variance based on the same
choices of Œ≤min and Œ≤max. We always have p1(x) ‚âàN (0, I) under these settings.

Solving the reverse VP SDE is similar to solving the reverse VE SDE. Specifically, we follow the
iteration rule below:


_Œ≤(tn)‚àÜt zn,_ (12)


**xn** 1 = (xn + Œ≤(tn)‚àÜtsŒ∏(x(tn), tn)) +
_‚àí_ 1 _Œ≤(tn)‚àÜt_

_‚àí_

where xN (0, I), zn (0, I) and n = N, N 1, _, 1._
_‚àºN_ p _‚àºN_ _‚àí_ _¬∑ ¬∑ ¬∑_


-----

C.2 DETAILS ON STOCHASTIC DIFFERENTIAL EDITING

In generation the process detailed in Algorithm 1 can also be repeated for K number of times as
detailed in Algorithm 2. Note that Algorithm 1 is a special case of Algorithm 2: when K = 1, we
recover Algorithm 1. For VE-SDE, Algorithm 2 converts a stroke painting to a photo-realistic image,
which typically modifies all pixels of the input. However, in cases such as image compositing and
stroke-based editing, certain regions of the input are already photo-realistic and therefore we hope to
leave these regions intact. To represent a specific region, we use a binary mask ‚Ñ¶ _‚àà{0, 1}[C][√ó][H][√ó][W]_
that evaluates to 1 for editable pixels and 0 otherwise. We can generalize Algorithm 2 to restrict
editing in the region defined by ‚Ñ¶.

For editable regions, we perturb the input image with the forward SDE and generate edits by reversing the SDE, using the same procedure in Algorithm 2. For uneditable regions, we perturb it as
usual but design the reverse procedure carefully so that it is guaranteed to recover the input. Specifically, suppose x ‚àà R[C][√ó][H][√ó][W] is an input image of height H, width W, and with C channels. Our
algorithm first perturbs x(0) = x with an SDE running from t = 0 till t = t0 to obtain x(t0).
Afterwards, we denoise x(t0) with separate methods for ‚Ñ¶ **x(t) and (1** **‚Ñ¶)** **x(t), where**
_‚äô_ _‚àí_ _‚äô_ _‚äô_
denotes the element-wise product and 0 _t_ _t0. For ‚Ñ¶_ **x(t), we simulate the reverse SDE (Song**
_‚â§_ _‚â§_ _‚äô_
et al., 2021) and project the results by element-wise multiplication with ‚Ñ¶. For (1 ‚àí **‚Ñ¶) ‚äô** **x(t), we**
set it to (1 ‚àí **‚Ñ¶) ‚äô** (x + œÉ(t)z), where z ‚àºN (0, I). Here we gradually reduce the noise magnitude
according to œÉ(t) to make sure ‚Ñ¶ _‚äô_ **x(t) and (1 ‚àí** **‚Ñ¶) ‚äô** **x(t) have comparable amount of noise.**
Moreover, since œÉ(t) ‚Üí 0 as t ‚Üí 0, this ensures that (1 ‚àí **‚Ñ¶) ‚äô** **x(t) converges to (1 ‚àí** **‚Ñ¶) ‚äô** **x,**
keeping the uneditable part of x intact. The complete SDEdit method (for VE-SDEs) is given in
Algorithm 3. We provide algorithm for VP-SDEs in Algorithm 4 and the corresponding masked
version in Algorithm 5.

With different inputs to Algorithm 3 or Algorithm 5, we can perform multiple image synthesis and
editing tasks with a single unified approach, including but not limited to the following:

-  Stroke-based image synthesis: We can recover Algorithm 2 or Algorithm 4 by setting all
entries in ‚Ñ¶ to 1.

-  Stroke-based image editing: Suppose x[(][g][)] is an image marked by strokes, and ‚Ñ¶ masks
the part that are not stroke pixels. We can reconcile the two parts of x[(][g][)] with Algorithm 3
to obtain a photo-realistic image.

-  Image compositing: Suppose x[(][g][)] is an image superimposed by elements from two images, and ‚Ñ¶ masks the region that the users do not want to perform editing, we can perform
image compositing with Algorithm 3 or Algorithm 5.

**Algorithm 2 Guided image synthesis and editing (VE-SDE)**

**Require: x[(][g][)]** (guide), t0 (SDE hyper-parameter), N (total denoising steps), K (total repeats)

‚àÜt _N_
_‚Üê_ _[t][0]_

**for k ‚Üê** 1 to K do

**z ‚àºN** (0, I)
**x** **x + œÉ(t0)z**
_‚Üê_
**for n ‚Üê** _N to 1 do_

_t_ _t0_ _N[n]_
_‚Üê_

**z ‚àºN** (0, I)
_œµ ‚Üê_ _œÉ[2](t) ‚àí_ _œÉ[2](t ‚àí_ ‚àÜt)

**x** **x + œµ[2]sŒ∏(x, t) + œµz**
_‚Üêp_

**end for**

**end for**
**Return x**


-----

**Algorithm 3 Guided image synthesis and editing with mask (VE-SDE)**

**Require: x[(][g][)]** (guide), ‚Ñ¶ (mask for edited regions), t0 (SDE hyper-parameter), N (total denoising
steps), K (total repeats)
‚àÜt _N_
_‚Üê_ _[t][0]_

**x0** **x**
**for ‚Üê k ‚Üê** 1 to K do

**z ‚àºN** (0, I)
**x ‚Üê** (1 ‚àí **‚Ñ¶) ‚äô** **x0 + ‚Ñ¶** _‚äô_ **x + œÉ(t0)z**
**for n ‚Üê** _N to 1 do_

_t_ _t0_ _N[n]_
_‚Üê_

**z ‚àºN** (0, I)
_œµ ‚Üê_ _œÉ[2](t) ‚àí_ _œÉ[2](t ‚àí_ ‚àÜt)

**x** (1 **‚Ñ¶)** (x0 + œÉ(t)z) + ‚Ñ¶ (x + œµ[2]sŒ∏(x, t) + œµz)
_‚Üêp_ _‚àí_ _‚äô_ _‚äô_

**end for**

**end for**
**Return x**


**Algorithm 4 Guided image synthesis and editing (VP-SDE)**

**Require: x[(][g][)]** (guide), t0 (SDE hyper-parameter), N (total denoising steps), K (total repeats)

‚àÜt _N_
_‚Üê_ _[t][0]_

_Œ±(t0) ‚Üê_ [Q]n[N]=1[(1][ ‚àí] _[Œ≤][(][ nt]N[0]_ [)‚àÜ][t][)]

**for k ‚Üê** 1 to K do

**z ‚àºN** (0, I)
**x** _Œ±(t0)x +_ 1 _Œ±(t0)z_
_‚Üê_ _‚àí_

**for n ‚Üêp** _N to 1 dop_

_t_ _t0_ _N[n]_
_‚Üê_

**zx ‚àºN** (0, I1 ) _Œ≤(t)‚àÜt z_
_‚Üê_ _‚àö1‚àíŒ≤(t)‚àÜt_ [(][x][ +][ Œ≤][(][t][)‚àÜ][t][s][Œ∏][(][x][, t][)) +]

**end for** p

**end for**
**Return x**


**Algorithm 5 Guided image synthesis and editing with mask (VP-SDE)**

**Require: x[(][g][)]** (guide), ‚Ñ¶ (mask for edited regions), t0 (SDE hyper-parameter), N (total denoising
steps), K (total repeats)
‚àÜt _N_
_‚Üê_ _[t][0]_

**x0** **x**
_‚Üê_
_Œ±(t0) ‚Üê_ [Q]i[N]=1[(1][ ‚àí] _[Œ≤][(][ it]N[0]_ [)‚àÜ][t][)]

**for k ‚Üê** 1 to K do

**z ‚àºN** (0, I)
**x ‚Üê** [(1 ‚àí ‚Ñ¶) ‚äô _Œ±(t0)x0 + ‚Ñ¶_ _‚äô_ _Œ±(t0)x +_ 1 ‚àí _Œ±(t0)z]_

**for n ‚Üê** _N to 1 dop_ p p

_t_ _t0_ _N[n]_
_‚Üê_

**z ‚àºN** (0, I)
_Œ±(t) ‚Üê_ [Q]i[n]=1[(1][ ‚àí] _[Œ≤][(][ it]N[0]_ [)‚àÜ][t][)]

**x** (1 ‚Ñ¶) ( _Œ±(t)x0 +_ 1 _Œ±(t)z) + ‚Ñ¶_ 1
_‚Üê_ _‚àí_ _‚äô_ _‚àí_ _‚äô_ _‚àö1‚àíŒ≤(t)‚àÜt_ [(][x][ +][ Œ≤][(][t][)‚àÜ][t][s][Œ∏][(][x][, t][)) +]

_Œ≤(t)‚àÜtn z)_ p p h

pend for io

**end for**
**Return x**


-----

D EXPERIMENTAL SETTINGS

D.1 IMPLEMENTATION DETAILS

Below, we add additional implementation details for each application. We use publicly available
pretrained SDE checkpoints provided by Song et al.; Ho et al.; Dhariwal & Nichol. Our code will
be publicly available upon publication.

**Stroke-based image synthesis.** In this experiment, we use K = 1, N = 500, t0 = 0.5, for SDEdit
(VP). We find that K = 1 to 3 work reasonably well, with larger K generating more realistic images
but at a higher computational cost.

For StyleGAN2-ADA, in-domain GAN and e4e, we use the official implementation with default
parameters to project each input image into the latent space, and subsequently use the obtained
latent code to produce stroke-based image samples.

**Stroke-based image editing.** We use K = 1 in the experiment for SDEdit (VP). We use t0 = 0.5,
_N = 500 for SDEdit (VP), and t0 = 0.45, N = 1000 for SDEdit (VE)._

**Image compositing.** We use CelebA-HQ (256√ó256) (Karras et al., 2017) for image compositing
experiments. More specifically, given an image from CelebA-HQ, the user will copy pixel patches
from other reference images, and also specify the pixels they want to perform modifications, which
will be used as the mask in Algorithm 3. In general, the masks are simply the pixels the users have
copied pixel patches to. We focus on editing hairstyles and adding glasses. We use an SDEdit model
pretrained on FFHQ (Karras et al., 2019). We use t0 = 0.35, N = 700, K = 1 for SDEdit (VE).
We present more results in Appendix E.2.

D.2 SYNTHESIZING STROKE PAINTING

**Human-stroke-simulation algorithm** We design a human-stroke-simulation algorithm in order
to perform large scale quantitative analysis on stroke-based generation. Given a 256√ó256 image,
we first apply a median filter with kernel size 23 to the image, then reduce the number of colors to 6
using the adaptive palette. We use this algorithm on the validation set of LSUN bedroom and LSUN
church outdoor, and subset of randomly selected 6000 images in the CelebA (256√ó256) test set to
produce the stroke painting inputs for Fig. 3a, Table 2 and Table 5. Additionally Fig. 30, Fig. 31 and
Fig. 32 show examples of the ground truth images, synthetic stroke paintings, and the corresponding
generated images by SDEdit. The simulated stroke paintings resemble the ones drawn by humans
and SDEdit is able to generate high quality images based on this synthetic input, while the baselines
fail to obtain comparable results.

**KID evaluation** KID is calculated between the real image from the validation set and the generated images using synthetic stroke paintings (based on the validation set), and the squared L2
distance is calculated between the simulated stroke paintings and the generated images.

**Realism-faithfulness trade-off** To search for the sweet spot for realism-faithfulness trade-off as
presented in Figure 3a, we select 0.01 and every 0.1 interval from 0.1 to 1 for t0 and generate
images for the LSUN church outdoor dataset. We apply the human-stroke-simulation algorithm
on the original LSUN church outdoor validation set and generate one stroke painting per image to
produce the same input stroke paintings for all choices of t0. As shown in Figure 33, this algorithm
is sufficient to simulate human stroke painting and we can also observe the realism-faithfulness
trade-off given the same stroke input. KID is calculated between the real image from the validation
set and the generated images, and the squared L2 distance is calculated between the simulated stroke
paintings and the generated images.

D.3 TRAINING AND INFERENCE TIME

We use open source pretrained SDE models provided by Song et al.; Ho et al.; Dhariwal & Nichol. In
general, VP and VE have comparable speeds, and can be slower than encoder-based GAN inversion


-----

methods. For scribble-based generation on 256√ó256 images, SDEdit takes 29.1s to generate one
image on one 2080Ti GPU. In comparison, StyleGAN2-ADA (Karras et al., 2020a) takes around
72.8s and In-domain GAN 2 (Zhu et al., 2020a) takes 5.2s using the same device and setting. We
note that our speed is in general faster than optimization-based GAN inversions while slower than
encoder-based GAN inversions. The speed of SDEdit could be improved by recent works on faster
SDE sampling.

E EXTRA EXPERIMENTAL RESULTS

E.1 EXTRA RESULTS ON LSUN DATASETS

**Stroke-based image generation.** We present more SDEdit (VP) results on LSUN bedroom in
Fig. 21. We use t0 = 0.5, N = 500, and K = 1. We observe that, SDEdit is able to generate
realistic images that share the same structure as the input paintings when no paired data is provided.

**Stroke-based image editing.** We present more SDEdit (VP) results on LSUN bedroom in Fig. 22.
SDEdit generates image edits that are both realistic and faithful to the user edit, while avoids making
undesired modifications on pixels not specified by users. See Appendix D for experimental settings.

E.2 EXTRA RESULTS ON FACE DATASETS

**Stroke-based image editing.** We provide intermediate step visualizations for SDEdit in Fig. 23.
We present extra SDEdit results on CelebA-HQ in Fig. 24. We also presents results on CelebA-HQ
(1024√ó1024) in Fig. 29. SDEdit generates images that are both realistic and faithful (to the user
edit), while avoids introducing undesired modifications on pixels not specified by users. We provide
experiment settings in Appendix D.

**Image compositing.** We focus on editing hair styles and adding glasses. We present more SDEdit
(VE) results on CelebA-HQ (256√ó256) in Fig. 25, Fig. 26, and Fig. 27. We also presents results
on CelebA-HQ (1024√ó1024) in Fig. 28. We observe that SDEdit can generate both faithful and
realistic edited images. See Appendix D for experiment settings.

**Attribute classification with stroke-based generation.** In order to further evaluate how the models convey user intents with high level user guide, we perform attribute classification on stroke-based
generation for human faces. We use the human-stroke-simulation algorithm on a subset of randomly
selected 6000 images from CelebA (256√ó256) test set to create the stroke inputs, and apply Microsoft Azure Face API[4] to detect fine-grained face attributes from the generated images. We choose
gender and glasses to conduct binary classification, and hair color to perform multi-class classification on the images. Images where no face is detected will be counted as providing false and to the
classification problems. Table 5 shows the classification accuracy, and SDEdit (VP) outperforms all
other baselines in all attributes of choice.

E.3 CLASS-CONDITIONAL GENERATION WITH STROKE PAINTING

In addition to user guide, SDEdit is able to also leverage other auxiliary information and models
to obtain further control of the generation. Following Song et al. (2021) and Dhariwal & Nichol
(2021), we present an extra experiment on class-conditional generation with SDEdit. Given a timedependent classifier pt(y **x), for SDEdit (VE) one can solve the reverse SDE:**
_|_


dx(t) = ( **x log pt(x) +** **x log pt(y** **x))** dt +
_‚àí_ [d[][œÉ]d[2]t[(][t][)]] _‚àá_ _‚àá_ _|_
 

and use the same sampling procedure defined in Section 3.


d[œÉ[2](t)]

d ¬Øw (13)
dt


[4https://github.com/Azure-Samples/cognitive-services-quickstart-code/](https://github.com/Azure-Samples/cognitive-services-quickstart-code/tree/master/python/Face)
[tree/master/python/Face](https://github.com/Azure-Samples/cognitive-services-quickstart-code/tree/master/python/Face)


-----

(a) Dataset image (b) User guide (c) GAN output (d) GAN blending

Figure 16: Post-processing samples from GANs by masking out undesired changes, yet the artifacts
are strong at the boundaries even with blending.

Methods Gender Glasses Hair - Blond Hair - Black Hair - Grey

In-domain GAN 1 0.5232 0.6355 0.5527 0.5722 0.5398
In-domain GAN 2 0.0202 0.0273 0.1806 0.3158 0.0253
StyleGAN2-ADA 0.0127 0.0153 0.1720 0.3105 0.0145
e4e 0.6175 0.6623 0.6731 0.6510 0.7233
SDEdit (ours) **0.8147** **0.9232** **0.8487** **0.7490** **0.8928**

Table 5: Attribute classification results with simulated stroke inputs on CelebA. SDEdit (VP) outperforms all baseline methods in all attribute selected in the experiment. Details can be found in
Appendix E.2.

For SDEdit (VP), we follow the class guidance setting in Dhariwal & Nichol (2021) and solve:


**xn** 1 =
_‚àí_


1 _Œ≤(tn)‚àÜt_ (xn+Œ≤(tn)‚àÜtsŒ∏(x(tn), tn))+Œ≤(tn)‚àÜt‚àáx log pt(y | xn)+
_‚àí_


_Œ≤(tn)‚àÜt zn,_

(14)


Fig. 34 shows the ImageNet (256√ó256) class-conditional generation results using SDEdit (VP).
Given the same stroke inputs, SDEdit is capable of generating diverse results that are consistent
with the input class labels.

E.4 EXTRA DATASETS

We present additional stroke-based image synthesis results on LSUN cat and horse dataset for
SDEdit (VP). Fig. 35 presents the image generation results based on input stroke paintings with
various levels of details. We can observe that SDEdit produce images that are both realistic and
faithful to the stroke input on both datasets. Notice that for coarser guide (e.g. the third row in
Fig. 35), we choose to slightly sacrifice faithfulness in order to obtain more realistic images by
selecting a larger t0 = 0.6, while all the other images in Fig. 35 are generated with t0 = 0.5.

E.5 EXTRA RESULTS ON BASELINES

SDEdit preserves the un-masked regions automatically, while GANs do not. We tried postprocessing samples from GANs by masking out undesired changes, yet the artifacts are strong at
the boundaries. We further tried blending on GANs (GAN blending) with StyleGAN2-ADA, but the
artifacts are still distinguishable (see Fig. 16).

F HUMAN EVALUATION

F.1 STROKE-BASED IMAGE GENERATION

Specifically, we synthesize a total of 400 bedroom images from stroke paintings for each method. To
quantify sample quality, we ask the workers to perform a total of 1500 pairwise comparisons against
SDEdit to determine which image sample looks more realistic. Each evaluation HIT contains 15
pairwise comparisons against SDEdit, and we perform 100 such evaluation tasks. The reward per
task is kept as 0.2$. Since each task takes around 1 min, the wage is around 12$ per hour. For each
question, the workers will be shown two images: one generated image from SDEdit and the other


-----

Figure 17: The instruction shown to MTurk workers for pairwise comparison.

Figure 18: The UI shown to MTurk workers for pairwise comparison.

from the baseline model using the same input. The instruction is: ‚ÄúWhich image do you think is
**more realistic‚Äù (see Fig. 17 and Fig. 18).**

To quantify user satisfactory score (faithfulness+realism), we ask a different set of workers to perform another 3000 pairwise comparisons against SDEdit. For each question, the workers will be
shown three images: the input stroke painting (guide), one generated image from SDEdit based on
the stroke input, and the other from the baseline model using the same input. Each evaluation HIT
contains 15 pairwise comparisons against SDEdit, and we perform 200 such evaluation tasks. The
reward per task is kept as 0.2$. Since each task takes around 1 min, the wage is around 12$ per
hour. The instruction is: ‚ÄúGiven the input painting, how would you imagine this image to look like
in reality? Choose the image that looks more reasonable to you. Your selection should based on
how realistic and less blurry the image is, and whether it shares similarities with the input‚Äù (see
Fig. 19 and Fig. 20).

Figure 19: The instruction shown to MTurk workers for pairwise comparison.


-----

Figure 20: The UI shown to MTurk workers for pairwise comparison.

F.2 IMAGE COMPOSITING ON CELEBA-HQ

To quantitatively evaluate our results, we generate 936 images based on the user inputs. To quantify
realism, we ask MTurk workers to perform 1500 pairwise comparisons against SDEdit pre-trained
on FFHQ (Karras et al., 2019) to determine which image sample looks more realistic. Each evaluation HIT contains 15 pairwise comparisons against SDEdit, and we perform 100 such evaluation
tasks. The reward per task is kept as 0.2$. Since each task takes around 1 min, the wage is around
12$ per hour. For each question, the workers will be shown two images: one generated image from
SDEdit and the other from the baseline model using the same input. The instruction is: ‚ÄúWhich
image do you think was more realistic?‚Äù.

To quantify user satisfactory score (faithfulness + realism), we ask different workers to perform
another 1500 pairwise comparisons against SDEdit pre-trained on FFHQ to decide which generated
image matches the content of the inputs more faithfully. Each evaluation HIT contains 15 pairwise
comparisons against SDEdit, and we perform 100 such evaluation tasks. The reward per task is kept
as 0.2$. Since each task takes around 1 min, the wage is around 12$ per hour. For each question,
the workers will be shown two images: one generated image from SDEdit and the other from the
baseline model using the same input. The instruction is: ‚ÄúWhich is a better polished image for the
input? An ideal polished image should look realistic, and matches the input in visual appearance
(e.g., they look like the same person, with matched hairstyles and similar glasses)‚Äù.


-----

Painting Diverse outputs


Figure 21: Stroke-based image generation on bedroom images with SDEdit (VP) pretrained on
LSUN bedroom.


-----

Original

Input

Edited

Figure 22: Stroke-based image editing on bedroom images with SDEdit (VP) pretrained on LSUN
bedroom. SDEdit generates image edits that are both realistic and faithful (to the user edit), while
avoids making undesired modifications on pixels not specified by users

Add Perturb

scribble with SDE

Original Edited Input n=500

(a) User edit image (b) Perturb the image with SDE

n=500 n=400 n=300 n=200 n=100 n=0

(c) Reverse SDE process


Figure 23: Stroke-based image editing. (a) Given an image, users will first modify the image using
stroke, and provide a mask which describes the pixels covered by stroke. (b) The edited image will
then be fed into SDEdit. SDEdit will first perturb the image with an SDE, and then simulate the
reverse SDE (see Algorithm 5). (c) We provide visualization of the intermediate steps of reversing
SDE used in SDEdit.


-----

Original

Input

Edited

Figure 24: Stroke-based image editing on CelebA-HQ images with SDEdit. SDEdit generates image
edits that are both realistic and faithful (to the user edit), while avoids making undesired modifications on pixels not specified by users.

Original

Input

Edited

Figure 25: Image compositing on CelebA-HQ images with SDEdit. We edit the images to have
brown hair. The model is pretrained on FFHQ.


-----

Original

Input

Edited

Figure 26: Image compositing on CelebA-HQ images with SDEdit. We edit the images to wear
glasses. The model is pretrained on FFHQ.

Original

Input


Edited

Figure 27: Image compositing on CelebA-HQ images with SDEdit. We edit the images to have
blond hair. The model is pretrained on FFHQ.


-----

(a) Original dataset image. (b) User edited input. (c) SDEdit results.

Figure 28: Image compositing results with SDEdit (VE) on CelebA-HQ (resolution 1024√ó1024).
The SDE model is pretrained on FFHQ.

(a) Original dataset image. (b) User edited input. (c) SDEdit results.

Figure 29: Stroke-based image editing results with SDEdit (VE) on CelebA-HQ (resolution
1024√ó1024). The SDE model is pretrained on FFHQ.

Ground truth Simulated stroke StyleGAN2-ADA In-domain 1 In-domain 2 e4e SDEdit

Figure 30: Stroke-based image generation with simulated stroke paintings inputs on bedroom images
with SDEdit (VP) pretrained on LSUN bedroom dataset.


-----

Ground truth Simulated stroke StyleGAN2-ADA e4e SDEdit

Figure 31: Stroke-based image generation with simulated stroke paintings inputs on church images
with SDEdit (VP) pretrained on LSUN church outdoor dataset.

Ground truth Simulated stroke StyleGAN2-ADA In-domain 1 In-domain 2 e4e SDEdit


Figure 32: Stroke-based image generation with simulated stroke paintings inputs on human face
images with SDEdit (VP) pretrained on CelebA dataset.

Stroke painting ùë°! = 0.01 ùë°! = 0.1 ùë°! = 0.2 ùë°! = 0.3 ùë°! = 0.4

|Stroke painting|ùë° = 0.01 !|
|---|---|
|||



ùë°! = 0.5 ùë°! = 0.6 ùë°! = 0.7 ùë°! = 0.8 ùë°! = 0.9 ùë°! = 1.0

Figure 33: Trade-off between faithfulness and realism shown with stroke-based image generation
with simulated stroke painting inputs on church images with SDEdit (VP) pretrained on LSUN
church outdoor dataset.


-----

Painting Tabby Jaguar Lion Tiger


Painting


Painting German shepherd Boxer Red fox Coyote

Figure 34: Class-conditional image generation from stroke paintings with different class labels by
SDEdit (VP) pretrained on ImageNet.


Painting Output

Cat

Horse

Figure 35: Stroke-based image generation with stroke inputs on cat and horse images with SDEdit
(VP) pretrained on LSUN cat and horse dataset. Notice that for coarser guide (e.g. the third row),
we choose to slightly sacrifice faithfulness in order to obtain more realistic images by selecting a
larger t0 = 0.6, while all the other images are generated with t0 = 0.5.


-----

