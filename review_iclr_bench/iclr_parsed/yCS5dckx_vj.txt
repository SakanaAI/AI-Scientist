# TOWARDS DEMYSTIFYING REPRESENTATION LEARN## ING WITH NON-CONTRASTIVE SELF-SUPERVISION

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Non-contrastive methods of self-supervised learning (such as BYOL and SimSiam) learn representations by minimizing the distance between two views of the
same image. These approaches have achieved remarkable performance in practice, but it is not well understood how the representation is learned based on the
augmentation process. Tian et al. (2021) explained why the representation does
not collapse to zero and proposed DirectPred that sets the predictor directly. In
our work, we analyze a generalized version of DirectPred, called DirectSet(Î±).
We show that in a simple linear network, DirectSet(Î±) provably learns a desirable
projection matrix and also reduces the sample complexity on downstream tasks.
Our analysis suggests that weight decay acts as an implicit threshold that discard
the features with high variance under augmentation, and keep the features with
low variance. Inspired by our theory, we simplify DirectPred by removing the expensive eigen-decomposition step. On CIFAR-10, CIFAR-100, STL-10 and ImageNet, DirectCopy, our simpler and more computationally efficient algorithm,
rivals or even outperforms DirectPred.

1 INTRODUCTION

Self-supervised learning recently emerges as a promising direction to learn representations without
manual labels. While contrastive learning (Oord et al., 2018; Tian et al., 2019; Bachman et al.,
2019; He et al., 2020; Chen et al., 2020a) minimizes the distance of representation between positive pairs, and maximizes such distances between negative pairs, recently, non-contrastive selfsupervised learning (abbreviated as nc-SSL) is able to learn nontrivial representation with only
positive pairs, using an extra predictor and a stop-gradient operation. Furthermore, the learned
representation shows comparable (or even better) performance for downstream tasks (e.g., image
classification) (Grill et al., 2020; Chen & He, 2020). This brings about two fundamental questions: (1) why the learned representation does not collapse to trivial (i.e., constant) solutions, and
(2) without negative pairs, what representation nc-SSL learns from the training and how the learned
representation reduces the sample complexity in downstream tasks.

While many theoretical results on contrastive SSL (Arora et al., 2019; Lee et al., 2020; Tosh et al.,
2020; Wen & Li, 2021) do exist, similar study on nc-SSL has been very rare. As one of the first
work towards this direction, Tian et al. (2021) show that while the global optimum of the noncontrastive loss is indeed a trivial one, following gradient direction in nc-SSL, one can find a local
optimum that admits a nontrivial representation. Based on their theoretical findings on gradientbased methods, they proposed a new approach, DirectPred, that directly sets the predictor using the
eigen-decomposition of the correlation matrix of input before the predictor, rather than updating it
with gradient methods. As a method for nc-SSL, DirectPred shows comparable or better performance in multiple datasets, including CIFAR-10 (Krizhevsky et al., 2009), STL-10 (Coates et al.,
2011) and ImageNet (Deng et al., 2009), compared to BYOL (Grill et al., 2020) and SimSiam (Chen
& He, 2020) that optimize the predictor using gradient descent.

While Tian et al. (2021) address the first question, i.e., why the learned representation does not
collapse, they do not address the second question, i.e., what representation is learned in nc-SSL and
how the learned representation is related to the data distribution and augmentation process and in
turn whether it reduces the sample complexity in downstream tasks.


-----

**Main Contributions.** In this paper, we make a first attempt towards the second question, by studying a family of algorithms named DirectSet(Î±), in which the DirectPred algorithm proposed by Tian
et al. (2021) is a special case with Î± = 1/2. Our contribution is two-folds:

First, we perform a theoretical analysis on DirectSet(Î±) with linear networks. Our analysis shows
that there exists an implicit threshold, determined by weight decay parameter Î·, that governs which
features are learned and which are discarded. More specifically, the threshold is applied to the variance of the feature across different data augmentations (or â€œviewsâ€) of the same instance: nuisance
_features (features with high variances under augmentation) are discarded, while invariant features_
(i.e., with low variances) are kept. We further make a formal statement on the sample complexity
of the learning process and performance guarantees of the downstream tasks, in the linear setting
similar to Tian et al. (2021). To our knowledge, this is the first sample complexity result in nc-SSL.

Second, we show that DirectCopy, a special case of DirectSet(Î±) when Î± = 1, performs comparably
with (or even outperforms) DirectPred in downstream tasks in CIFAR-10, CIFAR-100, STL-10
and ImageNet. In DirectCopy, the predictor can be set without the expensive eigen-decomposition
operation, which makes DirectCopy much simpler and more efficient than DirectPred.

**Related works. In nc-SSL, different techniques are proposed to avoid collapsing. BYOL and Sim-**
Siam use an extra predictor and stop gradient operation. Beyond these, BatchNorm (including its
variants (Richemond et al., 2020)), de-correlation (Zbontar et al., 2021; Bardes et al., 2021; Hua
et al., 2021), whitening (Ermolov et al., 2021), centering (Caron et al., 2021), and online clustering (Caron et al., 2020) are all effective ways to enforce implicit contrastive constraints among
samples for collapsing prevention. We study BYOL and SimSiam as representative nc-SSL methods.

**Organization. The paper is organized as follows. Section 2-3 introduce DirectSet(Î±), prove it**
learns a projection matrix onto the invariant features, and the learned representation reduces sample
complexity in downstream tasks. Section 4 demonstrates that DirectCopy achieves comparable or
even better performance than the original DirectPred algorithm in various datasets, and Section 5
shows ablation experiments. Finally, limitation and future works are discussed in Section 6-7.

2 PRELIMINARIES

2.1 NOTATIONS
We use Id to denote the dÃ—d identity matrix and simply write I when the dimension is clear. For any
linear subspacethe projection matrix S in R P[d]S, we use equals UU PS[âŠ¤] âˆˆ, where the columns ofR[d][Ã—][d] to denote the projection matrix on U constitute a set of orthonormal bases S. More precisely,
for subspace S. We use N (Âµ, Î£) to denote the Gaussian distribution with mean Âµ and covariance Î£.

We use âˆ¥Â·âˆ¥ to denote spectral norm for a matrix, or â„“2 norm for a vector and use âˆ¥Â·âˆ¥F to denote
Frobenius norm for a matrix. For a real symmetric matrixd _A âˆˆ_ R[d][Ã—][d] whose eigen-decomposition is
_i=1_ _[Î»][i][u][i][u]i[âŠ¤][,][ we use][ |][A][|][ to denote][ P]i[d]=1_ _[|][Î»][i][|][ u][i][u]i[âŠ¤][. If][ A][ is also positive semi-definite, we use][ A][Î±]_

to denoteP _i=1_ _[Î»]i[Î±][u][i][u]i[âŠ¤]_ [for any positive][ Î±][ âˆˆ] [R][.]

2.2 DIRECT[P][d] SET(Î±) AND DIRECTCOPY

In nc-SSL, recent methods as BYOL (Grill et al., 2020) and SimSiam (Chen & He, 2020) employ a
dual pair of Siamese networks (Bromley et al., 1994): one side is a composition of an online network
(including a projector) and a predictor network, the other side is a target network (see Figure 1 for
a simple example). The target network has the same architecture as the online network, but has
potentially different weights. Given an input x, two augmented views x1, x2 are generated, and the
network is trained to match the representation of x1 (through the online network and the predictor
network) and the representation of x2 (through the target network). More precisely, suppose the
online network and the target network are two mappings fÎ¸, fÎ¸a : R[d] _7â†’_ R[h] and the predictor
network is a mapping gÎ¸p : R[h] _7â†’_ R[h], the network is trained to minimize the following loss:

2

_gÎ¸p (fÎ¸(x1))_ _fÎ¸a_ (x2)

_L(Î¸, Î¸p, Î¸a) := [1]_ _._

2 [E][x][1][,x][2] _gÎ¸p (fÎ¸(x1))_ _[âˆ’]_ [StopGrad]  _âˆ¥fÎ¸a_ (x2)âˆ¥ 

In BYOL and SimSiam, the online network and the target network are trained by running gradient
methods on L. The target network is not trained by gradient methods; instead, it is directly set with


-----

BYOL: â„“% loss
Gradient methods ğ‘”!"(ğ‘“! [ğ‘¥]" [)]

Stop-Gradient

Predictor

DirectSet(ğ›¼):

ğ‘Š" [(ğœƒ]"[)]

ğ¹[%]
ğ‘Š" [=]

âˆ¥ğ¹[%] âˆ¥ [+ ğœ–ğ¼,] ğ‘“![(ğ‘¥]"[)] ğ‘“!![(ğ‘¥]#[)]

with ğ¹= ğ”¼&!ğ‘“' [ğ‘¥]# [ğ‘“]' [ğ‘¥]# [(]

Online Target

Gradient EMA of ğ‘Š
methods ğ‘Š(ğœƒ) ğ‘Š! [(ğœƒ]![)]


ğ‘¥# Augmentation ğ‘¥$

ğ‘¥

Figure 1: Problem Setup. Comparison between BYOL and DirectSet(Î±) on a linear network.

the weights in the online network (Chen & He, 2020) or an exponential moving average (EMA) of
the online network (Grill et al., 2020; He et al., 2020; Chen et al., 2020b).

The DirectSet(Î±) algorithm, as shown in Figure 1, directly sets the predictor based on the correlation
matrix F of the predictor inputs:

_F_ _[Î±]_
_Wp =_

_âˆ¥F_ _[Î±]âˆ¥_ [+][ ÏµI,]

where F = Ex1 _fÎ¸(x1)fÎ¸(x1)[âŠ¤]. In practice, F is estimated by a moving average over batches. That_
is, _F[Ë†] = ÂµF[Ë†] + (1 âˆ’_ _Âµ)EB[fÎ¸(x1)fÎ¸(x1)[âŠ¤]], where EB is the expectation over one batch._

In the original DirectPred proposed by Tian et al. (2021), Î± is fixed at 1/2. To compute _F[Ë†][1][/][2], one_
needs to first compute the eigen-decomposition of _F[Ë†], and then taking the root of each eigenvalue._
This step of eigen-decomposition can be expensive especially when the representation dimension
_h is high. To avoid the eigen-decomposition step, we propose DirectCopy (Î± = 1), in which the_
predictor Wp is a direct copy of the _F[Ë†] (with normalization and regularization)[1]. As we shall see,_
DirectCopy enjoys both theoretical guarantees and strong empirical performance.

3 THEORETICAL ANALYSIS OF DIRECTSET(Î±)

Deep linear networks have been widely used as a tractable theoretical model for studying nonconvex
loss landscapes (Kawaguchi, 2016; Du & Hu, 2019; Laurent & Brecht, 2018) and nonlinear learning
dynamics (Saxe et al., 2013; 2019; Lampinen & Ganguli, 2018; Arora et al., 2018). However, most
of them are for supervised learning setting. Tian et al. (2021) analyzed nc-SSL on a linear network,
but did not analyze their proposed approach DirectPred. Here, we analyze the representation learning process of DirectSet(Î±) on a minimal setting where the online network fÎ¸ is a single linear layer.
We also verify DirectSet(Î±) works for practical nonlinear deep models and realistic datasets.

3.1 SETUP

In this subsection, we define the network model, data distribution and simplify DirectSet(Î±) algorithm for our theoretical analysis. We consider the following network model (see Figure 1),
**Assumption 1 (Linear network model). The online, predictor and target network are all single-layer**
_linear network without bias, with weight matrices denoted as W, Wp, Wa âˆˆ_ R[d][Ã—][d] _respectively._

For the data distribution, we assume the input space is a direct sum of a invariant feature subspace
and a nuisance feature subspace. Specifically, we assume
**Assumption 2 (Data distribution). The input x is sampled from N** (0, Id), and its augmented view
_x1, x2 are independently sampled from N_ (x, Ïƒ[2]PB), where B is a (d âˆ’ _r)-dimensional subspace._
_We denote S as the orthogonal subspace of B in R[d]._

1Computing the spectral norm of Ë†F is much faster than computing the eigen-decomposition of _F[Ë†], because_
the former only needs the top eigen-vector of _F[Ë†]. Table 4 shows that the spectral norm can also be replaced by_
Frobenius norm or no normalization, and similar performance can be achieved.


-----

In this simple data distribution, subspace S corresponds to the features that are invariant to augmentations and its orthogonal subspace B is the nuisance subspace which the augmentation changes. We
will prove that DirectSet(Î±) can learn the projection matrix onto S subspace. Note in the previous
work (Tian et al., 2021), they assumed the covariance of the augmentation distribution to be Ïƒ[2]I and
did not study what representation is learned.

For the convenience of analysis, we consider a simplified version of DirectSet(Î±). We compute the
loss function without normalizing the two representations, so the population loss is

_L(W, Wa, Wp) := [1]_ (1)

2 [E][x][1][,x][2][ âˆ¥][W][p][Wx][1][ âˆ’] [StopGrad][ (][W][a][x][2][)][âˆ¥][2][,]

and the empirical loss is

_n_

2

_LË†(W, Wp, Wa) := [1]_ _WpWx[(]1[i][)]_ 2 [)] _,_ (2)

2n _i=1_ _[âˆ’]_ [StopGrad][(][W][a][x][(][i][)]

X

where x[(][i][)]â€™s are independently sampled from (0, I), and augmented views x[(]1[i][)] and x[(]2[i][)] are inde_N_
pendently sampled from (x[(][i][)], Ïƒ[2]PB). To train our model, first, we initialize W as Î´I with Î´ a
_N_
positive real number. We run gradient flow or gradient descent on online network W with weight
decay Î·, and set the the target network Wa = W. For clarity of presentation, when training on the
practice; when training on the empirical loss, we setpopulation loss, we set Wp as (W Exxx[âŠ¤]W _[âŠ¤])[Î±]_ = ( WWWp as[âŠ¤] (W)[Î±] _n[1]instead ofni=1_ _[x][(] ([i][)]W[[][x][(]E[i][)]x[]]1[âŠ¤]x[W]1x[ âŠ¤][âŠ¤]1[)][W][Î±][.][ âŠ¤][ Here, we][)][Î±][ as in]_

set the predictor regularization Ïµ = 0 and its influence will be studied in Section 5.
P

In the following, DirectSet(Î±) is shown to recover the projection matrix PS with polynomial number
of samples. Furthermore, given that the learned matrix is close to PS, the sample complexity on
downstream tasks is reduced.

3.2 GRADIENT FLOW ON POPULATION LOSS


In this section, we show that DirectSet(Î±) running on the population loss with infinitesimal learning
rate and Î· weight decay can learn the projection matrix onto the invariant feature subspace S.
**Theorem 1. Suppose network architecture and data distribution follow Assumption 1 and Assump-**
_tion 2, respectively. Suppose we initialize online network W as Î´I, and run DirectSet(Î±) on popu-_
_lation loss (see Eqn. 1) with infinitesimal step size and Î· weight decay. If we set the weight decay_

1/(2Î±)

_coefficient Î·_ 4(1+1Ïƒ[2]) _[,][ 1]4_ _and initialization scale Î´ >_ 1âˆ’[âˆš]21âˆ’4Î· _, then W converges to_
_âˆˆ_

1/(2Î±)

1+[âˆš]21âˆ’4Î·  _PS when time goes to infinity._  
 

Theorem 1 shows that when the weight decay is in certain range, and when the initialization is
large enough, the online network can converge to the desired projection matrix PS [2]. In sequel,
we explain how the dynamics of W leads to a projection matrix and how the weight decay and
initialization scale come into play. We leave the full proof in Appendix B.1. We also consider the
setting when Wp is set as (W Ex1 _x1x[âŠ¤]1_ _[W][ âŠ¤][)][Î±][ in Appendix B.4 and extend the result to deep linear]_
networks in Appendix C.

Due to the identity initialization, we can ensure that W is always a real symmetric matrix and is
simultaneously diagonalizable with PB. We can then analyze the evolution of each eigenvalue in W
separately. Under our assumptions, it turns out that all the eigenvalues whose eigenvectors lie in the
_B subspace share the same value Î»B, and all the eigenvalues in the S subspace share the value Î»S_
as shown in the following time dynamics:

_Î»Ë™_ _B = Î»B_ _âˆ’(1 + Ïƒ[2]) |Î»B|[4][Î±]_ + |Î»B|[2][Î±] _âˆ’_ _Î·_ _,_ _Î»Ë™_ _S = Î»S_ _âˆ’|Î»S|[4][Î±]_ + |Î»S|[2][Î±] _âˆ’_ _Î·_ _._ (3)
h i h i

Next, we show Î»B converges to zero and Î»S converges to a positive number, which immediately
implies that W converges to some scaling of PS.

1/(2Î±)

2Note that Theorem 1 also holds with negative initialization Î´ < 1âˆ’[âˆš]21âˆ’4Î· _, in which case W_
_âˆ’_

1/(2Î±)

1+[âˆš]1 4Î·  
converges to 2 _âˆ’_ _PS. Our other results can be extended to negative Î´ in a similar way._
_âˆ’_
 


-----

|O|Col2|ğœ† "# ğœ† "$ ğœ†"|
|---|---|---|


evolvement of the eigenvalues of F when itâ€™s trained by DirectCopy with Ïµ = 0.2 on STL-10. With weight

ğœ†!

O ğœ†!

ğœ†"

Bad Good
Basin Basin

O ğœ†"# ğœ†"$ ğœ†"

Figure 2: Left: With appropriate weight decay, Î»B always converge to zero; Î»S converges to zero when itâ€™s
initialized in the bad basin and converges to positive Î»[+]S [when itâ€™s initialized in the good basin.][ Middle:][ The]

decay Î· = 0.0004 (bottom), the eigen-spectrum at epoch 95 has sharp drop; while the drop is much milder
when Î· = 0 (top). Right: Similar phenomenon on CIFAR-10 with Ïµ = 0.3.

Similar as the analysis in Tian et al. (2021), when Î· > 4(1+1Ïƒ[2]) _[,][ we know][ Ë™]Î»B < 0 for any Î»B > 0_

and Î»B = 0 is a stable stationary point, as illustrated in Figure 2 (top, left). Therefore, as long as
_Î· >_ 4(1+1Ïƒ[2]) [,][ Î»][B][ must converge to zero. When][ 0][ < Î· <][ 1]4 _[,][ there are three non-negative solutions]_

1/(2Î±) 1/(2Î±)

to _Î»[Ë™]_ _S = 0, which are 0, Î»[âˆ’]S_ [=] 1âˆ’[âˆš]21âˆ’4Î· and Î»[+]S [=] 1+[âˆš]21âˆ’4Î· _. As illustrated in_

Figure 2 (bottom, left), if initialization _Î´ > Î»_ _[âˆ’]S_ [(good basin),][ Î»][S][ converges to a positive value] _[ Î»]S[+][;]_
if 0 < Î´ < Î»[âˆ’]S [(bad basin),][ Î»][S][ converges to zero.]

**Thresholding role of weight decay in feature learning:** While Tian et al. (2021) shows why ncSSL does not collapse, one key question is how nc-SSL learns useful features and how the method
determines which feature is learned. Now it is clear: the weight decay factor Î· makes a call on what
features should be learned. Nuisance features subject to significant change under data augmentation
has larger variance Ïƒ[2] and 4(1+1Ïƒ[2]) _[< Î·][, the eigenspace corresponds to this feature goes to zero;]_

on the other hand, invariant features that are robust to data augmentation has much smaller Ïƒ[2] and
1

4(1+Ïƒ[2]) _[> Î·][ and these features are kept. In our above analysis,][ B][ subspace corresponds to the]_
nuisance features and collapses to zero; S subspace corresponds to the invariant features (whose
variance was assumed as zero for simplicity) and is kept after training.

Figure 2 (middle and right) shows the spectrum of F (which is the correlation matrix of the predictor
inputs) when the network is trained by DirectSet(1) with and without weight decay Î· on STL10
and CIFAR10: when Î· = 0, the eigen-spectrum of F in later epochs does not have a sharp drop
compared with the case of Î· = 0.0004. This means that the nuisance features are not significantly
suppressed when Î· = 0.

Therefore, it is crucially important to choose weight decay appropriately: a too small Î· may not be
sufficient to suppress the nuisance features; a too large Î· can also collapse the invariant features. As
shown in Section 5, both cases lead to worse downstream performance.

3.3 GRADIENT DESCENT ON EMPIRICAL LOSS

In this section, we then proceed to prove that DirectCopy (one special case of DirectSet(Î±) with
_Î± = 1) successfully learns the projection matrix given polynomial number of samples._
**Theorem 2. Suppose network architecture and data distribution are as defined in Assumption 1**
_and Assumption 2, respectively. Suppose we initialize online network as Î´I, and run DirectCopy_
_on empirical loss (see Eqn. 2) with Î³ step size and Î· weight decay. Suppose the noise scale Ïƒ[2]_

_is a positive constant, the weight decay coefficient Î·_ 4(1+1+Ïƒ[2]Ïƒ/[2]4) _[,][ 1+3]4(1+[Ïƒ]Ïƒ[2][/][2])[4]_ _and the initialization_
_âˆˆ_
 


-----

_scale Î´ is a constant at least 1/âˆš2. Choose the step size Î³ as a small enough constant. For any_

_accuracy Ë†Ïµ > 0, given n â‰¥_ _poly(d, 1/ÏµË†) number of samples, with probability at least 0.99 there_
_exists t = O(log(1/ÏµË†)) such that (here_ _Wt is the online network weights at the t-th step):_

1 + 1 4Î·

_Wt_ [f] _[âˆš]_ _âˆ’_ _PS_ _Ïµ._
_âˆ’_ r 2
f _[â‰¤]_ [Ë†]

The proof proceeds by first proving that gradient descent on the population loss converges in linear
rate and then couples the gradient descent dynamics on empirical loss and that on population loss.
See the detailed proof in Appendix B.2.

3.4 SAMPLE COMPLEXITY ON DOWNSTREAM TASKS

In this section, we show that the learned representations can indeed reduce the sample complexity
on the downstream tasks. We consider the following data distribution for the down-stream task:

**Assumption 3 (Downstream data distribution). Each input x[(][i][)]** _is sampled from N_ (0, Id) and its
_label y[(][i][)]_ = _x[(][i][)], w[âˆ—]_ + Î¾[(][i][)], where w[âˆ—] _is the ground truth vector with unit â„“2 norm and Î¾[(][i][)]_ _is_
_independently sampled from N_ (0, Î²[2]). We assume the ground truth w[âˆ—] _lies on an r-dimensional_
_subspace S and we denote the projection matrix on subspace S simply as P_ _._

In practice, usually the semantically relevant features (S subspace here) are invariant to augmentations and the nuisance features (orthogonal subspace of S) have high variance under augmentations.
Therefore, by previous analysis, we expect DirectSet(Î±) to learn the projection matrix P.

Suppose {(x[(][i][)], y[(][i][)])}i[n]=1 [are][ n][ training samples. Each input][ x][(][i][)][ is transformed by a matrix][ Ë†]P âˆˆ
R[d][Ã—][d] (for example the learned online network W ) to get its representation _Px[Ë†]_ [(][i][)]. The regularized

2

loss is then defined as _L[Ë†](w) :=_ 21n _ni=1_ _PxË†_ [(][i][)], w _y[(][i][)]_ + _[Ï]2_

_âˆ’_ _[âˆ¥][w][âˆ¥][2][ .][ In the below theorem,]_

we show that when _P_ _P_ P D E
_âˆ’_ [Ë†] _F_ [is small, the above ridge regression can recover the ground truth][ w][âˆ—]

given only O(r) number of samples.

**Theorem 3. Suppose the downstream data distribution is as defined in Assumption 3. Suppose**
_P[Ë†]_ _P_ _Ïµ with Ë†Ïµ < 1. Choose the regularizer coefficient Ï = Ë†Ïµ[1][/][3]. For any Î¶ < 1/2, given_

_n â‰¥ âˆ’O(r +log(1F_ _[â‰¤]_ [Ë†] _/Î¶)) number of samples, with probability at least 1_ _âˆ’_ _Î¶, the training loss minimizer_
_wË† satisfies_

_âˆšr +_ log(1/Î¶)

_P[Ë†] Ë†w âˆ’_ _w[âˆ—]_ _â‰¤_ _O_ _ÏµË†[1][/][3]_ + Î² pâˆšn ! _._

In the above theorem, when n is at least O _Î²[2](r+log(1ÏµË†[2][/][3]_ _/Î¶))_ _, we have_ _P[Ë†] Ë†w_ _w[âˆ—]_ _O(Ë†Ïµ[1][/][3])._

_âˆ’_ _â‰¤_

Note that if we directly estimate Ë†w without transforming the inputs by  _P[Ë†], we need â„¦(d) number_
of samples to ensure that âˆ¥wË† âˆ’ _w[âˆ—]âˆ¥â‰¤_ _o(1) (Wainwright, 2019). The proof of Theorem 3 follows_
from bounding the difference between _P[Ë†] Ë†w and w[âˆ—]_ by matrix concentration inequalities and matrix
perturbation bounds. The full proof is in Appendix B.3.

4 EMPIRICAL PERFORMANCE OF DIRECTCOPY

In the previous analysis, we show DirectSet(Î±), and in particular DirectCopy (DirectSet(Î±) with
_Î± = 1), could recover the input feature structure with polynomial samples and make the down-_
stream task more sample efficient in a simple linear setting. Compared with the original DirectPred
(DirectSet(Î±) with Î± = 1/2), DirectCopy is a simpler and computationally more efficient algorithm
since it directly set the predictor as the correlation matrix F, without the eigen-decomposition step.
By our analysis in Theorem 1, DirectCopy also learns the projection matrix PS with larger scale [3]

1/(2Î±)

3Recall that in Theorem 1 under DirectSet(Î±), online matrix W converges to 1+[âˆš]21âˆ’4Î· _PS. So_

with a larger Î±, the scalar in front of PS becomes larger.  


-----

compared with DirectPred, which suggests that the invariant features learned by DirectCopy are
stronger and more distinguishable. Next, we show that DirectCopy is on par with (or even outperforms) the original DirectPred in various datasets, when coupling with deep nonlinear models on
real datasets.

4.1 RESULTS ON STL-10, CIFAR-10 AND CIFAR-100

We use ResNet-18 (He et al., 2016) as the backbone network, a two-layer nonlinear MLP as the
projector, and a linear predictor. Unless specified otherwise, SGD is used as the optimizer with
weight decay Î· = 0.0004. To evaluate the quality of the pre-trained representations, we follow
the linear evaluation protocol. Each setting is repeated 5 times to compute the mean and standard
deviation. The accuracy is reported as â€œmeanÂ±stdâ€. Unless explicitly specified, we use learning rate
_Î³ = 0.01, regularization Ïµ = 0.2 on STL-10; Î³ = 0.02, Ïµ = 0.3 on CIFAR-10 and Î³ = 0.03, Ïµ = 0.3_
on CIFAR-100. See more detailed experiment settings in Appendix A.

_STL-10_

|Col1|Num of epochs 100 300 500|
|---|---|


|DirectCopy DirectPred DirectPred (freq=5) SGD baseline|77.83Â±0.56 77.86Â±0.16 77.54Â±0.11 75.06Â±0.52|82.01Â±0.28 78.77Â±0.97 79.90Â±0.66 75.25Â±0.74|82.95Â±0.29 78.86Â±1.15 80.28Â±0.62 75.25Â±0.74|
|---|---|---|---|


|Col1|epochs 100|
|---|---|



_CIFAR-100_ Table 2: ImageNet Top-1 accu
|DirectCopy DirectPred DirectPred (freq=5) SGD baseline|84.02Â±0.37 85.21Â±0.23 84.93Â±0.29 84.49Â±0.20|89.17Â±0.12 88.88Â±0.15 88.83Â±0.10 88.57Â±0.15|89.62Â±0.10 89.52Â±0.04 89.56Â±0.13 89.33Â±0.27|
|---|---|---|---|


|DirectCopy DirectPred SGD Baseline|68.8 68.5 68.6|
|---|---|


|DirectCopy DirectPred DirectPred (freq=5) SGD baseline|55.40Â±0.19 56.60Â±0.27 56.43Â±0.21 54.94Â±0.50|61.06Â±0.14 61.65Â±0.18 62.01Â±0.22 60.88Â±0.59|62.23Â±0.06 62.68Â±0.35 63.15Â±0.27 61.42Â±0.89|
|---|---|---|---|



Table 1: STL-10/CIFAR-10/CIFAR-100 Top-1 accuracy of DirectCopy.
The numbers for DirectPred, DirectPred (freq=5) and SGD baseline on
STL-10/CIFAR-10 are obtained from Tian et al. (2021).

**STL-10:** We evaluate the quality of the learned representation after each epoch, and report the
best accuracy in the first 100/300/500 epochs in Table 1. DirectCopy achieves substantially better
performance than the original DirectPred and SGD baseline, especially when trained with longer
epochs. DirectPred (freq=5) means the predictor is set by DirectPred every 5 batchs, and is trained
with gradient updates in other batchs, which outperforms DirectPred in later epochs, but is still much
worse than DirectCopy. The SGD baseline is obtained by training the linear predictor using SGD.

**CIFAR-10/100:** For CIFAR-10, DirectCopy is slighly worse than DirectPred at epoch 100, but
catches up and gets even better performance in epoch 300 and 500 (Table 1). For CIFAR-100, at
earlier epochs, the performance of DirectCopy is not as good as DirectPred, but the gap gradually
diminishes in later epochs. Both DirectCopy and DirectPred outperfoms the SGD baseline. DirectPred (freq=5) achieves even better performance, but at the cost of a more complicated algorithm.

4.2 RESULTS ON IMAGENET

Following BYOL (Grill et al., 2020), we use ResNet-50 as the backbone and a two-layer MLP as
the projector. We use LARS (You et al., 2017) optimizer and trains the model for 100 epochs. See
more detailed experiment settings in Appendix A.

For fairness, we compare DirectCopy to the gradient-based baseline which uses the same-sized linear
predictor as ours. As shown in Table 2, at 100-epoch, this baseline achieves 68.6 top-1 accuracy,
which is already significantly higher than BYOL with two-layer predictors reported in the literature
(e.g., Chen & He (2020) reports 66.5 top-1 under 100-epoch training). DirectCopy using normalized
_F accumulated with EMA Âµ = 0.99 on the correlation matrix, regularization parameter Ïµ = 0.01_
achieves 68.8 under the same setting, better than this strong baseline. In contrast, DirectPred (Tian
et al., 2021) achieves 68.5, slightly lower than the linear baseline.


-----

|O|Bad Basin|Good Basin ğœ†"# ğœ†"$ ğœ†|
|---|---|---|

|O|Good Basin ğœ†"# ğœ†"$ ğœ†|
|---|---|


STL-10, DirectCopy with Ïµ = 1 completely fails. On CIFAR-10, although DirectCopy with Ïµ = 1

ğœ†" ğœ†" ğœ†"

Bad Good Good Bad
Basin Basin Basin Basin

Increase ğœ– Increase ğœ–

O ğœ†"# ğœ†"$ ğœ†" O ğœ†"# ğœ†"$ ğœ†" O ğœ†"

Figure 3: Left: Change of _Î»[Ë™]_ _S when predictor regularization Ïµ increases. Right: Eigenvalues of F when_
trained by DirectCopy under different Ïµ on CIFAR-10 for 100 epochs.

5 ABLATION STUDY

In this section, we study the influence of predictor regularization Ïµ, normalization method, weight
decay and degree Î± on the performance of DirectCopy.

**Predictor regularization:** Table 3 shows that when the predictor regularization Ïµ increases, the
performance of DirectCopy on STL-10 and CIFAR-10 improves at first and then deteriorates. On

achieved reasonable performance at epoch 300, itâ€™s still much worse than Ïµ = 0.3.

To better understand the role of Ïµ, we analyze the simple linear setting as in Section 3.1 while
setting Wp = WW _[âŠ¤]_ + ÏµI. Recall that Î»B is the eigenvalue of W in B subspace and Î»S is that in S
subspace. When the weight decay is appropriate, Î»B still converges to zero. On the other hand, the
dynamics for Î»S is as follows:

_Î»Ë™_ _S =_ _Î»S_ _Î»[2]S_ [+][ Ïµ][ âˆ’] [1][ âˆ’âˆš][1][ âˆ’] [4][Î·] _Î»[2]S_ [+][ Ïµ][ âˆ’] [1 +][ âˆš][1][ âˆ’] [4][Î·] _._
_âˆ’_ 2 2

   

Increasing Ïµ shifts the two positive stationary points Î»[âˆ’]S _[, Î»]S[+]_ [towards zero. As illustrated in Figure 3]

(left), as Ïµ increases, when Î»[+]S [is still positive, the good attraction basin expands, which means][ Î»][S]
can converge to a positive value from a smaller initialization; when Î»[+]S [shifts to zero,][ Î»][S][ converges]
to zero regardless the initialization size. See the full analysis in Appendix D.

Intuitively, a reasonable Ïµ can alleviate representation collapse, but a too large Ïµ also encourages
representation collapse. As shown in Figure 3 (right), when Ïµ increases from zero, more eigenvalues
of F becomes large; but when Ïµ exceeds 0.3, eigenvalues of F begin to collapse.

**Normalization on F** **:** In our experiments, we have been normalizing F by its spectral norm
before adding the regularization: Wp = F/ âˆ¥F _âˆ¥_ + ÏµI. It turns out that we can also normalize
_F by its Frobenius norm or simply skip the normalization step. In Table 4, we see comparable_
performance from DirectCopy with Frobenius normalization or no normalization, especially when
trained longer.


_STL-10_

|Col1|Number of epochs 100 300|
|---|---|



_CIFAR-10_

|Ïµ = 0 Ïµ = 0.1 Ïµ = 0.2 Ïµ = 1|76.57Â±0.66 78.05Â±0.14 77.83Â±0.56 31.10Â±0.80|81.19Â±0.39 81.60Â±0.15 82.01Â±0.28 31.10Â±0.80|
|---|---|---|


|Ïµ = 0 Ïµ = 0.1 Ïµ = 0.3 Ïµ = 1|80.53Â±1.14 83.97Â±0.25 84.02Â±0.37 57.38Â±11.62|86.07Â±0.71 88.58Â±0.11 89.17Â±0.12 83.15Â±4.24|
|---|---|---|



Table 3: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with varying regularization Ïµ.


_STL-10_

|Col1|Number of epochs 100 300|
|---|---|



_CIFAR-10_

|Spectral Frobenius None|77.83Â±0.56 77.71Â±0.18 77.81Â±0.20|82.01Â±0.28 82.06Â±0.28 82.00Â±1.24|
|---|---|---|


|Spectral Frobenius None|84.02Â±0.37 84.33Â±0.25 81.76Â±0.34|89.17Â±0.12 89.62Â±0.14 89.21Â±0.17|
|---|---|---|



Table 4: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with F matrix normalized by spectral
norm/Frobenius norm or no normalization.


**Weight decay:** Table 5 shows that when weight decay Î· increases, the performance of DirectCopy
improves at first and then deteriorates. This fits our analysis on simple linear networks. Basically,
when the weight decay Î· increases, it can suppress the nuisance features more effectively, but a too
large weight decay also collapses the useful features.


-----

_STL-10_


_STL-10_

|Col1|Number of epochs 100 300|
|---|---|



_CIFAR-10_

|Î± = 2 Î± = 1 Î± = 1/2 Î± = 1/4|76.80Â±0.22 80. 77.83Â±0.56 82. 77.82Â±0.37 77. 76.82Â±0.36 76.|90Â±0.18 01Â±0.28 83Â±0.37 82Â±0.36|
|---|---|---|


|Î± = 2 Î± = 1 Î± = 1/2 Î± = 1/4|82.96Â±0.56 88. 84.02Â±0.37 89. 84.88Â±0.21 88. 84.78Â±0.21 87.|60Â±0.11 17Â±0.12 32Â±0.57 82Â±0.32|
|---|---|---|



Table 6: STL-10/CIFAR-10 Top-1 accuracy of
DirectSet(Î±) with varying degree Î±.


Number of epochs

100 300


_CIFAR-10_

|Î· = 0 Î· = 0.0004 Î· = 0.001 Î· = 0.01|71.94Â±0.93 77.83Â±0.56 77.65Â±0.16 58.12Â±0.94|
|---|---|


|Î· = 0 Î· = 0.0004 Î· = 0.001 Î· = 0.01|79.15Â±0.08 84.02Â±0.37 83.91Â±0.33 65.31Â±1.19|
|---|---|



Table 5: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with varying weight decay.


**Predictor degree:** We compare DirectCopy against DirectSet(Î±) with Î± = 2, 1/2, 1/4. Table 6

shows that DirectCopy outperforms other algorithms on STL-10. On CIFAR-10, DirectCopy is
slightly worse at epoch 100, but catches up in later epochs.

6 BEYOND LINEAR MODELS: LIMITATIONS AND DISCUSSION

Figure 4: Eigenvalues of F when trained by DirectCopy, BYOL with linear predictor and BYOL with twolayer nonlinear predictor on CIFAR-10 for different epochs. Top-1 accuracy at 500 epoch is 89.62 for DirectCopy, 88.83 for BYOL with linear predictor and 90.25 for BYOL with two-layer nonlinear predictor.

As a linear model used to study the behavior of nc-SSL, our model does not capture all of its
intriguing empirical phenomena. For example, we observed that the discarded nuisance features
gradually come back after training over longer epochs. Moreover, whether it comes back or not is
related to the downstream task performance. In Figure 4 on CIFAR-10 dataset, both DirectCopy
and BYOL with two-layer nonlinear predictor show this resurgence of nuisance features, as well as
strong performance, while BYOL with linear predictor does not seem to learn new features even
when trained longer, which might explain its worse performance.

One conjecture is that at the beginning of training, weight decay prioritize the invariant features (i.e.,
low variance under augmentation) over nuisance ones. The invariant features then grow, building
their own supporting low-level features. After that, the nuisance feature, which is also useful, are
gradually picked up in later stage. Since the low-level features are already trained through previous
steps of back-propagation, the nuisance features are encouraged to use them as the supporting features, rather than creating their own. In contrast, if we train both the invariant and nuisance features
simultaneously, they will compete over the limited pool of low-level supporting features defined by
the capacity of the network, leading to worse learned representations. We believe understanding
these phenomena require analysis on the non-linear networks, and we leave it as future work.

7 CONCLUSION

In this paper, we have proved DirectSet(Î±) can learn the desirable projection matrix in a linear
network setting and reduce the sample complexity on down-stream tasks. Our analysis sheds light on
the crucial role of weight decay in nc-SSL, which discards the features that have high variance under
augmentations and keep the invariant features. Inspired by the analysis, we also designed a simpler
and more efficient algorithm DirectCopy, which achieves comparable or even better performance
than the original DirectPred (Tian et al., 2021) on various datasets.

We view our paper as an initial step towards demystifying the representation learning in nc-SSL.
Many mysteries lie beyond the explanation of the current theory and we leave them for future work.


-----

REFERENCES

Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In ICML. PMLR, 2018.

Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. In ICLR, 2019.

Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. arXiv preprint arXiv:1906.00910, 2019.

Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization
for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.

Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard SÂ¨ackinger, and Roopak Shah. Signature verification using aâ€œ siameseâ€ time delay neural network. NeurIPS, 1994.

Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS, 2020.

Mathilde Caron, Hugo Touvron, Ishan Misra, HervÂ´e JÂ´egou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers. _arXiv preprint_
_arXiv:2104.14294, 2021._

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.

Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint
_arXiv:2011.10566, 2020._

Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.

Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In International conference on artificial intelligence and statistics, 2011.

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR, 2009.

Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In
_ICML, 2019._

Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for selfsupervised representation learning. In International Conference on Machine Learning, pp. 3015â€“
3024. PMLR, 2021.

Rong Ge, Qingqing Huang, and Sham M Kakade. Learning mixtures of gaussians in high dimensions. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pp.
761â€“770. ACM, 2015.

Jean-Bastien Grill, Florian Strub, Florent AltchÂ´e, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint
_arXiv:2006.07733, 2020._

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In CVPR, 2020.

Tianyu Hua, Wenxiao Wang, Zihui Xue, Yue Wang, Sucheng Ren, and Hang Zhao. On feature
decorrelation in self-supervised learning. ICCV, 2021.

Kenji Kawaguchi. Deep learning without poor local minima. NeurIPS, 2016.


-----

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.

Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer
learning in deep linear networks. In ICLR, 2018.

Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are
global. In ICML, pp. 2902â€“2907. PMLR, 2018.

Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:
Provable self-supervised learning. arXiv preprint arXiv:2008.01064, 2020.

Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.

Pierre H. Richemond, Jean-Bastien Grill, Florent AltchÂ´e, Corentin Tallec, Florian Strub, Andrew
Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, and Michal Valko. Byol works
even without batch statistics. arXiv, 2020.

Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.

Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic
development in deep neural networks. Proc. Natl. Acad. Sci. U. S. A., 2019.

Gilbert W Stewart. On the perturbation of pseudo-inverses, projections and linear least squares
problems. SIAM review, 19(4):634â€“662, 1977.

Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint
_arXiv:1906.05849, 2019._

Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics
without contrastive pairs. arXiv preprint arXiv:2102.06810, 2021.

Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redundancy, and linear models. arXiv preprint arXiv:2008.10150, 2020.

Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
_arXiv:1011.3027, 2010._

Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.

Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press, 2019.

Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised
contrastive learning. arXiv preprint arXiv:2105.15134, 2021.

Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks.
_arXiv:1708.03888, 2017._

Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and StÂ´ephane Deny. Barlow twins: Self-supervised
learning via redundancy reduction. ICML, 2021.


-----

A DETAILED EXPERIMENT SETTING

**STL-10, CIFAR-10, CIFAR-100** : We use ResNet-18 (He et al., 2016) as the backbone network,
a two-layer nonlinear MLP (with batch normalization, ReLU activation, hidden layer width 512,
output width 128) as the projector, and a linear predictor. Unless specified otherwise, SGD is used
as the optimizer with momentum 0.9, weight decay Î· = 0.0004 and batch size 128. The EMA
parameter for the target network is set as 0.996 and the EMA parameter Âµ of the correlation matrix _F[Ë†]_
is set as 0.5. Our code is adapted from Tian et al. (2021) [4], and we follow the same data augmentation
process.

To evaluate the quality of the pre-trained representations, we follow the linear evaluation protocol.
Each setting is repeated 5 times to compute the mean and standard deviation. The accuracy is
reported as â€œmeanÂ±stdâ€. Unless explicitly specified, we use learning rate Î³ = 0.01, regularization
_Ïµ = 0.2 on STL-10; Î³ = 0.02, Ïµ = 0.3 on CIFAR-10 and Î³ = 0.03, Ïµ = 0.3 on CIFAR-100._

**ImageNet** : Following BYOL (Grill et al., 2020), we use ResNet-50 as the backbone and a twolayer MLP (with batch normalization, ReLU, hidden layer width 4096, output width 256) as the
projector. We use LARS (You et al., 2017) optimizer and trains the model for 100 epochs, with a
batch size 4096. The learning rate is 7.2, which is linearly scaled from the base learning rate 0.45
at batch size 256. Other setups such as weight decay (Î· = 1e[âˆ’][6]), target EMA (scheduled from 0.99
to 1), augmentation recipe (color jitters, blur, etc.), and linear evaluation protocol are the same as
BYOL.

B PROOFS OF SINGLE-LAYER LINEAR NETWORKS

B.1 GRADIENT FLOW ON POPULATION LOSS

In this section, we give the proof of Theorem 1, which shows that DirectSet(Î±) running on the
population loss with infinitesimal learning rate and Î· weight decay can learn the projection matrix
onto subspace S.
**Theorem 1. Suppose network architecture and data distribution follow Assumption 1 and Assump-**
_tion 2, respectively. Suppose we initialize online network W as Î´I, and run DirectSet(Î±) on popu-_
_lation loss (see Eqn. 1) with infinitesimal step size and Î· weight decay. If we set the weight decay_

1/(2Î±)

_coefficient Î·_ 4(1+1Ïƒ[2]) _[,][ 1]4_ _and initialization scale Î´ >_ 1âˆ’[âˆš]21âˆ’4Î· _, then W converges to_
_âˆˆ_

1/(2Î±)

1+[âˆš]1 4Î·    

2 _âˆ’_ _PS when time goes to infinity._

 

As we already mentioned in the main text, Theorem 1 is proved by analyzing each eigenvalue of W
separately. We show that the eigenvalues in the B subspace converge to zero, and the eigenvalues in
the S subspace converge to the same positive number, which immediately implies that W converges
to a scaling of the projection matrix PS.

**Proof of Theorem 1. We can compute the gradient in terms of W as follows,**


_âˆ‡L(W_ ) =Ex1,x2 _Wp[âŠ¤]_ [(][W][p][Wx][1] _[âˆ’]_ _[W][a][x][2][)][ x]1[âŠ¤]_
=Wp[âŠ¤] _WpW_ Ex1 _x1x[âŠ¤]1_ _[âˆ’]_ _[W][a][E][x]1[,x]2_ _[x][2][x][âŠ¤]1_ _._

Note that the two augmented views x1, x  2 are sampled by first sampling input _x from N_ (0, Id), and
then independently sampling x1, x2 from (x, Ïƒ[2]PB). Therefore, we know Ex1 _x1x[âŠ¤]1_ [=][ I][ +][ Ïƒ][2][P][B]
_N_
and Ex1,x2 _x2x[âŠ¤]1_ [=][ I.][ Recall that we run gradient flow on][ W][ with weight decay][ Î·,][ so the dynamics]
on W is as follows:
_WË™_ =Wp[âŠ¤][(][âˆ’][W][p][W] [(][I][ +][ Ïƒ][2][P][B][) +][ W][a][)][ âˆ’] _[Î·W,]_

where the first term comes from the gradient and the second term is due to weight decay.

Since W is initialized as Î´I, and Wa = W, Wp = (WW _[âŠ¤])[Î±], so we know initially W, Wp, Wa, I and_
_PB are all simultaneously diagonalizable, which then implies_ _W[Ë™]_ is simultaneously diagonalizable

4Their open source code is at https://github.com/facebookresearch/luckmatters/tree/main/ssl


-----

with W . This argument can continue to show that at any time point, W, Wp, Wa, I and PB are
all simultaneously diagonalizable. Since W is always a real symmetric matrix, we have Wp =
(WW _[âŠ¤])[Î±]_ = |W _|[2][Î±]_ _. The dynamics on W can then be written as_

_WË™_ = _W_ ( _W_ _W_ (I + Ïƒ[2]PB) + W ) _Î·W_
_|_ _|[2][Î±]_ _âˆ’|_ _|[2][Î±]_ _âˆ’_

=W (I + Ïƒ[2]PB) _W_ + _W_ _Î·_ _._
_âˆ’_ _|_ _|[4][Î±]_ _|_ _|[2][Î±]_ _âˆ’_
 

Let the eigenvalue decomposition of W be _i=1_ _[Î»][i][u][i][u]i[âŠ¤][,][ with span][(][{][u][d][âˆ’][r][+1][,][ Â· Â· Â·][, u][d][}][)][ equals to]_
subspace B. We can separately analyze the dynamics of each Î»i. Furthermore, we know Î»1, _, Î»r_
_Â· Â· Â·_
have the same value Î»S and Î»d _r+1,_ _, Î»d have the same value Î»B. Next, we separately show that_
_âˆ’_ _Â· Â· Â·_ [P][d]
_Î»B converge to zero and Î»S converges to a positive value._

**Dynamics for Î»B:** We can write down the dynamics for Î»B as follows:

_Î»Ë™_ _B = Î»B_ _âˆ’(1 + Ïƒ[2]) |Î»B|[4][Î±]_ + |Î»B|[2][Î±] _âˆ’_ _Î·_

Similar as the analysis in Tian et al. (2021), whenh _Î· >_ 4(1+1Ïƒ[2]) _[,][ we know]i_ [ Ë™]Î»B < 0 for any Î»B > 0

and Î»B = 0 is a critical point. This means, as long as Î· > 4(1+1Ïƒ[2]) [,][ Î»][B][ must converge to zero.]

**Dynamics for Î»S:** We can write down the dynamics for Î»S as follows:

_Î»Ë™_ _S = Î»S_ _âˆ’|Î»S|[4][Î±]_ + |Î»S|[2][Î±] _âˆ’_ _Î·_ _._
h i

When 0 < Î· < 14 _[,][ we know][ Ë™]Î»S > 0 for Î»[2]S[Î±]_ 1âˆ’[âˆš]21âˆ’4Î· _,_ [1+][âˆš]2[1][âˆ’][4][Î·] and _Î»[Ë™]_ _S < 0 for Î»[2]S[Î±]_
_âˆˆ_ _âˆˆ_

1+[âˆš]21âˆ’4Î· _,_ _. Furthermore, we know_ _Î»[Ë™]_ _S = 0 when_ _Î»[2]S[Î±]_ = 1+[âˆš]21âˆ’4Î· _. Therefore, as long as_

_âˆ_

0 < Î· < [1]4 [and initialization] _[ Î´][2][Î±][ >][ 1][âˆ’âˆš]2[1][âˆ’][4][Î·]_, we know Î»[2]S[Î±] [converges to][ 1+][âˆš]2[1][âˆ’][4][Î·] _._

1/(2Î±)

Overall, we know when 4(1+1Ïƒ[2]) _[< Î· <][ 1]4_ [and][ Î´ >] 1âˆ’[âˆš]21âˆ’4Î· _, we have Î»B converge to_

1/(2Î±) 1/(2Î±)

zero and Î»S converge to 1+[âˆš]21âˆ’4Î· _. That is, matrix_ _W converges to_ 1+[âˆš]21âˆ’4Î· _PS._
    â–¡

B.2 GRADIENT DESCENT ON EMPIRICAL LOSS

In this section, we prove that DirectCopy successfully learns the projection matrix given polynomial
number of samples.
**Theorem 2. Suppose network architecture and data distribution are as defined in Assumption 1**
_and Assumption 2, respectively. Suppose we initialize online network as Î´I, and run DirectCopy_
_on empirical loss (see Eqn. 2) with Î³ step size and Î· weight decay. Suppose the noise scale Ïƒ[2]_

_is a positive constant, the weight decay coefficient Î·_ 4(1+1+Ïƒ[2]Ïƒ/[2]4) _[,][ 1+3]4(1+[Ïƒ]Ïƒ[2][/][2])[4]_ _and the initialization_
_âˆˆ_

_scale Î´ is a constant at least 1/âˆš2. Choose the step size Î³ as a small enough constant. For any_

_accuracy Ë†Ïµ > 0, given n â‰¥_ _poly(d, 1/ÏµË†) number of samples, with probability at least 0.99 there_
_exists t = O(log(1/ÏµË†)) such that (here_ _Wt is the online network weights at the t-th step):_

1 + 1 4Î·

_Wt_ [f] _[âˆš]_ _âˆ’_ _PS_ _Ïµ._
_âˆ’_ r 2
f _[â‰¤]_ [Ë†]

When running gradient descent on the empirical loss, the eigenspace of _Wt can shift and become no_
longer simultaneously diagonalizable with PB. So we cannot independently analyze each eigenvalue
of _Wt as before, which brings significant challenge into the analysis. Instead of directly analyzing[f]_
the dynamics of _Wt, we first show that the gradient descent iterates Wt on the population loss_
converges to[f] _PS in linear rate, and then show that_ _Wt stays close to Wt within certain iterations._

[f]

[f]


-----

**Lemma 1. In the setting of Theorem 2, let Wt be the gradient descent iterations on the population**
_loss L. Given any accuracy Ë†Ïµ > 0, for any t â‰¥_ _C log(1/ÏµË†), we have_

1 + 1 4Î·

_[âˆš]_ _âˆ’_ _PS_ _Ïµ,_

2

r

_[â‰¤]_ [Ë†]

_where C is a positive constant._

_[W][t][ âˆ’]_

The proof of Lemma 1 is similar as the gradient flow analysis in Section 3.2. Next, we show that the
gradient descent trajectory on the empirical loss stays close to the gradient descent trajectory on the
population loss within O(log(1/ÏµË†)) iterations.

**Lemma 2. In the setting of Theorem 2, let Wt be the gradient descent iterations on the population**
_loss and let_ _Wt be the gradient descent iterations on the empirical loss. For any accuracy Ë†Ïµ > 0,_
_given n â‰¥_ _poly(d, 1/ÏµË†) number of samples, with probability at least 0.99, for any t â‰¤_ _C log(1/ÏµË†),_
_we have_

[f]
_Wt_ _Wt_ _Ïµ,Ë†_
_âˆ’_ _â‰¤_

_where the constant C comes from Lemma 1._

[f]

Then the proof of Theorem 2 directly follows from Lemma 1 and Lemma 2.

**Proof of Theorem 2. According to Lemma 1, we know given any accuracy Ë†Ïµ[â€²], for t = C log(1/ÏµË†),**
we have

1 + 1 4Î·

_[âˆš]_ _âˆ’_ _PS_ _Ïµ[â€²],_

2

r

_[â‰¤]_ [Ë†]

where C is a positive constant.

_[W][t][ âˆ’]_

According to Lemma 2, we know given n â‰¥ poly(d, 1/ÏµË†[â€²]) number of samples, with probability at
least 0.99,
_Wt_ _Wt_ _ÏµË†[â€²]._
_âˆ’_ _â‰¤_

Therefore, we have [f]

1 + 1 4Î· 1 + 1 4Î·

_Wt_ _[âˆš]_ _âˆ’_ _PS_ _[âˆš]_ _âˆ’_ _PS_ _Wt_ _Wt_ 2Ë†Ïµ[â€²].
_âˆ’_ r 2 r 2 [+] _âˆ’_ _â‰¤_

Replacingf Ë†Ïµ[â€²] by Ë†Ïµ/2 finishes the proof.[â‰¤] [f] â–¡

_[W][t][ âˆ’]_

In section B.2.1, we give the proof of Lemma 1 and Lemma 2. Proofs of some technical lemmas are
left in Appendix B.5.

B.2.1 PROOFS FOR LEMMA 1 AND LEMMA 2

**Proof of Lemma 1. Similar as in Theorem 1, we can show that at any step t, Wt is simultaneously**
diagonalizable with Wa,t, Wp,t, I and PB. The update on Wt is as follows,

_Wt+1 = Wt + Î³Wt_ _âˆ’(I + Ïƒ[2]PB)Wt[4]_ [+][ W][ 2]t _[âˆ’]_ _[Î·]_ _._
  

Let the eigenvalue decomposition of Wt be _i=1_ _[Î»][i,t][u][i][u]i[âŠ¤][,][ with span][(][{][u][d][âˆ’][r][+1][,][ Â· Â· Â·][, u][d][}][)][ equals]_
to subspace B. We can separately analyze the dynamics of each Î»i,t. Furthermore, we know
_Î»1,t,_ _, Î»r,t have the same value Î»S,t and Î»d_ _r+1,t,_ _, Î»d,t have the same value Î»B,t. Next,_
_Â· Â· Â·_ [P][d]âˆ’ _Â· Â· Â·_
we separately show that Î»B,t converge to zero and Î»S,t converges to a positive value in linear rate.

**Dynamics of Î»B,t:** We show that

0 _Î»B,t_ (1 _Î³C1)[t]Î´_
_â‰¤_ _â‰¤_ _âˆ’_

for any step size Î³ â‰¤ _C2, where C1, C2 are two positive constants._


-----

According to the gradient update, we have

_Î»B,t+1 = Î»B,t + Î³Î»B,t_ _âˆ’(1 + Ïƒ[2])Î»[4]B,t_ [+][ Î»]B,t[2] _[âˆ’]_ _[Î·]_ _._

We only need to prove that for any Î»B,t [0, Î´], we have 
_âˆˆ_

_âˆ’(1 + Ïƒ[2])Î»[4]B,t_ [+][ Î»]B,t[2] _[âˆ’]_ _[Î·][ =][ âˆ’][Î˜(1)][.]_

This is true since Î· 4(1+1+Ïƒ[2]Ïƒ/[2]4) _[,][ 1+3]4(1+[Ïƒ]Ïƒ[2][/][2])[4]_ and Ïƒ[2], Î´ are two positive constants.
_âˆˆ_
 


**Dynamics of Î»S:** We show that

0 â‰¤ _S,t_ _[âˆ’]_ [1 +][ âˆš]2[1][ âˆ’] [4][Î·] _[â‰¤]_ [(1][ âˆ’] _[Î³C][3][)][t]_ 2

for any step size Î³ â‰¤ _C[Î»]4,[2] where C3, C4 are two positive constants.[Î´][2][ âˆ’]_ [1 +][ âˆš][1][ âˆ’] [4][Î·]

There are two cases to consider: when the initialization scale Î´[2] [1/2, [1+][âˆš]2[1][âˆ’][4][Î·]
_âˆˆ_

1 + 1 4Î·

0 â‰¤ [1 +][ âˆš]2[1][ âˆ’] [4][Î·] _âˆ’_ _Î»[2]B,t_ _[â‰¤]_ [(1][ âˆ’] _[Î³C][3][)][t]_ _âˆš2_ _âˆ’_ _âˆ’_ _Î´[2]_

 

when the initialization scale Î´[2] _>_ [1+][âˆš]2[1][âˆ’][4][Î·] _, we prove_

0 â‰¤ _Î»[2]B,t_ _[âˆ’]_ [1 +][ âˆš]2[1][ âˆ’] [4][Î·] _â‰¤_ (1 âˆ’ _Î³C3)[t]_ _Î´[2]_ _âˆ’_ [1 +][ âˆš]2[1][ âˆ’] [4][Î·]

 

We focus on the second case; the proof for the first case is similar.

According to the gradient update, we have


], we prove


_Î»S,t+1 =Î»S,t + Î³Î»S,t_ _âˆ’Î»[4]S,t_ [+][ Î»]S,t[2] _[âˆ’]_ _[Î·]_
 

=Î»S,t âˆ’ _Î³Î»S,t_ _Î»[2]S,t_ _[âˆ’]_ [1][ âˆ’âˆš]2[1][ âˆ’] [4][Î·] _Î»[2]S,t_ _[âˆ’]_ [1 +][ âˆš]2[1][ âˆ’] [4][Î·]

   

We only need to show that Î»S,t _Î»[2]S,t_ 2 = Î˜(1) for any Î»[2]S,t 2 _, Î´]. This is_

_[âˆ’]_ [1][âˆ’âˆš][1][âˆ’][4][Î·] _[âˆˆ]_ [[][ 1+][âˆš][1][âˆ’][4][Î·]

true because Î· 4(1+1+Ïƒ[2]Ïƒ/[2]4) _[,][ 1+3]4(1+[Ïƒ]Ïƒ[2][/][2])[4]_ and Ïƒ[2], Î´ are two positive constants.
_âˆˆ_
 


Overall, we know that there exists constant step size such that after t = O(log(1/ÏµË†)) steps, we have


1 + 1 4Î·

_[âˆš]_ _âˆ’_


0 _Î»B,t_ _ÏµË† and_
_â‰¤_ _â‰¤_


r

_[Î»][S,t][ âˆ’]_

1 + 1 4Î·

_[âˆš]_ _âˆ’_



_[â‰¤]_ _Ïµ.[Ë†]_

_n_

1

_x[(]1[i][)][[][x]2[(][i][)][]][âŠ¤]_

_n_

_i=1_ !!

X


This then implies,

1 + 1

_[âˆš]_ _âˆ’_

2

r

_[W][t][ âˆ’]_

**Proof of Lemma 2. We know the update on** _Wt is_

_n_

1

_Wt+1 âˆ’_ _W[f]t = Î³W[f]p,t[âŠ¤]_ _âˆ’W[f]p,tW[f]t_ _n_ _i=1_ _x[f][(]1[i][)][[][x]1[(][i][)][]][âŠ¤]_

X

and the update onf _Wt is_


_PS_



_[â‰¤]_ _Ïµ.[Ë†]_


+ _Wa,t_

[f]


_Î³Î·W[f]t,_
_âˆ’_


_Wt+1 âˆ’_ _Wt = Î³Wp,t[âŠ¤]_ _âˆ’Wp,tWt_ _I + Ïƒ[2]PB_ + Wa,t _âˆ’_ _Î³Î·Wt._

Next, we bound _Wt+1_ _Wt_ (Wt+1  _Wt)_ _. According to Lemma 3, we know with probability _  
_âˆ’_ [f] _âˆ’_ _âˆ’_

at least 1 âˆ’ _O(d[2]) exp_ _âˆ’â„¦(Ë†Ïµ[â€²][2]n/d[2])_ _,_

_n_ [f] _n_ _n_

  


_x[(]1[i][)][[][x]1[(][i][)][]][âŠ¤]_ _[âˆ’]_ _[I][ âˆ’]_ _[Ïƒ][2][P][B]_
_i=1_

X


_x[(]1[i][)][[][x]2[(][i][)][]][âŠ¤]_ _[âˆ’]_ _[I]_
_i=1_

X


_x[(][i][)][x[(][i][)]][âŠ¤]_ _âˆ’_ _I_ _Ïµ[â€²]._
_i=1_ _[â‰¤]_ [Ë†]

X


-----

Recall that we set _Wa,t =_ _Wt and set Wa,t as Wt, so we have_ _Wa,t âˆ’_ _Wa,t_ = _Wt âˆ’_ _Wt_ _._

1 _n_
Also since we set _Wp,t =_ _Wt_ _n_ _i=1_ _[x][(][i][)][[][x][(][i][)][]][âŠ¤][ f]Wt[âŠ¤]_ and set Wp,t = WtWt[âŠ¤][,][ we have]

[f] [f] [f] [f]

_Wp,t_ _Wp,t_ = O _Wt_ _Wt_   + Ë†PÏµ[â€²][] since _Wt_ = O(1).
_âˆ’_ [f] _âˆ’_ [f] _âˆ¥_ _âˆ¥_


Combing the above bounds and recall Î³ is a constant, we have

[f] [f]

_Wt+1_ _Wt_ (Wt+1 _Wt)_ = O _Wt_ _Wt_ + Ë†Ïµ[â€²][] _._
_âˆ’_ [f] _âˆ’_ _âˆ’_ _âˆ’_

Therefore, 

[f] [f]

_Wt âˆ’_ _Wt_ _â‰¤_ _C1[t]Ïµ[Ë†][â€²],_

where C1 is a constant larger than 1. So for any t â‰¤ _C log(1/ÏµË†), we have_
_Wt_ _Wt_ [f] _C1[C][ log(1][/]Ïµ[Ë†])ÏµË†[â€²]_ (1/ÏµË†)[C][2] _ÏµË†[â€²],_
_âˆ’_ _â‰¤_ _â‰¤_

for some positive constant C2. Choosing Ë†Ïµ[â€²] = Ë†Ïµ[C][2][+1], we know as long as n poly(d, 1/ÏµË†), with
probability at least 0.99, for any[f] t â‰¤ _C log(1/ÏµË†), we have_ _â‰¥_
_Wt_ _Wt_ _Ïµ.Ë†_
_âˆ’_ _â‰¤_

â–¡

[f]

B.3 SAMPLE COMPLEXITY ON DOWN-STREAM TASKS


In this section, we give a proof for Theorem 3, which shows that the learned representations can
indeed reduce sample complexity in downstream tasks.
**Theorem 3. Suppose the downstream data distribution is as defined in Assumption 3. Suppose**
_P[Ë†]_ _P_ _Ïµ with Ë†Ïµ < 1. Choose the regularizer coefficient Ï = Ë†Ïµ[1][/][3]. For any Î¶ < 1/2, given_

_n â‰¥ âˆ’O(r +log(1F_ _[â‰¤]_ [Ë†] _/Î¶)) number of samples, with probability at least 1_ _âˆ’_ _Î¶, the training loss minimizer_
_wË† satisfies_

_âˆšr +_ log(1/Î¶)

_P[Ë†] Ë†w âˆ’_ _w[âˆ—]_ _â‰¤_ _O_ _ÏµË†[1][/][3]_ + Î² pâˆšn ! _._

Suppose (x[(][i][)], y[(][i][)]) _i=1_ [are][ n][ training samples in the downstream task, let][ X][ âˆˆ] [R][n][Ã—][d][ be the data]
_{_ _}[n]_
matrix with its i-th row equal to x[(][i][)]. Denote y âˆˆ R[n] as the label vector with its i-th entry as y[(][i][)].
Each input x[(][i][)] is transformed by a matrix _P[Ë†] âˆˆ_ R[d][Ã—][d] to get its representation _Px[Ë†]_ [(][i][)]. The regularized
loss can be written as

2

_L(w) := [1]_ _XPw[Ë†]_ _y_ + _[Ï]_

2n _âˆ’_ 2 _[âˆ¥][w][âˆ¥][2][ .]_

This is the ridge regression problem on inputs {( Px[Ë†] [(][i][)], y[(][i][)])}i[n]=1[, and the unique global minimizer]
_wË† has the following close form:_

1 _âˆ’1 1_
_wË† =_ _n_ _PË†[âŠ¤]X_ _[âŠ¤]XP[Ë†] + ÏI_ _n_ _PË†[âŠ¤]X_ _[âŠ¤]y_ (4)
 

With the above closed form of Ë†w, the proof of Theorem 3 follows by bounding the difference between _P[Ë†] Ë†w and w[âˆ—]_ by matrix concentration inequalities and matrix perturbation bounds. Some proofs
of technical lemmas are left in Appendix B.5.

**Proof of Theorem 3. Denoting** _P[Ë†] as P + âˆ†, we know_ âˆ† _F_ _ÏµË† by assumption. We can also write_
_y as Xw[âˆ—]_ + Î¾ where Î¾ âˆˆ R[n] is the noise vector with its âˆ¥ i-th entry equal toâˆ¥ _â‰¤_ _Î¾[(][i][)]. Then, we can divide_
_wË† into two terms,_

1 _âˆ’1 1_
_wË† =_ _n_ _PË†[âŠ¤]X_ _[âŠ¤]XP[Ë†] + ÏI_ _n_ _PË†[âŠ¤]X_ _[âŠ¤]y_
 

1 _âˆ’1 1_ 1 _âˆ’1 1_
= _n_ _PË†[âŠ¤]X_ _[âŠ¤]XP[Ë†] + ÏI_ _n_ _[P][ âŠ¤][X]_ _[âŠ¤]_ [(][Xw][âˆ—] [+][ Î¾][) +] _n_ _PË†[âŠ¤]X_ _[âŠ¤]XP[Ë†] + ÏI_ _n_ [âˆ†][âŠ¤][X] _[âŠ¤]_ [(][Xw][âˆ—] [+][ Î¾][)]
   


Letâ€™s first give an upper bound for the second term that comes from the error term âˆ†[âŠ¤].


-----

**Upper bounding** _n1_ _P[Ë†][âŠ¤]X_ _[âŠ¤]XP[Ë†] + ÏI_ _âˆ’1 1n_ [âˆ†][âŠ¤][X] _[âŠ¤]_ [(][Xw][âˆ—] [+][ Î¾][)] We first bound the norm of

1  
_n_ [âˆ†][âŠ¤][X] _[âŠ¤][Xw][âˆ—][.][ According to Lemma 5, we know with probability at least][ 1][ âˆ’]_ [exp(][âˆ’][â„¦(][n][))][,]
_âˆš[1]n_ âˆ†[âŠ¤]X _[âŠ¤]_ _Ïµ). Since Xw[âˆ—]_ is a standard Gaussian vector with dimension n, according
_F_

_[â‰¤]_ _[O][(Ë†]_

to Lemma 8, with probability at least 1 exp( â„¦(n)), _âˆš[1]n_ _Xw[âˆ—]_ _O(1). Therefore, we have_
_âˆ’_ _âˆ’_ _â‰¤_

_n[1]_ [âˆ†][âŠ¤][X] _[âŠ¤][Xw][âˆ—]_ _â‰¤_ _O(Ë†Ïµ)._

Then we bound the norm of _n1_ [âˆ†][âŠ¤][X] _[âŠ¤][Î¾][.]_ According to Lemma 8, we know

with probability at least 1 exp( â„¦(n)), _âˆš[1]n_ _Î¾_ _O(Î²). According to Lemma 6, we_
_âˆ’_ _âˆ’_ _â‰¤_

know with probability at least 1 âˆ’ _Î¶/3,_ âˆ†[âŠ¤]X _[âŠ¤]Î¾Â¯_ _â‰¤_ _O_ _ÏµË†_ log(1/Î¶) _. Therefore, we have_

_n[1]_ [âˆ†][âŠ¤][X] _[âŠ¤][Î¾]_ _O_ _Î²ÏµË†[âˆš]log(1âˆšn_ _/Î¶)_ _._  p 
_â‰¤_
 

Since Î»min _n1_ _P[Ë†][âŠ¤]X_ _[âŠ¤]XP[Ë†] + ÏI_ _â‰¥_ _Ï, we have_ _n1_ _P[Ë†][âŠ¤]X_ _[âŠ¤]XP[Ë†] + ÏI_ _âˆ’1_ _[â‰¤]_ _Ï[1]_ _[.][ Combining with]_

above bound on _n[1]_ [âˆ†][âŠ¤][X] _[âŠ¤]_ [(][Xw][âˆ—] [+][ Î¾][)], we know with probability at least  1 _âˆ’_ exp(âˆ’â„¦(n)) _âˆ’_ _Î¶/3,_

1 _âˆ’1 1_ _ÏµË†_ _Ïµ_ log(1/Î¶)

_n_ _PË†[âŠ¤]X_ _[âŠ¤]XP[Ë†] + ÏI_ _n_ [âˆ†][âŠ¤][X] _[âŠ¤]_ [(][Xw][âˆ—] [+][ Î¾][)] _Ï_ [+][ Î²][Ë†] _Ï[âˆš]n_ _._

  _[â‰¤]_ _[O]_ p !

**Analyzing** _n1_ _P[Ë†][âŠ¤]X_ _[âŠ¤]XP[Ë†] + ÏI_ _âˆ’1 1n_ _[P][ âŠ¤][X]_ _[âŠ¤]_ [(][Xw][âˆ—] [+][ Î¾][)] We can write _n1_ _P[Ë†][âŠ¤]X_ _[âŠ¤]XP[Ë†]_ as

1
_n_ _[P][ âŠ¤][X]_ _[âŠ¤][XP][ +][ E,][ where]_ 

_E = [1]_

_n_ [âˆ†][âŠ¤][X] _[âŠ¤][XP][ + 1]n_ _[P][ âŠ¤][X]_ _[âŠ¤][X][âˆ†+ 1]n_ [âˆ†][âŠ¤][X] _[âŠ¤][X][âˆ†][.]_


Letâ€™s first bound the spectral norm of XP. Since P is a projection matrix on an r-dimensional
subspace S, we can write P as UU _[âŠ¤], where U âˆˆ_ R[d][Ã—][r] has columns as an orthonormal basis of
subspace S. According to Lemma 4, we know with probability at least 1 âˆ’ exp(âˆ’â„¦(n)),

1 1

â„¦(1) _Ïƒmin_ _Ïƒmax_ _O(1)._
_â‰¤_ _âˆšnXU_ _â‰¤_ _âˆšnXU_ _â‰¤_

   

Since âˆ¥U _âˆ¥â‰¤_ 1, we have _âˆš[1]n_ _XP_ = _âˆš[1]n_ _XUU_ _[âŠ¤]_ _â‰¤_ _O(1)._

According to Lemma 5, we know with probability at least 1 âˆ’ exp(âˆ’â„¦(n)),
1
_âˆšnXâˆ†_ _F_ _â‰¤_ _O(Ë†Ïµ)._

So overall, we know _E_ _E_ _F_ _O(Ë†Ïµ)._
_âˆ¥_ _âˆ¥â‰¤âˆ¥_ _âˆ¥_ _â‰¤_

Then, we can write
1 _âˆ’1_ 1 _âˆ’1_

_n_ _PË†[âŠ¤]X_ _[âŠ¤]XP[Ë†] + ÏI_ = _n_ _[P][ âŠ¤][X]_ _[âŠ¤][XP][ +][ ÏI]_ + F.

   


According to the perturbation bound for matrix inverse (Lemma 11), we have âˆ¥F _âˆ¥â‰¤_ _O(_ _ÏÏµ[Ë†][2][ )][.][ Then,]_

we have
1 _âˆ’1 1_ 1 _âˆ’1 1_

_n_ _PË†[âŠ¤]X_ _[âŠ¤]XP[Ë†] + ÏI_ _n_ _[P][ âŠ¤][X]_ _[âŠ¤]_ [(][Xw][âˆ—] [+][ Î¾][) =] _n_ _[P][ âŠ¤][X]_ _[âŠ¤][XP][ +][ ÏI]_ _n_ _[P][ âŠ¤][X]_ _[âŠ¤][Xw][âˆ—]_

   

+ F [1]

_n_ _[P][ âŠ¤][X]_ _[âŠ¤][Xw][âˆ—]_

1 _âˆ’1_ 1

+ + F

_n_ _[P][ âŠ¤][X]_ _[âŠ¤][XP][ +][ ÏI]_ _n_ _[P][ âŠ¤][X]_ _[âŠ¤][Î¾]_

  !


-----

We first show that the first term is close to w[âˆ—]. Let the eigenvalue decomposition of _n[1]_ _[P][ âŠ¤][X]_ _[âŠ¤][XP]_

be V Î£V _[âŠ¤], where V â€™s columns are an orthonormal basis for subspace S. Here Î£ âˆˆ_ R[r][Ã—][r] is the
diagonal matrix that contains all the eigenvalues of _n[1]_ _[P][ âŠ¤][X]_ _[âŠ¤][XP]_ [. According to Lemma 4, we]

know that with probability at least 1 âˆ’ exp(âˆ’â„¦(n)), all the non-zero eigenvalues of _n[1]_ _[P][ âŠ¤][X]_ _[âŠ¤][XP]_

are Î˜(1).

Then, itâ€™s not hard to show that


1 _âˆ’1 1_

_n_ _[P][ âŠ¤][X]_ _[âŠ¤][XP][ +][ ÏI]_ _n_ _[P][ âŠ¤][X]_ _[âŠ¤][XP][ âˆ’]_ _[P]_

 

This immediately implies that



_[â‰¤]_ _[O][(][Ï][)][.]_


1 _âˆ’1 1_

_n_ _[P][ âŠ¤][X]_ _[âŠ¤][XP][ +][ ÏI]_ _n_ _[P][ âŠ¤][X]_ _[âŠ¤][Xw][âˆ—]_ _[âˆ’]_ _[w][âˆ—]_

  _[â‰¤]_ _[O][(][Ï][)]_

Next, we bound the norm of the second term F _n[1]_ _[P][ âŠ¤][X]_ _[âŠ¤][Xw][âˆ—][.][ Similar as before, we know]_

with probability at least 1 âˆ’ exp(âˆ’â„¦(n)), _âˆš[1]n_ _Xw[âˆ—]_ _â‰¤_ _O(1) and_ _âˆš[1]n_ _P_ _[âŠ¤]X_ _[âŠ¤]_ _â‰¤_ _O(1). There-_
fore, we have

1 1 Ë†Ïµ
_n_ _[P][ âŠ¤][X]_ _[âŠ¤][Xw][âˆ—]_ _[â‰¤âˆ¥][F]_ _[âˆ¥]_ _âˆšnP_ _[âŠ¤]X_ _[âŠ¤]_ _âˆšnXw[âˆ—]_ _[â‰¤]_ _[O]_  _Ï[2]_  _._

Finally, letâ€™s bound the third term[F][ 1] 1n _[P][ âŠ¤][X]_ _[âŠ¤][XP][ +][ ÏI]_ _âˆ’1 + F_ _n1_ _[P][ âŠ¤][X]_ _[âŠ¤][Î¾.][ We first bound the]_

norm of _n[1]_ _[P][ âŠ¤][X]_ _[âŠ¤][Î¾.][ with probability at least] _ [ 1][ âˆ’] [exp(][âˆ’][â„¦(] _[n][))][,][ we know]_ _[ âˆ¥][Î¾][âˆ¥â‰¤]_ [2][Î²][âˆš][n.][ Therefore,]

we know _n[1]_ _[P][ âŠ¤][X]_ _[âŠ¤][Î¾]_ _â‰¤_ _O(Î²/[âˆš]n)_ _P_ _[âŠ¤]X_ _[âŠ¤]Î¾Â¯_ _, where Â¯Î¾ = Î¾/ âˆ¥Î¾âˆ¥_ _. According to Lemma 7,_

with probability at least 1 âˆ’ _Î¶/3, we have_ _P_ _[âŠ¤]X_ _[âŠ¤]Î¾Â¯_ _â‰¤_ _âˆšr + O(_ log(1/Î¶)). Overall,

with probability at least 1 exp( â„¦(n)) _Î¶/3,_
_âˆ’_ _âˆ’_ _âˆ’_ p
1 _âˆšrÎ² +_ log(1/Î¶)Î²

_n_ _[P][ âŠ¤][X]_ _[âŠ¤][Î¾]_ _âˆšn_ _._

_[â‰¤]_ _[O]_ p !

Itâ€™s not hard to verify that for any vector 1n _[P][ âŠ¤][X]_ _[âŠ¤][XP][ +][ ÏI]_ _âˆ’1 + F_ _v_ _â‰¤_ _O(âˆ¥vâˆ¥). Since v_ _n[1]âˆˆ[P][ âŠ¤][X]R[d][âŠ¤][Î¾][ lies on subspace]in the subspace[ S,] S,[ we have] we have_
  1   _âˆ’1_ 1 _âˆšrÎ² +_ log(1/Î¶)Î²

_n_ _[P][ âŠ¤][X]_ _[âŠ¤][XP][ +][ ÏI]_ + F _n_ _[P][ âŠ¤][X]_ _[âŠ¤][Î¾]_ _âˆšn_ _._

  ! _[â‰¤]_ _[O]_ p !

Combining the above analysis and taking a union bound over all the events, we know
with probability at least 1 âˆ’ exp(âˆ’â„¦(n)) âˆ’ 2Î¶/3,

_Ïµ_ _Ïµ_ _Ïµ_ log(1/Î¶) _âˆšrÎ² +_ log(1/Î¶)Î²

_âˆ¥wË† âˆ’_ _w[âˆ—]âˆ¥_ = O _Ï + Ï[Ë†]_ [+ Ë†]Ï[2][ +][ Î²][Ë†]pÏ[âˆš]n + pâˆšn !

Suppose n â‰¥ _O(log(1/Î¶)) and setting Ï = Ë†Ïµ[1][/][3], we further have with probability at least 1 âˆ’_ _Î¶,_

_Ïµ[2][/][3][p]log(1/Î¶)_ _âˆšrÎ² +_ log(1/Î¶)Î²

_âˆ¥wË† âˆ’_ _w[âˆ—]âˆ¥_ =O _ÏµË†[1][/][3]_ + _[Î²][Ë†]_ _âˆšn_ + pâˆšn !

_âˆšr +_ log(1/Î¶)

_â‰¤O_ _ÏµË†[1][/][3]_ + Î² pâˆšn ! _,_


where the last inequality assumes Ë†Ïµ < 1.


-----

We can also bound _P[Ë†] Ë†w âˆ’_ _w[âˆ—]_ as follows,
_P[Ë†] Ë†w âˆ’_ _w[âˆ—]_ = _P[Ë†] Ë†w âˆ’_ _P Ë†w + P Ë†w âˆ’_ _Pw[âˆ—]_

_â‰¤_ _P[Ë†] Ë†w âˆ’_ _P Ë†w_ + âˆ¥P Ë†w âˆ’ _Pw[âˆ—]âˆ¥_

_â‰¤_ _P[Ë†] âˆ’_ _P_ _âˆ¥wË†âˆ¥_ + âˆ¥P _âˆ¥âˆ¥wË† âˆ’_ _w[âˆ—]âˆ¥_

_âˆšr +_ log(1/Î¶)

_â‰¤ÏµOË†_ 1 + Ë†Ïµ[1][/][3] + Î² _âˆšn_

p


_âˆšr +_
_ÏµË†[1][/][3]_ + Î²


log(1/Î¶)
_âˆšn_

p


+ O


_âˆšr +_
_ÏµË†[1][/][3]_ + Î²


log(1/Î¶)
_âˆšn_

p


_â‰¤O_


B.4 ANALYSIS WITH Wp := (W Ex1 _x1x[âŠ¤]1_ _[W][ âŠ¤][)][Î±]_

In this section, we prove that DirectSet(Î±) can also learn the projection matrix when we set Wp :=
(W Ex1 _x1x[âŠ¤]1_ _[W][ âŠ¤][)][Î±][. For the network architecture and data distribution, we follow exactly the same]_
setting as in Section 3.2. Therefore, we know Wp := (W Ex1 _x1x[âŠ¤]1_ _[W][ âŠ¤][)][Î±][ = (][W]_ [(][I][ +] _[Ïƒ][2][P][B][)][W][ âŠ¤][)][Î±][.]_

**Theorem 4. Suppose network architecture and data distribution are as defined in Assumption 1 and**
_Assumption 2, respectively. Suppose we initialize online network W as Î´I, and run DirectPred(Î±)_
_on population loss (see Eqn. 1) with infinitesimal step size and Î· weight decay. Suppose we set Wa =_
_W and Wp = (W_ Ex1 _x1x[âŠ¤]1_ _[W][ âŠ¤][)][Î±][.][ Assuming the weight decay coefficient][ Î·][ âˆˆ]_ 4(1+Ïƒ1[2])[1+2][Î±][,][ 1]4

1/(2Î±) 1/(2Î±)

_and initialization scale Î´ >_ 1âˆ’[âˆš]21âˆ’4Î· _, we know W converges to_ 1+[âˆš]21âˆ’4Î· _PS_

_when time goes to infinity._    

The only difference from Theorem 4 is that now the initialization Î´ is only required to be larger than
1

4(1+Ïƒ[2])[1+2][Î±][ . The proof is almost the same as in Theorem 1.]

**Proof of Theorem 4. Similar as in the proof of Theorem 1, we can write the dynamics on W is as**
follows:
_WË™_ =Wp[âŠ¤][(][âˆ’][W][p][W] [(][I][ +][ Ïƒ][2][P][B][) +][ W][a][)][ âˆ’] _[Î·W]_

= _W_ [2](I + Ïƒ[2]PB) ( _W_ [2](I + Ïƒ[2]PB) _W_ (I + Ïƒ[2]PB) + W ) _Î·W_
_âˆ’_ _âˆ’_

=W (I + Ïƒ[2]PB)[1+2][Î±] _W_ + _W_ _Î·_ _._
_âˆ’_ _[Î±]_ _|_ _|[4][Î±]_ _|_ _|[2][Î±]_ _âˆ’[Î±]_
 


**Dynamics for Î»B:** We can write down the dynamics for Î»B as follows:

_Î»Ë™_ _B = Î»B_ _âˆ’(1 + Ïƒ[2])[1+2][Î±]_ _|Î»B|[4][Î±]_ + |Î»B|[2][Î±] _âˆ’_ _Î·_

When Î· > 4(1+Ïƒ1[2])[1+2][Î±][,][ we know]h[ Ë™]Î»B < 0 for any Î»B > 0 and Î»B = 0i is a critical point. This

means, as long as Î· > 4(1+Ïƒ1[2])[1+2][Î±][,][ Î»][B][ must converge to zero.]


**Dynamics for Î»S:** The dynamics is same as when setting Wp = (WW _[âŠ¤])[Î±],_

_Î»Ë™_ _S = Î»S_ _âˆ’|Î»S|[4][Î±]_ + |Î»S|[2][Î±] _âˆ’_ _Î·_ _._
h i

so when 0 < Î· < 4[1] [and initialization][ Î´][2][Î±][ >][ 1][âˆ’âˆš]2[1][âˆ’][4][Î·], we know Î»[2]S[Î±] [converges to][ 1+][âˆš]2[1][âˆ’][4][Î·] _._

1/(2Î±)

Overall, we know when 4(1+Ïƒ1[2])[1+2][Î±][ < Î· <][ 1]4 [and][ Î´ >] 1âˆ’[âˆš]21âˆ’4Î· _, we have Î»B converge to_

1/(2Î±) 1/(2Î±)

zero and Î»S converge to 1+[âˆš]21âˆ’4Î· _. That is, matrix_ _W converges to_ 1+[âˆš]21âˆ’4Î· _PS._
    â–¡


-----

B.5 TECHNICAL LEMMAS

_OLemma 3.(d/ÏµË†[2]), with probability at least Suppose {x[(][i][)], x[(]1[i][)][, x] 12[(][i] âˆ’[)][}]i[n]O=1(d[are sampled as decribed in Section 3.][2]) exp_ _âˆ’â„¦(Ë†Ïµ[2]n/d[2])_ _, we have_ _Suppose n â‰¥_
  


_x[(]1[i][)][[][x]1[(][i][)][]][âŠ¤]_ _[âˆ’]_ _[I][ âˆ’]_ _[Ïƒ][2][P][B]_
_i=1_

X


_x[(]1[i][)][[][x]2[(][i][)][]][âŠ¤]_ _[âˆ’]_ _[I]_
_i=1_

X


_x[(][i][)][x[(][i][)]][âŠ¤]_ _âˆ’_ _I_
_i=1_

X


_n_ _x[(]1[i][)][[][x]1[(][i][)][]][âŠ¤]_ _[âˆ’]_ _[I][ âˆ’]_ _[Ïƒ][2][P][B]_ _[,]_ _n_ _x[(]1[i][)][[][x]2[(][i][)][]][âŠ¤]_ _[âˆ’]_ _[I]_ _[,]_ _n_ _x[(][i][)][x[(][i][)]][âŠ¤]_ _âˆ’_ _I_ _Ïµ._

_i=1_ _i=1_ _i=1_ _[â‰¤]_ [Ë†]

X X X

**Proof of Lemma 3. For each x[(]1[i][)][, we can write it as][ x][(][i][)][ +][ z]1[(][i][)]** where x[(][i][)] _âˆ¼N_ (0, I) and z1[(][i][)]
(0, Ïƒ[2]PB). So we have
_N_

_n_ _n_

1

_x[(]1[i][)][[][x]1[(][i][)][]][âŠ¤]_ [= 1] _x[(][i][)][x[(][i][)]][âŠ¤]_ + z1[(][i][)][[][z]1[(][i][)][]][âŠ¤] [+][ x][(][i][)][[][z]1[(][i][)][]][âŠ¤] [+][ z]1[(][i][)][[][x][(][i][)][]][âŠ¤][] _._

_n_ _n_

_i=1_ _i=1_

X X 


According to Lemma 9, we know as long as n â‰¥ _O(d/ÏµË†[2]), with probability at least 1 âˆ’_
exp(âˆ’â„¦(Ë†Ïµ[2]n)), _n_


1

_x[(][i][)][x[(][i][)]][âŠ¤]_ _I_

_n_ _âˆ’_

_i=1_

X

Similarly, with probability at least 1 âˆ’ exp(âˆ’â„¦(Ë†Ïµ[2]n)),



_[â‰¤]_ _Ïµ.[Ë†]_


1

_z1[(][i][)][[][z]1[(][i][)][]][âŠ¤]_ _[âˆ’]_ _[Ïƒ][2][P][B]_ _Ïµ._

_n_

_i=1_ _[â‰¤]_ [Ë†]

X

Next we bound _n[1]_ _ni=1_ _[x][(][i][)][[][z]1[(][i][)][]][âŠ¤]_ _. We know each entry in matrix_ _n[1]_ _ni=1_ _[x][(][i][)][[][z]1[(][i][)][]][âŠ¤]_ [is the]

average of n zero-mean O(1)-subexponential independent random variables. Therefore, according

P P

to the Bernsteinâ€™s inequality, for any fixed entry (k, l), with probability at least 1 âˆ’ exp _âˆ’ÏµË†[2]n/d[2][]_ _,_

_n_  

1

_x[(][i][)][z1[(][i][)][]][âŠ¤]_ _Ïµ/d.Ë†_

" _n_ _i=1_ #k,l _â‰¤_

X

Taking a union bound over all the entries, we know with probability at least 1 âˆ’ _d[2]_ exp _âˆ’ÏµË†[2]n/d[2][]_ _,_

_n_ _n_

1 1  

_x[(][i][)][z1[(][i][)][]][âŠ¤]_ _x[(][i][)][z1[(][i][)][]][âŠ¤]_ _Ïµ.Ë†_

_n_ _n_ _â‰¤_

_i=1_ _[â‰¤]_ _i=1_ _F_

X X

The same analysis also applies to _n[1]_ _ni=1_ _[z]1[(][i][)][[][x][(][i][)][]][âŠ¤]_ _. Combing all the bounds, we know with_

probability at least 1 _O(d[2]) exp_ â„¦(Ë†Ïµ[2]n/d[2]) _,_

P

_âˆ’_ _âˆ’_
  


1

_x[(]1[i][)][[][x]1[(][i][)][]][âŠ¤]_ _[âˆ’]_ _[I][ âˆ’]_ _[Ïƒ][2][P][B]_ _Ïµ._

_n_

_i=1_ _[â‰¤]_ [4Ë†]

X

Similarly, we can prove that with probability at least 1 âˆ’ _O(d[2]) exp_ _âˆ’â„¦(Ë†Ïµ[2]n/d[2])_
 


1

_x[(]1[i][)][[][x]2[(][i][)][]][âŠ¤]_ _[âˆ’]_ _[I]_ _Ïµ._

_n_

_i=1_ _[â‰¤]_ [4Ë†]

X

Changing Ë†Ïµ to Ë†Ïµ[â€²]/4 finishes the proof. â–¡

**Lemma 4. Let X âˆˆ** R[n][Ã—][d] _be a standard Gaussian matrix, and let U âˆˆ_ R[d][Ã—][r] _be a matrix with_
_orthonormal columns. Suppose n â‰¥_ 2r, with probability at least 1 âˆ’ exp(âˆ’â„¦(n)), we know

1 1

â„¦(1) _Î»min_ _Î»max_ _O(1)._
_â‰¤_ _n_ _[U][ âŠ¤][X]_ _[âŠ¤][XU]_ _â‰¤_ _n_ _[U][ âŠ¤][X]_ _[âŠ¤][XU]_ _â‰¤_

   


-----

**Proof of Lemma 4. Since U has orthonormal columns, we know XU is a n Ã— r matrix with each**
entry independently sampled from N (0, 1). According to Lemma 9, we know when n â‰¥ 2r, with
probability at least 1 âˆ’ exp(âˆ’â„¦(n)),

1 1

â„¦(1) _Ïƒmin_ _Ïƒmax_ _O(1)._
_â‰¤_ _âˆšnXU_ _â‰¤_ _âˆšnXU_ _â‰¤_

   

This immediately implies that

1 1

â„¦(1) _Î»min_ _Î»max_ _O(1)._
_â‰¤_ _n_ _[U][ âŠ¤][X]_ _[âŠ¤][XU]_ _â‰¤_ _n_ _[U][ âŠ¤][X]_ _[âŠ¤][XU]_ _â‰¤_

   

â–¡

**Lemma 5. Let âˆ†** _be a d_ _Ã—_ _d matrix with Frobenius norm Ë†Ïµ, and let X be a n_ _Ã—_ _d standard Gaussian_
_matrix. We know with probability at least 1 âˆ’_ exp(âˆ’â„¦(n)),
1
_âˆšnXâˆ†_ _F_ _â‰¤_ _O(Ë†Ïµ)._


**Proof of Lemma 5. Let the singular value decomposition of âˆ†** be U Î£V _[âŠ¤], where U, V have or-_
thonormal columns and Î£ is a diagonal matrix with diagonals equal to singular values Ïƒiâ€™s. Since
_âˆ¥âˆ†âˆ¥F = Ë†Ïµ, we know_ _i=1_ _[Ïƒ]i[2]_ [= Ë†]Ïµ[2].

Since U is an orthonormal matrix, we know[P][d] _X[Ë†] := XU is still an n Ã— d standard Gaussian matrix._
Next, we bound the Frobenius norm of _X := X[Ë†]_ Î£. Itâ€™s not hard to verify that all the entries in _X are_
sum of independent and sub-exponential random variables, we have for everyindependent Gaussian variables and _Xij[e] âˆ¼N_ (0, Ïƒj[2][)][.][ According to the Bernsteinâ€™s inequality for] t > 0, [e]

[e]

_t[2]_ _t_

Pr ï£® _iâˆˆ[nX],jâˆˆ[d]_ _Xij[2]_ _[âˆ’]_ _[n]Ïµ[Ë†][2]_ _â‰¥_ _tï£¹_ _â‰¤_ 2 exp "âˆ’c min _iâˆˆ[n],jâˆˆ[d]_ _[Ïƒ]j[4]_ _,_ maxjâˆˆ[d] Ïƒj[2] !# _._

ï£° e ï£» P

Since _j=1_ 2[Ïƒ]j[2] = âˆ¥âˆ†âˆ¥F[2] = Ë†Ïµ[2], we know maxjâˆˆ[d] Ïƒj[2] _â‰¤_ _ÏµË†[2]. We also have_ _jâˆˆ[d]_ _[Ïƒ]j[4]_ _â‰¤_

_j_ [d] _[Ïƒ]j[2]_ = Ë†Ïµ[4]. Therefore, we have
_âˆˆ_ [P][d] [P]

P 

2
_t_

Pr ï£® _i_ [n],j [d] _Xij[2]_ _[âˆ’]_ _[n]Ïµ[Ë†][2]_ _â‰¥_ _tï£¹_ _â‰¤_ 2 exp âˆ’c min  _nÏµË†[4][, t]ÏµË†[2]_  _._

_âˆˆ_ Xâˆˆ

ï£° e ï£» 2

Replacing t by nÏµË†[2], we concluded that with probability at least 1 exp( â„¦(n)), _X_ _Ïµ[2]._

Furthermore, since _V_ _[âŠ¤]_ = 1, we have _âˆ’_ _âˆ’_ _F_ _[â‰¤]_ [2][n][Ë†]

[e]

1 1 1
_âˆšnXâˆ†_ _F_ = _âˆšn_ _XV_ _[âŠ¤]_ _F_ _â‰¤_ _âˆšn_ _X_ _F_ _âˆ¥V âˆ¥â‰¤_ _O(Ë†Ïµ)._
e e â–¡

**Lemma 6. Let âˆ†[âŠ¤]** _be a dÃ—d matrix with Frebenius norm Ë†Ïµ and let X_ _[âŠ¤]_ _be a dÃ—n standard Gaussian_
_matrix. Let_ _Î¾[Â¯] be a unit vector with dimension n. We know with probability at least 1 âˆ’_ _Î¶/3,_
âˆ†[âŠ¤]X _[âŠ¤]Î¾Â¯_ _â‰¤_ _O(Ë†Ïµ_ log(1/Î¶)).
p

**Proof of Lemma 6. Let the sigular value decomposition of âˆ†[âŠ¤]** be U Î£V _[âŠ¤]. We know X_ _[âŠ¤]Î¾[Â¯] is a d-_
dimensional standard Gaussian vector. Further, we know V _[âŠ¤]X_ _[âŠ¤]Î¾[Â¯] is also a d-dimensional standard_
Gaussian vector. So Î£V _[âŠ¤]X_ _[âŠ¤]Î¾[Â¯] has independent Gaussian entries with its i-th entry distributed_
as N (0, Ïƒi[2][)][.][ According to the Bernsteinâ€™s inequality for sum of independent and sub-exponential]
random variables, we have for every t > 0,

_t2_
Pr Î£V _[âŠ¤]X_ _[âŠ¤]Î¾Â¯_ _âˆ’_ _ÏµË†[2]_ _â‰¥_ _t_ _â‰¤_ 2 exp _âˆ’c min_ _ÏµË†[4][, t]ÏµË†[2]_ _._
h i   

[2]


-----

Choosing t as O(Ë†Ïµ[2] log(1/Î¶)), we know with probability at least 1 âˆ’ _Î¶/3, we have_
Î£V _[âŠ¤]X_ _[âŠ¤]Î¾Â¯_ _â‰¤_ _O_ _ÏµË†[2]_ log(1/Î¶) _._

Since _U_ = 1, we further have   
_âˆ¥_ _âˆ¥_ [2]
âˆ†[âŠ¤]X _[âŠ¤]Î¾Â¯_ = _U_ Î£V _[âŠ¤]X_ _[âŠ¤]Î¾Â¯_ _â‰¤âˆ¥U_ _âˆ¥_ Î£V _[âŠ¤]X_ _[âŠ¤]Î¾Â¯_ _â‰¤_ _O_ _ÏµË†_ log(1/Î¶)
 p  â–¡

**Lemma 7. Let P âˆˆ** R[d][Ã—][d] _be a projection matrix on a r-dimensional subspace, and let_ _Î¾[Â¯] be a unit_
_vector in R[d]. Let X_ _[âŠ¤]_ _be a d Ã— n standard Gaussian matrix that is independent with P and Î¾. With_
_probability at least 1 âˆ’_ _Î¶/3, we have_
_P_ _[âŠ¤]X_ _[âŠ¤]Î¾Â¯_ _â‰¤_ _[âˆš]r + O(plog(1/Î¶))._

**Proof of Lemma 7. Since P is a projection matrix on an r-dimensional subspace, we can write P**
as UU _[âŠ¤], where U âˆˆ_ R[d][Ã—][r] has orthonormal columns. We know U _[âŠ¤]X_ _[âŠ¤]_ is still a standard Gaussian
matrix with dimension r Ã— n. Furthermore, U _[âŠ¤]X_ _[âŠ¤]Î¾[Â¯] is an r-dimensional standard Gaussian vector._
According to Lemma 8, with probability at least 1 âˆ’ _Î¶/3, we have_
_U_ _[âŠ¤]X_ _[âŠ¤]Î¾Â¯_ _â‰¤_ _[âˆš]r + O(_ log(1/Î¶)).

Since _U_ = 1, we further have p
_âˆ¥_ _âˆ¥_
_P_ _[âŠ¤]X_ _[âŠ¤]Î¾Â¯_ = _UU_ _[âŠ¤]X_ _[âŠ¤]Î¾Â¯_ _â‰¤âˆ¥U_ _âˆ¥_ _U_ _[âŠ¤]X_ _[âŠ¤]Î¾Â¯_ _â‰¤_ _[âˆš]r + O(_ log(1/Î¶)).
p â–¡

C ANALYSIS OF DEEP LINEAR NETWORKS

In this section, we extend the analysis in Section 3.2 to deep linear networks. We consider the same
data distribution as defined in Assumption 2. We consider the following network,
**Assumption 4 (Deep linear network). The online network is an l-layer linear networks**
_WlWl_ 1 _W1 with each Wi_ R[d][Ã—][d]. The target network has the same architecture with weight
_matricesâˆ’_ _W Â· Â· Â·a,lWa,l_ 1 _Wa,1 âˆˆ. For convenience, we denote W as WlWl_ 1 _W1 and denote Wa_
_as Wa,lWa,lâˆ’1 Â· Â· Â· Wâˆ’_ _a, Â· Â· Â·1._ _âˆ’_ _Â· Â· Â·_

**Training procedure:** At the initialization, we initialize each Wi as Î´[1][/l]Id. Through the training,
we fix Wp as _WW_ _[âŠ¤][][Î±]_ and fix each Wa,i as Wi. We run gradient flow on every Wi with weight
decay Î·. The population loss is
 

_L(_ _Wi_ _, Wp,_ _Wa,i_ ) := [1]
_{_ _}_ _{_ _}_ 2 [E][x][1][,x][2][ âˆ¥][W][p][W][l][W][l][âˆ’][1][ Â· Â· Â·][ W][1][x][1][ âˆ’] [StopGrad][(][W][a,l][W][a,l][âˆ’][1][ Â· Â· Â·][ W][a,][1][x][2][)][âˆ¥][2][ .]

**Theorem 5. Suppose the data distribution and network architecture satisfies Assumption 2 and As-**
_sumption 4, respectively. Suppose we train the network as described above. Assuming the weight_
_decay coefficient_


_Î·_ 2Î±l(2Î±l+2lâˆ’2)[1+ 1]Î± _[âˆ’]_ _Î±l[1]_ _Î±_ _[âˆ’]_ _Î±l[1]_ _, and initialization scale Î´_
_âˆˆ_  (4Î±l+2[1] _lâˆ’2)[2+ 1]Î±_ _[âˆ’]_ _Î±l[1]_ (1+Ïƒ[2])[1+ 1]Î± _[âˆ’]_ _Î±l[1]_ _[,][ 2][Î±l](4[(2]Î±l[Î±l]+2[+2]lâˆ’[l][âˆ’]2)[2)][2+ 1][1+ 1]Î±_ _[âˆ’]_ _Î±l[1]_  _â‰¥_

2Î±l+2l 2 2Î±
4Î±l+2lâˆ’âˆ’2 _, we know[1]_ _W converges to cPS as time goes to infinity, where c is a positive number_
 2Î±l+2l 2 2Î±

_within_ 4Î±l+2lâˆ’âˆ’2 _, 1_ _._
  

Similar as in the setting of single-layer linear networks, we prove Theorem 5 by analyzing the
dynamics of the eigenvalues of W. Note that with constant Î±, the upper/lower bounds for Î· and
scalar c in the Theorem are always constants no matter how large l is.

**Proof of Theorem 5. For j** _i, we use W[j:i] to denote WjWj_ 1 _Wi and for j < i have_
_W[j:i] = I. We use similar notations for â‰¥_ _Wa,[j:i]. For each Wi, we can compute its dynamics asâˆ’_ _Â· Â· Â·_
follows:
_WË™_ _i =_ _WpW[l:i+1]_ _âŠ¤_ _WpW_ (I + Ïƒ[2]PB) _W[i_ 1:1] _âŠ¤_ + _WpWa,[l:i+1]_ _âŠ¤_ _Wa_ _Wa,[i_ 1:1] _âŠ¤_ _Î·Wi._
_âˆ’_ _âˆ’_ _âˆ’_ _âˆ’_
              


-----

Itâ€™s clear that through the training all Wiâ€™s remains the same and they are simultaneously diagonalizable with Wp, I and PB. We also have Wa = W and Wp = |W _|[2][Î±]_ _. Since we will ensure that W_
is always positive semi-definite so Wp = _W_ = W [2][Î±] = Wi[2][Î±l]. So the dynamics for each Wi
_|_ _|[2][Î±]_
can be simplified as follows:

_WË™_ _i =_ _Wi[4][Î±l][+2][l][âˆ’][1](I + Ïƒ[2]PB) + Wi[2][Î±l][+2][l][âˆ’][1]_ _Î·Wi._
_âˆ’_ _âˆ’_

Let the eigenvalue decomposition of Wi be _i=1_ _[Î½][i][u][i][u]i[âŠ¤][,][ with span][(][{][u][d][âˆ’][r][+1][,][ Â· Â· Â·][, u][d][}][)][ equals to]_
subspace B. We can separately analyze the dynamics of each Î½i. Furthermore, we know Î½1, _, Î½r_
_Â· Â· Â·_
have the same value Î½S and Î½d _r+1,_ _, Î½d have the same value Î½B. We can write down the dy-_
_âˆ’_ _Â· Â· Â·_ [P][d]
namics for Î½S and Î½B as follows,

_Î½Ë™S =_ _Î½S[4][Î±l][+2][l][âˆ’][1]_ + Î½S[2][Î±l][+2][l][âˆ’][1] _Î·Î½S,_
_âˆ’_ _âˆ’_

_Î½Ë™B =_ _Î½B[4][Î±l][+2][l][âˆ’][1](1 + Ïƒ[2]) + Î½B[2][Î±l][+2][l][âˆ’][1]_ _Î·Î½B._
_âˆ’_ _âˆ’_

Let Î»S be the eigenvalue of W corresponding to eigen-directions u1, Â· Â· Â·, ur, and let Î»B be the
eigenvalue of W corresponding to eigen-directions udâˆ’r+1, Â· Â· Â·, ud. We know Î»S = Î½S[l] [and][ Î»][B][ =]
_Î½B[l]_ _[.][ So we can write down the dynamics for][ Î»][B][ as follows,]_

_Î»Ë™_ _B = lÎ½B[l][âˆ’][1]Î½Ë™B =_ _lÎ½B[4][Î±l][+3][l][âˆ’][2](1 + Ïƒ[2]) + lÎ½B[2][Î±l][+3][l][âˆ’][2]_ _lÎ·Î½B[l]_
_âˆ’_ _âˆ’_

= _lÎ»[4]B[Î±][+3][âˆ’][2][/l](1 + Ïƒ[2]) + lÎ»B[2][Î±][+3][âˆ’][2][/l]_ _lÎ·Î»B,_
_âˆ’_ _âˆ’_

and similarly for Î»S we have


_Î»Ë™_ _S =_ _lÎ»[4]S[Î±][+3][âˆ’][2][/l]_ + lÎ»[2]S[Î±][+3][âˆ’][2][/l] _lÎ·Î»S._
_âˆ’_ _âˆ’_

**Dynamics for Î»B:** We can write the dynamics on Î»B as follows,

_Î»Ë™_ _B = lÎ»Bg(Î»B),_

where g(Î»B) := _Î»B[4][Î±][+2][âˆ’][2][/l](1 + Ïƒ[2]) + Î»B[2][Î±][+2][âˆ’][2][/l]_ _Î·. We show that when Î· is large enough,_
_âˆ’_ _âˆ’_
_g(Î»B) is negative for any positive Î»B. We compute the maximum value of g(Î»B) for Î»B > 0. We_
first compute the derivative of g as follows:

_g[â€²](Î»B) =_ (4Î± + 2 2/l)(1 + Ïƒ[2])Î»B[4][Î±][+1][âˆ’][2][/l] + (2Î± + 2 2/l)Î»[2]B[Î±][+1][âˆ’][2][/l]
_âˆ’_ _âˆ’_ _âˆ’_
=Î»B[2][Î±][+1][âˆ’][2][/l] _âˆ’(4Î± + 2 âˆ’_ 2/l)(1 + Ïƒ[2])Î»[2]B[Î±] [+ (2][Î±][ + 2][ âˆ’] [2][/l][)] _._

2Î±l+2l 2
Itâ€™s clear that( (4Î±l+22Î±ll+22)(1+lâˆ’2 _gÏƒ[2][â€²])([,]Î»[ +]B[âˆ]) >[)][.][ Therefore, the maximum value of] 0 for _ _Î»[2]B[Î±]_ _âˆˆ_ (0, (4Î±l+2lâˆ’2)(1+âˆ’ _[ g]Ïƒ[(][2][Î»])[B][)][ and][)][ for positive][ g][â€²][(][Î»][B][)][ <][ Î»][B][ 0][ takes at][ for][ Î»][ Î»]B[2][Î±]B[âˆ—]_ [=]âˆˆ

_âˆ’_ [1]

(4Î±l+22Î±ll+2âˆ’2)(1+lâˆ’2 _Ïƒ[2])_ 2Î± and

 2Î±l + 2l 2 2+ _Î±[1]_ _[âˆ’]_ _Î±l[1]_ 2Î±l + 2l 2 1+ _Î±[1]_ _[âˆ’]_ _Î±l[1]_

_g(Î»[âˆ—]B[) =][ âˆ’]_ _âˆ’_ (1 + Ïƒ[2]) + _âˆ’_ _Î·_

(4Î±l + 2l 2)(1 + Ïƒ[2]) (4Î±l + 2l 2)(1 + Ïƒ[2]) _âˆ’_

 _âˆ’_   _âˆ’_ 

2Î±l(2Î±l + 2l 2)[1+][ 1]Î± _[âˆ’]_ _Î±l[1]_
= _âˆ’_

(4Î±l + 2l âˆ’ 2)[2+][ 1]Î± _[âˆ’]_ _Î±l[1]_ (1 + Ïƒ[2])[1+][ 1]Î± _[âˆ’]_ _Î±l[1]_ _[âˆ’]_ _[Î·.]_

As long as Î· > 2Î±l(2Î±l+2lâˆ’2)[1+ 1]Î± _[âˆ’]_ _Î±l[1]_

(4Î±l+2lâˆ’2)[2+ 1]Î± _[âˆ’]_ _Î±l[1]_ (1+Ïƒ[2])[1+ 1]Î± _[âˆ’]_ _Î±l[1]_ _[,][ we know][ g][(][Î»][B][)][ <][ 0][ for any][ Î»][B][ >][ 0][, which]_

further implies that _Î»[Ë™]_ _B < 0 for any Î»B > 0. So Î»B converges to zero._


**Dynamics for Î»S :** We can write down the dynamics on Î»S as follows,

_Î»Ë™_ _S = lÎ»Sh(Î»S),_

where h(Î»S) = _Î»S[4][Î±][+2][âˆ’][2][/l]_ + Î»[2]S[Î±][+2][âˆ’][2][/l] _Î·. We compute the derivative of h as follows:_
_âˆ’_ _âˆ’_

_h[â€²](Î»S) = Î»[2]S[Î±][+1][âˆ’][2][/l]_ _âˆ’(4Î± + 2 âˆ’_ 2/l)Î»[2]S[Î±] [+ (2][Î±][ + 2][ âˆ’] [2][/l][)] _._
  


-----

[1] [1]
So h(Î»S) is increasing in (0, 24Î±lÎ±l+2+2llâˆ’22 2Î± ) and is decreasing in ( 24Î±lÎ±l+2+2llâˆ’22 2Î±, ). The maxi_âˆ’_ [1] _âˆ’_ _âˆ_

mum value of h for positive Î»S takes at Î»[âˆ—]S [=] 42Î±lÎ±l+2+2llâˆ’22 2Î± and we have 
_âˆ’_
  _Î±_ _Î±l_

_[âˆ’]_ [1]

_h(Î»[âˆ—]S[) = 2][Î±l][(2][Î±l][ + 2][l][ âˆ’]_ [2)][1+][ 1] _Î·._

(4Î±l + 2l âˆ’ 2)[2+][ 1]Î± _[âˆ’]_ _Î±l[1]_ _âˆ’_

As long as Î· < [2][Î±l](4[(2]Î±l[Î±l]+2[+2]lâˆ’[l][âˆ’]2)[2)][2+ 1][1+ 1]Î± _[âˆ’]Î±_ _[âˆ’]Î±l[1]_ _Î±l[1]_ _, we have h(Î»[âˆ—]S[)][ >][ 0][.][ Furthermore, since][ h][ is increasing in]_

(0, Î»[âˆ—]S[)][ and is decreasing in][ (][Î»]S[âˆ—] _[,][ âˆ][)][ and][ h][(0)][, h][(][âˆ][)][ <][ 0][,][ we know there exists][ Î»][âˆ’]S_ _S[)][, Î»][+]S_
(Î»[âˆ—]S[,][ âˆ][)][ such that][ h][(][Î»][S][)][ <][ 0][ in][ (0][, Î»][âˆ’]S [)][,][ h][(][Î»][S][)][ >][ 0][ in][ (][Î»]S[âˆ’][, Î»]S[+][)][ and][ h][(][Î»][S][)][ <][âˆˆ][ 0][(0][ in][, Î»][ (][âˆ—][Î»]S[+][,][ âˆ][âˆˆ][)][.]
Therefore, as long as Î´ _Î»[âˆ—]S_ _[> Î»]S[âˆ’][,][ we have][ Î»][S][ converges to][ Î»]S[+][.][ Since][ h][(1)][ <][ 0][,][ we know]_

[1] _â‰¥_
_Î»[+]S_ _[âˆˆ]_ [(] 24Î±lÎ±l+2+2llâˆ’âˆ’22 2Î±, 1).
 

Overall as long as Î· 2Î±l(2Î±l+2lâˆ’2)[1+ 1]Î± _[âˆ’]_ _Î±l[1]_ _Î±_ _[âˆ’]_ _Î±l[1]_ _, we know W_
_âˆˆ_  (4Î±l+2lâˆ’2)[2+ 1]Î± _[âˆ’]_ _Î±l[1]_ (1+Ïƒ[2])[1+ 1]Î± _[âˆ’]_ _Î±l[1]_ _[,][ 2][Î±l](4[(2]Î±l[Î±l]+2[+2][1]lâˆ’[l][âˆ’]2)[2)][2+ 1][1+ 1]Î±_ _[âˆ’]_ _Î±l[1]_ 

converges to cPS, where c is a positive number within ( 24Î±lÎ±l+2+2llâˆ’âˆ’22 2Î±, 1). â–¡
 

D ANALYSIS OF PREDICTOR REGULARIZATION.


In this section, we study the influence of predictor regularization in a simple linear setting. In
particular, we consider the same setting as in Section 3.2 except that we set Wp := (WW _[âŠ¤])[Î±]_ + ÏµI.

**Theorem 6. In the setting of Theorem 1 except that we set Wp = (WW** _[âŠ¤])[Î±]_ + ÏµI. We have



[1]
_âˆ’_ _Ïµ, 0_ 2Î±, we have W con



-  when Ïµ [0, [1+][âˆš]2[1][âˆ’][4][Î·] ), as long as Î´ > max 1âˆ’[âˆš]21âˆ’4Î·
_âˆˆ_ [1]

_verges to_ 1+[âˆš]21âˆ’4Î· _Ïµ_ 2Î± PS;  

_âˆ’_

 

-  when Ïµ 2 _, W always converges to zero._
_â‰¥_ [1+][âˆš][1][âˆ’][4][Î·]


**Proof of Theorem 6. We can write the dynamics of W as follows,**
_WË™_ =Wp[âŠ¤][(][âˆ’][W][p][W] [(][I][ +][ Ïƒ][2][P][B][) +][ W][a][)][ âˆ’] _[Î·W]_

2
=W (I + Ïƒ[2]PB) _W_ + ÏµI + _W_ + ÏµI _Î·_ _._
_âˆ’_ _|_ _|[2][Î±]_ _|_ _|[2][Î±]_ _âˆ’_
     

Let the eigenvalue decomposition of W be _i=1_ _[Î»][i][u][i][u]i[âŠ¤][,][ with span][(][{][u][d][âˆ’][r][+1][,][ Â· Â· Â·][, u][d][}][)][ equals to]_
subspace B. We can separately analyze the dynamics of each Î»i. Furthermore, we know Î»1, _, Î»r_
_Â· Â· Â·_
have the same value Î»S and Î»d _r+1,_ _, Î»d have the same value Î»B._
_âˆ’_ _Â· Â· Â·_ [P][d]

**Dynamics for Î»B:** We can write down the dynamics for Î»B as follows:

2

_Î»Ë™_ _B = Î»B_ _âˆ’(1 + Ïƒ[2])_ _|Î»B|[2][Î±]_ + Ïµ + _|Î»B|[2][Î±]_ + Ïµ _âˆ’_ _Î·_

When Î· > 4(1+1Ïƒ[2]) _[,][ we still know]_ [ Ë™]Î»B < 0 for any Î»B > 0 and _Î»B = 0 is a critical point. So_ _Î»B_

converges to zero.

**Dynamics for Î»S:** We can write down the dynamics for Î»S as follows:

2

_Î»Ë™_ _S =Î»S_ _âˆ’_ _|Î»S|[2][Î±]_ + Ïµ + _|Î»S|[2][Î±]_ + Ïµ _âˆ’_ _Î·_

     

= _Î»S_ _Î»S_ + Ïµ _Î»S_ + Ïµ _,_
_âˆ’_ _|_ _|[2][Î±]_ _âˆ’_ [1][ âˆ’âˆš]2[1][ âˆ’] [4][Î·] _|_ _|[2][Î±]_ _âˆ’_ [1 +][ âˆš]2[1][ âˆ’] [4][Î·]

   

where the second inequality assumes 0 < Î· < [1]4 _[.][ We have]_


-----

[1]
_âˆ’_ _Ïµ, 0_ 2Î±, we have Î»S con



-  when Ïµ [0, [1+][âˆš]2[1][âˆ’][4][Î·] ), as long as Î´ > max 1âˆ’[âˆš]21âˆ’4Î·
_âˆˆ_ [1]

1+[âˆš]1 4Î· 2Î±  
verges to 2 _âˆ’_ _Ïµ_ _> 0;_

_âˆ’_

 

-  when Ïµ 2 _, Î»S always converges to zero._
_â‰¥_ [1+][âˆš][1][âˆ’][4][Î·]

E TECHNICAL TOOLS

E.1 NORM OF RANDOM VECTORS


The following lemma shows that a standard Gaussian vector with dimension n has â„“2 norm concentrated at _n._

_[âˆš]_

**Lemma 8 (Theorem 3.1.1 in Vershynin (2018)). Let X = (X1, X2, Â· Â· Â·, Xn) âˆˆ** R[n] _be a random_
_vector with each entry independently sampled from N_ (0, 1). Then

Pr[ _âˆ¥xâˆ¥âˆ’_ _[âˆš]n_ _â‰¥_ _t] â‰¤_ 2 exp(âˆ’t[2]/C [2]),

_where C is an absolute constant._

E.2 SINGULAR VALUES OF GAUSSIAN MATRICES

The following lemma shows a tall random Gaussian matrix is well-conditioned with high probability.

**Lemma 9 (Corollary 5.35 in Vershynin (2010)). Let A be an N Ã— n matrix whose entries are**
_independent standard normal random variables. Then for every t â‰¥_ 0 with probability at least
1 âˆ’ 2 exp(âˆ’t[2]/2) one has
_âˆšN_ _n_ _t_ _smin(A)_ _smax(A)_ _âˆšN +_ _n + t_

_âˆ’_ _[âˆš]_ _âˆ’_ _â‰¤_ _â‰¤_ _â‰¤_ _[âˆš]_

E.3 PERTURBATION BOUND FOR MATRIX PSEUDO-INVERSE

With a lowerbound on Ïƒmin(A), we can get bounds for the perturbation of pseudo-inverse.

**Lemma 10 (Theorem 3.4 in Stewart (1977)). Consider the perturbation of a matrix A âˆˆ** R[m][Ã—][n] :
_B = A + E. Assume that rank(A) = rank(B) = n, then_
_B[â€ ]_ _âˆ’_ _A[â€ ]_ _â‰¤_ _âˆš2_ _A[â€ ]_ _B[â€ ]_ _âˆ¥Eâˆ¥_ _._

The following corollary is particularly useful for us.

**Lemma 11 (Lemma G.8 in Ge et al. (2015)). Consider the perturbation of a matrix A âˆˆ** R[m][Ã—][n] :
_B = A + E where_ _E_ _Ïƒmin(A)/2. Assume that rank(A) = rank(B) = n, then_
_âˆ¥_ _âˆ¥â‰¤_
_B[â€ ]_ _A[â€ ]_ 2âˆš2 _E_ _/Ïƒmin(A)[2]._
_âˆ’_ _â‰¤_ _âˆ¥_ _âˆ¥_


-----

