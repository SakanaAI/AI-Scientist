# ON THE GENERALIZATION OF WASSERSTEIN ROBUST FEDERATED LEARNING

**Anonymous authors**
Paper under double-blind review

ABSTRACT

In Federated Learning (FL), participating clients typically possess non-i.i.d. data,
posing a significant challenge to generalization to unseen distributions. To address
this, we propose a Wasserstein distributionally robust optimization scheme called
WAFL. Leveraging its duality, we frame WAFL as an empirical surrogate risk
minimization problem, and solve it using a novel local SGD-based algorithm with
convergence guarantees. We show that the robustness of WAFL is more general
than related approaches, and the generalization bound is robust to all adversarial
distributions inside the Wasserstein ball (ambiguity set). Since the center location and radius of the Wasserstein ball can be suitably modified, WAFL shows its
applicability not only in robustness but also in domain adaptation. Through empirical evaluation, we demonstrate that WAFL generalizes better than the vanilla
FedAvg in non-i.i.d. settings, and is more robust than other related methods in
distribution shift settings. Further, using benchmark datasets we show that WAFL
is capable of generalizing to unseen target domains.

1 INTRODUCTION

Federated Learning (FL) (KoneÀácn¬¥y et al., 2016; McMahan et al., 2017) has emerged as a cuttingedge technique in distributed and privacy-preserving machine learning. The nature of highly nonindependent and identically distributed (non-i.i.d.) data in clients‚Äô devices poses an important challenge to FL commonly called statistical heterogeneity. The global model trained on this data using
the de facto FedAvg algorithm (McMahan et al., 2017) has been shown to generalize poorly to
individual clients‚Äô data, and further to unseen distributions on new clients as they enter the network.

Several solutions to data heterogeneity have been proposed. Personalized FL (Fallah et al., 2020;
Deng et al., 2020a; Dinh et al., 2021; Li et al., 2021) and multi-task FL (Smith et al., 2018) are
_client-adaptive approaches, where a personalized model is adapted to each client. From another_
perspective, distributionally robust FL trains a model using a worst-case objective over an ambiguity
_set (Mohri et al., 2019; Du et al., 2020; Reisizadeh et al., 2020; Deng et al., 2020b). This approach_
is client-uniform because a single global model is judiciously learned to deliver uniformly good
performance not only for all training clients but also for new/unseen clients with unknown data distributions. It is specifically useful when test distributions drift away from the training distributions.

A natural question when designing distributionally robust FL frameworks is generalization: how can
minimizing the training error also bound the test error (generalization bounds)? In FL, Mohri et al.
(2019) proposed agnostic FL where a model is designed to be robust against any ambiguity set as a
convex combination of the clients‚Äô distributions. Reisizadeh et al. (2020) applied the general affine
covariate shift ‚Äì used in the standard adversarial robust training ‚Äì into FL training. In characterizing
the generalization bounds, while Mohri et al. (2019) relied on the standard Rademacher complexity,
Reisizadeh et al. (2020) use the margin-based technique developed by Bartlett et al. (2017).

In this work, we take a different approach called WAsserstein distributionally robust FL (WAFL for
short). The ambiguity set in WAFL is a Wasserstein ball of all adversarial distributions in close
proximity to the nominal data distribution at the center. Our main contributions are:

-  We propose a distributionally robust optimization problem to address statistical heterogeneity in FL. By controlling the center and radius of the Wasserstein ball, we show that WAFL
is robust to a wider range of adversarial distributions than is agnostic or adversarial FL.


-----

-  To make WAFL more amenable to distributed optimization, we transform the original problem into a minimization of the empirical surrogate risk. We propose a local SGD-based algorithm to solve this surrogate problem. With additional Lipschitz smoothness conditions,
standard techniques can be applied to find the convergence rate for the proposed algorithm.

-  We show how WAFL‚Äôs output can reduce the test error by bounding its excess risk. We call
this the robust generalization bound as it is applicable to all adversarial distributions inside
the Wasserstein ball. By scaling the Wasserstein radius based on local data sizes, we show
this bound is applicable to the true (unknown) data distribution among all clients.

-  We show WAFL‚Äôs extra flexibility in controlling the location of the Wasserstein ball by
adjusting the nominal distribution. This enables applications such as multi-source domain
adaptation and robustness to all clients‚Äô unknown distributions with minimal Wasserstein
radius.

-  Experimentally, we discuss how to control this radius for a given center location by finetuning a robust hyperparameter. We show that WAFL generalizes better than FedAvg in
non-i.i.d. settings and further outperforms existing robust methods in distribution shift settings. We finally explore WAFL‚Äôs capability in transferring knowledge from multi-source
domains to related target domains with much less data and/or without labels.

2 RELATED WORK

**Federated Learning was introduced in response to three challenges of machine learning at scale:**
massive data quantities at the edge, communication-critical networks of participating devices, and
privacy-preserving learning without central data storage (KoneÀácn¬¥y et al., 2016; McMahan et al.,
2017). The de facto federated optimization algorithm ‚Äì FedAvg (McMahan et al., 2017) ‚Äì is based
on local stochastic gradient descent (SGD) and averaging and is often considered a baseline in FL.

Most challenges of FL are categorized into systems heterogeneity and statistical heterogeneity. The
former focuses on communication problems such as connection loss and bandwidth minimization.
This motivated some prior works to design more communication-efficient methods (KoneÀácn¬¥y et al.,
2016; 2017; Suresh et al., 2017). On the other hand, statitical heterogeneity is concerned with
clients‚Äô non-i.i.d. data, which is the main cause behind aggregating very different models leading to
one which does not perform well on any data distribution. To address this, many ideas have been
introduced. Li et al. (2020) provided much theoretical analysis of FL non-i.i.d. settings. Zhao et al.
(2018) proposed an FL framework which globally shares a small subset of data among clients to train
the model with non-i.i.d. data. Smith et al. (2018) introduced a multi-task FL framework in which
each client individually learns its own data pattern while borrowing information from other clients.
Mansour et al. (2020) proposed three approaches to adapt the FL model to enable personalization, in
reponse to distribution shift. Several personalized FL models have also been developed, including
Fallah et al. (2020); Deng et al. (2020a); Dinh et al. (2021); Li et al. (2021).

**Wasserstein Distributionally Robust Optimization (WDRO) aims to lean a robust model against**
adversarially manipulated data. The unknown data distribution is assumed to lie within a Wasserstein
ball centered around the empirical distribution (Kuhn et al., 2019). WDRO has received attention as
a promising tool for training parametric models, both in centralized and federated learning settings.

In centralized learning, many studies have proposed solutions based on WDRO problems for certain machine learning tasks (Shafieezadeh Abadeh et al., 2015; Gao & Kleywegt, 2016; Esfahani
& Kuhn, 2017; Chen & Paschalidis, 2018; Sinha et al., 2020; Blanchet et al., 2019; ShafieezadehAbadeh et al., 2019; Gao et al., 2020). For instance, Shafieezadeh Abadeh et al. (2015) considered
a robust logistic regression model under the assumption that the probability distributions lie in a
Wasserstein ball. Chen & Paschalidis (2018); Blanchet et al. (2019); Gao et al. (2020) leveraged
WDRO to recover regularization formulations in classification and regression. Gao & Kleywegt
(2016) proposed a minimizer based on a tractable approximation of the local worst-case risk. Esfahani & Kuhn (2017) used WDRO to formulate the search for the largest perturbation range as an
optimization problem and solve its dual problem. Sinha et al. (2020) introduced a robustness certificate based on a Lagrangian relaxation of the loss function which provably robust against adversarial
input distributions within a Wasserstein ball centered around the original input distribution.


-----

In FL, only a few works have explored the Wasserstein distance (Reisizadeh et al., 2020; Diamandis
et al., 2021). Specially, Reisizadeh et al. (2020) proposed FedRobust based on adversarial robust
training to enhance robustness. Regarding distributionally robust learning, Deng et al. (2020b) proposed DRFA, a communication efficient distributed algorithm. Besides, based on the agnostic FL
framework suggested by Mohri et al. (2019), Du et al. (2020) introduced AgnosticFair, a two-player
adversarial minimax game between the learner and the adversary, to achieve fairness.

3 WASSERSTEIN ROBUST FEDERATED LEARNING

3.1 EXPECTED RISK AND EMPIRICAL RISK MINIMIZATION IN FEDERATED LEARNING

In a federated setting, there are m clients, and each client i ‚àà [m] := {1, . . ., m} has its data generating distribution Pi supported on domain Zi := (Xi, Yi). Consider the parametrized hypothesis
class H = _hŒ∏ | Œ∏ ‚àà_ R[d], where each member hŒ∏ is a mapping from Xi to Yi parametrized by
_Œ∏. With zi := (xi, yi)_ _i, we use ‚Ñì(zi, hŒ∏), shorthand for ‚Ñì(yi, hŒ∏(xi)), to represent the cost of_
 _‚ààZ_
predictingsquare loss h ‚ÑìŒ∏((zxi, hi) when the ground-truth label isŒ∏) = ‚Ñì(yi, hŒ∏(xi)) = (Œ∏[T]xi _y yi)i[2]. For example, ifcan be considered. In FL, all clients collaborate hŒ∏(xi) = Œ∏[T]xi and yi ‚àà_ R, a
_‚àí_
with a server to find a global model Œ∏ such that the following sum weighted risk is minimized:


_ŒªiEZi‚àºPi_ _‚Ñì(Zi, hŒ∏)_ _,_ (1)
_i=1_

X  


min
_Œ∏‚ààR[d]_


where EZi‚àºPi _‚Ñì(Zi, hŒ∏)_ is client i‚Äôs expected risk and Œªi ‚â• 0 represents the relative ‚Äúweight‚Äù
of client i and _i=1_ _[Œª][i][ = 1][.]_ Therefore, Œª := [Œª1, . . ., Œªm][‚ä§] belongs to a simplex ‚àÜ:=
_Œª ‚àà_ R[m] : Œª ‚âΩ 0 and Œª[‚ä§]1m = 1 . Define by PŒª := _i=1_ _[Œª][i][P][i][ the mixed clients‚Äô distribution]_
over m domains [P] :=[m] 1, . . ., _m_ . We denote by Z _PŒª a random data point Z generated by_
 _Z_ _{Z_ _Z_ _}_ _‚àº_
_PŒª, which means that the domain of client i is chosen with probability[P][m]_ **P(Z = Zi) = Œªi first, then**
a data point zi ‚ààZi is selected with probability P(Zi = zi), Zi ‚àº _Pi._

While the underlying distributions[ni] _PŒª. We abuse the notation [ Pnii] are unknown, clients have access to finite observations to denote the set of client i‚Äôs both observable data points and zi ‚àà_
their indexes. Let ‚àº _Pni :=_ _n1i_ _zi_ [ni] _[Œ¥][z]i_ [be the empirical distribution of][ P][i][, where][ Œ¥][z]i [is the Dirac]

_‚àà_
point mass at zi. In general, we use the notation for quantities that are dependent on the training

P

data. Define by _PŒª[b] :=_ _i=1_ _[Œª][i][ b]Pni the mixed empirical distribution of n =_ _i=1_ _[n][i][ training data]_
from m clients. The empirical risk minimization (ERM) problem of (1) is as follows: b

_m_ _m_

[b] [P][m] _ni_ 1 [P][m]

_Œ∏min‚ààR[d]EZ‚àºPŒª_ _‚Ñì(Z, hŒ∏)_ = _i=1_ _ŒªiEZi‚àºPni_ _‚Ñì(Zi, hŒ∏)_ = _i=1_ _n_  _ni_ _zi_ [ni] _‚Ñì(zi, hŒ∏),_ (2)

b   X b   X X‚àà

where Œªi = ni/n is often chosen in ERM of the standard FL (McMahan et al., 2017).

3.2 WASSERSTEIN ROBUST RISK IN FEDERATED LEARNING


Models resulting from (2) have been shown to be vulnerable to adversarial attacks and to lack of
robustness to distribution shifts. We consider a robust variant of the ERM framework, involving the
worst-case risk with respect to the p-Wasserstein distance between two probability measures. Given
a set Z, define d : Z √ó Z ‚Üí [0, ‚àû) to be the cost of ‚Äútransportation‚Äù between its two points.[1]
Suppose P and Q are two distributions on Z. Let Œ†(P, Q) be the set of probability measures œÄ on
_Z √ó Z whose marginals are P and Q, called their couplings. In other words, œÄ(A, Z) = P_ (A) and
_œÄ(Z, A) = Q(A), ‚àÄA ‚äÇZ. The p-Wasserstein distance between P and Q is defined as_

1/p

_Wp(P, Q) =_ _œÄ‚ààŒ†(infP,Q)_ **E(Z,Z‚Ä≤)‚àºœÄ** _d[p](Z, Z_ _[‚Ä≤])_ _._ (3)

   

This distance represents the minimum cost of transporting one distribution to another, where the
cost of moving a unit point mass is determined by the ground metric on the space of uncertainty
realizations. In this work, we mainly work with p = 2. Let _p(P, œÅ) :=_ _Q : Wp(P, Q)_ _œÅ_
_B_ _{_ _‚â§_ _}_

1The function d must satisfy non-negativity, lower semi-continuity and d(z, z) = 0, ‚àÄz ‚ààZ.


-----

denote the Wasserstein ball centered at P (i.e., nomimal distribution) and having radius œÅ ‚â• 0. We
modify (2) into the following Wasserstein robust risk minimization in FL problem:

WAFL: _Œ∏min‚ààR[d]_ supQ‚ààB(PŒª,œÅ) **[E][Z][‚Ä≤][‚àº][Q]** _‚Ñì(Z_ _[‚Ä≤], hŒ∏)_ _._ (4)

There are several merits to this framework. First, then b _ambiguity set_ _p(PŒª[o], œÅ) contains all (contin-_
_B_
uous or discrete) distributions Q that can be converted from the (discrete) nominal distribution _PŒª_
at a bounded transportation cost œÅ. Second, Wasserstein distances can be approximated from the[b]
samples. Based on the non-asymptotic convergence results of Fournier & Guillin (2015), we can

[b]
specify a suitable value for œÅ to probabilistically bound Wp(P, Q) by the distance between their
empirical distributions Wp(P, _Q) (e.g., for multi-source domain adaptation)._

In any robust optimization problem, the ambiguity set is a key ingredient to defining the level of
robustness. We will compare[b] WAFL[b] in (4) with other approaches in terms of their ambiguity set.

**Agnostic** **FL.** Using this approach, existing techniques (Mohri et al., 2019; Deng et al., 2020b) minimize the
worst-case loss
maxŒª ‚àÜ **[E][Z][‚àº]P[b]Œª** _‚Ñì(Z, hŒ∏)_ _,_
_‚àà_

hence its distributional ambiguity set is _Q_ ‚àÜ := _PŒª : Œª ‚àà_
‚àÜ _. While Agnostic FL‚Äôs ambiguity set is the static convex_
hull of _Pni_ _i_ [m][,][ WAFL][‚Äôs ambiguity set][ B][(][ b]PŒª b, œÅ) can be

_‚àà_
adjusted by controlling the robustness level œÅ and by position
Figure 1: Example of four FL clients

ing the ball center using b _Œª, which is useful for domain adapta-_

with data distributions P1, . . ., P4. The

tion. Therefore, B(PŒª, œÅ) can cover Q‚àÜ by using appropriate shaded area (Agnostic FL‚Äôs ambiguity
values for œÅ and Œª (see Fig. 1). set) is covered by the blue ball with ra
**Adversarial robust FL.[b]** Using this approach, Reisizadeh dius œÅ and centered at _PŒª (Wasserstein_

ambiguity set). For domain adaptation

et al. (2020) combines a general affine covariate shift in stan
with Q as a target domain, the nom
dard adversarial robust training with FL. Most existing tech- [b]

inal distribution (multi-source domain)

niques using this approach (Goodfellow et al., 2015; Kurakin
et al., 2017; Carlini & Wagner, 2017; Madry et al., 2019; is shifted to _PŒª‚Ä≤ such that W2(PŒª‚Ä≤_ _, Q)_

is minimal.

Tram`er et al., 2020) define an adversarial perturbation u at

[b] [b]

a data point Z, and minimize the worst-case loss over all perturbations: maxu‚ààU EZ‚àºPŒª _‚Ñì(Z +_
_u, hŒ∏)_, where the ambiguity set is := _u_ R[d][+1] : _u_ _œµ_ . In Appendix A, we show that
_U_ _‚àà_ _‚à•_ _‚à•‚â§_ b 
the Wasserstein ambiguity set also contains the perturbation points induced by the solution to this
 
adversarial robust training problem.

3.3 WAFL: ALGORITHM DESIGN AND CONVERGENCE ANALYSIS


The original form of WAFL in (4) is not friendly for distributed algorithm design. Fortunately, the
_Wasserstein robust risk (or_ (PŒª, œÅ)-worst-case risk) has its dual formulation as follows (Gao &
_B_
Kleywegt, 2016; Sinha et al., 2020)
_Q_ sup(PŒª,œÅ) **EZ[‚Ä≤][b]‚àºQ** _‚Ñì(Z_ _[‚Ä≤], hŒ∏)_ = infŒ≥‚â•0 _Œ≥œÅ[2]_ + EZ‚àºPŒª _œÜŒ≥(Z, Œ∏)_ _,_ (5)
_‚ààB_
   b  

where œÜŒ≥(zi, Œ∏) := sup[b] _Œ∂_ _‚Ñì(Œ∂, hŒ∏)_ _Œ≥d[2](Œ∂, zi)_, and d[2](z, z[‚Ä≤]) = _x_ _x[‚Ä≤]_ + Œ∫ _y_ _y[‚Ä≤]_ _, Œ∫ >_
_‚ààZ_ _‚àí_ _‚à•_ _‚àí_ _‚à•[2]_ _|_ _‚àí_ _|[2]_
0. The crux of using the dual is that the inner supremum problem (finding œÜŒ≥) is easily solvable
when its objective is well-conditioned: if _‚Ñì_ is L-smooth and _d is 1-strongly convex, setting Œ≥ > L_
ensures that Œ∂ _‚Ñì(Œ∂, hŒ∏)_ _Œ≥d[2](Œ∂, z) is strongly concave, and using gradient ascent for the inner_
_7‚Üí_ _‚àí_
supremum problem (for finding œÜŒ≥) ensures linear convergence. For each zi let zi[‚àó] [be the solution to]
supŒ∂ _‚Ñì(Œ∂, hŒ∏)_ _Œ≥d[2](Œ∂, zi)_ . Then, _Œ∏œÜŒ≥(zi, Œ∏) =_ _Œ∏‚Ñì(zi[‚àó][, h][Œ∏][)][ (Sinha et al., 2020, Lemma 1).]_
_‚ààZ_ _‚àí_ _‚àá_ _‚àá_

Rather than find the optimal _Œ≥ in (5), we consider Œ≥ as a control parameter, and instead solve the_
following client-decomposable problem, which is amenable to distributed algorithm design.

_m_

_Œ∏min‚ààR[d]EZ‚àºPŒª_ _œÜŒ≥(Z, Œ∏)_ = _i=1_ _ŒªiEZi‚àºPni_ _œÜŒ≥(Zi, Œ∏)_ _._ (6)

b   X b  []


-----

**Algorithm 1: Local SGD for WAFL**

1: for t = 0, 1, ..., T ‚àí 1 do // Global rounds



[P] [P]

|2: 3: 4: 5: 6: 7: 8: 9:|Sample a subset of clients S ‚äÇm t for client i ‚ààS in parallel do t Set local parameters: Œ∏(t,0) = Œ∏t // Communication i for k = 0, 1, ..., K ‚àí1 do // Local rounds Sample a mini-batch Di from client i‚Äôs dataset Œ∏ i(t,k+1) = Œ∏ i(t,k) ‚àí |DŒ∑ ziP ‚ààDi‚àáŒ∏œÜ(z i, Œ∏ i(t,k)) i| Send Œ∏(t,K)to server // Communication i Server: update Œ∏(t+1) =P Œª Œ∏(t,K)/P Œª i‚ààSt i i i‚ààSt i|
|---|---|


This motivates the development of Algorithm 1 for solving (6). The structure of WAFL is similar
to FedAvg with T communication rounds. We further note three key components of Algorithm 1
common to FL methods. First, client sampling (line 2) refers to the partial participation of clients
in each global round. Second, each client performs K local steps (line 5) before sending its local
model to the server,. Finally, stochastic approximation of a client‚Äôs gradient using a mini-batch
(lines 6 and 7) is necessary when the data size is large. The main difference between WAFL and
FedAvg is that WAFL aims to minimize the risk with respect to the surrogate loss œÜŒ≥, rather than ‚Ñì.

We show that the convergence of WAFL can be similarly characterized as that of FedAvg, the de
facto FL algorithm based on local SGD updates (McMahan et al., 2017). In FedAvg optimization,
we seek to establish the convergence when using the original loss function ‚Ñì. On the other hand, in
WAFL the convergence is with respect to the surrogate loss œÜŒ≥, through which the local and global
risks are defined by Fi(Œ∏) := EZi‚àºPi _œÜŒ≥(Zi, Œ∏)_ and F (Œ∏) := EZ‚àºPŒª _œÜŒ≥(Z, Œ∏)_, respectively.

We first make the following assumptions, common to analyses of Wasserstein-robust optimiza-   
tion (Sinha et al., 2020). Unless stated otherwise, all norms are the Euclidean norm.
**Assumption 1. The function d : Z √ó Z ‚Üí** R+ is continuous, and d(¬∑, z0) is 1-strongly convex,
_z0_ _._
_‚àÄ_ _‚ààZ_
**Assumption 2. The loss function ‚Ñì** : Z √ó H ‚Üí R is Lipschitz continuous as follows

(a) _‚Ñì(z, hŒ∏)_ _‚Ñì(z[‚Ä≤], hŒ∏)_ _Lz_ _z_ _z[‚Ä≤]_ _,_ (b) _‚Ñì(z, hŒ∏)_ _‚Ñì(z, hŒ∏‚Ä≤_ ) _LŒ∏_ _Œ∏_ _Œ∏[‚Ä≤]_ _._
_‚à•_ _‚àí_ _‚à•‚â§_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚àí_ _‚à•‚â§_ _‚à•_ _‚àí_ _‚à•_

**Assumption 3. The loss function ‚Ñì** : Z √ó H 7‚Üí R is Lipschitz smooth as follows

_Œ∏‚Ñì(z, hŒ∏)_ _Œ∏‚Ñì(z, hŒ∏‚Ä≤_ ) _LŒ∏Œ∏_ _Œ∏_ _Œ∏[‚Ä≤]_ _,_ _z‚Ñì(z, hŒ∏)_ _z‚Ñì(z[‚Ä≤], hŒ∏)_ _Lzz_ _z_ _z[‚Ä≤]_ _,_
_‚à•‚àá_ _‚àí‚àá_ _‚à•‚â§_ _‚à•_ _‚àí_ _‚à•_ _‚à•‚àá_ _‚àí‚àá_ _‚à•‚â§_ _‚à•_ _‚àí_ _‚à•_

_Œ∏‚Ñì(z, hŒ∏)_ _Œ∏‚Ñì(z[‚Ä≤], hŒ∏)_ _LŒ∏z_ _z_ _z[‚Ä≤]_ _,_ _z‚Ñì(z, hŒ∏)_ _z‚Ñì(z, hŒ∏‚Ä≤_ ) _LzŒ∏_ _Œ∏_ _Œ∏[‚Ä≤]_ _._
_‚à•‚àá_ _‚àí‚àá_ _‚à•‚â§_ _‚à•_ _‚àí_ _‚à•_ _‚à•‚àá_ _‚àí‚àá_ _‚à•‚â§_ _‚à•_ _‚àí_ _‚à•_

Given Assumption 3, it has been shown that the mapping Œ∏ _œÜŒ≥(_ _, Œ∏) is L-smooth with L =_
_7‚Üí_ _¬∑_
_LŒ∏Œ∏ +_ _[L]Œ≥[Œ∏z]L[L]zz[zŒ∏]_ _[, Œ≥ > L][zz][ (Sinha et al., 2020) (more detail in Appendix Lemma 2). In addition, we]_

_‚àí_
make the following assumptions common to FL analysis (Wang et al., 2021).
**Assumption 4. The unbiased stochastic approximation of** _Fi(Œ∏), denoted by gœÜi_ (Œ∏) :=
_‚àá_

_Œ∏œÜŒ≥(zi, Œ∏), zi_ _Pi, has œÉ[2]-uniformly bounded variance, i.e., E_ _gœÜi_ (Œ∏) _Fi(Œ∏)_ _œÉ[2]._
_‚àá_ _‚àº_ _‚à•_ _‚àí‚àá_ _‚à•[2][i]_ _‚â§_

**Assumption 5. The difference between the local gradient ‚àáFi(Œ∏h) and the global gradient ‚àáF** (Œ∏)
_is ‚Ñ¶-uniformly bounded, i.e., maxi supŒ∏_ _Fi(Œ∏)_ _F_ (Œ∏) ‚Ñ¶.
_‚à•‚àá_ _‚àí‚àá_ _‚à•‚â§_

Assuming complete participation of clients in every round (St = m, ‚àÄt), using standard techniques
in Wang et al. (2021), we have:
**Theorem 1 (WAFL‚Äôs convergence for convex loss function). Let Assumptions 1‚Äì5 hold and the**
_mapping Œ∏_ _‚Ñì(z, hŒ∏) be convex. Denote by_ _Œ∏[¬Ø][(][t,k][)]_ _the ‚Äúshadow‚Äù sequence, defined as_ _Œ∏[¬Ø][(][t,k][)]_ =
_m_ _7‚Üí_
_i=1_ _[Œª][i][Œ∏]i[(][t,k][)]_ _and by Œ∏[‚àó]_ _the optimal solution to minŒ∏‚ààRd F_ (Œ∏). If the client learning rate satisfies

2 2

P 1 _D_ _D_ 3 _D_ 3

_Œ∑_ min _,_ 1 2 1 1 2 1 1 1 2 _,_
_‚â§_ 3L _[,]_ 2‚àöKT ŒõœÉ 48 3 K 3 T 3 L 3 ‚Ñ¶[¬Ø] 3 _[,]_ 40 3 KT 3 L 3 ‚Ñ¶[¬Ø] 3
 

b


-----

_then we have_


1 _T ‚àí1_ _K‚àí1_ _LD2_ _œÉDŒõ_ 21 13 ‚Ñ¶[¬Ø] 32 D 34 13 ‚Ñ¶[¬Ø] 23 D
E _F_ (Œ∏[¬Ø][(][t,k][)]) _F_ (Œ∏[‚àó]) + _[L]_ 1 2 + _[L]_ 2
 _KT_ _t=0_ _k=0_ _‚àí_  _‚â§O_ _KT_ [+][ b]‚àöKT _K_ 3 T 3 _T_ 3

X X

_œÉ[2]_
_where ÀÜœÉ[2]_ := _|Di|_ _[,][ D][ :=][ ‚à•][Œ∏][(0)][ ‚àí]_ _[Œ∏][‚àó][‚à•]_ _[and][ Œõ :=][ P]i[m]=1_ _[Œª]i[2][.]_

We provide a proof of Theorem 1 in Appendix C.


1

_KT_




_T ‚àí1_

_t=0_

X


(7)


4 ROBUST GENERALIZATION BOUNDS

We show the generalization and robustness properties of WAFL‚Äôs output by bounding its excess risk.
Denote the loss class by := ‚Ñì := _z_ _‚Ñì(z, h), h_, where we use f (resp. fŒ∏) to
_F_ _‚ó¶H_ _{_ _7‚Üí_ _‚ààH}_ _‚ààF_
represent a generic loss (resp. a loss function parametrized by Œ∏).
**Definition 1. Denote the expected risk and surrogate of Wasserstein robust risk of an arbitrary f**,
respectively, as follows

L(PŒª, f ) := EZ‚àºPŒª _‚Ñì(Z, h)_ and L[Œ≥]œÅ[(][P][Œª][, f] [) :=][ E][Z][‚àº][P]Œª _œÜŒ≥(Z, f_ ) + Œ≥œÅ[2].

Then their excess risks are defined respectively as follows  h i

E(PŒª, f ) := L(PŒª, f ) inf and E[Œ≥]œÅ[(][P][Œª][, f] [) :=][ L]œÅ[Œ≥][(][P][Œª][, f] [)][ ‚àí] [inf] _œÅ[(][P][Œª][, f][ ‚Ä≤][)][.]_
_‚àí_ _f_ _[‚Ä≤]‚ààF_ [L][(][P][Œª][, f][ ‚Ä≤][)] _f_ _[‚Ä≤]‚ààF_ [L][Œ≥]

If a distribution Q is in the ambiguity set (PŒª, œÅ), we can bound its excess risk E(Q, f ) as follows.
_B_
**Lemma 1. Let Assumption 2 (a) holds and Œ≥ ‚â•** _Lz/œÅ. For all f ‚ààF and for all Q ‚ààB(PŒª, œÅ),_

E[Œ≥]œÅ[(][P][Œª][, f] [)][ ‚àí] _[g][(][œÅ, Œ≥][)][ ‚â§]_ [E][(][Q, f] [)][ ‚â§] [E]œÅ[Œ≥][(][P][Œª][, f] [) +][ g][(][œÅ, Œ≥][)][,]

_where g(œÅ, Œ≥) := 2LzœÅ + |Œ≥ ‚àí_ _Œ≥[‚àó]|œÅ[2], and Œ≥[‚àó]_ := arg minŒ≥‚Ä≤‚â•0 LœÅ[Œ≥][‚Ä≤] [(][P][Œª][, f] [)][.]

We provide a proof of Lemma 1 in Appendix D.
**Remark 1. Lemma 1 shows that the lower and upper bounds for E(Q, f** ) can be analyzed using
E[Œ≥]œÅ[(][P][Œª][, f] [)][ and a two-component error term][ g][(][œÅ, Œ≥][)][. The first component,][ 2][L][z][œÅ][, says that when][ œÅ]
is increased ‚Äì to allow for larger Wasserstein distance between the nominal PŒª and any worst-case
distribution Q ‚Äì the difference between the excess risks E(Q, f ) and E[Œ≥]œÅ[(][P][Œª][, f] [)][ increases, and this]
error is amplified at most by the Lipschitz constant Lz of the mapping z 7‚Üí _‚Ñì(z, ¬∑). The second_
component, |Œ≥ ‚àí _Œ≥[‚àó]|œÅ[2], addresses the sub-optimality error of a chosen value of Œ≥, which is amplified_
when Œ≥ is drifted away from the optimal Œ≥[‚àó]. Note that L[Œ≥]œÅ[‚àó] [(][P][Œª][, f] [)][ is the same as][ B][(][P][Œª][, œÅ][)][-worst-]
case risk thanks to the strong duality (5), which is obtained with œÅ > 0.

Denote by _Œ∏[œµ]_ Œò an Œµ-minimizer to the surrogate ERM, i.e., EZ _PŒª_ _œÜŒ≥(Z, fŒ∏[Œµ]_ [)]
_‚àà_ _‚àº_ _‚â§_
inf _Œ∏‚ààŒò EZ‚àºPŒª_ _œÜ(Z, fŒ∏)_ + Œµ, where Œò ‚äÇ R[d] is a parameter class, we obtain the following result.b  b 

[b]

**Theorem 2 (Robust Generalization Bounds)b**   **. Let Assumptions 2 and 3 hold, Œ≥** max _Lzz, Lz/œÅ_ _,_
_‚â•_
_and_ _‚Ñì(z, h)_ _M‚Ñì. We have the following result for all Q_ (PŒª, œÅ)
_|_ _| ‚â§_ _‚ààB_ 


48C(Œò)
+ 2M‚Ñì
_‚àöni_


2 log(2m/Œ¥)

_ni_


E(Q, fŒ∏[Œµ] [)][ ‚â§] _Œªi_ + 2M‚Ñì + Œµ + g(œÅ, Œ≥)

_i=1_ " _‚àöni_ s _ni_ #

b X

_with probability at least 1_ _Œ¥, where C(Œò) := LŒ∏_ _‚àû0_ log (Œò, Œò, œµ)dœµ and (Œò, Œò, œµ)
_‚àí_ _N_ _‚à•¬∑ ‚à•_ _N_ _‚à•¬∑ ‚à•_

_denotes the œµ-covering number of Œò w.r.t a metric_ Œò as the norm on Œò.

R ‚à•¬∑ ‚à•p

We provide a proof of Theorem 2 in Appendix E. We sketch the proof as follows: first bound
E[Œ≥]œÅ[(][P][Œª][, f]Œ∏[b][Œµ] [)][ using standard excess risk decomposition and uniform convergence with Rademacher]
complexity. Then leverage the upper-bound of Lemma 1 to bound E(Q, f ), _Q_ (PŒª, œÅ), based
_‚àÄ_ _‚ààB_
on the bound of E[Œ≥]œÅ[(][P][Œª][, f]Œ∏[b][Œµ] [)][. The result shows that using][ WAFL][ to minimize the surrogate of]
Wasserstein robust empirical risk also controls the robustness and generalization. As an example


-----

_H =_ _‚ü®Œ∏, ¬∑‚ü©_ _, Œ∏ ‚àà_ Œò with Œò = _Œ∏ ‚àà_ R[d] : ‚à•Œ∏‚à•2 ‚â§ _C_ . The diameter of Œò is supŒ∏,Œ∏‚Ä≤‚ààŒò‚à•Œ∏ ‚àí _Œ∏[‚Ä≤]‚à•_ =
2C, thus (Œò, 2, œµ) = (1 + 2C/œµ)[d], and C(Œò) 3CLŒ∏‚àöd/2 (Lee & Raginsky, 2018).
 _N_ _‚à•¬∑ ‚à•_  _‚â§_

Generally, the radius of Wasserstein ball œÅ can be considered hyperparameter that needs fine-tuning
(e.g., through cross-validation). In principle, œÅ should not be too large to become over-conservative,
which can hurt the empirical average performance, but also not too small to become similar to the
ERM, and thus can lack robustness. From a statistical standpoint, we are interested in learning how
to scale œÅ w.r.t. the sample size ni, i [m], such that the generalization of the WAFL solution
_‚àà_
_Œ∏[Œµ]_ w.r.t the true distribution PŒª is guaranteed, while still ensuring robustness w.r.t all distributions
inside the Wasserstein ball. Using the result from Fournier & Guillin (2015), which shows that _Pni_
converges in Wasserstein distance to the trueb _Pi at a specific rate, we obtain the following_

**Corollary 1. With all assumptions as in Theorem 2, defining œÅn :=** _mi=1_ _[Œª][i]œÅ[b][Œ¥/m]ni_ _[, we have]_ [b]

_m_

48C(Œò) 2 log(4m/Œ¥) qP

E(PŒª, fŒ∏[Œµ] [)][ ‚â§] _Œªi_ + 2M‚Ñì + g(œÅn, Œ≥) + Œµ

_i=1_ " _‚àöni_ s _ni_ #

b X min 2/d,1/2

log(c1/Œ¥) _{_ _}_

_c2n_ _if n_ _c2_ _,_

_with probability at least 1 ‚àí_ _Œ¥, where_ _œÅ[Œ¥]n_ [:=] Ô£±  log(c1/Œ¥) 1/Œ± _‚â•_ [log(][c][1][/Œ¥][)]

Ô£¥Ô£≤ _c2n_ _if n <_ [log(]c[c]2[1][/Œ¥][)] _._

b  

We provide a proof of Corollary 1 in Appendix F.Ô£¥Ô£≥

5 CHOOSING Œª: APPLICATIONS

We focus on two applications: multi-source domain adaptation and generalization to all client distributions. We provide insights about choosing the client weights Œª for these applications.

**Multi-source domain adaptation: Consider the multi-source domain distribution PŒª (Mansour**
et al., 2021). Lee & Raginsky (2018) show that solving the minimax risk with the Wasserstein
ambiguity set can help transfer data/knowledge from the source domain PŒª to a different, but related,
target domain Q. They bound the distance Wp(PŒª, Q) using the triangle inequality

_Wp(PŒª, Q)_ _Wp(PŒª,_ _PŒª) + Wp(PŒª,_ _Q) + Wp(Q, Q),_ (8)
_‚â§_

where _PŒª and_ _Q are the empirical versions of PŒª and Q, respectively. While Wp(PŒª,_ _PŒª) and_

[b] [b] [b] [b]

_Wp(Q, Q) can be probabilistically bounded with a confidence parameter Œ¥_ (0, 1) according to
_‚àà_
Fournier & Guillin (2015),[b] [b] _Wp(PŒª,_ _Q) can be deterministically computed using linear or convex[b]_
programming (Peyr¬¥[b] e & Cuturi, 2019).
In FL context, in order to have a better bound for W2(PŒª, Q) similar to (8), it is straightforward to

[b] [b]
choose Œª = arg minŒª‚Ä≤‚àà‚àÜ _W2(PŒª‚Ä≤_ _,_ _Q). To relax this problem into a form solvable using existing_
approaches, observe that W2(PŒª, Q) ‚â§ [P]i[m]=1 _[Œª][i][W][2][(][ b]Pni_ _, Q) due to the convexity of Wasserstein_
distance. We then consider the following upper-bound to[b] [b] _m_ minŒª‚àà‚àÜ _W2(PŒª,_ _Q):_

[b]

_Œªmin‚àÜ_ _i=1_ _[Œª][i][W][2][(][ b]Pni_ _,_ _Q) =: œÅ[‚ãÜ],_ (9)
_‚àà_ [b] [b]

X

which is a linear program, considering each W2(Pni _,_ _Q) can be found by efficiently solving convex_

[b]

programs especially with entropic regularization and the Sinkhorn algorithm (Cuturi, 2013).
**Corollary 2. Denote the solution to (9) by Œª[‚ãÜ], and assume that domain[b]** [b] _Q generates nQ i.i.d. data_
_points. With probability at least 1 ‚àí_ _Œ¥, we have_

_m_

_W2(PŒª‚ãÜ_ _, Q) ‚â§_ _W2(PŒª‚ãÜ_ _,_ _PŒª‚ãÜ_ ) + W2(PŒª‚ãÜ _,_ _Q) + W2(Q,_ _Q) ‚â§_ _i=1_ _[Œª]i[‚ãÜ]œÅ[Œ¥/m]ni_ + œÅ[‚ãÜ] + _œÅ[Œ¥/]nQ[2][.]_

rX

The proof of this corollary is similar to that of Corollary 3 in Appendix B.[b] [b] [b] [b] [b] b

**Covering all client distributions in the Wasserstein ball: Suppose we want to cover all client**
distributions inside a Wassertein ball so that the generalization and robustness result by WAFL in
Theorem 2 is applicable to all clients‚Äô distributions. We show in Appendix B that this is a problem
of finding Œª such that the Wasserstein distance between PŒª and Pj, ‚àÄj, is as small as possible.


-----

6 EXPERIMENTS

We first show how to change the level of worst-case perturbations by varying the robust parameter
_Œ≥. To show the generalizability and robustness of WAFL, we evaluate WAFL under non-i.i.d. and_
distribution shift settings. We compare WAFL with baseline robust methods and non-robust FedAvg.
Finally, in Appendix G.4, we show WAFL‚Äôs capability in domain adaptation.

**Experimental settings. We use two datasets in two non-i.i.d. settings. We distribute the first dataset**
‚Äì MNIST (Lecun et al., 1998) ‚Äì to 100 clients and use a multinomial logistic regression model to
model a convex setting. For the second dataset ‚Äì CIFAR-10 (Krizhevsky, 2009) ‚Äì we use 20 clients
and a CNN model employed in McMahan et al. (2017) to model a non-convex setting. We set
_Œªi = ni/n as the client weights. In optimization, we randomly sample St = 10 clients to participate_
in training at each communication round. More detail can be found in Appendix G.1.


**Effect of Œ≥ on the worst-case risk perturbations.** We
first examine the relationship between the robust parameter Œ≥
and the average worst-case perturbations ÀÜœÅ, defined as ÀÜœÅ[2] =
**EZ‚àºPŒª** _d[2](Z, Z)_, where _Z is the adversarial perturbation of_
_Z. As shown in Fig. 2, smaller values of Œ≥ correspond to larger_
b  
worst-case perturbations ÀÜœÅ on both the MNIST and CIFAR-10

[b] [b]
datasets. For the rest of the experiment, rather than control ÀÜœÅ directly, we set Œ≥ on the opposite direction to control the level of
robustness.


Average worst-case perturbation


1.50

1.25

1.00

0.75

0.50

0.00

10 1 10[0] 10[1] 10[2] 10[3] 10[4]

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||WAFL: WAFL:|MNIST CIFAR-10|


1/

Figure 2: Varying ÀÜœÅ by Œ≥.


**Generalization and robustness of WAFL. We consider** _PŒª and_ _Q as the empirical distribution of_
the training samples and test samples of all clients, respectively. By varying Œ≥, we aim to train a
global model robust to any empirical test distribution _Q. To show the generalization and robustness[b]_ [b]
of WAFL, we train and evaluate it in two different scenarios. First, in the clean data scenario, the
global robust model is trained with different values of Œ≥ and then evaluated on given clients‚Äô test data

[b]
(similar to traditional FL). Second, in the distribution shift setting, the training process is similar;
however, the global robust model is evaluated when there are distribution shifts at the clients‚Äô test
data. To obtain these shifts, we use the common PGD attack method in Madry et al. (2019) under
_l_ -norm to generate an œµ-level perturbation on clients‚Äô test data. We choose the l -norm as it
_‚àû_ _‚àû_
shows benefits in adversaries and gives large perturbations. Following Madry et al. (2019), we fix
the number of gradient steps to generate adversarial examples with tavd = 40, œµ = 0.3, Œ± = 0.01
for MNIST and tavd = 10, œµ = 8/255, Œ± = 2/255 for CIFAR-10 with a batch size of 64. We note
that this setting is similar to the adversarial poisoning attacks and the main purpose of this setting is
to increase the Wasserstein distance between _PŒª and_ _Q, thereby verifying the robustness of WAFL._

The performance of WAFL in both scenarios on MNIST and CIFAR-10 is shown in Fig. 3. When
the clients‚Äô data is clean, the Wasserstein distance between[b] [b] _PŒª and_ _Q is relatively small, and training_
WAFL with small Œ≥ (large ÀÜœÅ) gives the worse performance on the test set. With a sufficiently large Œ≥,
WAFL is less robust and has the same generalization with FedAvg. By carefully fine-turning Œ≥ in the

[b] [b]
range [0.5, 1] for MNIST and [10, 20] for CIFAR-10, WAFL shows an improvement over FedAvg.
_Œ≥ in this scenario plays the same role as a regularization parameter to handle non-i.i.d. data._

By adding distribution shifts, we increase the Wasserstein distance between _PŒª and_ _Q. By vary-_
ing Œ≥, we train a global robust model with different levels of worst-case perturbation to handle the
distribution shifts. Small values of Œ≥ generate larger ambiguity sets (PŒª, ÀÜœÅ) and increase the ro-[b] [b]
_B_
bustness of WAFL, thus increase the chance _Q lies inside_ (PŒª, ÀÜœÅ). In Fig. 3, WAFL shows better
_B_
performance with smaller Œ≥ values. However, as in mentioned in Sec. 4, when[b] ÀÜœÅ is much larger or
smaller than the level of distribution shifts (Œ≥ 0.01 or Œ≥ 1 for MNIST and Œ≥ 0.1 or Œ≥ > 10
_‚â§[b]_ _‚â•_ [b] _‚â§_
for CIFAR-10), WAFL performs inefficiently: too small Œ≥ hurts the empirical performance, while
too large Œ≥ makes WAFL lack robustness. For every scenario, Œ≥ needs to be tuned correspondingly.
In our experiments, by carefully choosing Œ≥, WAFL not only handles distribution shifts or common data poisoning attacks but also provides better performance than FedAvg in non-i.i.d. data and
heterogeneous settings. Specifically, we set Œ≥ = 0.5 for MNIST and Œ≥ = 10 for CIFAR-10.

**Comparison with other robust methods.** We compare WAFL with three baselines: two
common robust methods in FL called FedPGM and FedFGSM, and one non-robust method Fe

-----

MNIST

0.01 0.05 0.1 0.5 1 5 10 50


MNIST

0.01 0.05 0.1 0.5 1 5 10 50


CIFAR

0.01 0.05 0.1 0.5 1 5 10 50


CIFAR

0.01 0.05 0.1 0.5 1 5 10 50


0.90

0.85

0.80

0.75

0.70

0.65

0.60

0.55

0.50


0.70

0.65

0.60

0.55

0.50

0.45

0.40


2.00

1.75

1.50

1.25

1.00

0.75

0.50

0.25


2.75

2.50

2.25

2.00

1.75

1.50

1.25

1.00


WAFL: Clean data FedAvg: Clean data WAFL: Distribution shifts FedAvg: Distribution shifts


Figure 3: Global accuracy and loss of with different values of Œ≥ on MNIST and CIFAR-10 under clean data
and distribution shifts (40% of clients are affected by PGD attack). The blue vertical line indicates the value of


MNIST


CIFAR


0.9

0.8

0.7

0.6

0.5

0.4


2.50

2.25

2.00

1.75

1.50

1.25

1.00

0.75

0.50


0.70

0.65

0.60

0.55

0.50

0.45

0.40

0.35

0.30


2.8

2.6

2.4

2.2

2.0

1.8

1.6

1.4

1.2

1.0

|D attack(Œ≥ = 0.05 for|Col2|Col3|Col4|
|---|---|---|---|
|MNIST||||
|||||
|||||
|||||

|0).|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
|CIF||AR|||
||||||
||||||
||||||


0.2 0.4 0.6 0.8 0 0.2 0.4 0.6 0.8

Proportion of attacked clients Proportion of attacked clients


0.2 0.4 0.6 0.8

Proportion of attacked clients

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|0 0.2 0.4 0.6 0.8 Proportion of attacked clients||||


WAFL FedPGM FedFGSM FedAvg


Figure 4: Comparison with other robust methods on MNIST and CIFAR-10 at different proportion of attacked
clients (clients are affected by distribution shifts).

dAvg. FedPGM and FedFGSM are FedAvg with adversarial training using the projected gradient method PGD (Madry et al., 2019) and the fast-gradient method FGSM (Goodfellow et al.,
2015), respectively. In FedPGM and FedFGSM, in each local update, all clients solve Œ¥[‚àó] =
arg max‚à•Œ¥‚à•‚àû‚â§œµ _‚Ñì(hŒ∏(z + Œ¥), y)_ using projection onto an l‚àû-norm to find the worst-case perturbation Œ¥. While FedPGD uses tavd gradient steps to find Œ¥[‚àó], FedFGSM uses only one gradient

step. We use the same values of œµ and Œ± from the distribution shifts setting for projection. For a
fair comparison, both WAFL and FedPGM have the same value tavd and all algorithms have the
same number of local updates K. We also train WAFL with the value of Œ≥ generating the same
level perturbation of œµ in FedPGM and FedFGSM. We study different proportions of clients having
distribution shifts in Fig. 4. The robust accuracy of all methods decreases when the percentage of
clients having distribution shifts increases. Especially, the FedAvg‚Äôs performance drops dramatically when the percentage of attacked clients reaches 80%. For all scenarios, WAFL outperforms
all the baselines. Specifically, in the case of 80% attacked clients, the improvements in accuracy of
WAFL over FedPGM, FedFGSM, and FedAvg are 7%, 8%, and 33% for MNIST and 2.5%, 4.5%,
and 18% for CIFAR-10, respectively.


7 CONCLUSION

In this paper, we present WAFL, a Wasserstein distributionally robust optimization framework, to
tackle the issue of statistical heterogeneity in federated learning. We first remodel the duality of
the worst-case risk to an empirical surrogate risk minimization problem, then solve it using a local
SGD-based algorithm with convergence analysis. We show that WAFL is more general in terms
of robustness compared to related approaches, and obtains an explicit robust generalization bound
with respect to all unknown distributions in the Wasserstein ambiguity set. Through numerical
experiments, we demonstrate that WAFL generalizes better than the standard FedAvg baseline in
non-i.i.d. settings, and outperforms related methods with respect to robustness to distribution shifts.
Moreover, WAFL reveals its capability in generalizing to unseen data distributions.


-----

REFERENCES

Peter Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
[networks. arXiv:1706.08498 [cs, stat], December 2017. URL http://arxiv.org/abs/1706.08498.](http://arxiv.org/abs/1706.08498)
arXiv: 1706.08498.

Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust Wasserstein Profile Inference and Applications to Machine Learning. Journal of Applied Probability, 56(3):830‚Äì857, September 2019.
[ISSN 0021-9002, 1475-6072. doi: 10.1017/jpr.2019.49. URL http://arxiv.org/abs/1610.05627.](http://arxiv.org/abs/1610.05627)
arXiv: 1610.05627.

Nicholas Carlini and David Wagner. Towards Evaluating the Robustness of Neural Networks.
_[arXiv:1608.04644 [cs], March 2017. URL http://arxiv.org/abs/1608.04644. arXiv: 1608.04644.](http://arxiv.org/abs/1608.04644)_

Ruidi Chen and Ioannis Ch Paschalidis. A Robust Learning Approach for Regression Models Based
on Distributionally Robust Optimization. Journal of Machine Learning Research, 19(13):1‚Äì48,
[2018. ISSN 1533-7928. URL http://jmlr.org/papers/v19/17-295.html.](http://jmlr.org/papers/v19/17-295.html)

Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Proceed_ings of the 26th International Conference on Neural Information Processing Systems - Volume_
_2, NIPS‚Äô13, pp. 2292‚Äì2300, Red Hook, NY, USA, 2013. Curran Associates Inc._

Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive Personalized Feder[ated Learning. arXiv:2003.13461 [cs, stat], November 2020a. URL http://arxiv.org/abs/2003.](http://arxiv.org/abs/2003.13461)
[13461. arXiv: 2003.13461.](http://arxiv.org/abs/2003.13461)

Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Distributionally Robust Federated Averaging. In Advances in Neural Information Processing Systems, volume 33, pp. 15111‚Äì
15122. Curran Associates, Inc., 2020b. [URL https://proceedings.neurips.cc/paper/2020/hash/](https://proceedings.neurips.cc/paper/2020/hash/ac450d10e166657ec8f93a1b65ca1b14-Abstract.html)
[ac450d10e166657ec8f93a1b65ca1b14-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/ac450d10e166657ec8f93a1b65ca1b14-Abstract.html)

Theo Diamandis, Yonina C. Eldar, Alireza Fallah, Farzan Farnia, and Asuman Ozdaglar. A Wasserstein Minimax Framework for Mixed Linear Regression. arXiv:2106.07537 [cs, math, stat], June
[2021. URL http://arxiv.org/abs/2106.07537. arXiv: 2106.07537.](http://arxiv.org/abs/2106.07537)

Canh T. Dinh, Nguyen H. Tran, and Tuan Dung Nguyen. Personalized Federated Learning with
[Moreau Envelopes. arXiv:2006.08848 [cs, stat], March 2021. URL http://arxiv.org/abs/2006.](http://arxiv.org/abs/2006.08848)
[08848. arXiv: 2006.08848.](http://arxiv.org/abs/2006.08848)

Wei Du, Depeng Xu, Xintao Wu, and Hanghang Tong. Fairness-aware Agnostic Federated Learning.
_[arXiv:2010.05057 [cs], October 2020. URL http://arxiv.org/abs/2010.05057. arXiv: 2010.05057.](http://arxiv.org/abs/2010.05057)_

Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven Distributionally Robust Optimization Using the Wasserstein Metric: Performance Guarantees and Tractable Reformulations.
_arXiv:1505.05116 [math, stat], June 2017._ [URL http://arxiv.org/abs/1505.05116.](http://arxiv.org/abs/1505.05116) arXiv:
1505.05116.

Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach. In Advances in Neural Infor_[mation Processing Systems, volume 33, pp. 3557‚Äì3568. Curran Associates, Inc., 2020. URL https:](https://proceedings.neurips.cc/paper/2020/hash/24389bfe4fe2eba8bf9aa9203a44cdad-Abstract.html)_
[//proceedings.neurips.cc/paper/2020/hash/24389bfe4fe2eba8bf9aa9203a44cdad-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/24389bfe4fe2eba8bf9aa9203a44cdad-Abstract.html)

Nicolas Fournier and Arnaud Guillin. On the rate of convergence in wasserstein distance of the
empirical measure. _Probability Theory and Related Fields, 162:707, 2015._ doi: 10.1007/
s00440-014-0583-7.

Yaroslav Ganin and Victor Lempitsky. Unsupervised Domain Adaptation by Backpropagation. In
_[International Conference on Machine Learning, pp. 1180‚Äì1189. PMLR, June 2015. URL https:](https://proceedings.mlr.press/v37/ganin15.html)_
[//proceedings.mlr.press/v37/ganin15.html. ISSN: 1938-7228.](https://proceedings.mlr.press/v37/ganin15.html)

Rui Gao and Anton J. Kleywegt. Distributionally Robust Stochastic Optimization with Wasserstein
[Distance. arXiv:1604.02199 [math], July 2016. URL http://arxiv.org/abs/1604.02199. arXiv:](http://arxiv.org/abs/1604.02199)
1604.02199.


-----

Rui Gao, Xi Chen, and Anton J. Kleywegt. Wasserstein Distributionally Robust Optimization and
[Variation Regularization. arXiv:1712.06050 [cs, math, stat], October 2020. URL http://arxiv.org/](http://arxiv.org/abs/1712.06050)
[abs/1712.06050. arXiv: 1712.06050.](http://arxiv.org/abs/1712.06050)

Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial
[Examples. arXiv:1412.6572 [cs, stat], March 2015. URL http://arxiv.org/abs/1412.6572. arXiv:](http://arxiv.org/abs/1412.6572)
1412.6572.

Jochen Gorski, Frank Pfeuffer, and Kathrin Klamroth. Biconvex sets and optimization with biconvex
functions: a survey and extensions. Mathematical Methods of Operations Research, 66(3):373‚Äì
[407, December 2007. ISSN 1432-5217. doi: 10.1007/s00186-007-0161-1. URL https://doi.org/](https://doi.org/10.1007/s00186-007-0161-1)
[10.1007/s00186-007-0161-1.](https://doi.org/10.1007/s00186-007-0161-1)

J.J. Hull. A database for handwritten text recognition research. _IEEE Transactions on Pattern_
_Analysis and Machine Intelligence, 16(5):550‚Äì554, May 1994. ISSN 1939-3539. doi: 10.1109/_
34.291440.

Jakub KoneÀácn¬¥y, H. Brendan McMahan, Daniel Ramage, and Peter Richt¬¥arik. Federated Optimization: Distributed Machine Learning for On-Device Intelligence. arXiv:1610.02527 [cs], October
[2016. URL http://arxiv.org/abs/1610.02527. arXiv: 1610.02527.](http://arxiv.org/abs/1610.02527)

Jakub KoneÀácn¬¥y, H. Brendan McMahan, Felix X. Yu, Peter Richt¬¥arik, Ananda Theertha Suresh,
and Dave Bacon. Federated Learning: Strategies for Improving Communication Efficiency.
_[arXiv:1610.05492 [cs], October 2017. URL http://arxiv.org/abs/1610.05492. arXiv: 1610.05492.](http://arxiv.org/abs/1610.05492)_

Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. pp. 60, 2009.

Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shafieezadeh-Abadeh.
Wasserstein Distributionally Robust Optimization: Theory and Applications in Machine Learning. _arXiv:1908.08729 [cs, math, stat], August 2019._ [URL http://arxiv.org/abs/1908.08729.](http://arxiv.org/abs/1908.08729)
arXiv: 1908.08729.

Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial Machine Learning at Scale.
_arXiv:1611.01236 [cs, stat], February 2017._ [URL http://arxiv.org/abs/1611.01236.](http://arxiv.org/abs/1611.01236) arXiv:
1611.01236.

Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278‚Äì2324, November 1998. ISSN 1558-2256. doi:
10.1109/5.726791. Conference Name: Proceedings of the IEEE.

Jaeho Lee and Maxim Raginsky. Minimax Statistical Learning with Wasserstein distances. In
_Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018._
URL [https://papers.nips.cc/paper/2018/hash/ea8fcd92d59581717e06eb187f10666d-Abstract.](https://papers.nips.cc/paper/2018/hash/ea8fcd92d59581717e06eb187f10666d-Abstract.html)
[html.](https://papers.nips.cc/paper/2018/hash/ea8fcd92d59581717e06eb187f10666d-Abstract.html)

Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and Robust Federated
[Learning Through Personalization. arXiv:2012.04221 [cs, stat], June 2021. URL http://arxiv.](http://arxiv.org/abs/2012.04221)
[org/abs/2012.04221. arXiv: 2012.04221.](http://arxiv.org/abs/2012.04221)

Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the Convergence of
[FedAvg on Non-IID Data. arXiv:1907.02189 [cs, math, stat], June 2020. URL http://arxiv.org/](http://arxiv.org/abs/1907.02189)
[abs/1907.02189. arXiv: 1907.02189.](http://arxiv.org/abs/1907.02189)

Quande Liu, Cheng Chen, Jing Qin, Qi Dou, and Pheng-Ann Heng. FedDG: Federated Domain
Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency
Space. arXiv:2103.06030 [cs], March 2021.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards Deep Learning Models Resistant to Adversarial Attacks. arXiv:1706.06083 [cs, stat],
[September 2019. URL http://arxiv.org/abs/1706.06083. arXiv: 1706.06083.](http://arxiv.org/abs/1706.06083)

Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three Approaches for
Personalization with Applications to Federated Learning. arXiv:2002.10619 [cs, stat], July 2020.
[URL http://arxiv.org/abs/2002.10619. arXiv: 2002.10619.](http://arxiv.org/abs/2002.10619)


-----

Yishay Mansour, Mehryar Mohri, Jae Ro, Ananda Theertha Suresh, and Ke Wu. A Theory of
Multiple-Source Adaptation with Limited Target Labeled Data. In Proceedings of The 24th Inter_national Conference on Artificial Intelligence and Statistics, pp. 2332‚Äì2340. PMLR, March 2021._
[URL https://proceedings.mlr.press/v130/mansour21a.html. ISSN: 2640-3498.](https://proceedings.mlr.press/v130/mansour21a.html)

H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag¬®uera y
Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data.
_arXiv:1602.05629 [cs], February 2017._ [URL http://arxiv.org/abs/1602.05629.](http://arxiv.org/abs/1602.05629) arXiv:
1602.05629.

Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic Federated Learning.
_arXiv:1902.00146 [cs, stat], January 2019._ [URL http://arxiv.org/abs/1902.00146.](http://arxiv.org/abs/1902.00146) arXiv:
1902.00146.

Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram
Swami. The Limitations of Deep Learning in Adversarial Settings. arXiv:1511.07528 [cs, stat],
[November 2015. URL http://arxiv.org/abs/1511.07528. arXiv: 1511.07528.](http://arxiv.org/abs/1511.07528)

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance
Deep Learning Library. In Advances in Neural Information Processing Systems 32, Vancouver,
BC, Canada, 2019.

Xingchao Peng, Zijun Huang, Yizhe Zhu, and Kate Saenko. Federated Adversarial Domain Adaptation. arXiv:1911.02054 [cs], December 2019.

Gabriel Peyr¬¥e and Marco Cuturi. Computational optimal transport: With applications to data science. Foundations and Trends¬Æ in Machine Learning, 11(5-6):355‚Äì607, 2019. ISSN 1935-8237.
[doi: 10.1561/2200000073. URL http://dx.doi.org/10.1561/2200000073.](http://dx.doi.org/10.1561/2200000073)

Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust Federated
Learning: The Case of Affine Distribution Shifts. arXiv:2006.08907 [cs, math, stat], June 2020.
[URL http://arxiv.org/abs/2006.08907. arXiv: 2006.08907.](http://arxiv.org/abs/2006.08907)

Filippo Santambrogio. Optimal Transport for Applied Mathematicians: Calculus of Variations,
_PDEs, and Modeling._ Progress in Nonlinear Differential Equations and Their Applications.
Birkh¬®auser Basel, 2015. ISBN 978-3-319-20827-5. doi: 10.1007/978-3-319-20828-2. URL
[https://www.springer.com/gp/book/9783319208275.](https://www.springer.com/gp/book/9783319208275)

Soroosh Shafieezadeh Abadeh, Peyman Mohajerin Mohajerin Esfahani, and Daniel Kuhn. Distributionally Robust Logistic Regression. In Advances in Neural Information Processing Sys_[tems, volume 28. Curran Associates, Inc., 2015. URL https://papers.nips.cc/paper/2015/hash/](https://papers.nips.cc/paper/2015/hash/cc1aa436277138f61cda703991069eaf-Abstract.html)_
[cc1aa436277138f61cda703991069eaf-Abstract.html.](https://papers.nips.cc/paper/2015/hash/cc1aa436277138f61cda703991069eaf-Abstract.html)

Soroosh Shafieezadeh-Abadeh, Daniel Kuhn, and Peyman Mohajerin Esfahani. Regularization via
[Mass Transportation. arXiv:1710.10016 [cs, math, stat], July 2019. URL http://arxiv.org/abs/](http://arxiv.org/abs/1710.10016)
[1710.10016. arXiv: 1710.10016.](http://arxiv.org/abs/1710.10016)

Shai Shalev-Shwartz and Shai Ben-David. _Understanding Machine Learning: From Theory to_
_Algorithms. Cambridge University Press, USA, 2014. ISBN 978-1-107-05713-5._

Aman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi. Certifying Some Distributional Robustness with Principled Adversarial Training. arXiv:1710.10571 [cs, stat], May 2020.
[URL http://arxiv.org/abs/1710.10571. arXiv: 1710.10571.](http://arxiv.org/abs/1710.10571)

Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar. Federated Multi-Task
[Learning. arXiv:1705.10467 [cs, stat], February 2018. URL http://arxiv.org/abs/1705.10467.](http://arxiv.org/abs/1705.10467)
arXiv: 1705.10467.

Ananda Theertha Suresh, Felix X. Yu, Sanjiv Kumar, and H. Brendan McMahan. Distributed Mean
[Estimation with Limited Communication. arXiv:1611.00429 [cs], September 2017. URL http:](http://arxiv.org/abs/1611.00429)
[//arxiv.org/abs/1611.00429. arXiv: 1611.00429.](http://arxiv.org/abs/1611.00429)


-----

Florian Tram`er, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble Adversarial Training: Attacks and Defenses. arXiv:1705.07204 [cs, stat],
[April 2020. URL http://arxiv.org/abs/1705.07204. arXiv: 1705.07204.](http://arxiv.org/abs/1705.07204)

Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H. Brendan McMahan, Blaise Aguera y
Arcas, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data,
[et al. A Field Guide to Federated Optimization. arXiv:2107.06917 [cs], July 2021. URL http:](http://arxiv.org/abs/2107.06917)
[//arxiv.org/abs/2107.06917. arXiv: 2107.06917.](http://arxiv.org/abs/2107.06917)

Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
[Learning with Non-IID Data. arXiv:1806.00582 [cs, stat], June 2018. URL http://arxiv.org/abs/](http://arxiv.org/abs/1806.00582)
[1806.00582. arXiv: 1806.00582.](http://arxiv.org/abs/1806.00582)


-----

A ADVERSARIAL ROBUST FL‚ÄôS AMBIGUITY SET V.S. WASSERTEIN BALL

We show that using the Wasserstein ambiguity set contains the perturbation points induced by the
solution to the Adversarial Robust FL approach. As we present in Sec. 3.2, existing techniques for
adversarial training robust models (Goodfellow et al., 2015; Papernot et al., 2015; Kurakin et al.,
2017; Carlini & Wagner, 2017; Madry et al., 2019; Tram`er et al., 2020) define an adversarial perturbation u at a data point Z, and minimize the following worst-case loss over all possible perturbations

maxu **[E][Z][‚àº]P[b]Œª** _‚Ñì(Z + u, hŒ∏)_ _,_ (10)
_‚ààU_

h i

where the ambiguity set U := _u ‚àà_ R[d][+1] : ‚à•u‚à•‚â§ _œµ_ . To compare this approach with Wassersteinrobust FL, we relate the above problem to its counterpart defined in the probability space of input as

follows
max **E ÀúZ** _Q_ _‚Ñì( Z, h[Àú]_ _Œ∏)_ _,_ (11)
_QÀú‚ààQ(œµ)_ _‚àº_ [Àú]
h i

where Q(œµ) := _QÀú : P_ _‚à•Z[Àú] ‚àí_ _Z‚à•‚â§_ _œµ_ = 1, Z ‚àº _P[b]Œª,_ _Z[Àú] ‚àº_ _Q[Àú]_ _. Considering u[‚àó]_ as a solution to

problem (10), we see that the distributionn   _Q[‚Ä≤]_ of perturbation points (i.e.,o _Z[Àú] := (Z + u[‚àó])_ _Q[‚Ä≤])_
_‚àº_
in problem (10) belongs to the feasible set Q(œµ) in problem (11) (If not, then P _‚à•u[‚àó]‚à•‚â§_ _œµ_ _< 1, a_
contradiction). Next, consider an arbitrary distribution _Q[Àú]_ (œµ) in problem (11), with any _Z[Àú]_ _Q_
_‚ààQ_   _‚àº_ [Àú]
and Z _PŒª, we have_
_‚àº_ [b]

w.p.1
_‚à•Z[Àú] ‚àí_ _Z‚à•_ _‚â§_ _œµ =‚áí_ **EZ‚àºPŒª,Z[Àú]‚àºQ[Àú]** _‚à•Z[Àú] ‚àí_ _Z‚à•_ _‚â§_ _œµ =‚áí_ _œÄ‚ààŒ†(infPŒª,Q[Àú])_ **E(Z,Z** _‚Ä≤)‚àºœÄ_ _‚à•Z[Àú] ‚àí_ _Z‚à•_ _‚â§_ _œµ,_
b    

which implies that W1(PŒª, _Q[Àú])_ _œµ,_ _Q_ (œµ), and thus (œµ[b]) 1(PŒª, œµ). We have shown
_‚â§_ _‚àÄ_ [Àú] ‚ààQ _Q_ _‚äÇB_
that the Wasserstein ambiguity set contains the perturbation points induced by the solution to the
adversarial robust training problem (10).

[b] [b]

B CHOOSING Œª: GENERALIZING TO ALL CLIENT DISTRIBUTIONS

We show that by calibrating appropriate Œª value, our proposed algorithm will be capable of generalizing to all client distributions. Suppose we want to cover all client distributions inside a Wassertein
ball so that the generalization and robustness result by WAFL in Theorem 2 is applicable to all
clients‚Äô distributions. This is the problem of finding Œª such that the Wasserstein distance between
_PŒª and Pj, ‚àÄj, is as small as possible. Instead of directly finding the minimum Wasserstein ra-_
dius that cover all client distributions, we will leverage the popular Wasserstein barycenter problem
(Peyr¬¥e & Cuturi, 2019). Specifically, consider the problem

_Œªmin‚àà‚àÜ_ _[W][2][(][ b]PŒª, P_ [‚ô•]) s.t. _P_ [‚ô•] = arg minQ‚ààP _mi=1_ _[Œª][i][W][2][(][ b]Pni_ _, Q),_ (12)

X

where P [‚ô•] is the Wasserstein bary center w.r.t the solution Œª[‚àó] to this problem. Even though the
solution is not straightforward, we propose to solve its tractable upper-bound:

_m_

_Œª_ ‚àÜmin,Q _i=1_ _[Œª][i][W][2][(][ b]Pni_ _, Q)._ (13)
_‚àà_ _‚ààP_

X

This is a bi-convex problem, which is convex w.r.t to Œª (resp. Q) when fixing Q (resp. Œª). Thus,
we can use alternative minimization (Gorski et al., 2007) to find a local solution to this problem.
Denoting _Œª[Àú] as the solution to (12) and (Œª[‚àó], P_ _[‚àó]) as a local solution to (13), we obtain_

_m_
_W2(PŒªÀú[, P][ ‚ô•][)][ ‚â§]_ _[W][2][(][ b]PŒª‚àó_ _, P_ _[‚àó]) ‚â§_ _i=1_ _[Œª]i[‚àó][W][2][(][ b]Pni_ _, P_ _[‚àó]) =: œÅ[‚àó]._ (14)
X

**Corollary 3. For all client j** [m], with probability at least 1 _Œ¥, we have_

[b] _‚àà_ _‚àí_

_W2(PŒª‚àó_ _, Pj) ‚â§_ _W2(PŒª‚àó_ _,_ _PŒª‚àó_ ) + W2(PŒª‚àó _, P_ _[‚àó]) + W2(P_ _[‚àó],_ _Pnj_ ) + W2(Pnj _, Pj)_

_m_

_‚â§_ _i=1_ _[Œª][b]i[‚àó]œÅ[Œ¥/m]ni_ + œÅ[‚àó][b]+ _Œª[œÅ][‚àó]j_ + _œÅ[Œ¥/]nj[2]_ [b] [b]

rX

[b] b


-----

_Proof. The first line is by triangle inequality._ The second line is by following facts: (i)

**P** _W2(PŒª‚àó_ _,_ _PŒª‚àó_ ) _‚â•_ _mi=1_ _[Œª]i[‚àó]œÅ[Œ¥/m]ni_ _‚â§_ _Œ¥/2 according to (37), (ii) W2(P_ _[‚àó],_ _Pnj_ ) =

_ŒªjhW2(P_ _[‚àó],Pnj )_ _œÅ[‚àó]_ i

union bound.Œªj [b] [b] _‚â§_ _Œªj_ [, and (iii)]qP **[ P]** _W[b]2(Pnj_ _, Pj) ‚â•_ _œÅ[Œ¥/]nj[2]_ _‚â§_ _Œ¥/2 according to (35), and (iv) using[b]_
 

[b] b

C PROOF OF THEOREM 1

Our proof is based on the analysis of local SGD for FL presented in Wang et al. (2021).

Fix some zi. Define œï(Œ∂, Œ∏; zi) := ‚Ñì(Œ∂, hŒ∏) _Œ≥d(Œ∂, zi). Since ‚Ñì_ is Lzz-smooth and d is 1-strongly
_‚àí_
convex, œï(Œ∂, Œ∏; zi) is (Œ≥ _Lzz)-strongly concave with respect to Œ∂, given that Œ≥ > Lzz._
_‚àí_
**Lemma 2. Let zi[‚àó]** [= arg max]Œ∂ _[œï][(][Œ∂, Œ∏][;][ z][i][)][. Therefore,][ œÜ][Œ≥][(][Œ∏][;][ z][i][) =][ œï][(][z]i[‚àó][, Œ∏][;][ z][i][)][.][ Let][ ‚Ñì]_ _[satisfy]_
_‚ààZ_
_Assumption 3. Then œÜŒ≥ is differentiable, and_

_œÜŒ≥(zi, Œ∏)_ _œÜŒ≥(zi, Œ∏[‚Ä≤])_ _L_ _Œ∏_ _Œ∏[‚Ä≤]_ _,_
_‚à•‚àá_ _‚àí‚àá_ _‚à•‚â§_ _‚à•_ _‚àí_ _‚à•_

_with L = LŒ∏Œ∏ +_ _[L]Œ≥[Œ∏z]‚àíL[L]zz[zŒ∏]_ _[when][ Œ≥ > L][zz][.]_


The proof can be found in (Sinha et al., 2020, Lemma 1). Lemma 2 implies that œÜŒ≥ is L-smooth.


Define gi(Œ∏) := 1i _zi_ _i_

_|D_ _|_ _‚ààD_

_[‚àá][Œ∏][ œÜ][Œ≥][(][z][i][, Œ∏][)][, then we have]_

2[i]

P

**E** _‚à•gi(Œ∏) ‚àí‚àáFi(Œ∏)‚à•[2][i]_ = E [1]i _‚àáŒ∏ œÜŒ≥(zi, Œ∏) ‚àí‚àáFi(Œ∏)_
h h _|D_ _|_ _zXi‚ààDi_

1

_gœÜi_ (Œ∏) _Fi(Œ∏)_ _œÉ[2]_ (by Assumption 4.) (15)

_‚â§_ _i_ _‚àí‚àá_ _‚â§_ _[œÉ][2]i_ [:=][ b]

_|D_ _|_ **[E]** _|D_ _|_
h

[2][i]

With the shadow sequence _Œ∏[¬Ø][(][t,k][)]_ = _i=1_ _[Œª][i][Œ∏]i[(][t,k][)], we have_

_m_ _m_ _m_

_Œ∏¬Ø[(][t,k][+1)]_ = _ŒªiŒ∏i[(][t,k][+1)]_ = [P][m]Œªi _Œ∏i[(][t,k][)]_ _Œ∑gi(Œ∏i[(][t,k][)])_ = Œ∏[¬Ø][(][t,k][)] _Œ∑_ _Œªigi(Œ∏i[(][t,k][)])._

_‚àí_ _‚àí_

_i=1_ _i=1_ _i=1_

X X    X


**Lemma 3. If the client learning rate satisfies Œ∑ ‚â§** 31L _[, then]_

1 _K‚àí1_ _m_ _m_ _K‚àí1_ 1

_K_ _kX=0_ **EF** (Œ∏[¬Ø][(][t,k][)]) ‚àí _F_ (Œ∏[‚àó]) _‚â§2Œ∑œÉÀÜ[2]_ Xi=1 _Œª[2]i_ ! + L Xi=1 _Œªi_ _kX=0_ _K_ **[E]h‚à•Œ∏i[(][t,k][)]** _‚àí_ _Œ∏[¬Ø][(][t,k][)]‚à•[2][i]_

1
+ _Œ∏[(][t][)]_ _Œ∏[‚àó]_ **E** _Œ∏[(][t][+1)]_ _Œ∏[‚àó]_ _._

2Œ∑K _‚à•_ _‚àí_ _‚à•[2]_ _‚àí_ _‚à•_ _‚àí_ _‚à•[2][i!]_

h

_Proof. Since_ _Œ∏[¬Ø][(][t,k][+1)]_ = Œ∏[¬Ø][(][t,k][)] _‚àí_ _Œ∑_ _i=1_ _[Œª][i][g][i][(][Œ∏]i[(][t,k][)]), by parallelogram law_

_m_

_Œªi_ _gi(Œ∏i[(][t,k][)]),_ _Œ∏[¬Ø][(][t,k][+1)]_ _Œ∏[‚àó][E]_ = [P][1][m] _Œ∏[(][t,k][)]_ _Œ∏[‚àó]_ _Œ∏[(][t,k][+1)]_ _Œ∏[(][t,k][)]_ _Œ∏[(][t,k][+1)]_ _Œ∏[‚àó]_
_‚àí_ 2Œ∑ _‚à•[¬Ø]_ _‚àí_ _‚à•[2]_ _‚àí‚à•[¬Ø]_ _‚àí_ [¬Ø] _‚à•[2]_ _‚àí‚à•[¬Ø]_ _‚àí_ _‚à•[2][]_
_i=1_

X D 

(16)


Fact: Fi(Œ∏) = EZi‚àºPi _œÜŒ≥(Zi, Œ∏)_ is Lipschitz smooth with L = LŒ∏Œ∏ + _[L]Œ≥[Œ∏z]‚àíL[L]zz[zŒ∏]_ [when][ Œ≥ > L][zz][.]

With the assumption that Œ∏ _‚Ñì(z, hŒ∏) is convex, we have Œ∏_ _Fi(Œ∏) is convex._

 _7‚Üí_  _7‚Üí_

Since Fi is convex and L-smooth,

_Fi(Œ∏[¬Ø][(][t,k][+1)]) ‚â§_ _Fi(Œ∏i[(][t,k][)]) +_ D‚àáFi(Œ∏i[(][t,k][)]), _Œ∏[¬Ø][(][t,k][+1)]_ _‚àí_ _Œ∏i[(][t,k][)]E_ + _[L]2_ _[‚à•]Œ∏[¬Ø][(][t,k][+1)]_ _‚àí_ _Œ∏i[(][t,k][)]‚à•[2]_

_‚â§Fi(Œ∏[‚àó]) +_ D‚àáFi(Œ∏i[(][t,k][)]), _Œ∏[¬Ø][(][t,k][+1)]_ _‚àí_ _Œ∏[‚àó][E]_ + _[L]2_ _[‚à•]Œ∏[¬Ø][(][t,k][+1)]_ _‚àí_ _Œ∏i[(][t,k][)]‚à•[2]_

_Fi(Œ∏[‚àó]) +_ _Fi(Œ∏i[(][t,k][)]),_ _Œ∏[¬Ø][(][t,k][+1)]_ _Œ∏[‚àó][E]_ + L _Œ∏[(][t,k][+1)]_ _Œ∏[(][t,k][)]_ + L _Œ∏i[(][t,k][)]_ _Œ∏[(][t,k][)]_ _._ (17)
_‚â§_ _‚àá_ _‚àí_ _‚à•[¬Ø]_ _‚àí_ [¬Ø] _‚à•[2]_ _‚à•_ _‚àí_ [¬Ø] _‚à•[2]_
D


-----

From (16) and (17), we have


_F_ (Œ∏[¬Ø][(][t,k][+1)]) ‚àí _F_ (Œ∏[‚àó]) =


_Œªi_ _Fi(Œ∏[¬Ø][(][t,k][+1)])_ _F_ (Œ∏[‚àó])
_‚àí_
_i=1_

X 


_m_ _m_

_Œªi_ _Fi(Œ∏i[(][t,k][)])_ _gi(Œ∏i[(][t,k][)]),_ _Œ∏[¬Ø][(][t,k][+1)]_ _Œ∏[‚àó][E]_ + L _Œ∏[(][t,k][+1)]_ _Œ∏[(][t,k][)]_ + L _Œªi_ _Œ∏i[(][t,k][)]_ _Œ∏[(][t,k][)]_

_‚â§_ _‚àá_ _‚àí_ _‚àí_ _‚à•[¬Ø]_ _‚àí_ [¬Ø] _‚à•[2]_ _‚à•_ _‚àí_ [¬Ø] _‚à•[2]_

_i=1_ _i=1_

X D X

+ [1] _Œ∏[(][t,k][)]_ _Œ∏[‚àó]_ _Œ∏[(][t,k][+1)]_ _Œ∏[(][t,k][)]_ _Œ∏[(][t,k][+1)]_ _Œ∏[‚àó]_ _._ (18)

2Œ∑ ‚à•[¬Ø] _‚àí_ _‚à•[2]_ _‚àí‚à•[¬Ø]_ _‚àí_ [¬Ø] _‚à•[2]_ _‚àí‚à•[¬Ø]_ _‚àí_ _‚à•[2][]_

We have

_[m]_
**E** _Œªi_ _Fi(Œ∏i[(][t,k][)])_ _gi(Œ∏i[(][t,k][)]),_ _Œ∏[¬Ø][(][t,k][+1)]_ _Œ∏[‚àó][Ei]_

_‚àá_ _‚àí_ _‚àí_
_i=1_

hX D

_[m]_
= E _Œªi_ _Fi(Œ∏i[(][t,k][)])_ _gi(Œ∏i[(][t,k][)]),_ _Œ∏[¬Ø][(][t,k][+1)]_ _Œ∏[(][t,k][)][Ei]_ (since E _gi(Œ∏i[(][t,k][)])_ = _Fi(Œ∏i[(][t,k][)]) given_ _Œ∏[¬Ø][(][t,k][)], Œ∏[‚àó])_

_‚àá_ _‚àí_ _‚àí_ [¬Ø] _‚àá_
_i=1_

hX Dm  

_‚â§_ [3]2 _[Œ∑][ ¬∑][ E]_ _‚à•_ _Œªi_ _‚àáFi(Œ∏i[(][t,k][)]) ‚àí_ _gi(Œ∏i[(][t,k][)])_ _‚à•[2][i]_ + 6[1]Œ∑ **[E]** _‚à•Œ∏[¬Ø][(][t,k][+1)]_ _‚àí_ _Œ∏[¬Ø][(][t,k][)]‚à•[2][i]_ (by Peter Paul inequality)

_i=1_

h mX    h

_‚â§_ 2Œ∑œÉÀÜ[2] _Œª[2]i_ + 6[1]Œ∑ **[E]** _‚à•Œ∏[¬Ø][(][t,k][+1)]_ _‚àí_ _Œ∏[¬Ø][(][t,k][)]‚à•[2][i],_ (19)
Xi=1  h


Plugging (19) back to the conditional expectation of (18), and noting that Œ∑ ‚â§


1

3L [, we have]


_F_ (Œ∏[¬Ø][(][t,k][+1)]) _F_ (Œ∏[‚àó]) + [1]
_‚àí_ 2Œ∑
i


_‚à•Œ∏[¬Ø][(][t,k][+1)]_ _‚àí_ _Œ∏[‚àó]‚à•[2]_ _‚àí‚à•Œ∏[¬Ø][(][t,k][)]_ _‚àí_ _Œ∏[‚àó]‚à•[2]_



_‚à•Œ∏[¬Ø][(][t,k][+1)]_ _‚àí_ _Œ∏[¬Ø][(][t,k][)]‚à•[2]_ + L



_Œªi_ _Œ∏i[(][t,k][)]_ _Œ∏[(][t,k][)]_
_‚à•_ _‚àí_ [¬Ø] _‚à•[2]_
_i=1_

X


_‚â§2Œ∑œÉÀÜ[2]_


_Œª[2]i_
_i=1_

X


3Œ∑

_[‚àí]_ _[L]_


2Œ∑œÉÀÜ[2][][ m] _Œª[2]i_ + L
_‚â§_

_i=1_

X 


_Œªi_ _Œ∏i[(][t,k][)]_ _Œ∏[(][t,k][)]_
_‚à•_ _‚àí_ [¬Ø] _‚à•[2]_
_i=1_

X


By convexity of F and telescoping k from 0 to K ‚àí 1, we have

1 _K‚àí1_ _m_ _m_ _K‚àí1_ 1

_K_ _kX=0_ **EF** (Œ∏[¬Ø][(][t,k][)]) ‚àí _F_ (Œ∏[‚àó]) _‚â§2Œ∑œÉÀÜ[2]_ Xi=1 _Œª[2]i_ ! + L Xi=1 _Œªi_ _kX=0_ _K_ **[E]h‚à•Œ∏i[(][t,k][)]** _‚àí_ _Œ∏[¬Ø][(][t,k][)]‚à•[2][i]_

1
+ _Œ∏[(][t,][0)]_ _Œ∏[‚àó]_ **E** _Œ∏[(][t,K][)]_ _Œ∏[‚àó]_ _._

2Œ∑K ‚à•[¬Ø] _‚àí_ _‚à•[2]_ _‚àí_ h‚à•[¬Ø] _‚àí_ _‚à•[2][i]_

Since _Œ∏[¬Ø][(][t,][0)]_ = Œ∏[(][t][)] and _Œ∏[¬Ø][(][t,K][)]_ = Œ∏[(][t][+1)], we complete the proof.

**Lemma 4 (Bounded client drift). Assuming the client learning rate satisfies Œ∑ ‚â§** 31L _[, we have]_

**E** _Œ∏i[(][t,k][)]_ _Œ∏[(][t,k][)]_ _Œ∑[2](24K_ [2][ ¬Ø]‚Ñ¶[2] + 20K‚Ñ¶[¬Ø] [2]).
_‚à•_ _‚àí_ [¬Ø] _‚à•[2][i]_ _‚â§_
h

_where_ ‚Ñ¶[¬Ø] [2] := max _œÉ[2], ‚Ñ¶[2]_ _._


_Proof._

b

2[]
**E** _Œ∏1[(][t,k][+1)]_ _Œ∏2[(][t,k][+1)]_ = E _Œ∏1[(][t,k][)]_ _Œ∏2[(][t,k][)]_ _Œ∑_ _g1(Œ∏1[(][t,k][)])_ _g2(Œ∏1[(][t,k][)])_
_‚àí_ _‚àí_ _‚àí_ _‚àí_
    

= _Œ∏1[(][t,k][)]_ _Œ∏2[(][t,k][)]_ 2Œ∑[2] _g1(Œ∏1[(][t,k][)])_ _F1(Œ∏1[(][t,k][)]), Œ∏1[(][t,k][)]_ _Œ∏2[(][t,k][)]_
_‚à•_ _‚àí_ _‚à•[2]_ _‚àí_ _‚àí‚àá_ _‚àí_
D E


2Œ∑ _F2(Œ∏1[(][t,k][)])_ _g2(Œ∏2[(][t,k][)]), Œ∏1[(][t,k][)]_ _Œ∏2[(][t,k][)]_
_‚àí_ _‚àá_ _‚àí_ _‚àí_
D E

2Œ∑ _F1(Œ∏1[(][t,k][)])_ _F2(Œ∏2[(][t,k][)]), Œ∏1[(][t,k][)]_ _Œ∏2[(][t,k][)]_ + Œ∑[2] _g1(Œ∏1[(][t,k][)])_ _g2(Œ∏2[(][t,k][)])_ _._ (20)
_‚àí_ _‚àá_ _‚àí‚àá_ _‚àí_ _‚à•_ _‚àí_ _‚à•[2]_
D E


-----

The second term (and similarly for the third term) is bounded as follows

_g1(Œ∏1[(][t,k][)])_ _F1(Œ∏1[(][t,k][)]), Œ∏1[(][t,k][)]_ _Œ∏2[(][t,k][)]_
_‚àí_ _‚àí‚àá_ _‚àí_
D 1 E

_‚â§_ 6Œ∑K _Œ∏1[(][t,k][)]_ _‚àí_ _Œ∏2[(][t,k][)]_ + [3][Œ∑K]2 _g1(Œ∏1[(][t,k][)]) ‚àí‚àáF1(Œ∏1[(][t,k][)])_ (by Peter Paul inequality)

1
= 6Œ∑K _Œ∏1[(][t,k][)]_ _‚àí_ _Œ∏2[(][t,k][)]_ [2] + [3][Œ∑K]2 _œÉ[2]_ (by (15)) [2]

Since maxi supŒ∏ _Fi(Œ∏)_ [2]F (Œ∏) b‚Ñ¶ (Assumption 5), the 4th-term is bounded as
_‚à•‚àá_ _‚àí‚àá_ _‚à•‚â§_

_F1(Œ∏1[(][t,k][)])_ _F2(Œ∏2[(][t,k][)]), Œ∏1[(][t,k][)]_ _Œ∏2[(][t,k][)]_
_‚àí_ _‚àá_ _‚àí‚àá_ _‚àí_
D E


_F_ (Œ∏1[(][t,k][)]) _F_ (Œ∏2[(][t,k][)]), Œ∏1[(][t,k][)] _Œ∏2[(][t,k][)]_ + 2‚Ñ¶ _Œ∏1[(][t,k][)]_ _Œ∏2[(][t,k][)]_
_‚â§‚àí_ _‚àá_ _‚àí‚àá_ _‚àí_ _‚à•_ _‚àí_ _‚à•_
D E

_‚â§‚àí_ _L[1]_ _[‚à•‚àá][F]_ [(][Œ∏]1[(][t,k][)]) ‚àí‚àáF (Œ∏2[(][t,k][)])‚à•[2] + 2‚Ñ¶‚à•Œ∏1[(][t,k][)] _‚àí_ _Œ∏2[(][t,k][)]‚à•_ (by smoothness and convexity)

1

_‚â§‚àí_ _L[1]_ _[‚à•‚àá][F]_ [(][Œ∏]1[(][t,k][)]) ‚àí‚àáF (Œ∏2[(][t,k][)])‚à•[2] + 6Œ∑K 1 _‚àí_ _Œ∏2[(][t,k][)]‚à•[2]_ + 6Œ∑K‚Ñ¶[2] (by Young‚Äôs inequality)

_[‚à•][Œ∏][(][t,k][)]_

The last term is bounded as follows


_g1(Œ∏1[(][t,k][)])_ _g2(Œ∏2[(][t,k][)])_
_‚à•_ _‚àí_ _‚à•[2]_

5 _g1(Œ∏1[(][t,k][)])_ _F1(Œ∏1[(][t,k][)])_ + _F1(Œ∏1[(][t,k][)])_ _F_ (Œ∏1[(][t,k][)]) + _F_ (Œ∏1[(][t,k][)]) _F_ (Œ∏2[(][t,k][)])
_‚â§_ _‚àí‚àá_ _‚à•‚àá_ _‚àí‚àá_ _‚à•[2]_ _‚à•‚àá_ _‚àí‚àá_ _‚à•[2]_


+ _F_ (Œ∏2[(][t,k][)]) _F2(Œ∏2[(][t,k][)])_ [2] + _g2(Œ∏2[(][t,k][)])_ _F2(Œ∏2[(][t,k][)])_
_‚à•‚àá_ _‚àí‚àá_ _‚à•[2]_ _‚àí‚àá_

5 _F_ (Œ∏1[(][t,k][)]) _F_ (Œ∏2[(][t,k][)]) + 10(œÉ[2] + ‚Ñ¶[2]) (by (15) and Assumption 5)[2][]
_‚â§_ _‚à•‚àá_ _‚àí‚àá_ _‚à•[2]_

Substituting the above four bounds back to (20) gives (note thatb _Œ∑ ‚â§_ 31L [)]

2

**Eh‚à•Œ∏+ 61[(][t,k]Œ∑[+1)][2]K‚àíœÉ[2]Œ∏2[(]+ 12[t,k][+1)]Œ∑[2]‚à•K[2][i]‚Ñ¶‚â§[2]** + 101 + Œ∑K[2][1](œÉ[2]‚à•+ ‚Ñ¶Œ∏1[(][t,k][2][)])‚àí _Œ∏2[(][t,k][)]‚à•[2]_ _‚àí_ _Œ∑_ _L_ _[‚àí]_ [5][Œ∑]‚àáF (Œ∏1[(][t,k][)]) ‚àí‚àáF (Œ∏2[(][t,k][)])

1 + [1] _Œ∏1[(][t,k][)]_ _Œ∏2[(][t,k][)]_ + 6Œ∑[2]KœÉ[2] + 12Œ∑[2]K‚Ñ¶[2] + 10Œ∑[2](œÉ[2] + ‚Ñ¶[2]).
_‚â§_ _Kb_ _‚à•_ _‚àí_ _‚à•[2]_ b
 

Unrolling recursively, we obtain b b

_K_
1 + 1/K 1

**E** _Œ∏1[(][t,k][+1)]_ _‚àí_ _Œ∏2[(][t,k][+1)]_ _‚â§_ 1/K _‚àí_ 6Œ∑[2]KœÉ[2] + 12Œ∑[2]K‚Ñ¶[2] + 10Œ∑[2](œÉ[2] + ‚Ñ¶[2])

  

h h i

[2][i] _‚â§_ 12Œ∑[2]K [2]œÉ[2] + 24Œ∑[2]K [2]‚Ñ¶[2]b+ 20Œ∑[2]K(œÉ[2] + ‚Ñ¶[2]) b

_‚â§_ _Œ∑[2](24K_ [2][ ¬Ø]‚Ñ¶[2] + 20K‚Ñ¶[¬Ø] [2]).

_K_ b b
where we use the fact that 1+11/K/K _‚àí1_ _K(e_ 1) 2K, and ‚Ñ¶[¬Ø] [2] := max _œÉ[2], ‚Ñ¶[2]_ .

_‚â§_ _‚àí_ _‚â§_

  

By convexity, for any i, 

b

**E** _Œ∏i[(][t,k][+1)]_ _Œ∏[(][t,k][+1)]_ _Œ∑[2](24K_ [2][ ¬Ø]‚Ñ¶[2] + 20K‚Ñ¶[¬Ø] [2]).
_‚à•_ _‚àí_ [¬Ø] _‚à•[2][i]_ _‚â§_
h

Substituting the result of Lemma 4 to Lemma 3, and telescoping over t, we obtain


1

_T_

h


_T ‚àí1_

_t=0_

X


_K‚àí1_ _D[2]_

_F_ (Œ∏[¬Ø][(][t,k][)]) _F_ (Œ∏[‚àó]) _œÉ[2]Œõ + Œ∑[2]L(24K_ [2][ ¬Ø]‚Ñ¶[2] + 20K‚Ñ¶[¬Ø] [2]),
_‚àí_ _‚â§_ 2Œ∑KT [+ 2][Œ∑][ÀÜ]
_k=0_

X i


-----

where D := ‚à•Œ∏[(0)] _‚àí_ _Œ∏[‚àó]‚à•, Œõ :=_



[(0)] _‚àí_ _Œ∏[‚àó]‚à•, Œõ :=_ _i=1_ _Œª[2]i_ [. By optimizing][ Œ∑][ on the R.H.S, we obtain]

_K‚àí1_ P _LD2_ _œÉDŒõ_ 21 13 ‚Ñ¶[¬Ø] 23 D 34 13 ‚Ñ¶[¬Ø] 23 D

_F_ (Œ∏[¬Ø][(][t,k][)]) _F_ (Œ∏[‚àó]) + _[L]_ 1 2 + _[L]_ 2
_kX=0_ _‚àí_ i _‚â§O_ _KT_ [+][ b]‚àöKT _K_ 3 T 3 _T_ 3


1

_KT_

h


_T ‚àí1_

_t=0_

X


when


_Œ∑ = min_


3L _[,]_


1 2 1 1 2
48 3 K 3 T 3 L 3 ‚Ñ¶[¬Ø] 3 _[,]_


1 1 1
40 3 KT 3 L 3 ‚Ñ¶[¬Ø]


_KT_ ŒõœÉb


D PROOF OF LEMMA 1

We first prove the following fact:


**Fact 1:**
(a) L(Q, f ) ‚â§ L[Œ≥]œÅ[(][P][Œª][, f] [)][,] _‚àÄf ‚ààF, Q ‚ààB(PŒª, œÅ)._

(b) inf _œÅ[(][P][Œª][, f][ ‚Ä≤][)][,]_ _Q_ (PŒª, œÅ).
_f_ _[‚Ä≤]‚ààF_ [L][(][Q, f][ ‚Ä≤][)][ ‚â§] _f[inf][‚Ä≤]‚ààF_ [L][Œ≥] _‚àÄ_ _‚ààB_

For (a), we have

L(Q, f ) ‚â§ _P_ _[‚Ä≤]‚ààBsup(PŒª,œÅ)L(P_ _[‚Ä≤], f_ ) = infŒ≥[‚Ä≤]‚â•0nŒ≥[‚Ä≤]œÅ[2] + EZ‚àºPŒª hœÜŒ≥(Z, f )io

_Œ≥œÅ[2]_ + EZ _PŒª_ _œÜŒ≥(Z, f_ ) =: L[Œ≥]œÅ[(][P][Œª][, f] [)][,]
_‚â§_ _‚àº_

where the equality is due to strong duality result by Gao & Kleywegt (2016).h i


For (b), defining fPŒª := arg minf ‚Ä≤‚ààF LœÅ[Œ≥][(][P][Œª][, f][ ‚Ä≤][)][, we have]

_finf[‚Ä≤]‚ààF_ [L][(][Q, f][ ‚Ä≤][)][ ‚â§] [L][(][Q, f][P][Œª] [)][ ‚â§] _P_ _[‚Ä≤]‚ààBsup(PŒª,œÅ)L(P_ _[‚Ä≤], fPŒª_ ) (21)

= infŒ≥[‚Ä≤]‚â•0 _Œ≥[‚Ä≤]œÅ[2]_ + EZ‚àºPŒª _œÜŒ≥(Z, fPŒª_ ) (22)

n h io

_‚â§_ _Œ≥œÅ[2]_ + EZ‚àºPŒª _œÜŒ≥(Z, fPŒª_ ) (23)

= inf _œÅ[(][P][Œª][, f]h[ ‚Ä≤][)][.]_ i (24)
_f_ _[‚Ä≤]‚ààF_ [L][Œ≥]

We next prove the second fact:


**Fact 2:**
(a) L[Œ≥]œÅ[(][P][Œª][, f] [)][ ‚â§] [L][(][Q, f] [) + 2][L][z][œÅ][ +][ |][Œ≥][ ‚àí] _[Œ≥][‚àó][|][œÅ][2][,]_ _‚àÄf ‚ààF, Q ‚ààB(PŒª, œÅ)_

(b) inf _œÅ[(][P][Œª][, f][ ‚Ä≤][)][ ‚â§]_ [inf]
_f_ _[‚Ä≤]‚ààF_ [L][Œ≥] _f_ _[‚Ä≤]‚ààF_ [L][(][Q, f][ ‚Ä≤][) + 2][L][z][œÅ][ +][ |][Œ≥][ ‚àí] _[Œ≥][‚àó][|][œÅ][2][.]_

For (a), we have:

L[Œ≥]œÅ[(][P][Œª][, f] [) =] sup L(P _[‚Ä≤], f_ ) + L[Œ≥]œÅ[(][P][Œª][, f] [)][ ‚àí] sup L(P _[‚Ä≤], f_ )
P _[‚Ä≤]‚ààB(PŒª,œÅ)_  n _P_ _[‚Ä≤]‚ààB(PŒª,œÅ)_ o

_‚â§_ L(Q, f ) + 2LzœÅ + **EZ‚àºPŒª** [œÜŒ≥(Z, f )] + œÅ[2]Œ≥ ‚àí _Œ≥min[‚Ä≤]_ 0 _œÅ[2]Œ≥[‚Ä≤]_ + EZ‚àºPŒª [œÜŒ≥‚Ä≤ (Z, f )]
n o  _‚â•_ n o[]

_‚â§_ L(Q, f ) + 2LzœÅ + œÅ[2](Œ≥ ‚àí _Œ≥[‚àó]) + EZ‚àºP_ _œÜŒ≥(Z, f_ ) ‚àí _œÜŒ≥‚àó_ (Z, f )
h i

= L(Q, f ) + 2LzœÅ + œÅ[2](Œ≥ ‚àí _Œ≥[‚àó]) + EZ‚àºP_  _Œ∂sup‚ààZ_ n‚Ñì(Œ∂, h) ‚àí _Œ≥d(Œ∂, Z)o_ _‚àí_ _Œ∂sup‚ààZ_ n‚Ñì(Œ∂, h) ‚àí _Œ≥[‚àó]d[2](Œ∂, Z)o[]_

= L(Q, f ) + 2LzœÅ + (Œ≥ ‚àí _Œ≥[‚àó])_ _œÅ[2]_ _‚àí_ **EZ‚àºP** _Œ∂sup‚ààZ_ _d[2](Œ∂, Z)_
 h i

L(Q, f ) + 2LzœÅ + _Œ≥_ _Œ≥[‚àó]_ _œÅ[2],_
_‚â§_ _|_ _‚àí_ _|_


-----

where the first inequality is due to Proposition 1, and the last inequality is because we choose Œ≥ ‚â•
_Lz/œÅ and that fact that Œ≥[‚àó]_ _Lz/œÅ by Lemma 1 of Lee & Raginsky (2018)._
_‚â§_

For (b), defining fQ := arg minf L(Q, f ), we have
_‚ààF_

inf _œÅ[(][P][Œª][, f][ ‚Ä≤][)][ ‚â§]_ [L][Œ≥]œÅ[(][P][Œª][, f][Q][)] (25)
_f_ _[‚Ä≤]‚ààF_ [L][Œ≥]

L(Q, fQ) + 2LzœÅ + _Œ≥_ _Œ≥[‚àó]_ _œÅ[2]_ (26)
_‚â§_ _|_ _‚àí_ _|_

= inf (27)
_f_ _[‚Ä≤]‚ààF_ [L][(][Q, f][ ‚Ä≤][) + 2][L][z][œÅ][ +][ |][Œ≥][ ‚àí] _[Œ≥][‚àó][|][œÅ][2][,]_


where the second line is due to Fact 2(a).

Combining all facts, we complete the proof. Specifically, by adding two inequalities in Fact 1(a)
and Fact 2(b), we obtain the upperbound of Lemma 1. Similarly, adding two inequalities in Fact
**1(b) and Fact 2(a), we obtain the lowerbound of this lemma.**

Finally, we provide the proof of the following proposition that was used in proving Fact 2(a).

**Proposition 1. Let Assumption 2 (a) holds. For any f ‚ààF and for all Q ‚ààB(PŒª, œÅ), we have**

sup L(P _, f_ ) L(Q, f ) + 2LzœÅ.
_P_ _[‚Ä≤]‚ààB(PŒª,œÅ)_ _[‚Ä≤]_ _‚â§_

_Proof. Denote P_ _[‚àó]_ := arg max L(P _[‚Ä≤], f_ ). We have
_P_ _[‚Ä≤]‚ààB(PŒª,œÅ)_

_P_ _[‚Ä≤]‚ààBsup(PŒª,œÅ)L(P_ _[‚Ä≤], f_ ) = L(Q, f ) + _P_ _[‚Ä≤]‚ààBsup(PŒª,œÅ)L(P_ _[‚Ä≤], f_ ) ‚àí L(Q, f )

_‚â§_ L(Q, f ) + |L(P _[‚àó], f_ ) ‚àí L(Q, f )|,

_‚â§_ L(Q, f ) + Lz **EZ‚àºP ‚àó** []‚Ñì(Z, h)/Lz _‚àí_ **EZ‚àºQ** _‚Ñì(Z, h)/Lz_

L(Q, f ) + LzW1(P _, Q)_
_‚â§_ _[‚àó]_   

L(Q, f ) + Lz _W2(P_ _, PŒª) + W2(PŒª, Q)_ (28)
_‚â§_ _[‚àó]_

L(Q, f ) + Lz2œÅ,

 

_‚â§_


where the fourth line are due to Kantorovich-Rubinstein dual representation theorem, i.e.,


**EZ‚àºP** _h(Z)_ _‚àí_ **EZ‚àºQ** _h(Z)_ : h(¬∑) is 1-Lipschitz
   


_W1(P, Q) = sup_


and the fifth line is due to W1(P _, Q)_ _W2(P_ _, Q) and triangle inequality._

_[‚àó]_ _‚â§_ _[‚àó]_

E PROOF OF THEOREM 2

_Proof. To simplify notation, we denote Œ¶ := œÜŒ≥_ = _z_ _œÜŒ≥(z, f_ ), f where = _fŒ∏, Œ∏_
_‚ó¶F_ _{_ _7‚Üí_ _‚ààF}_ _F_ _‚àà_
Œò R[d], which represents the composition of œÜŒ≥ with each of the loss function fŒ∏ parametrized
_‚äÇ_ 
by Œ∏ belonging to the parameter class Œò.

Defining fPŒª arg minf LœÅ[Œ≥][(][P][Œª][, f] [)][ and][ b]Œ∏[‚àó] argmin **EZ** _PŒª_ _œÜŒ≥(Z, fŒ∏)_ such that
_‚àà_ _‚ààF_ _‚àà_ _Œ∏‚ààŒò_ _‚àº_

L[Œ≥]œÅ[(][ b]PŒª, fŒ∏‚àó ) = infŒ∏ Œò **EZ‚àºPŒª** _œÜŒ≥(Z, fŒ∏)_ + Œ≥œÅ[2][i], we decompose the excess risk as follows:b  
_‚àà_

h b  


-----

E[Œ≥]œÅ[(][P][Œª][, f]Œ∏[b][Œµ] [) =][ L]œÅ[Œ≥][(][P][Œª][, f]Œ∏[b][Œµ] [)][ ‚àí] [inf] _œÅ[(][P][Œª][, f]_ [)]
_f_ [L][Œ≥]
_‚ààF_

= L[Œ≥]œÅ[(][P][Œª][, f]Œ∏[b][Œµ] [)][ ‚àí] [L]œÅ[Œ≥][(][P][Œª][, f][P]Œª [)]

= L[Œ≥]œÅ[(][P][Œª][, f]Œ∏[b][Œµ] [)][ ‚àí] [L]œÅ[Œ≥][(][ b]PŒª, fŒ∏[Œµ] [)] + L[Œ≥]œÅ[(][ b]PŒª, fŒ∏[Œµ] [)][ ‚àí] [L]œÅ[Œ≥][(][ b]PŒª, fŒ∏[‚àó] [)]
h b i h b _Œµ_ b i

_‚â§_

+ L[Œ≥]œÅ[(][ b]PŒª, fŒ∏[‚àó] [)][ ‚àí] [L]œÅ[Œ≥][(][ b]PŒª, fPŒª ) |+ L[Œ≥]œÅ[(][ b]PŒª, fP{zŒª ) ‚àí L[Œ≥]œÅ[(][P][Œª][, f][P]}Œª [)]
h b 0 i h i

_‚â§_

_‚â§_ 2 supœÜ|Œ≥ _‚ààŒ¶_ **EZ‚àºPŒª** [œÜ{zŒ≥(Z, fŒ∏)] ‚àí **EZ}‚àºPŒª** [[][œÜ][Œ≥][(][Z, f][Œ∏][)]] + Œµ

_m_ b

_‚â§_ 2 supœÜŒ≥ _‚ààŒ¶_ _i=1_ _Œªi_ **EZi‚àºPi** [œÜŒ≥(Zi, fŒ∏)] ‚àí **EZi‚àºPi** [[][œÜ][Œ≥][(][Z][i][, f][Œ∏][)]] + Œµ

_m_ X b

_‚â§_ 2 _i=1_ _Œªi supœÜŒ≥_ _‚ààŒ¶_ **EZi‚àºPi** [œÜŒ≥(Zi, fŒ∏)] ‚àí **EZi‚àºPi** [[][œÜ][Œ≥][(][Z][i][, f][Œ∏][)]] + Œµ

X b

_m_

2 log(2m/Œ¥)

_Œªi_ 4Ri(Œ¶) + 2M‚Ñì + Œµ with probability at least 1 _Œ¥,_ (29)

_‚â§_ _i=1_  s _ni_  _‚àí_

X

where the first inequality is due to optimization error and definition of _Œ∏[‚àó]. The second inequality is_
due to the fact that |[P]i[m]=1 _[Œª][i][a][i][| ‚â§]_ [P]i[m]=1 _[Œª][i][|][a][i][|][,][ ‚àÄ][a][i][ ‚àà]_ [R][ and][ Œª][i][ ‚â•] [0][. The third inequality is because]
pushing the sup inside increases the value. For the last inequality, using the facts that (i) _œÜŒ≥(z, f_ )

[b] _|_ _| ‚â§_
_M‚Ñì_ due to _M‚Ñì_ _‚Ñì(z, h)_ _œÜŒ≥(z, f_ ) supz _‚Ñì(z, h)_ _M‚Ñì_ and (ii) the Rademacher complexity
of the function class ‚àí _‚â§_ Œ¶ defined by ‚â§ Ri(Œ¶) = ‚â§ **E[sup‚ààZ** _œÜŒ≥_ Œ¶ _n ‚â§1i_ _nk=1i_ _[œÉ][k][œÜ][Œ≥][(][Z][k][, f][Œ∏][)]][ where the expecta-]_
_‚àà_

i.i.d.
tion is w.r.t both Zk _Pi and i.i.d. Rademacher random variableP_ _œÉk independent of Zk,_ _k_ [ni],
_‚àº_ _‚àÄ_ _‚àà_
we have


2 log(2m/Œ¥)

_œÜsupŒ≥_ Œ¶ **EZi‚àºPi** [œÜŒ≥(Zi, fŒ∏)] ‚àí **EZi‚àºPi** [[][œÜ][Œ≥][(][Z][i][, f][Œ∏][)]] _‚â•_ 2Ri(Œ¶) + M‚Ñìs _ni_ (30)
_‚àà_

b

with probability ‚â§ _Œ¥/m due to the standard symmetrization argument and McDiarmid‚Äôs inequal-_
ity (Shalev-Shwartz & Ben-David, 2014, Theorem 26.5). Multiplying Œªi to both sides of (30),
summing up the inequalities over all i ‚àà [n], and using union bound, we obtain (29).

Define a stochastic process _XœÜŒ≥_ _œÜŒ≥_ _‚ààŒ¶_

_ni_

   1

_XœÜŒ≥ :=_ _œÉkœÜŒ≥(Zk, fŒ∏)_
_‚àöni_

_k=1_

X

which is zero-mean because E _XœÜŒ≥_ = 0 for all œÜŒ≥ Œ¶. To upper-bound Rn(Œ¶), we first show
_‚àà_
that _XœÜŒ≥_ _œÜŒ≥_ _‚ààŒ¶_ [is a sub-Gaussian process with respect to the following pseudometric] 
   _œÜŒ≥_ _œÜ[‚Ä≤]Œ≥_ _œÜŒ≥(z, fŒ∏)_ _œÜŒ≥(z, fŒ∏[‚Ä≤]_ ) _._ (31)

_‚àí_ _‚àû_ [:= sup]z _‚àí_
_‚ààZ_

For any t ‚àà R, using Hoeffding inequality with the fact that œÉk, k ‚àà [n], are i.i.d. bounded random
variable with sub-Gaussian parameter 1, we have

_t_ _ni_

**E** exp _t_ _XœÜŒ≥ ‚àí_ _XœÜ‚Ä≤Œ≥_ = E "exp _‚àöni_ _k=1_ _œÉk (œÜŒ≥ (Zk, fŒ∏) ‚àí_ _œÜ (Zk, fŒ∏‚Ä≤_ ))!#
h   i _t_ X _ni_

= **E** exp _œÉ1 (œÜŒ≥ (Z1, fŒ∏)_ _œÜŒ≥ (Z1, fŒ∏‚Ä≤_ ))
_‚àöni_ _‚àí_
   

_t[2]_ _œÜŒ≥_ _œÜ[‚Ä≤]Œ≥_

exp _‚àí_ _‚àû_ _._
_‚â§_ 2 !

[2]


-----

Then, invoking Dudley entropy integral, we have

_‚àû_
_‚àöni Ri(Œ¶) = E sup_ _XœÜŒ≥_ 12
_œÜŒ≥_ Œ¶ _‚â§_ 0
_‚àà_ Z


log (Œ¶, _, œµ)dœµ_ (32)
_N_ _‚à•¬∑‚à•‚àû_


We will show that when Œ∏ _‚Ñì(z, hŒ∏) is LŒ∏-Lipschitz by Assumption 2, then Œ∏_ _œÜŒ≥(z, fŒ∏) is also_
_7‚Üí_ _7‚Üí_
_LŒ∏-Lipschitz as follows._

_œÜŒ≥(z, fŒ∏)_ _œÜŒ≥(z, fŒ∏[‚Ä≤]_ ) = sup inf _‚Ñì(Œ∂, hŒ∏)_ _Œ≥d(Œ∂, z)_ _‚Ñì(Œ∂_ _[‚Ä≤], hŒ∏‚Ä≤_ ) + Œ≥d(Œ∂ _[‚Ä≤], z)_
_‚àí_ _Œ∂‚ààZ_ _Œ∂[‚Ä≤]‚ààZ_ _‚àí_ _‚àí_

n o

sup _‚Ñì(Œ∂, hŒ∏)_ _‚Ñì(Œ∂, hŒ∏‚Ä≤_ )
_‚â§_ _Œ∂‚ààZ_ _‚àí_

n o

sup _‚Ñì(Œ∂, hŒ∏)_ _‚Ñì(Œ∂, hŒ∏[‚Ä≤]_ )
_‚â§_ _Œ∂‚ààZ_ _‚àí_

_LŒ∏_ _Œ∏_ _Œ∏[‚Ä≤]_ _,_
_‚â§_ _‚à•_ _‚àí_ _‚à•_

which implies
_œÜŒ≥ ‚àí_ _œÜ[‚Ä≤]Œ≥_ _‚àû_ _[‚â§]_ _[L][Œ∏][‚à•][Œ∏][ ‚àí]_ _[Œ∏][‚Ä≤][‚à•][.]_

Therefore, by contraction principle (Shalev-Shwartz & Ben-David, 2014), we have

_N (Œ¶, ‚à•¬∑‚à•‚àû, œµ) ‚â§N (Œò, ‚à•¬∑‚à•, œµ/LŒ∏) ._ (33)

Substituting (33) and (32) into (29), we obtain


48C(Œò)
+ 2M‚Ñì
_‚àöni_


2 log(2m/Œ¥)

_ni_


48C(Œò) 2 log(2m/Œ¥)

E[Œ≥]œÅ[(][P][Œª][, f]Œ∏[b][Œµ] [)][ ‚â§] _Œªi_ + 2M‚Ñì + Œµ, (34)

_i=1_ " _‚àöni_ s _ni_ #

X

which will be substituted into the upper-bound in Lemma 1 to complete the proof.


F PROOF OF CORROLARY 1

We now present how we adapt the result from Fournier & Guillin (2015) to prove Corollary 1
**Proposition 2 (Measure concentration (Fournier & Guillin, 2015, Theorem 2)). Let P be a probabil-**
_i.i.d._
_ity distribution on a bounded set Z. Let_ _Pn denote the empirical distribution of Z1, . . ., Zn_ _‚àº_ _P._
_Assuming that there exist constants a > 1 such that A := EZ‚àºP_ exp(‚à•Z‚à•[a]) _< ‚àû_ _(i.e., P is a_
_light-tail distribution). Then, for any œÅ > 0,_

[b]  

_c1 exp_ _c2nœÅ[max][{][d/p,][2][}][]_ _if œÅ_ 1
**P** _Wp(Pn, P_ ) ‚â• _œÅ_ _‚â§_ _c1 exp (‚àíc2nœÅ[a])_ _if œÅ > ‚â§ 1_
h i   ‚àí

_where c1, c2 are constants depending on[b]_ _a, A and d._


As a consequence of this proposition, for any Œ¥ > 0, we have


min 2/d,1/2

log(c1/Œ¥) _{_ _}_

_c2n_ if n _c2_ _,_

1/Œ± _‚â•_ [log(][c][1][/Œ¥][)] (35)

log(c1/Œ¥) 

_c2n_ if n < [log(]c[c]2[1][/Œ¥][)] _._



_W2(Pn, P_ ) ‚â§ _œÅ[Œ¥]n_ _‚â•_ 1 ‚àí _Œ¥ where_ _œÅ[Œ¥]n_ [:=]
i

[b] b b


In Proposition 2, Fournier & Guillin (2015) show that the empirical distribution _Pn converges in_
Wasserstein distance to the true P at a specific rate. This implies that judiciously scaling the radius
of Wasserstein balls according to (35) provides natural confidence regions for the data-generating

[b]
distribution P .

By the duality of transport cost (Santambrogio, 2015, p.261), we have


_Wp[p][(][¬µ, ŒΩ][) =]_ sup
_œï(x)+œà(y)‚â§d[p](x,y)_


_œï d¬µ + œà dŒΩ =_ sup _Tf_ (¬µ, ŒΩ), _p_ 1,
_œï(x)+œà(y)‚â§d[p](x,y)_ _‚àÄ_ _‚â•_


-----

which is the supremum of linear functionals Tf : P √ó P _7‚Üí_ R defined by Tf (¬µ, ŒΩ) =
_‚ü®(¬µ, ŒΩ), (œï, œà)‚ü©; therefore, (¬µ, ŒΩ) 7‚Üí_ _Wp[p][(][¬µ, ŒΩ][)][ is convex. Thus we have]_

_[m]_ _m_
_W2[2][(][P][Œª][,][ b]PŒª) = W2[2]_ _Œªi(Pni_ _, Pi)_ _‚â§_ _ŒªiW2[2][(][ b]Pni_ _, Pi)._ (36)

_i=1_ _i=1_

X  X

Then, we have [b]

_m_

_m_

**P** _W2(PŒª,_ _PŒª) ‚â•_ _i=1_ _[Œª][i]œÅ[b][Œ¥/m]ni_ = P _W2[2][(][P][Œª][,][ b]PŒª) ‚â•_ _ŒªiœÅ[Œ¥/m]ni_

_i=1_

h rX i h _[m]_ X _m_ i

[b] b

_‚â§_ **P** _ŒªiW2[2][(][ b]Pni_ _, Pi) ‚â•_ _ŒªiœÅ[Œ¥/m]ni_

_i=1_ _i=1_

_mhX_ X i

b

_‚â§_ **P** _W2[2][(][ b]Pni_ _, Pi) ‚â•_ _œÅ[Œ¥/m]ni_

_i=1_

Xm h i

b

= **P** _W2(Pni_ _, Pi)_ _œÅ[Œ¥/]ni[2][m]_

_‚â•_
_i=1_

Xm h i

_Œ¥_ [b] b

(37)

_‚â§_ 2m [=][ Œ¥]2 _[,]_

_i=1_

X

where the first inequality is due to (36), the second inequality is due to the union bound, and the last
inequality is due to Proposition 2 and (35).

1/2
According to (28), by setting œÅ = _mi=1_ _[Œª][i]œÅ[b][Œ¥/m]ni_ in Theorem 2 and using union bound, we
complete the proof. P 

G ADDITIONAL EXPERIMENTAL SETTINGS AND RESULTS

G.1 DATASETS

Table 1: Statistics of all datasets using in the experiments.

Total Num labels Samples / client
Dataset _m_
samples / client Mean Std

CIFAR-10 20 43,098 3 2154 593.8
MNIST 100 70,000 2 700 313.4
MNIST-M 100 70,000 2 700 322.4

We distribute all datasets to clients as follows:

-  MNIST: A handwritten digit dataset (Lecun et al., 1998) including 70, 000 instances belonged to 10 classes. We distribute dataset to m = 100 clients and each client has a different
local data size with only 2 of the 10 classes.

-  CIFAR-10: An object recognition dataset (Krizhevsky, 2009) including 60, 000 colored
images belonged to 10 classes. We partition the dataset to m = 20 clients and there are 3
labels per client. Each client has a different local data size.

-  Three handwritten digit: MNIST, MNIST-M (Ganin & Lempitsky, 2015), and USPS
(Hull, 1994). We distribute one dataset, for example, MNIST to 100 source clients, and
leave MNIST-M to the target client. Each source client has only 2 labels over 10 labels. As
the number of data samples in USPS is relatively small amount (9,298 samples), we only
use this dataset for the target client.

We standardize and randomly split all datasets with 75% and 25% for training and testing, respectively. In domain adaptation, the target client only has test data. The statistics of all datasets are
summarized in Tab. 1.


-----

MNIST: Clean Data

50 100 150 200


MNIST: Clean Data

50 100 150 200


MNIST: Under Distribution Shifts

0 50 100 150 200


MNIST: Under Distribution Shifts

0 50 100 150 200


1.8

1.6

1.4

1.2

1.0

0.8

0.6

2.2

2.0

1.8

1.6

1.4

1.2


0.8

0.7

0.6

0.5

0.4

0.55

0.50

0.45

0.40

0.35

0.30

0.25

0.20


0.6

0.5

0.4

0.3


3.0

2.5

2.0

1.5

2.2

2.1

2.0

1.9

1.8

1.7

1.6

1.5


CIFAR: Under Distribution Shifts

WAFL : = 0.05


0.45

0.40

0.35

0.30

Global Accuracy

0.25

0.20


CIFAR: Clean Data

50 100 150 200


CIFAR: Clean Data


CIFAR: Under Distribution Shifts

0 50 100 150 200


50 100 150 200

T


50 100 150 200


WAFL : = 0.5


Figure 5: Convergence of WAFL.


G.2 MODELS

The details of models for each dataset is provided as follows:



-  MNIST: We use a multinomial logistic regression model (MLR) with a cross-entropy loss
function and an L2-regularization term.

-  CIFAR-10: We use a CNN model employed in McMahan et al. (2017).



-  Set of three handwriten digits: We use a multinomial logistic regression model (MLR)
with a cross-entropy loss function and an L2-regularization term.

In all settings, we randomly sample 10 clients to participate in training the global robust model
in each communication round, and set the number of local epochs to K = 2 and the number of
communication rounds to T = 200. All experiments were conducted using PyTorch (Paszke et al.,
2019).


G.3 CONVERGENCE OF WAFL

We verify the convergence of WAFL under two cases: clean data (no attacked clients) and distri_bution shifts (where 40% of clients are attacked). In each case, we use two datasets: MNIST and_
CIFAR-10 and employ the same setup as in Sec. 6. Specifically, for MNIST, we distribute the dataset
to 100 clients and set Œ≥ = 0.05. For CIFAR-10, we use 20 clients and set Œ≥ = 0.5. We use T = 200
communication iterations.

To show WAFL‚Äôs convergence, we plot both the original loss (using the function ‚Ñì) and global
accuracy in Fig. 5.


G.4 DOMAIN ADAPTATION.

We show that WAFL demonstrates its superior capability in domain adaptation by collaboratively
learning from a multi-source domain _PŒª, and applying it to an unseen, but related, target domain_
_Q. In federated domain adaptation (Peng et al., 2019; Liu et al., 2021), source domains often need_
access to the private data of target domains to find a common representation to aid training. However,[b]
this violates the data privacy assumption of FL. By contrast, the construction of the model inb WAFL
does not involve such private data.

Our experimental set up is as follows. We consider three datasets: MNIST (mt), MNIST-M (mm)
and USPS (up), which are widely used in domain adaptation. Among these datasets, we consider


-----

Table 2: Domain adaptation performance of WAFL and FedAvg as an accuracy on the target dataset.

Algorithm **_mt‚Üímm_** **_mt‚Üíup_** **_mm‚Üíup_** **_mm‚Üímt_** Average

WAFL **40.23** **66.94** **62.04** **79.27** **62.12**
FedAvg 37.71 66.08 58.60 75.80 59.55

one dataset as a source domain (distributed to 100 clients), and another dataset as a target domain
(assumed to be on one client). For example, if mt is used as the source and mm the domain, we
use mt‚Üímm to denote this case. Specifically, the mm dataset is distributed to 100 clients to learn
a global model, then the model is tested on the mt dataset. We compare WAFL against the vanilla
FedAvg as a baseline, and report the accuracy on the target dataset in several scenarios in Tab. 2.

WAFL markedly improves from FedAvg in all cases. On average, the accuracy of WAFL is 2.5 percentage point higher than that of FedAvg. This indicates WAFL‚Äôs benefits in transferring knowledge
from multi-sourced, private client distributions to a new target distribution.


-----

