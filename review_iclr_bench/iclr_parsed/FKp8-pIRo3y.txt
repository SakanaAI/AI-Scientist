## WISH YOU WERE HERE: HINDSIGHT GOAL SELECTION
### FOR LONG-HORIZON DEXTEROUS MANIPULATION

**Todor Davchev[1][,][2][,][â€ ][âˆ—], Oleg Sushkov[1][,][â€ ], Jean-Baptiste Regli[1], Stefan Schaal[3], Yusuf Aytar[1],**
**Markus Wulfmeier[1], Jon Scholz[1]**

1DeepMind, 2University of Edinburgh, 3Intrinsic LLC

ABSTRACT

Complex sequential tasks in continuous-control settings often require agents to
successfully traverse a set of â€œnarrow passagesâ€ in their state space. Solving
such tasks with a sparse reward in a sample-efficient manner poses a challenge
to modern reinforcement learning (RL) due to the associated long-horizon nature
of the problem and the lack of sufficient positive signal during learning. Various
tools have been applied to address this challenge. When available, large sets of
demonstrations can guide agent exploration. Hindsight relabelling on the other hand
does not require additional sources of information. However, existing strategies
explore based on task-agnostic goal distributions, which can render the solution
of long-horizon tasks impractical. In this work, we extend hindsight relabelling
mechanisms to guide exploration along task-specific distributions implied by a
small set of successful demonstrations. We evaluate the approach on four complex,
single and dual arm, robotics manipulation tasks against strong suitable baselines.
The method requires far fewer demonstrations to solve all tasks and achieves a
significantly higher overall performance as task complexity increases. Finally, we
investigate the robustness of the proposed solution with respect to the quality of
input representations and the number of demonstrations.

1 INTRODUCTION

Recent advances in model-free reinforcement learning (RL) have enabled successful applications in
a variety of practical, real-world tasks (Gu et al., 2017; Levine et al., 2018; Riedmiller et al., 2018;
OpenAI et al., 2019; Sharma et al., 2020; Gupta et al., 2021). Given the complexity of these tasks,
additional information such as demonstrations often plays an important role (Vecerik et al., 2019;
Zuo et al., 2020; Davchev et al., 2020; Sutanto et al., 2020; Luo et al., 2021). These methods are
particularly useful to robotics as they enable efficient learning with only sparse rewards, which are
easier to define. However, scaling such solutions to more complex long-horizon sequential settings
with limited number of demonstrations remains an open challenge (Gupta et al., 2020).

Solving long-horizon dexterous manipulation tasks is an important problem as it enables a wide range of useful robotic
applications ranging from object-handling tasks, common in
collaborative settings, through to contact-rich dual arm manipulations, often seen in manufacturing. Consider the bi-manual
3.5mm jack cable insertion problem as illustrated in Figure 1.
Solving such tasks with vanilla demo-driven RL often fails due
to the difficulty of assigning credit over long horizons in the
presence of noisy exploration and sparse rewards.

Self-supervision mechanisms like Hindsight Experience Replay
(HER) (Andrychowicz et al., 2017) offer an alternative to ease
the sparse reward problem by providing a stronger learning sig- Figure 1: HinDRL solving a 3.5mm
nal through additional goal-reaching tasks which are generated jack cable insertion task.
from the agentâ€™s trajectories. While the more frequent reward can be beneficial, the agentâ€™s trajectory

_âˆ—Work done during an internship at DeepMind; â€  Denotes equal contribution._


-----

distribution is often considerably more complex and non-stationary in comparison to the target-task
distribution. When the final task is very complex and the main task reward is not perceived, this
can lead to learning a capable, goal-reaching policy for states close to the agentâ€™s initial states while
unable to complete the actual task.

Our contribution is based on the insight that demonstration states can be viewed as samples of
the target task distribution, and can therefore be used to constrain the self-supervision process to
task-relevant goals. We introduce a framework for task-constrained goal-conditioned RL that flexibly
combines demonstrations with hindsight relabelling. Unlike HER, which learns a general goalconditioned agent, we train a goal-conditioned agent specialized at achieving goals which directly
lead to the task solution. The approach further allows to smoothly vary the task relevance of the
relabelling process. Unlike conventional goal-conditioned RL, we enable agents to solve tasks
through utilising abstract goal formulations, such as inserting a 3.5mm jack, common in the context
of complex sequential tasks. We achieve this through using a continuously improving target goal
distribution for the online goal selection stage. We refer to our method as a Hindsight Goal Selection
for Demo-Driven RL or HinDRL for short. We demonstrate that the proposed solution can solve tasks
where both demonstration-driven RL and its self-supervised version with HER struggle. Specifically,
we show that HinDRL can dramatically reduce the number of required demonstration by an order of
magnitude on most considered tasks.

2 RELATED LITERATURE

Using demonstrations in RL is a popular tool for robot learning. Classic approaches, such as (Atkeson
& Schaal, 1997; Peters & Schaal, 2008; Tamosiunaite et al., 2011), use expert demonstrations to
extract good initial policies before fine-tuning with RL. Kim et al. (2013); Chemali & Lazaric (2015)
use demonstrations to learn an imitation loss function to bootstrap learning. However, those solutions
are not applied to complex sequential tasks. Follow up work that use deep neural networks enable the
application of demo-driven RL to more complex tasks such as Atari(Aytar et al., 2018; Pohlen et al.,
2018; Hester et al., 2018), or robotics (Vecerik et al., 2017; 2019; Paine et al., 2018). We build upon
DPGfD(Vecerik et al., 2019), which we explain in more details in Section 3.

The Atari literature often requires large number of environment steps (Salimans & Chen, 2018; Bacon
et al., 2017). This can result in hardware wear and tear if applied on a physical robotics system.
Alternatively, a number of robotics solutions assume the existence of a manually defined structure,
e.g. through specifying an explicit set of primitive skills (Stulp et al., 2012; Xie et al., 2020), or a
manually defined curricula (Davchev et al., 2020; Luo et al., 2021). However, hand-crafting structure
can be sub-optimal in practice as it varies across tasks. In contrast, restricting the search space by
greedily solving for specific goals (Foster & Dayan, 2002; Schaul et al., 2015) can be combined with
implicitly defined curricula through retroactive goal selection (Kaelbling, 1993; Andrychowicz et al.,
2017). Hindsight goal assignment has been successfully applied to multi-task RL (Li et al., 2020;
Eysenbach et al., 2020), reset-free RL (Sharma et al., 2021) but also for batch RL (Kalashnikov et al.,
2021; Chebotar et al., 2021) and hierarchical RL (Wulfmeier et al., 2020; 2021). However, these
approaches explore based on task-agnostic goal distributions and compensate with additional policy
structure such as allowing for longer training to concurrently learn a hierarchy, or using large amounts
of offline data. In this work, we extend hindsight relabelling mechanisms to guide exploration along
task-specific distributions implied from a limited number of demonstrations and study its performance
against strong task-agnostic goal distributions.

Combining hindsight relabelling with demonstrations is not a novel concept. Ding et al. (2019)
relabels demonstrations in hindsight to improve generative adversarial imitation learning. The
proposed solution is used to learn a goal-conditioned reward function that can be applied on demodriven RL for hindsight relabelling. Gupta et al. (2020) propose a hierarchical data relabelling
mechanism that uses demonstrations to extract goal-conditioned hierarchical imitation-based policies
and apply them to multi task RL. Nair et al. (2018a) fuses demo-driven RL similar to us but uses
HERâ€™s final-goal sampling mechanism and combines it with structured resets to specific demo
states to overcome the difficulties of exploration for long-horizon tasks, which would be difficult to
achieve in a continuous robotics task in practice. Zuo et al. (2020) uses demonstrations to bootstrap
TD3 and combines it with HER. However, in all those works, hindsight goals were always chosen
directly from the agentâ€™s own trajectory. Instead, we consistently select goals directly from a task
_distribution implied from a collection of successful demonstrations. Hindsight goal selection has_
previously been generalized via adopting representation learning. Florensa et al. (2019) applied HER


-----

**if successful** **Online Goal**
**Selection Stage**

**Agent Rollout**

**Encoder**

**. . .** **Goal DB**

#### ð…

**t** **t + 1** **T**

**Encoder**

**Encoder** **Encoder** **Encoder** **DemonstrationsTask**

**Goal**

**Hindsight Goal**
**Selection Stage**

**HER** **Task-specific goal**

**candidates**

agent rollout **Select K goals in**

start **hindsight** start

**for each timestep**

**Replay Buffer** Candidate Goal z g **and relabel** target

Candidate Goal

Sample z from agent's trajectoryStrategy a)g Sample z from demonstrationsg Strategy b)


Figure 2: The HinDRL pipeline performing a 3.5mm jack cable insertion. The input to the policy are encoded
target goal and the current robot state. At train time, the produced episodes are relabelled in hindsight using
a choice of sampling strategies. We consider two main strategies for sampling goals in hindsight. Strategy a)
shows the standard hindsight goal selection strategies that select goals from states within the trajectory being
relabelled; and strategy b) shows a mechanism that focuses on sampling goals directly from some collection of
successful trajectories. The resulted relabelled data is fed into the replay buffer.

to a pixels-to-torque reaching task where sample efficiency was not a direct constraint. Nair et al.
(2018b) used a Î² _âˆ’balanced variational auto encoder (Î²_ -VAE) to enable learning from visual inputs
with a special reward. While Î² -VAE is broadly a powerful tool for unsupervised representation
learning, it does not take into account the temporal nature of sequential robotics tasks. This makes
them sub-optimal in the context of our work (Chen et al., 2021). In contrast, contrastive-learning
based techniques such as temporal cycle consistency (TCC) (Dwibedi et al., 2019) can provide more
informative representations as previously discussed in concurrent work (Zakka et al., 2022). In this
work we combine HinDRL, a framework for goal-conditioned demo-driven RL with both learnt and
engineered representations and provide a comprehensive sensitivity analysis to the quality of the
encoder and the dependency against the number of demonstrations used.

3 PRELIMINARIES

Consider a finite-horizon discounted Markov decision process (MDP), M = (S, _A,_ _P,_ _r,_ _Î³,_ _T_ ) with
transition probability P : S _Ã—_ _A_ _Ã—_ _S 7â†’_ [0, 1]. Let the current state and goal s, _g âˆˆ_ _S âŠ†_ R[n][s] be elements in
the state space S, and a âˆˆ _A âŠ†_ R[n][a] denote the desired robot action. We define a sparse environmental
reward r(s) that assigns 0/1 reward only at the final state of an episode. Let Ïˆ(Â·) be an encoding
function that embeds a given state and goal to a latent state z = Ïˆ(s) and a latent goal g = Ïˆ(g).
Let Î¶ = **xt0,...,** **xT** be a trajectory with a discrete horizon T, a discount function Î³( ), a state
_Â·_
action tuple  _xt = (st,_ _at_ _,_ _g) and a trajectory return R(Î¶_ ) = âˆ‘t[T]=t0 _[Î³][r][(][s][t]_ [)][. In this setting, a transition]
(st _,_ _at_ _,_ _st+1,_ _g,_ _r = 0) can be â€™relabelledâ€™ as (st_ _,_ _at_ _,_ _st+1, Ë†g â‰ˆ_ _st+1,_ _r = 1) and both the original and_
relabelled transitions can be used for training. A separate, goal conditioned reward r(zt+1, Ë†zg) =
1[zt+1 = Ë†zg] uses the latent state and goal and assigns reward during relabelling. Finally, we also
assume access to D demonstration trajectories D = {Î¶ _j}[D]j=0_ [that reach the goals][ {]g[ Ë†] j}[D]j=1 [retrieved]
from the final state of the demonstration. Then, a goal-conditioned deterministic control policy
parameterised by Î¸, Ï€Î¸ (a|z, _zg) selects an action a given a latent state z and goal zg. In this context,_
we define an optimal policy Ï€ _[âˆ—]_ = argmaxÏ€âˆˆÏ€Â¯ _[J][(][Ï€][)][, where][ J][(][Ï€][) =][ E]s0âˆ¼p(s0),gâˆ¼Ï€(g)[[][R][(][Î¶]_ [)]][.]

DPGfD(Vecerik et al., 2019) is a powerful algorithm that bootstraps learning through a BC loss
applied directly to the actor. This loss is typically defined as the L2 distance between the predicted
by the policy actions and the true actions of a demonstration and is decayed proportionally to the
number of environmental steps. Gradual conversion from using a BC loss to using the actor-critic
loss is typically done using Q filtering or gradually decaying the BC loss. We employ the latter. The
algorithm uses an L2 regularised actor-critic loss with a distributional critic, introduced in (Bellemare


-----

et al., 2017). Using BC loss with DDPG was recently applied to a goal-conditioned setting (Ding
et al., 2019) but goal-conditioned policy learning was never used with demo-driven distributional RL.

4 METHODOLOGY

This section describes our method, HinDRL. First, we extend DPGfD to its goal-conditioned interpretation and introduce a mechanism that deals with intermediate goals. Next, we introduce two
demonstration-driven goal sampling strategies and discuss how to use them for goal-conditioned
policy learning. Finally, we discuss how to extract temporally consistent representations and define a
distance-based goal-conditioned reward function. We summarise our framework in Figure 2.

4.1 GOAL-CONDITIONED DEMO-DRIVEN REINFORCEMENT LEARNING
Similar to DPGfD we combine reinforcement and imitation learning. We utilise a data set of
demonstrations, D = {(s0, _a0,_ _sg,...)t[k][)][}][D]k=1_ [which are used to seed a replay buffer for RL, and for]
supervised training of the policy. We used both 6D Cartesian velocity and a binary open/close actions,
which we modelled using a standard regression for the predicted velocity and a classification for the
binary prediction. We use encoded state z and encoded goal zg as input to the policy network,

2
_LBC =_ _Ï€(z,_ _zg)c_ _ac_ 2 âˆ‘ _a[o]b[log][(][Ï€][(][z][,]_ _[z][g][)][o]b[)][.]_ (1)
_||_ _âˆ’_ _||[2]_ _[âˆ’]_ _i=1_

The BC loss is applied only to transitions from successful trajectories.

For RL we utilized the standard deterministic policy-gradient (DPG, Silver et al. (2014)) loss for
the deterministic actions (velocities), and stochastic value-gradient (SVG, Heess et al. (2015)) for
the stochastic actions (gripper positions). We used the Gumbel-Softmax trick (Jang et al., 2016;
Maddison et al., 2016) to reparameterize the binary actions for the SVG loss. Following Vecerik et al.
(2019) we applied both the BC and RL losses to train the policy, and annealed the BC loss throughout
training to allow the agent to outperform the expert.

We use a distributional critic (Bellemare et al., 2017), modeling the Q-function with a categorical
distribution between 0 and 1 (with 60 evenly spaced bins). The learning loss is the KL-divergence
between our current estimate and a projection of the 1-step return on the current bins. Therefore, the
loss becomes,

_LTD = KL(Q(zt_ _,_ _at_ _,_ _zg)||Î¦(rt +_ _Î³(t)_ _âˆ—_ _Qtarget_ (zt _,_ _Ï€(zt_ _,_ _zg),_ _zg))),_ (2)

where Î¦ is an operator that projects a distribution on the set of bins.

4.2 GOAL SELECTION FROM DEMONSTRATIONS

Typically, goal-conditioned policy learning has two separate stages for goal selection (Andrychowicz
et al., 2017): an online stage where a policy is conditioned on a specific goal during a policy rollout,
and a hindsight goal selection stage where the produced trajectory is being relabelled in hindsight
(See Figure 2). While Andrychowicz et al. (2017) only consider the original goal in the online stage,
we describe below how this perspective can be extended to increase robustness.

**Online goal selection A target goal of a sequential task, such as inserting a plug into a socket, is in**
principle more abstract than the physical skill of reaching a specific configuration. That is, being
an Îµ distance away from a target configuration might still result in a failed insertion. We mitigate
this issue by maintaining a separate goal database, Gdb, comprised of final states over successful
executions. We bootstrap this database with the final states from D but we also continuously grow
_Gdb as learning progresses. Conceptually, the more examples of final goal states we collect, the better_
understanding of the target goal of the current task we will have. In this context, for every episode
we sample a new Ë†zg from a distribution over the set of all possible goals in Gdb. We denote this as
we always store the resultedRG = {zg âˆˆ _Gdb : p(zg) > 0} Ë† and we refer to this process as online goal-conditioning. In this work,zg-conditioned trajectory, Î¶_, in the replay buffer. As in classic RL, the
last step of Î¶ is rewarded using the environment reward r(st ).

**Hindsight goal selection In contrast, hindsight goal selection is the process of retroactively sampling**
candidate goal states from some goal distribution p(zg) after an episode rollout. Here, zg are not
conceptually related to the abstract formulation of a target task as Ë†zg is. Instead, they represent
different stages from the behaviour that results in solving the abstract task. The way this stage works
is we sample new goals for each time step of the trajectory Î¶ using some candidate goal distribution


-----

**Algorithm 1 HinDRL**

1: Initial Î¸, Ï†, D = {Ï„ _[k]}k[D]=1[,][ RB][ =][ /]0_

2: Policy Ï€Î¸, encoder ÏˆÏ†, p(s0)
3: // if trainable, learn encoder offline here
4: // encode demos, add to replay buffer
5: RB â† _RB_ [S] _D_
6: // relabel trajectories and add to RB
7: RB _RB_ [S] {generate_samples(Ï„ _[k],_ _D)_ _k=1_
_â†_ _}[D]_
8: // add final state from demos to goal db
9: Gdb = Gdb _Ï„Tk_ _k=1_
10: for iter i _iters{_ _max[}][D] do_
_âˆˆ_ S
11:12: _sgË†0 âˆ¼ â†Gpdb(s // target goal0) // initial state_

13: // trajectory Î¶ = (zt _,_ _at_ _,_ _zt+1,_ _rt_ _,_ _zg)_ _t=1_
_{_ _}[T]_
14: _Î¶ â†_ rollout(s0, _g,_ _Ï€(Â·),_ _Ïˆ(Â·))_

15: **if success then**

16: // if successful, store last state in goal db

17:18: **end ifGdb â†** _Gdb_ _zT // zT âˆˆ_ _Î¶_
S

19: _RB â†_ _RB_ [S] _Î¶ // add rollout to RB_

20: // relabel Î¶ and add to RB

21: _RB â†_ _RB_ [S] generate_samples(Î¶ _,_ _D)_

22: // optionally, retrain Ï† here

23: _Î¸ â†_ _train_Ï€(R)_

24: end for


**Algorithm 2 Generate samples**

1: Input: trajectory, Î¶, task demos D
2: Ï‡ â† 0/ // relabelled transitions

3: for sampler âˆˆ sampling strategies list do
4: data â† 0/ // support data

5: **if sampler.name is Rollout-conditioned then**

6: data = Î¶ // HER

7: **else if sampler.name is Task-conditioned then**

8: data = D // Task

9: **end if**

10: // compose support for goal distribution

11:12: _R// sample new goals,G = {zg âˆˆ_ _data : p( Tzg =) > |Î¶ 0|}_

13: _zg_ _p(zg)_ _t=1_
14: _{// store relabelled transitions using Eq. 3 âˆ¼_ _}[T]_

15: _Ï‡_ _Ï‡_ (zt _,_ _at_ _,_ _zt+1,_ _r(zt+1,_ _zg),_ _zg)_ _t=1_
_â†_ _{_ _}[T]_
16: end for
17: Returns: Ï‡[S]


Figure 3: Algorithm for Hindsight goal selection for Demo-driven Reinforcement learning (HinDRL)

_p(zg) implied from some given behaviour. Typically we sample candidate goals from the future states_
in rollout trajectories. Then, we re-evaluate the given transition with the newly sampled goal and
assign a new reward. This is achieved with the help of a goal conditioned reward r(zt _,_ _zg) and not the_
environment reward from the previous paragraph. Thanks to the goal-conditioned formulation, fitting
a Q function from both of these sources of reward is well-posed. In this section we propose a way for
composing a hindsight goal distribution p(zg), that is targeted on the specific task at hand.

Candidate goals can be acquired through self-supervision (as in HER), where a goal distribution is
comprised of states from the agentâ€™s trajectory Î¶ . The agent retrospectively samples feasible goals
to â€™explainâ€™ its own behaviour under the current goal-conditioned policy Ï€. However, similar to
concurrent work (Gupta et al., 2020; Pertsch et al., 2020) we observed that choosing goals from
_p(zg|Î¶_ ) does not work well for long-horizon tasks.

However, hindsight goal selection does not need to be constrained to the same trajectory. Goals
can be sampled directly from a task distribution, implied from a set of successful trajectories, such
as demonstrations. Therefore, selecting task-specific goals can come from p(zg) with support
_Rsimilar to HER, this can benefit from alternative formulations too. In practice, the support of theG = {zg âˆˆ_ _D : p(zg) > 0}. We used a uniform distribution over the demonstration states. However,_
task-distribution, RG, can be enriched over time with additional positive task completions from the
training process too. However, we do not do it in this work.

Using the agentâ€™s trajectory Î¶ is useful when modelling the agentâ€™s behaviour. However, it can
struggle to scale beyond normal-horizon tasks, Gupta et al. (2020). On the other hand, using demodriven samplers can speed up training by directly modelling the task distribution. However, it requires
certain level of confidence over the quality of the task representation. These two support distributions
can complement each other and do not need to be disjoint. Joining both types of trajectories can be
particularly useful in cases where using just demonstration states can fail to make the sparse reward
problem easier, e.g. when the representations fail to capture the notion of progress. We provide an
ablation against joint-condition samplers in Appendix A.3, including using the union, Î¶ _D, and_
intersection Î¶ _D of the two sets of goals._ Our results indicate that doing so can improve the
performance of HinDRL with representations that do not preserve the notion of progress and speed[S]
up training for complex tasks (see Appendix A.4 and A.5).[T]

4.3 TIME-CONSISTENT REPRESENTATIONS

Synthesising goal-conditioned policies from raw state observations can be impractical in high
dimensional spaces. Using raw states can not only deteriorate the speed of learning (Hessel et al.,


-----

2018), but it also makes it difficult to define a comprehensive goal-conditioned reward for relabelling.
Instead, we encode our state observations into a lower dimensional latent space R[n][z], using an
encoder Ïˆ to obtain a latent state zt = Ïˆ(st ) and a latent goal zg = Ïˆ(g). Next, we propose both
expert-engineered and learnt time-consistent representations.

**Engineered representations In the presence of an expert, low dimensional representations can be**
programmed to include features that bear a notion of progress, e.g. through measuring the distance
from a target goal, as well as the notion of contact, such as whether an insertion was successful.
These notions vary across each task and we provide additional details for each of the engineered
encoders in Appendix A.9.

**Learnt representations We propose as an alternative to the engineered encoding, a self-supervised**
learnt representation that can be directly used with the provided demonstrations and refined over time.
We ensure consistent and aligned notion of episode progress in our learnt representation without the
need of labels by employing a differentiable cycle-consistency loss (TCC)(Dwibedi et al., 2019).
This allows us to learn a representation by finding correspondences across time between trajectories
of differing length, effectively preserving the notion of progress.

Training of TCC involves encoding an observation ot from trajectory U to a latent z[U]t [, and checking]
for cycle-consistency with another trajectory V that is not necessarily of the same length as U.
Cycle consistency is defined via nearest-neighbor â€“ if the nearest embedding in V to z[U]t [is][ z][V]m[, i.e.]
_nearest(z[U]t_ _[,][V]_ [) =][ z][V]m[, then][ z][U]t [and][ z][V]m [are cycle-consistent if and only if][ nearest][(][z][V]m[,][U][) =][ z][U]t [. Learning]
this requires a differentiable version of cycle-consistency measure, such as regression-based or
classification-based one. In this work, we employ the latter.

We additionally evaluate a non-temporal embedding using a Î² -VAE, similar to (Nair et al., 2018b)
(see Appendix A.5).
4.4 DISTANCE-BASED REWARD SPECIFICATION

Considering local smoothness in the learnt goal
representation space, we use a distance-based

# ... goal-conditioned reward function. This allows

us to increase the number of positive rewards
and therefore ease the hard exploration problem.
We particularly use,
_r(z,_ _zg) = 1[||z_ _âˆ’_ _zg|| < Îµ],_ (3)
for some threshold Îµ, where latent state z and
latent goal zg are considered similar if the l2
distance between them is below Îµ. We provide
more details on how we obtain the threshold in
Appendix A.8. Figure 4 illustrates the t-SNE
visualisation of a temporally aligned representation space learnt with TCC. The notion of
progress in TCC is consistent across all 100 plot
# ...

ted trajectories. We provide additional details

Figure 4: t-SNE of latent representations obtained with
TCC (Dwibedi et al., 2019) against novel successful in Appendix A.5. TCC is inherently capable of
trajectories. Temporal color-coding, purple is start and temporally aligning different in length and moyellow is goal. tion trajectories that pass through similar stages.

This makes it particularly suitable to use with distance-based metrics like Eq. 3.

The final proposed algorithm, together with the relabelling technique, is detailed in Algorithm 1.
5 EXPERIMENTS
We are interested in answering the following questions: i) is task-conditioned hindsight goal selection
useful for long-horizon sequential tasks; ii) does it improve over DPGfD and HERâ€™s sample efficiency;
iii) is HinDRL a demonstration efficient solution and how does it perform in a few-shot setting; and
iv) how useful are learnt time-consistent representations.

5.1 TASKS

We answer these questions based on four tasks with increasing complexity, implemented with MuJoCo
(Todorov et al., 2012). We consider an attempt successful if the agent gets under a threshold distance


-----

(a) (b) (c) (d) (e)

Figure 5: Tasks description a) is the parameterised reach task. Visiting the small yellow goals must happen
before reaching the middle goal. b) is dual arm reaching, we combine this with c) lifting, d) aligning in both
position and orientation and e) is the dual arm insertion.
from its target. The metrics include the running accuracy of completing the tasks and the overall
accumulated reward. All policies are executed in 10Hz with 12 random seeds. At test time we sample
a random goal from all successful goal states and for the agent. We use z = (s, _e(s))t and zg = e(g),_
for all encoders e(Â·) and always train with a sparse binary environment reward. All methods use
the same representation and rewards as HinDRL. The number K of goal samples is dependent on
the complexity of the task with further details in the next paragraphs and in Appendix A.10. We
evaluate HinDRL on two different robotic set ups and a total of four different tasks. Each environment
assumes a different robot which has different structure, action space and robot dynamics.

**Parameterised Reach: Here, we use a single 7DoF Sawyer robot to perform parameterised reaching.**
Parameterised reaching is the task of visiting two randomly located in free space way-points before
reaching for its target goal, Figure 5a. The continuous action space is comprised of the Cartesian 6
DoF pose of the robot and a 1 DoF representing the gripper open/close motion. The goal space is
equivalent to the state space. We use a total of two hindsight goal samples, zg, to relabel a single step.

**Bring Near: This is a dual-arm task situated in a cell with two Panda Franka Emika arms and hanging**
audio cables, Figure 5b-c. The agent has to use both robot arms to reach for two cables and bring
both tips within an Îµ distance from each other. The action space is 7DoF for each arm, representing
pose and grasp. The state and goal spaces are twice as large due to the dual nature of the task. Here
we use a total of four hindsight goal samples, zg, to relabel a single time step.

**Bring Near and Orient: This is a dual-arm task similar to above where the agent needs to reach and**
grasp the two cables. However, here it is also required to align both tips, that is position and reorient
them within a certain threshold. This task is significantly more complicated than the one above as
it requires an additional planning component. The two cables have rigid components at their tips,
a 3.5mm jack and a socket respectively. In order to be manipulated to a specific orientation those
cables have to be grasped at the rigid parts as grasping the flexible part of the cable prevents from
aligning the two tips (Figure 5d). We used a total of six goal samples, zg, to relabel a single time step.

**Bimanual Insertion: This is the most complex task we evaluate against. It has the same dual-arm set**
up as above but requires the complete insertion of the 3.5mm jack as well as the reaching, grasping
and alignment stages from the previous two bimanual tasks (Figure 5e). This task requires not only
careful planning but also very precise manipulation in order to successfully complete insertion. We
used a total of 12 hindsight goal samples, zg, to relabel a single time step for this task.

5.2 HINDSIGHT GOAL SELECTION FOR DEMO-DRIVEN RL: HINDRL

In this section we evaluate the utility of using different strategies for goal sampling in hindsight in
the context of demo-driven RL against a hand-engineered encoder. We compare the performance of
HinDRL against several strategies for hindsight goal sampling, as well as two non-goal-conditioned
baselines â€“ vanilla DPGfD and Behaviour Cloning (BC).

In addition, we combine DPGfD with hindsight relabelling using only the final reached state by the
agent, similar to (Nair et al., 2018a) although without a curriculum over start configurations and using
our goal-conditioned distributional agent as opposed to DDPG. We refer to this as HER (final). We
consider using samples from the future trajectory to relabel both the demonstrations and the agent
rollouts, similar to (Ding et al., 2019), although we do not consider using a stronger BC component
and use our goal-conditioned distributional agent as opposed to DDPG. we refer to this baseline as
HER (future). We report findings in Table 1. In all cases HinDRL performs best. Notably, assuming
a sufficient number of demonstrations, the DPGfD agent is able to solve both the parameterised reach
and the bring near tasks very well, but struggles on the full cable-insertion task. Both HER-based
samplers struggle on this task as well, but HER (future) performs better. HER is particularly good


-----

for shorter-horizon tasks like Bring Near. We do not expect to have any significant performance
benefits against HER on such tasks. In summary, the ability of HinDRL to specialize at achieving
goals on track to the specific task solution through the proposed task-constrained self-supervision
results in a superior performance across all considered long-horizon tasks.

**Environment** BC DPGfD HER (final) HER (future) HinDRL (Our)
Mean (Std) Mean (Std) Mean (Std) Mean (Std) Mean (Std)

Parameterised Reach (2wp) 82.05% (0.0) 88.62% (7.7) 84.92% (28.6) 86.92 (5.2) **92.61% (4.7)**
Bring Near 88.90% (0.0) **99.74% (1.0)** 32.03% (41.8) 97.75% (2.9) 98.79% (4.1)
Bring Near + Orient 57.89% (0.0) 83.52% (18.0) 8.33% (27.6) 23.28% (33.5) **90.77% (10.0)**
Bimanual Insertion 16.06% (0.0) 4.28% (4.4) 3.38% (3.3) 18.39% (12.3) **78.92% (10.3)**

Average 61.23% (0.0) 69.04% (7.8) 32.17% (25.32) 56.59% (12.9) **90.27% (7.3)**

Table 1: Records performance in terms of accuracy.

5.3 PERFORMANCE ON FEW-SHOT TASKS

Using demonstrations can significantly speed up

**Demonstrations** BC DPGfD HER (future) HinDRL (Our) the training time while still successfully outperMean (Std) Mean (Std) Mean (Std) Mean (Std) forming the expert eventually, (Vecerik et al.,

10 demos55 demos 16.06% (0.0)2.94% (0.0) 4.28% (4.4)0.0% (0.0) 0.0% (0.0)18.39% **78.92% (10.3)32.05% (23.3)** 2019). However, the final achieved performance
5 demos 0.52% (0.0) 0.0% (0.0) 0.0% (0.0) **17.78% (18.9)** in sparse reward settings depends on the com1 demos 0.13% (0.0) 0.0% (0.0) 0.0% (0.0) **12.58% (18.3)** plexity of the task and the number of provided

demonstrations. In this section we show how

Table 2: Performance against using different number of
demonstrations on Bimanual Insertion. using task-conditioned hindsight goal selection

can significantly reduce the demonstration requirements and relax the dependency of the RL agent to the performance of the BC loss. We evaluate
the ability of HinDRL to work in a few-shot setting. We train with 1, 5, 10 and 30 demonstrations
and report our findings against the Bring Near + Orient task in Figure 6. The figure shows the
overall achieved accuracy across different number of demonstrations. There, all solutions fail to solve
the task with a single demonstration. This shows that all considered DPGfD-based solutions were
dependent on the ability of the BC component to achieve more than 0% accuracy. HinDRL is able to
outperform the BC policy rather quickly on the 5-and-10-shot tasks while both DPGfD and HER
took significantly longer. Furthermore, HinDRL manages to maintain its final achieved accuracy
across the 5, 10 and 30-shot cases for the dedicated training budget while both DPGfD and BC didnâ€™t.
Table 2 shows the final achieved accuracy on the Bi-manual Insertion task. There, only HinDRL was
able to learn in the 10-5-1-shot scenarios. This indicates the ability of task-constrained hindsight
relabelling to bootstrap learning for long-horizon dexterous manipulation tasks, particularly with
high dimensional continuous action spaces.

Figure 6: Accuracy against different number of demonstrations on the Bring Near + Orient task. Each plot
shows the overall achieved accuracy during training. HinDRL consistently outperforms the alternatives. Success
with one demonstration depends on the selected demonstration. We choose it on a random principle.

5.4 STUDYING THE SPEED OF EXECUTION

We study the dependency of HinDRL on the number of demonstrations and the achieved accuracy of
the BC component. The results show a definite improvement over these two factors. However, this
does not necessarily mean that HinDRL is strictly better than the expertâ€™s performance. In practice, a
quick and nimble expert can still produce higher total output and be more valuable on average than
an RL solution even if it is less successful. Therefore, in this section we study the overall speed
of performing a task. We measure the speed of solving a task as a function of the per-step reward
accumulated over the course of the entire training. Therefore, an increased per-step reward means


-----

that an episode takes less environment steps to complete. We report our findings in Figure 7. We
provide an additional ablation over using different numbers of demonstrations in Appendix A.6. Our
findings indicate that HinDRL was always able to outperform the speed of execution obtained from
the BC policy while using only DPGfD or DPGfD with HER did not. HER is beneficial for the easier
tasks with insufficient number of demonstrations for DPGfD. However, HER alone is unable to solve
the longer-horizon complex sequential tasks.

Figure 7: Measuring per-step reward using the smallest number of demonstrations that resulted in learning.

5.5 ROBUSTNESS TO THE QUALITY OF THE ENCODER

In the above sections we study performance when using different hindsight goal samplers, HinDRL
sensitivity to the number of demonstrations and its ability to learn more efficient policies than
the provided expert. However, in all those cases we assumed access to a hand-engineered state
encoder. Informative representations is of central important to HinDRL as the algorithm uses the
state representations both as input to the RL agent but also during the relabelling stage to assign
reward, as discussed in Section 4. However, access to near perfect representations is not always
feasible. Therefore, we study the sensitivity of our solution to the quality of the encoder. We evaluate
the achieved performance against using learnt representations and raw input too. We compare learnt
representations. One is extracted with a Î² -VAE, for Î² = 0.5, and one with TCC. We hypothesise
that the notion of episode progress is of crucial importance to HinDRL. We report our findings in
Figure 8. Representations that preserve temporal consistency were able to achieve higher results on
all tasks. The dip in success rate is caused from annealing away the BC term in the actorâ€™s loss.

Using a learnt encoding obtained with TCC was capable of achieving near equivalent performance to
using an engineered encoding. We notice that using a support, RG for the goal distribution that is
comprised of the union between the task-conditioned distribution and HER help improve the Î² -VAE
performance, indicating that the agentâ€™s rollout was able to partially substitute the lack of notion of
progress. However, we provide this more detailed ablation in Appendix A.5. Unlike the engineered
encoding, the TCC-based representation does not contain any manipulation-specific priors.

Figure 8: Robustness to the quality of the encoder. Comparing the engineered encoder with raw state
observations and learnt Î² -VAE and TCC based encoders. The engineered encoder performs best with TCC
achieving close to commensurate performance.
6 CONCLUSION

We proposed an efficient goal-conditioned, demonstration-driven solution to solving complex sequential tasks in sparse reward settings. We introduce a novel sampling heuristic for hindsight relabelling
using goals directly from the demonstrations and combine it with both engineered and learnt encoders
that consistently preserve the notion of progress. We show that HinDRL outperforms competitive
baselines. Moreover, the method requires a considerably lower number of demonstrations. In the
future, we would like to extend this work to multi-task settings, employ vision and move towards the
context of batch RL, e.g. through employing distance learning techniques (Hartikainen et al., 2020)
to build on more informative goal spaces.


-----

REFERENCES

Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay.
_Advances in neural information processing systems, 30, 2017._

Christopher G Atkeson and Stefan Schaal. Robot learning from demonstration. In ICML, volume 97,
pp. 12â€“20. Citeseer, 1997.

Yusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu Wang, and Nando De Freitas. Playing
hard exploration games by watching youtube. Advances in neural information processing systems,
31, 2018.

Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of
_the AAAI Conference on Artificial Intelligence, volume 31, 2017._

Marc G Bellemare, Will Dabney, and RÃ©mi Munos. A distributional perspective on reinforcement
learning. In International Conference on Machine Learning, pp. 449â€“458. PMLR, 2017.

Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jacob Varley, Alex
Irpan, Benjamin Eysenbach, Ryan C Julian, Chelsea Finn, and Sergey Levine. Actionable models:
Unsupervised offline reinforcement learning of robotic skills. In Marina Meila and Tong Zhang
(eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of
_Proceedings of Machine Learning Research, pp. 1518â€“1528. PMLR, 18â€“24 Jul 2021. URL_
[https://proceedings.mlr.press/v139/chebotar21a.html.](https://proceedings.mlr.press/v139/chebotar21a.html)

Jessica Chemali and Alessandro Lazaric. Direct policy iteration with demonstrations. In Twenty_Fourth International Joint Conference on Artificial Intelligence, 2015._

Xin Chen, Sam Toyer, Cody Wild, Scott Emmons, Ian Fischer, Kuang-Huei Lee, Neel Alex, Steven H
Wang, Ping Luo, Stuart Russell, et al. An empirical investigation of representation learning for
imitation. 2021.

Todor Davchev, Kevin Sebastian Luck, Michael Burke, Franziska Meier, Stefan Schaal, and Subramanian Ramamoorthy. Residual learning from demonstration. arXiv preprint arXiv:2008.07682,
2020.

Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano Phielipp. Goal-conditioned imitation
learning. Advances in neural information processing systems, 32, 2019.

Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman.
Temporal cycle-consistency learning. In Proceedings of the IEEE/CVF Conference on Computer
_Vision and Pattern Recognition, pp. 1801â€“1810, 2019._

Ben Eysenbach, XINYANG GENG, Sergey Levine, and Russ R Salakhutdinov. Rewriting history with
inverse rl: Hindsight inference for policy improvement. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
[pp. 14783â€“14795. Curran Associates, Inc., 2020. URL https://proceedings.neurips.](https://proceedings.neurips.cc/paper/2020/file/a97da629b098b75c294dffdc3e463904-Paper.pdf)
[cc/paper/2020/file/a97da629b098b75c294dffdc3e463904-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/a97da629b098b75c294dffdc3e463904-Paper.pdf)

Carlos Florensa, Jonas Degrave, Nicolas Heess, Jost Tobias Springenberg, and Martin Riedmiller. Selfsupervised learning of image embedding for continuous control. arXiv preprint arXiv:1901.00943,
2019.

David Foster and Peter Dayan. Structure in the space of value functions. Machine Learning, 49(2):
325â€“346, 2002.

Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for
robotic manipulation with asynchronous off-policy updates. In 2017 IEEE international conference
_on robotics and automation (ICRA), pp. 3389â€“3396. IEEE, 2017._


-----

Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy
learning: Solving long-horizon tasks via imitation and reinforcement learning. In Leslie Pack
Kaelbling, Danica Kragic, and Komei Sugiura (eds.), Proceedings of the Conference on Robot
_Learning, volume 100 of Proceedings of Machine Learning Research, pp. 1025â€“1037. PMLR, 30_
[Octâ€“01 Nov 2020. URL https://proceedings.mlr.press/v100/gupta20a.html.](https://proceedings.mlr.press/v100/gupta20a.html)

Abhishek Gupta, Justin Yu, Tony Z Zhao, Vikash Kumar, Aaron Rovinsky, Kelvin Xu, Thomas Devlin,
and Sergey Levine. Reset-free reinforcement learning via multi-task learning: Learning dexterous
manipulation behaviors without human intervention. In 2021 IEEE International Conference on
_Robotics and Automation (ICRA), pp. 6664â€“6671. IEEE, 2021._

Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, and Sergey Levine. Dynamical distance learning for semi-supervised and unsupervised skill discovery. In International Conference on Learning
_[Representations, 2020. URL https://openreview.net/forum?id=H1lmhaVtvr.](https://openreview.net/forum?id=H1lmhaVtvr)_

Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa.
Learning continuous control policies by stochastic value gradients. In C. Cortes, N. Lawrence,
D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems,
[volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2015/file/148510031349642de5ca0c544f31b2ef-Paper.pdf)
[paper/2015/file/148510031349642de5ca0c544f31b2ef-Paper.pdf.](https://proceedings.neurips.cc/paper/2015/file/148510031349642de5ca0c544f31b2ef-Paper.pdf)

Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Thirty-second AAAI conference on artificial intelligence, 2018.

Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan,
John Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In
_Thirty-second AAAI conference on artificial intelligence, 2018._

Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
_preprint arXiv:1611.01144, 2016._

Leslie Pack Kaelbling. Learning to achieve goals. In IJCAI, pp. 1094â€“1099. Citeseer, 1993.

Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski,
Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv preprint arXiv:2104.08212, 2021.

Beomjoon Kim, Amir-massoud Farahmand, Joelle Pineau, and Doina Precup. Learning from limited
demonstrations. In NIPS, pp. 2859â€“2867. Citeseer, 2013.

Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learning handeye coordination for robotic grasping with deep learning and large-scale data collection. The
_International Journal of Robotics Research, 37(4-5):421â€“436, 2018._

Alexander Li, Lerrel Pinto, and Pieter Abbeel. Generalized hindsight for reinforcement learning.
_Advances in neural information processing systems, 33:7754â€“7767, 2020._

Jianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Wenzhao Lian, Chang Su, Mel Vecerik, Ning Ye,
Stefan Schaal, and Jonathan Scholz. Robust Multi-Modal Policies for Industrial Assembly via
Reinforcement Learning and Demonstrations: A Large-Scale Study. In Proceedings of Robotics:
_Science and Systems, Virtual, July 2021. doi: 10.15607/RSS.2021.XVII.088._

Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.

Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In 2018 IEEE International
_Conference on Robotics and Automation (ICRA), pp. 6292â€“6299. IEEE, 2018a._

Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual
reinforcement learning with imagined goals. Advances in neural information processing systems,
31, 2018b.


-----

OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur
Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas
Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei
Zhang. Solving rubikâ€™s cube with a robot hand. arXiv preprint, 2019.

Tom Le Paine, Sergio GÃ³mez Colmenarejo, Ziyu Wang, Scott Reed, Yusuf Aytar, Tobias Pfaff,
Matt W Hoffman, Gabriel Barth-Maron, Serkan Cabi, David Budden, et al. One-shot high-fidelity
imitation: Training large-scale deep nets with rl. arXiv preprint arXiv:1810.05017, 2018.

Karl Pertsch, Youngwoon Lee, and Joseph J Lim. Accelerating reinforcement learning with learned
skill priors. arXiv preprint arXiv:2010.11944, 2020.

Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180â€“1190, 2008.

Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden,
Gabriel Barth-Maron, Hado Van Hasselt, John Quan, Mel VeË‡cerÃ­k, et al. Observe and look further:
Achieving consistent performance on atari. arXiv preprint arXiv:1805.11593, 2018.

Martin A. Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van
de Wiele, Volodymyr Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playing [solving sparse reward tasks from scratch. CoRR, abs/1802.10567, 2018. URL http://arxiv.](http://arxiv.org/abs/1802.10567)
[org/abs/1802.10567.](http://arxiv.org/abs/1802.10567)

Tim Salimans and Richard Chen. Learning montezumaâ€™s revenge from a single demonstration. arXiv
_preprint arXiv:1812.03381, 2018._

Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators.
In International conference on machine learning, pp. 1312â€“1320. PMLR, 2015.

Archit Sharma, Michael Ahn, Sergey Levine, Vikash Kumar, Karol Hausman, and Shixiang Gu.
Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning. In
_Proceedings of Robotics: Science and Systems, Corvalis, Oregon, USA, July 2020. doi: 10.15607/_
RSS.2020.XVI.053.

Archit Sharma, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Autonomous
reinforcement learning via subgoal curricula. Advances in Neural Information Processing Systems,
34, 2021.

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine learning, pp.
387â€“395. PMLR, 2014.

Freek Stulp, Evangelos A Theodorou, and Stefan Schaal. Reinforcement learning with sequences
of motion primitives for robust manipulation. IEEE Transactions on robotics, 28(6):1360â€“1370,
2012.

Giovanni Sutanto, Katharina Rombach, Yevgen Chebotar, Zhe Su, Stefan Schaal, Gaurav S Sukhatme,
and Franziska Meier. Supervised learning and reinforcement learning of feedback models for
reactive behaviors: Tactile feedback testbed. arXiv preprint arXiv:2007.00450, 2020.

Minija Tamosiunaite, Bojan Nemec, AleÅ¡ Ude, and Florentin WÃ¶rgÃ¶tter. Learning to pour with a
robot arm combining goal and shape learning for dynamic movement primitives. Robotics and
_Autonomous Systems, 59(11):910â€“922, 2011._

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026â€“5033.
IEEE, 2012.

Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess,
Thomas RothÃ¶rl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep
reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817,
2017.


-----

Mel Vecerik, Oleg Sushkov, David Barker, Thomas RothÃ¶rl, Todd Hester, and Jon Scholz. A practical
approach to insertion with variable socket position using deep reinforcement learning. In 2019
_international conference on robotics and automation (ICRA), pp. 754â€“760. IEEE, 2019._

Markus Wulfmeier, Abbas Abdolmaleki, Roland Hafner, Jost Tobias Springenberg, Michael Neunert,
Noah Siegel, Tim Hertweck, Thomas Lampe, Nicolas Heess, and Martin Riedmiller. Compositional
Transfer in Hierarchical Reinforcement Learning. In Proceedings of Robotics: Science and Systems,
Corvalis, Oregon, USA, July 2020. doi: 10.15607/RSS.2020.XVI.054.

Markus Wulfmeier, Dushyant Rao, Roland Hafner, Thomas Lampe, Abbas Abdolmaleki, Tim
Hertweck, Michael Neunert, Dhruva Tirumala, Noah Siegel, Nicolas Heess, et al. Data-efficient
hindsight off-policy option learning. In International Conference on Machine Learning, pp.
11340â€“11350. PMLR, 2021.

Fan Xie, Alexander Chowdhury, M De Paolis Kaluza, Linfeng Zhao, Lawson Wong, and Rose Yu.
Deep imitation learning for bimanual robotic manipulation. Advances in Neural Information
_Processing Systems, 33:2327â€“2337, 2020._

Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, and Debidatta Dwibedi.
Xirl: Cross-embodiment inverse reinforcement learning. In Conference on Robot Learning, pp.
537â€“546. PMLR, 2022.

Guoyu Zuo, Qishen Zhao, Jiahao Lu, and Jiangeng Li. Efficient hindsight reinforcement learning
using demonstrations for robotic tasks with sparse rewards. International Journal of Advanced
_Robotic Systems, 17(1):1729881419898342, 2020._


-----

A APPENDIX

A.1 PREDICTING PROGRESS

We measure the performance of the learnt representations to encode
progress by running a KNN classification. First, we collect 500

**Model** Train Test

demonstrations on the full insertion task using manually defined way
points and a PD controller. We vary the starting configuration of VAE **88.6%** 63.6%

TCC 86.2% **79.2%**

each robot arm by 1[â—¦] for all its 7 joints. Then, we perform a CV
split over the collected trajectories and obtain training and validation
sets. We train both encoders using the training data. Once training Figure 9: Predicting episode
is complete, we process all training trajectories with each encoder progress with 10-fold KNN.
which results in two separate encoded data sets. We use those to
train two separate 10-fold KNN classifiers - one for each type of encoding. Then, we process the
never seen before validation set using each of the encoders and evaluate the accuracy of predicting
the episode progress with the KNN classifiers. Table 9 shows the results. It can be seen that the VAE
achieved much higher accuracy when evaluated on the training data as opposed to test, indicating it
has overfitted to it. In contrast, the TCC was much better at predicting the episode progress.


A.2 ABLATION OF THE ONLINE GOAL SELECTION STAGE

Goal-conditioned (gc) policy learning takes in as input a desired
goal state to solve for. However, in cases where the goal is as
abstract as â€™solve the taskâ€™ there may be a number of different goals
describing the same problem. Therefore, restricting the gc policy
to a single target goal state may negatively impact the learning
process. Additionally, using a small subset of goals, e.g. goals
corresponding to the final states of all demonstrations as done in
Nair et al. (2018a), may be insufficient too. In contrast, conditioning
on a wider range of goal states that solve the same task can better
capture the goal distribution describing the task at hand. As a result,
we randomly sample a plausible target goal state for each roll-out
during training (see Figure 2). We propose to continuously grow a Figure 10: Different types of ontarget goal distribution as training evolves and the agent starts solving line goal selections.
the training task. In this section, we compare the performance of our
agent when using a single target goal to represent solving the task, using as target goals only the goal
states from the provided near-optimal demonstrations, and continuously growing the goal database
as learning progresses. Our findings reported in Figure 10 show that continuously growing the goal
database allows for better and faster learning in the context of vaguely formulated goals such as the
3.5mm jack insertion considered in this work.

A.3 MIXING THE GOAL DISTRIBUTION SUPPORT

Mixing goal candidates taken from the agentâ€™s rollout and the provided demonstrations can help
for retroactive goal selection. A potential scenario is the one discussed in Appendix A.4. Having
noisy, sub-optimal representations that do not preserve the notion of progress can be problematic. An

(a) (b) (c) (d)

Figure 11: Measuring per-step reward using the smallest number of demonstrations that resulted in learning.


-----

alternative scenario could involve a relatively narrow set of demonstrations. This is particularly evident
in complex task settings where having a set of successful demonstrations can still be insufficient to
solve the task as there are a vast number of failure modes, like in the full insertion task, for example.
In this subsection, we focus on studying different candidate heuristics that can help relax these
constraints. We build upon the formulation for retroactive goal selection introduced in Section 4.2
which allows us to fuse together the HER-style relabelling strategies and demo-driven ones too.

**Using demonstration states as candidate goals:** The simplest version of this is a union over both
sets where the agent gets to sample a goal that comes from the rollout or the demonstration data. This
when the provided demonstrations are only partially useful to solving the task. Implicitly letting thecan be expressed as RG = {zg âˆˆ _Î¶_ _D : p(zg) > 0}. Such formulation can be useful, for example_
agent to sample goals that are part of its own rollout can help model useful behaviours that over time[S]
could help get us closer to the provided demonstrations.

An alternative version is when the goal distribution p(zg) is composed of the intersection over the two
types of goal distributions discussed in Section 4.2. In this case, the support of the goal distribution
becomesadopted representation does not have an encoded notion of progress. Therefore, choosing goals RG = {zg âˆˆ _Î¶_ _D : p(zg) > 0}. We find this intersection to be useful in cases where the_
that are Îµ close to states produced by the agentâ€™s dynamics can hypothetically act as regularisation[T]
_âˆ’_
over the choice of goals we use but still ensure staying close to the target task. We can collect all
qualifying goals for a time step t by iterating over the data set of successful trajectories obtained from
demonstration and compare to zt. Since zt and all zg are temporally consistent, we can use Eq. 3 to
prune the data set and pick the closest zg for each zt . We used a task-conditioned sampler where we
sample directly from a distribution implied from demonstrations. However, we can also compose
distributions comprised of mixing goals from the agentâ€™s rollout and the demonstrations. Here we
consider two different versions of this.

We report our overall results in Figure 11. Our results indicate that using the intersection over the
rollout trajectory and the demonstrated goal results in broadly similar resutls as the task conditioned
approach. However, the intersection based solution had much higher variance indicating that the
agent is is less stable where some seeds achieved near perfect performance and others were closer to
failure. We noticed that this type of goal conditioning can be useful when training with a VAE. That
is, this sampling strategy can be useful in cases where the quality of the representations and also of
the implied task distribution is poor, e.g. when they do not contain notion of progress or in low data
regimes. We report these details in Appendix A.4.

**Using relevant agent states as candidate goals:**
Utilising the demonstration states to collect candidate goal distributions can be a powerful tool
as we demonstrate in this work. However, in
complex tasks, such as the full insertion task
(Figure 1), relabeling the goals with just demonstration states can sometimes fail to make the
sparse-reward problem easier (as intended by
HER) since the resulting goal distributions are
still relatively narrow. This is particularly true (a) (b)
in the beginning of the training process when
there is a relatively large mismatch between Figure 12: Comparing different demo-driven support
the agentâ€™s Q function and the true underlying distributions.
dynamics the provided demonstrations follow.
Therefore, we consider an alternative method that can help speed up the training process. To this
end, we can form a collection of candidate goals that is jointly conditioned on both the agent and
the demonstratorâ€™s behaviours but is comprised of all zt that fall under the Îµ threshold defined in
Appendix A.8. In this setting, we focus on modelling the agentâ€™s behaviour directly by focusing only
on relevant to the task states as opposed to strictly targeting actual demonstration states. Figure 12
summarises our findings. While using this type of joint conditioning can speed up training (plot on
the right), it does not necessarily result in improved performance (plot on the left). We suspect that
mixing the goal distribution support can be potentially very useful to using less demonstrations or
partially useful demonstrations. We leave this study for future work.


-----

A.4 NOISE SENSITIVITY OF THE ENCODER: CHALLENGING THE NOTION OF PROGRESS

We compare the performance of both the joint- and taskconditioned samplers on the Bring Near + Orient task and
report the overall accuracy. We trained for a total of 150K
environmental steps and report our findings in Figure 13.
Relying on task-conditioned samples results in higher accuracy for the less noisy observations. This indicates that
having a stronger representation is directly related to the
agentâ€™s confidence in â€™understandingâ€™ the actual task it
has to solve. This study further confirms our conjecture
that notion of progress is paramount to solving complex
sequential tasks. Note that the relationship between the
level of noise and performance depicted in Figure 13 does
not affect the jointly conditioned relabelling strategies as

Figure 13: Injecting noise to the engineered

much as it does for task-conditioned relabelling. In fact,

encoder. Reach, Grasp, Lift, Orient Task.

a little bit of noise leads to improved performance for the
former while it slows down training for the latter. This indicates that relying on alternative mechanisms for indicating progress, such as conditioning on the agentâ€™s own trajectory can be useful when
progress is not successfully encoded in the representations used. This aligns with our motivation from
Section 4 that task-conditioning works when we are confident in the quality of the task distribution.
However, the proposed ablation in this section points towards an alternative mode that can potentially
compensate for this. Namely, implicitly informing the agent for the notion of progress e.g. through
building heuristics for hindsight goal selection that utilise both demonstrations and agent motion
can be useful. We hypothesise that retroactive relabelling using only goals that are both similar to
the demonstrations and aligned with the current agentâ€™s trajectory can be useful with respect to the
agentâ€™s current understanding of the dynamics, represented through its current Q function. Next, we
consider three different strategies for extracting candidate goals and discuss some of their benefits
and limitations.

A.5 QUALITY OF ENCODER: EXTENDED STUDY

Figure 14: Learnt representations.

The previous section suggests that a joint-conditioned sampler can be more useful when the used
representations do not encode notion of progress. In this section we compare using task-conditioned
and joint-conditioned encoders using learnt representations instead. We compare using TCC and
VAE and benchmark the results against a hand-engineered encoder. Even though TCC with a taskconditioned sampler achieved the closest results to the best hand-engineered solution across all tasks,
we can see that a joint-conditioned encoder can work better for states that do not encode notion of
progress.

Figure 14 illustrates the achieved results. We can see that using a Î² -VAE encoder worked best with
goal sampling from a joint-conditioned goal distribution,the intersection between both distributions still results in a data set comprised of goal candidates that RG = {zg âˆˆ _Î¶_ _D : p(zg) > 0}. Note that_
still belong to the implied from demonstrations task distribution. However, we only used the goals[T]
that were similar to the agent rollout. This result is connected to our observations from Appendix A.4
that a joint-conditioned sampler implicitly introduces a notion of progress via utilising the agentâ€™s
own motion at the retroactive goal selection stage.


-----

Another interesting observation is the final full insertionâ€™s task performance. Although TCC-based
representation came closest to the engineered representation, it was still much lower. The full insertion
task relies the most on the contact-rich manipulation to be completed when compared to the rest. The
engineered representation contains information relevant to the manipulation which is why we suspect
the gap between both learnt and engineered representation is much larger than the rest of the tasks. A
potentially exciting future direction is attempting to extract representations that preserve the notion
of contact as well as progress.

A.6 PER-STEP REWARD

(a) (b) (c) (d)

(e) (f) (g) (h)

(i) (j) (k) (l)

Figure 15: Measuring per-step reward.

A.7 PROGRESS-BASED WEIGHTING

Weighting down the BC and actor-critic losses associated
with intermediate states can have slight benefits to improving the speed of learning of goal-conditioned DPGfD.

Total Reward Although in practice there could be many different ways

of reweighing loss values, we found two particular ones
useful in our setting. One way of scaling such losses is
by choosing a fixed weight Ï‰ that scales down all non
Total Reward

(a) (b) terminal statesâ€™ losses during training by the same fixed

weight. An alternative weighting can be defined by using

Figure 16: Measuring per-step reward using a quadratically scaled weight using the episode progress.
the smallest number of demonstrations thatresulted in learning. That is, for batch b, we getLTD = Î» _[p]_ _âˆ—_ _LTD, where âˆ—_ indicates element-wise multi- LBC(b) = Î» _[p]_ _âˆ—_ _LBC(b) and_

plication and

_Î»_ _[p]_ = 1.0, if zt = T (4)
_Ï‰,_ otherwise _[,][ or][ Î»][ p][ =][ i][2][,][ for i][ âˆˆ{][ 1]T_ _[,...,][ T]T_
 _[}][.]_

We used Ï‰ = 0.1 in our experiments. Figure 16 illustrates an example of re-weighting on the Bring
Near + Orient task. Down-weighting intermediate states can lead to slightly faster learning and a


-----

higher variance performance. There is a difference between weighting states using a fixed value and
assigning quadratic weighting proportional to the episode progress.

A.8 COMPUTING THE THRESHOLD

There are multiple ways to obtain an Îµ threshold for our goal conditioned reward. We used the provided demonstrations to compute
the average distance Îµ = Âµ + kÏƒ, where Âµ and Ïƒ were extracted
using a rolling distance between consecutive states from the encoded
demonstrations, e.g.step gap in between the two states and ||zt[d] _[âˆ’]_ _[z]t[d]+m[||][, for a demonstration] k standard deviations. In[ d][ with an][ m]_
our tasks, m = 10 for all tasks but the bring near and orient and
the full insertion where we used m = 5. We use a rolling distance
over a window of time steps because we did not want to let time
step clusters often situated around the different â€œnarrow phasesâ€ of
a trajectory influence the average threshold. Broadly, we notice a
relationship between the size of the rolling window and the precision Figure 17: Ablating the rolling
and recall of the obtained goal-conditioned sparse reward function. window.
We ablate the importance of a rolling window size on the bring near
and orient task in Figure 17. There, it can be seen that too small of a rolling window size (such as 1)
might have a noticeable negative impact on learning due to the clusters situated around the different
phases. Effectively, too small of a window can affect the recall of our obtained reward which can be
detrimental to learning. In contrast, too large of a rolling window can affect the speed of learning due
to allowing for a more flexible threshold function. Using too large of a rolling window can reduce the
precision of the obtained threshold by rewarding too many false positive states. In terms of learning,
this can be detrimental to the speed of learning a successful policy and might result in converging to
poorer performance too.

A.9 COMPUTING THE ENGINEERED ENCODERS

The engineered goal encoders vary between tasks, but in all cases the encoding captures some notion
of progress of the agent through the set task.

**Parameterized Reach: The engineered encoder for this task concatenates the robot arm pose and a**
mask of which waypoints have been visited so far.

**Bring Near: The state encoder is the concatenation of the distance of the left and right grippers from**
their corresponding cables, the distance between the cable tips, and of whether the grippers have
grasped their respective cables.

**Bring Near and Orient: The encoder for this task is similar to Bring Near, but also adds the dot**
product between the z-axes of the left and right cable tips.

**Bimanual Insertion: The encoder for this task is similar to Bring Near and Orient, but adds the**
distance of the cable tip from the socket bottom, along the z-axis of the socket.

A.10 CHOOSING NUMBER OF HINDSIGHT GOAL SAMPLES

We followed the intuition that the complexity of the task guides the number of samples required to
capture the overall task distribution. That is, we used 2 samples for the simplest task of Parameterised
Reach, 4 for the Bring Near, 6 for Bring Near and Orient and 12 for the Bi-manual Insertion.


-----

