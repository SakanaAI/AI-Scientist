# PRIVATE MULTI-WINNER VOTING FOR MACHINE LEARNING

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Private multi-winner voting is the task of revealing k-hot binary vectors that satisfy a bounded differential privacy guarantee. This task has been understudied
in the machine learning literature despite its prevalence in many domains such
as healthcare. We propose three new privacy-preserving multi-label mechanisms:
Binary, τ, and Powerset voting. Binary voting operates independently per label
through composition. τ voting bounds votes optimally in their ℓ2 norm. Powerset
voting operates over the entire binary vector by viewing the possible outcomes
as a power set. We theoretically analyze tradeoffs showing that Powerset voting
requires strong correlations between labels to outperform Binary voting. We use
these mechanisms to enable privacy-preserving multi-label learning by extending
the canonical single-label technique: PATE. We empirically compare our techniques with DPSGD on large real-world healthcare data and standard multi-label
benchmarks. We find that our techniques outperform all others in the centralized
setting. We enable multi-label CaPC and show that our mechanisms can be used to
collaboratively improve models in a multi-site (distributed) setting.

1 INTRODUCTION

Differential privacy techniques for machine learning have predominantly focused on two types of
mechanisms: for a given query, release either a noisy estimate using the Gaussian mechanism (Abadi
et al., 2016b) or the noisy arg max (Papernot et al., 2017). These two mechanisms appeal well to
_single-label classification per input (a.k.a. multi-class classification), which is a widely studied setting_
of supervised learning where a given input has only one class present. The Gaussian mechanism
can enable arbitrary queries of data, for instance, differentially private stochastic gradient descent
(DPSGD), which returns noisy gradients (Abadi et al., 2016b). Instead, the Gaussian noisy arg max
mechanism can only reveal the max count, which is used in the canonical Private Aggregation of
_Teacher Ensembles (PATE) (Papernot et al., 2017)._

In contrast, more real world tasks, such as multi-label classification (Tsoumakas & Katakis, 2007),
can be modeled using multi-winner elections (Elkind et al., 2017; Faliszewski et al., 2017), where
more than one candidates may win, simultaneously. Outside of the obvious situation of elections,
other multi-winner election scenarios include canonical computer visions tasks like object recognition,
which require models to recognize all the objects present in an image (Boutell et al., 2004). Further,
multi-winner problems arise when inferring the topics written about in a corpus of text (document):
e.g., a news article may discuss one or more topics like politics, finance, or education. Another
principal setting is healthcare, in which patient data (e.g., symptom reports or X-rays) may be
indicative of multiple conditions (Irvin et al., 2019; Johnson et al., 2019).

We thus focus on creating private mechanisms for releasing the outcome of a multi-winner election.
We first formalize the multi-winner election and then provide a solution, termed Binary voting, where
we apply a single-winner election mechanism independently to each of the k candidates. Though
simple, we prove that it is optimal when there is a lack of correlation among the outcomes of particular
candidates. We then derive tighter data-independent privacy bounds for multi-winner elections by
considering the situation where each voter is limited in their (ℓ2) votes for candidates, which we
call τ voting. By casting multi-winner elections to an analogous task of single-winner elections, we
create a third mechanism termed Powerset voting, which reveals the result for all k candidates jointly.


-----

By replacing the noisy arg max mechanism used in (single-label) PATE, we create multi-label PATE
which enables multi-label privacy-preserving classification with semi-supervised machine learning.

We empirically evaluate our algorithms on large-scale datasets, including the common multi-label
benchmark Pascal VOC and three healthcare datasets, CheXpert (Irvin et al., 2019), MIMIC (Johnson
et al., 2019), and PadChest (Bustos et al., 2020). We compare our work against DPSGD, which
can be directly applied to multi-label machine learning, and find that our approaches achieve new
state-of-the-art privately trained models, with 85% AUC (17% better than DPSGD) on the Pascal
VOC dataset. Because many multi-label settings may benefit from distributed, multi-site learning
(e.g., healthcare), we further integrate our methods with the collaborative protocol of Choquette-Choo
et al. (2021). We find that training with our mechanisms can improve model performance significantly
on large, real-world healthcare data with sensitive attributes.

Our main contributions are as follows:

1. We create three new DP aggregation mechanisms for private multi-winner voting: Binary voting, τ
voting, and Powerset voting. We theoretically analyze the privacy guarantees of these mechanisms,
finding that Binary voting performs better unless there is high correlation among labels.

2. We integrate our mechanisms with PATE, enabling private multi-label semi-supervised learning
that achieves SOTA performance on large real-world tasks: Pascal VOC and 3 healthcare datasets.

3. We enable multi-label collaborative learning in the multi-site scenario by integrating our mechanisms into CaPC. We find that multi-label CaPC significantly improves model performance.

2 BACKGROUND

2.1 DIFFERENTIAL PRIVACY FOR MACHINE LEARNING

Differential Privacy (DP) is a canonical framework for measuring the privacy leakage of a randomized
algorithm (Dwork et al., 2006). It requires the mechanism, the training algorithm in our case, to
produce statistically indistinguishable outputs on any pair of adjacent datasets, i.e., datasets differing
by only one and any data point. This bounds the probability of an adversary inferring properties of
the training data from the mechanism’s outputs.

**Definition 1 (Differential Privacy). A randomized mechanism M with domain D and range R**
_satisfies (ε, δ)-differential privacy if for any subset S ⊆R and any adjacent datasets d, d[′]_ _∈D, i.e._
_d_ _d[′]_ 1 1, the following inequality holds: Pr [ (d) ] _e[ε]Pr [_ (d[′]) ] + δ.
_∥_ _−_ _∥_ _≤_ _M_ _∈S_ _≤_ _M_ _∈S_

We use Rényi Differential Privacy (RDP) (Mironov, 2017) which enables tighter accounting for
our mechanisms using Gaussian noise because of its use of the Rényi-divergence to bound privacy
loss. RDP is a generalization of pure, δ = 0-DP so we convert to and report final budgets using
(ε, δ) − _DP_ .

**Definition 2. Rényi Differential Privacy (Mironov, 2017). We say that a mechanism M is (λ, ε)-**
_RDP with order λ ∈_ (1, ∞) if for all neighboring datasets X, X _[′]:_

_λ_

_Dλ(_ (X) (X _[′])) =_ 1 _pM(X)(θ)_ _ε_
_M_ _||M_ _λ −_ 1 [log][ E][θ][∼M][(][X] _[′][)]_ _pM(X_ _′)(θ)_   _≤_

It is convenient to consider RDP in its functional form as ε (λ), which is the RDP ε of mechanism
_M_
_M at order λ. We now state the result which our privacy analysis builds on._

**Lemma 2.1. RDP-Gaussian mechanism (for a single-label setting) (Mironov, 2017). Let f : X →**
_have bounded ℓ2 sensitivity for any two neighboring datasets X, X_ _[′], i.e.,_ _f_ (X) _f_ (X _[′])_ 2 ∆2.
_R_ _∥_ _−_ 2 _∥_ _≤_

_The Gaussian mechanism M(X) = f_ (X) + N (0, σ[2]) obeys RDP with εM(λ) = _[λ]2[∆]σ[2][2][ .]_

Another notable advantage of RDP over (ε, δ)-DP is that it composes naturally. This will be helpful
when analyzing our approach which repeatedly applies the same private mechanism to a dataset.

**Lemma 2.2. RDP-Composition (Mironov, 2017). Let mechanism M = (M1, . . ., Mt) where Mi**
_can potentially depend on outputs of M1, . . ., Mi−1. M obeys RDP with εM(·) =_ _i=1_ _[ε][M][i]_ [(][·][)][.]

[P][t]


-----

2.2 PATE AND CAPC

**PATE (Private Aggregation of Teacher Ensembles) is a model agnostic algorithm for training ML**
models with DP guarantees (Papernot et al., 2017). Rather than training a single model on a dataset,
PATE trains an ensemble of models from (disjoint) partitions of the dataset. Each model, called a
teacher, is then asked to predict on a test input and to vote for one class. Teacher votes are collected
into a histogram where ni(x) indicates the number of teachers who voted for class i. To preserve
privacy, PATE relies on the noisy argmax mechanism and only reveals a noisy aggregate prediction
(rather than revealing each prediction directly): argmax{ni(x) + N (0, σG[2] [)][}][, where the variance]
_σG[2]_ [of the Gaussian controls the privacy loss of revealing this prediction. A student model learns in]
a semi-supervised fashion by querying the ensemble of teachers for noisy labels, where each label
incurs additional privacy loss. Loose data-independent guarantees are obtained through advanced
composition (Dwork et al., 2014) and tighter data-dependent guarantees (see Appendix Section L) are
possible when there is high consensus among teachers on the predicted label (Papernot et al., 2018).
To further reduce privacy loss, Confident GNMax only reveals predictions that have high consensus:
maxi{ni(x)} is compared against a noisy threshold also sampled from a Gaussian with variance σT[2] [.]
PATE was created for single-label classification; our work extends it to the multi-label setting.

**CaPC (Confidential and Private Collaboration) is a distributed collaborative learning framework**
that improves on PATE (Choquette-Choo et al., 2021). In addition to protecting the privacy of
training data through differential privacy, CaPC also protects the confidentiality of test data and
model parameters by introducing new cryptographic primitives. Confidentiality refers to the notion
that no other party (or adversary) can view, in plaintext, the data of interest—it protects the inputs
to our mechanism. This differs from differential privacy which protects what can be inferred (in
our case about the training set for PATE models) from the outputs of our mechanism. Both protect
data privacy in different ways. In CaPC, each teacher model is trained by a distinct (and different)
protocol participant. The querying party initiates the protocol by (1) encrypting unlabeled data and
the other (answering) parties return an encrypted label. Then, (2) the encrypted vote is secretly
shared with both the answering party and the Privacy Guardian, where the Privacy Guardian follows
PATE to enforce DP. Finally, (3) the Privacy Guardian engages in the secure computation with the
querying party and returns a DP vote to the querying party. The detailed CaPC protocol can be found
in Algorithm 1 of Appendix J.

2.3 PRIOR WORK ON MULTI-LABEL CLASSIFICATION

The closest to our work is that of Zhu et al. (2020), which provides a privacy-preserving nearest
neighbors algorithm formulated using PATE. The nearest-neighbor training points are used as teachers,
where their labels are the votes. While this work is applicable to the multi-label setting, it assumes
the existence of a publicly-available embedding model, which is used to project high-dimensional
data onto a sufficiently-low-dimensional manifold for the meaningful computation of distances (to
determine the nearest neighbors). Instead, our approach does not make this assumption. Furthermore,
we also show in Section 3 that clipping votes to the ℓ1 ball, as done in Zhu et al. (2020), is not
optimal—given that noise is sampled from a Gaussian distribution. Instead, we obtain tighter
bounds by clipping votes to an ℓ2 ball. The multi-label voting problem can also be formulated
as simultaneously answering multiple counting queries which has been studied extensively in the
theoretical literature. Though data-independent optimal error bounds for the privacy loss were proven
by Dagan & Kur (2020), these mechanisms do not generally have data-dependent bounds and have
yet to show promise empirically in settings such as multi-label voting. Another alternative multi-label
method is differentially private stochastic gradient descent (DPSGD), which was proposed by Abadi
et al. (2016a) and applied to the multi-label setting by Zhang et al. (2021) to train small multi-label
models (k = 5 labels). We compare directly with their work and find that our methods perform
better. We present a detailed description of the related work in Appendix E. Finally, we stress that our
evaluation includes large-scale healthcare datasets with a natural need for privacy and the additional
challenge of being severely imbalanced.

3 DIFFERENTIAL PRIVACY MECHANISMS FOR MULTI-WINNER ELECTION

Multi-winner election systems are direct generalizations of popularly used single-winner elections.
They are adopted commonly in real-life elections and their comprehensive analysis can be found


-----

in Elkind et al. (2017) and Faliszewski et al. (2017). Here, we formally define a multi-winner system
and propose DP mechanisms to facilitate the private release of the outcome of a multi-winner election.

Compared to single-winner elections, multi-winner elections are more challenging for designing a DP
mechanism: instead of outputting a single scalar, multi-winner elections output a higher-dimensional
vector and thus require more noise to achieve the same level of privacy. DP mechanisms for singlewinner voting have been well studied before, where the noisy arg max achieves minimal privacy loss
due to its tight sensitivity as a result of information minimization (Dwork et al., 2014). However,
to the best of our knowledge, no prior work formally defines and analyzes DP mechanisms for
multi-winner election systems, hence we provide Definition 3 and the analysis. We provide formal
data-dependent and data-independent privacy loss bounds in Section 4.

**Definition 3. θ-multi-winner election. Given a set of n voters, each with a ballot of k candidates**
_(coordinates) b_ 0, 1 _such that_ **_b_** 2 _θ. The outcome of a θ-multi-winner election with_
_threshold T is defined as ∈{_ _}[k]_ _∥_ _∥_ _≤_


_f_ (b1, . . ., bn) ≜ **_bj[i] > T_**

 

_j=1_

 X 

[i][ :]  _[∈{][0][,][ 1][}][k][,]_

where T = _[n]2_ [and with the binary decision per label][ i][,][ P]j[n]=1 **_[b][j][[][i][]][ represents the number of positive]_**

votes.

3.1 DP MECHANISMS FOR ∞-MULTI-WINNER ELECTION

In an ∞-multi-winner[1] election, each elector can vote for any candidates in their ballots. We propose
to decompose the multi-winner election into k separate single-winner elections, where we instead
apply the well-studied noisy arg max separately to each of the k single-winner elections. We call this
Binary voting and state formally in Definition 4 below. For m instances of ∞-multi-winner elections,
the total privacy loss of this mechanism follows Lemma 2.2 which can then be tightly expressed as
(ε, δ)-DP guarantee for a chosen δ.

**Definition 4 (DP (Noisy Argmax) Binary Voting Mechanism). For an ∞-multi-winner election, the**
_binary voting mechanism is_


_σ(b1, . . ., bn) ≜_ **_bj[i] +_** (0, σG[2] [)] _> Ti_
_M_   _N_  

_j=1_

 X 

 

[i][ :]  _[∈{][0][,][ 1][}][k][,]_

where Ti = n − [P]j[n]=1 **_[b][j][[][i][]][ is the number of negative votes, which enables more noise to be added]_**
than when T = _[n]2_ [. Though there are many potential per-candidate Binary voting mechanisms, we]

base ours on the noisy argmax mechanism because of its advantageous privacy guarantees stemming
from its well-understood sensitivity: changing the training set to any adjacent one can only modify a
single ballot. We also leverage propose-test-release (Dwork & Lei, 2009) shown in Algorithm 2 of
Appendix K. Next, we show that Binary voting is optimal in the coordinate-independent setting.

**Definition 5. A function f** (X) : R[n] _→R[k]_ _is coordinate-independent if the i’th output coordinate of_
_f_ _, fi, is determined by a unique subset Pi(X) of the input X, such that ∩i[k]=1[P][i][ =][ ∅][.][ P][i][(][X][)][ indexes]_
_(partitions) the input X uniquely for each i and returns the chosen subset; the chosen columns are_
_dependent only on i, not the particular dataset X._

We first build an intuition for our guarantee and then provide the formal statement (in Proposition 3.1).
Recall from Section 2 that the crux of DP analysis is to bound the sensitivity of the random mechanism
_M, which is the noisy output of the function[2]_ _f_ .

**Definition 6. Sensitivity is the maximum deviation ∆pf =** max
(X,X _[′]):||X−X_ _[′]||1=1[||][f]_ [(][X][)][ −] _[f]_ [(][X] _[′][)][||][p][ of]_

_f_ _’s output for any adjacent pair of inputs X and X_ _[′], i.e., have a Hamming distance ||X −_ _X_ _[′]||1 of 1,_
_on their rows (Dwork et al., 2014)._

1 _k_

2We use ∞ to emphasize that any vote b ∈{0, 1} is allowed but a tighter bound exists: ∥b∥2 ≤ _√k < ∞._
Because the random mechanism differs minimally from the function, we use the two interchangeably.


-----

Observe that output fi of a Binary voting mechanism for a coordinate i depends only on the
corresponding i−th coordinate of the input ballot because we apply Binary voting independently
to each candidate. For any mechanism f, ∆pf is maximized when the mechanism’s output for
each coordinate i is flipped from predicted to not predicted or vice versa. The pair of teacher
votes achieving this differ in each coordinate i. This also means that any coordinate-independent
mechanism has sensitivity at least as large as the one of Binary voting mechanism because all k
coordinates can be flipped. If there were coordinate-dependence (e.g., if the output coordinates i and
_j both solely depended on the input coordinate i), it may not be possible to find such a single pair of_
databases across all coordinates, simultaneously, which would decrease the sensitivity and thus the
privacy loss. Proposition 3.1 below formally states our result. For more intuition, example functions,
as well as the full proof, see Appendix A and Proposition A.2 therein.
**Proposition 3.1. For a coordinate-independent multi-label function fmulti(X) : R[n]** _→R[k], the_
_ℓ1 sensitivity ∆1fmulti is equal to that of Binary voting applied per label and thus fmulti does not_
_provide a better privacy guarantee than k applications of Binary voting._

Note that Proposition 3.1 is exactly applicable to Binary voting if we assume candidate independence:
then each candidate on all ballots only influences a single winner. Analyzing these assumptions,
we also readily see two scenarios when a Binary voting mechanism can be outperformed. The first
scenario is when any input coordinate Xi explains > 1 output (coordinate dependence). Proposition A.4 of Appendix A proves that for fmulti to achieve a strictly lesser sensitivity, we necessarily
require coordinate dependence. If there is coordinate dependence, then there is some correlation
between the outputs of the mechanism (in our case, the labels). When this is the case, it may be
possible to achieve a tighter privacy loss bound by leveraging their correlations, especially if these
correlations are high (see Section D in the Appendix). The second case we observe is whenever p > 1
for the sensitivity, which is a direct result of Hölder’s Inequality (Hölder, 1889). We explore this in
Section 3.2 by clipping the sensitivity in the ℓ2 norm.

3.2 DP MECHANISMS FOR τ -MULTI-WINNER ELECTION

When the average ballot does not vote for all k candidates, it is possible to obtain tighter privacy
guarantees by considering the τ -multi-winner election. Here, we bound the ℓ2 norm of each ballot,
limiting the individual impact of a ballot. For this problem to be meaningful, we assume τ < _√k;_

otherwise, we could always use the Binary voting mechanism from Definition 4. Note that norm
_clipping is a popular method for bounding the influence of an individual’s vote on the sensitivity of_
the mechanism (Abadi et al., 2016b). We integrate this method into Binary voting in Definition 7.
**Definition 7 (DP τ Voting Mechanism). Mechanism for τ** **_-Multi-Winner Election. For a τ_** _-multi-_
_winner election the τ Voting mechanism is_


_Mσ(b1, . . ., bn) ≜_ _i :_



_∈{0, 1}[k], vj[i] ≜_ min(1,


**_vj[i] + N_** (0, σG[2] [)] _> Ti_
_j=1_ 

X


)bi,
**_bi_** 2
_∥_ _∥_


where we choose Ti = n − [P]j[n]=1 **_[v][j][[][i][]][, which represents the number of][ clipped][ negative votes. To]_**
analyze the privacy loss of this new mechanism, we propose an extension of Lemma 2.1 to make it
applicable to this multi-voting task.
**Lemma 3.2. RDP-Gaussian mechanism for a multi-label setting.** _Let f : X →R[k]_ _obey_
_f_ (X) _f_ (X _[′])_ 2 ∆2 for neighboring datasets X, X _[′]. The Gaussian mechanism_ (X) =
_∥_ _−_ _∥_ _≤_ 2 _M_

_f_ (X) + Nk(0, σ[2]I) obeys RDP with εM(λ) = _[λ]2[∆]σ[2][2][ .]_

We present proof in the Appendix C. Next, we illustrate how our mechanism offers superior privacy
guarantees to the similar ℓ1 clipping proposed in Zhu et al. (2020), which is not optimal for Gaussian
mechanisms. Our bound is no larger, and often much smaller, than the privacy bound in the ℓ1 norm.
We can see this by comparing the privacy loss of the two methods in the case of max possible change

2 2 1 1
(as in Definition 6): ϵM(λ) = _[λτ]σ[2][ 2][ =][ λ]2[∆]σ[2][2][ ≤]_ _[λ]2[∆]σ[2][2][ =][ λ][(2]2σ[τ][1][2][)][2]_ = _[λ]σ[2][τ][2][ 2]_ where in the first inequality

we use the fact that _x_ 2 _x_ 1. Empirically, we confirm that τ voting achieves a much tighter
bound on many multi-label datasets. Take ∥ _∥_ _≤∥_ _∥_ _τ1 to represent clipping in the ℓ1 norm and similarly for τ2._
On Pascal VOC, which has k = 20 labels, we find that we can set τ2 = 1.8 and τ1 = 3.4 without
deteriorating the utility of the mechanism (as measured using the balanced accuracy). In general, we


-----

select τ to be marginally larger than the average p-norm of labels in the training data. For example,
there are on average fewer than 2 positive labels out of 20 per query in the Pascal VOC dataset.
Here, ℓ2 norm clipping achieves a >6x tighter privacy loss compared to Zhu et al. (2020) and the
multi-voting of Section 3.1.

3.3 CASTING ∞-MULTI-WINNER ELECTIONS AS SINGLE-WINNER ELECTIONS

It is also possible to approximate multi-winner elections using single-winner voting mechanisms
by encoding each of the 2[k] different outcomes as a separate candidate for a single-winner voting
mechanism. We call this mechanism the powerset voting mechanism.
**Definition 8 (DP Powerset Mechanism for Multi-Winner Elections). Denote the Powerset operator as**
_P(·). For a ∞-multi-winner election with ordered outcomes P({0, 1}[k]), where |P({0, 1}[k])| = 2[k],_

_n_

_σ(b1, . . ., bn) ≜_ arg max 1bj = ( 0,1 _k)i +_ (0, σG[2] [)]
_M_ _i_  _P_ _{_ _}_ _N_ 

_j=1_

 X 

where 1 represents the indicator function. We first note that this is an[P][(][{][0][,][ 1][}][k][)][i][ :] _approximation_ _[∈{] of the Multi-[0][,][ 1][}][k][,]_
Winner problem. The outcome of the mechanism may differ from the plurality per candidate if there
is no ballot cast with those votes. When we compare our two voting mechanisms, we find that there
are two regimes where Powerset voting outperforms Binary voting.

_The first regime is when there are high correlations between the multi-winner candidates, i.e., there is_
strong coordinate dependence (see Section B.4). The distribution over outcomes can be approximated
by viewing the collection of single-winner outcomes as non-uniform balls and bins problem: each
distinct outcome i ∈ [1, . . ., 2[k]] has some probability Pi of being selected. Then, each of the n
ballots is an independent ball being thrown at these bins. We can estimate the probability of each of
the 2[k] single-winner outcomes. Doing so, if the likelihood of a single i−th outcome is much higher
than the rest, it results in decreased privacy loss.

_The second regime is when neither mechanism can leverage stronger data-dependent privacy guar-_
_antees. When the distribution of multi-winner outcomes and single-winner outcomes approaches a_
uniform distribution, the best guarantee either can achieve is data-independent privacy bound based on
the sensitivity of either mechanism and the amount of noise added. Because Binary voting composes
_k separate DP queries, the privacy loss using this approach is necessarily higher than from Powerset_
voting which only releases a single outcome. Instead, when there is coordinate independence, or
when each individual multi-winner candidate has a high probability of winning, or when the most
likely outcomes are approximately the same between Binary voting and Powerset voting, we find
that Binary voting performs better. See Appendix B.4 for our detailed theoretical analysis and
supporting empirical evidence. See Sections 4 below for a detailed analysis of the data-independent
and data-dependent privacy bounds of Binary voting and Powerset voting.

4 FROM PRIVATE ELECTIONS TO PRIVATE MULTI-LABEL CLASSIFICATION

In addition to the obvious application of making elections private, DP voting mechanisms play an
important role in privacy-preserving machine learning. Notably, PATE adopted the noisy arg max
mechanism, a canonical DP single-winner election mechanism, for training private single-label
models by transferring knowledge with semi-supervised learning. In this section, we discuss how to
use our proposed DP multi-winner mechanisms to build systems for private multi-label classification.

4.1 BINARY PATE, τ PATE AND POWERSET PATE

By replacing the noisy arg max mechanism in single-label PATE with one of our multi-winner
election mechanisms, we can enable multi-label PATE. In doing so, non-private multi-label models
can be trained on the private centralized dataset and used as teachers for semi-supervised learning.
We leverage a bank of unlabeled data, which is often widely available, and label m of these data
points with noisy estimates from the ensemble of teachers using our multi-winner mechanisms. We
then train a student model on the newly DP labeled data, preserving DP of the non-private teachers’
training data. We call the corresponding forms Binary PATE, τ pate, and Powerset PATE.


-----

Pascal VOC CheXPert MIMIC-CXR PadChest

1.0

0.8

0.6

0.4 ACC

Value of metric BAC

0.2

MAP

0.0

1000 400

Binary PATE

800 Binary PATE-DI 300

600 L2-DI

t-PATE 200

400

100

200

# of answered queries

0 0

0 10 20 0 10 20 0 10 20 0.0 2.5 5.0 7.5 10.0

(Gaussian noise scale) (Gaussian noise scale) (Gaussian noise scale) (Gaussian noise scale)


Figure 1: With sufficient consensus, the best query-utility tradeoff obtained lies in a regime of
_σG where the data-dependent bound is used. In the 1st row, we maximize the σG of τ PATE while_
maintaining sufficiently high values of the performance metrics. The chosen values for (σG, τ ) are
9, 1.8 for Pascal VOC, 10, 3 for MIMIC-CXR, and 7, 2.8 for CheXPert and 7, 2.7 PadChest. When
there is a lack of consensus (on PadChest), we see that the data-independent ℓ2 (L2-DI) mechanism
bound outperforms all others. For a well chosen τ, there is little-to-no impact on the consensus of
the data-dependent regime (c.f. Binary PATE and τ PATE which leverage the data-dependent bound
when it reduces privacy loss). Because of this, τ PATE achieves a competitive query-utility tradeoff.
See Figure 14 in Appendix I.5 for tuning of Binary PATE as an ∞-Winner Election.






Pascal VOC CheXpert MIMIC PadChest

1.0

Before CaPC Mean Before CaPC
Ensemble Mean Ensemble

0.9 After CaPC Mean After CaPC

0.8

0.7

0.6

Balanced accuracy

0.5

0.4

1 2 3 4 5 6 7 8 9 1011121314151617181920 AT CA CO ED EF EN FR LL LO PN PX AT CA CO ED EF EN FR LL LO PN PX ATCACOED EFEM FI FR HE IN MANOPT PN PX

Label Label Label Label

Figure 2: Using CaPC to improve model performance. Dashed lines represent mean balanced
_accuracy (BAC). We retrain a given model using additional data labelled by all other models from the_
same ensemble. We observe a mean increase of BAC by 2.0 percentage points on CheXpert.

Because we only replace aggregation mechanism in PATE, our new multi-label PATE algorithms
(and their corresponding voting mechanisms) can directly leverage the tight data-dependent analysis
of Papernot et al. (2018). In particular, when there is an empirically observed highly likely outcome,
we can often draw a much tighter bound on the privacy loss. Note that the privacy budget must be
sanitized before being released which can be done using smooth sensitivity, as shown by Papernot et al.
(2018, Algorithm 2). In absence of this empirical observation, when the noisy arg max mechanism
satisfies ε (λ)-RDP, our Binary voting and Binary PATE algorithm achieves a data-independent
_M_
bound of _i_ _[ε][M][(][λ][)][i][,][ τ]_ [-voting and][ τ][ PATE of][ λτ]σ[2][ 2][, and Powerset voting and Powerset PATE of]

_ε_ (λ), all per election (query). Note that τ PATE also leverages data-dependent analysis as with
_M_
Binary PATE, but with the additional clipping of votes.

[P][k]


5 EXPERIMENTAL EVALUATION

We carry out the evaluation on three medical datasets CheXpert (Irvin et al., 2019), MIMICCXR (Johnson et al., 2019), PadChest (Bustos et al., 2020), and on a vision dataset Pascal VOC
2012 (Everingham et al., 2010). We use the following metrics: (1) accuracy, (2) mean recall per class
denoted as balanced accuracy (BAC) (Brodersen et al., 2010), (3) area under the receiver operating
characteristic curve (AUC) (Hand & Till, 2001), and (4) mean average precision (mAP). We use
DenseNet-121 (Huang et al., 2017) for the medical datasets and ResNet-50 (He et al., 2016) for the
vision dataset. See Supplement Section G for dataset and model descriptions. We experiment with


-----

Table 1: Model improvements through retraining with multi-label CaPC

# OF
DATASET MODELS STATE PB (ε) ACC BAC AUC MAP


1 INITIAL -  .97 .85 .97 .85
50 BEFORE CAPC -  .93 .02 .59 .01 .88 .01 .54 .01

PASCAL VOC _±_ _±_ _±_ _±_

50 AFTER CAPC 10 **.94±.01** .62±.01 .88±.01 .54±.01
50 AFTER CAPC 20 **.94±.01** **.64±.01** **.89±.01** **.55±.01**

1 INITIAL -  .79 .78 .86 .72
CHEXPERT 50 BEFORE CAPC -  .77±.06 .66±.02 .75±.02 .58±.02
50 AFTER CAPC 20 .76±.07 **.69±.01** **.77±.01** **.59±.01**

1 INITIAL -  .90 .74 .84 .51
MIMIC 50 BEFORE CAPC -  .84±.07 .63±.03 .78±.03 .43±.02
50 AFTER CAPC 20 **.85±.05** **.64±.01** **.79±.01** **.45±.03**

1 INITIAL -  .86 .79 .90 .37
PADCHEST 10 BEFORE CAPC -  .90±.01 .64±.01 .79±.01 .16±.01
10 AFTER CAPC 20 .88±.01 .64±.01 .75±.01 .14±.01

_ε = 8 for 5 labels on CheXpert (see Appendix I.3), ε = 10 for predictions on PascalVOC, ε = 20 for_
11 labels on CheXpert or MIMIC, and also ε = 20 for 15 labels on CheXpert (see also Section I.9).

5.1 ANALYZING THE QUERY-UTILITY TRADEOFF IN PATE

In comparing how the privacy parameter σG impacts the query-utility tradeoff of each mechanism,
under a fixed ε = 20 across all datasets, we find that under high consensus Binary voting performs
_best and under lower consensus τ voting performs best. In Figure 1, we compare each multi-winner_
election mechanism used in τ PATE and find that each mechanism has a range of σG where it
performs best. In particular, as σG 0, no queries can be answered by any mechanism due to a lack
of noise to satisfy the chosen ε. For sufficiently small → _σG and with suitable consensus amongst voters,_
which is the case for Pascal VOC, CheXPert, and MIMIC-CXR, we find the data-dependent analysis
for Binary voting outperforms all others while remaining in a regime of high-performance metrics.
As σG, the τ voting mechanism outperforms all others, though in most cases at a decrease
in the performance metrics; however, on PadChest, this is the best bound. This can be explained →∞
by the fact that on PadChest we can only train 10 teachers before each individual model’s accuracy
degrades too much. An important distinction is that τ pate can perform worse than Binary PATE
when the chosen τ bound is too small (because ballot clipping can change the vote distribution). For
well-chosen values of τ we observe only marginal decrease in the number of queries answered and
the performance metrics (c.f. Figure 1 with Figures 13, 12, and 14 in Appendix I). Inspecting the
tradeoff of Powerset PATE (see Figure 8 in Appendix I.1), we find that a much lower noise can be
tolerated before a steep decline in performance metrics. Because of this, we find that much fewer
queries can be answered (see Tables 5 and 8 in Appendix I.1). Thus, we recommend τ PATE as the
de-facto mechanism.

5.2 PRIVATE CENTRALIZED LEARNING USING MULTI-LABEL PATE

We now show that even in the centralized setting, our multi-label PATE methods outperform the
competitive baselines. We train models on Pascal VOC and CheXpert which include 20 and 11
labels, respectively. We leverage the entire training set to train a single non-private model and a
single private model via DPSGD (Abadi et al., 2016a). For our multi-label PATE we instead train
50 teachers each on a separate disjoint partition of the centralized training set (and thus, with 1/50
number of samples compared with DPSGD and the non-private model). We use these teacher models
to privately train a student model using semi-supervised learning with MixMatch (Berthelot et al.,
2019) (where modifications were made to adapt MixMatch to the multi-label setting). Though the
DPSGD algorithm does not assume a public unlabeled dataset, we compare it with our methods
because DPSGD can be directly applied to this multi-winner setting. DPSGD for multi-label was
studied in prior work (Zhang et al., 2021), and cannot directly leverage public unlabeled data. On the
other hand, our PATE-based approaches leverage a public pool of unlabeled samples that have noisy
labels provided by the ensemble of teachers. The added noise protects the privacy of the centralized
training data. We then train the student on the newly labeled samples. The student model, non-private


-----

baseline, and DPSGD baseline were all pre-trained on ResNet-50 models. Observing Table 2, we see
that our Binary PATE algorithm outperforms all other privacy-preserving techniques by a significant
margin. Our Binary PATE does not use τ clipping or the confident GNMax improvement so as to
fairly compare with Powerset PATE. The non-private model achieves strong performance across
all metrics where the model trained using DPSGD incurs significant degradation across all metrics.

Though Binary PATE outperforms DPSGD and Powerset PATE, it falls short of the non-private model by a
wide margin, indicating much room for improvement Table 2: DPSGD vs PATE. Comparison bein multi-label privacy-preserving techniques—in par- tween standard non-private model, DPSGD,
ticular, in extreme multi-label settings, which we mo- Powerset and Binary multi-label PATE on the
tivate and expand on in Appendix D. We observe that Pascal VOC dataset in terms of utility.
Powerset PATE answers much fewer queries (leading **METHOD** **ACC** **BAC** **AUC** **MAP**
to less training data for the student model) than Bi- NON-PRIVATE .97 .85 .97 .85

DPSGD .92 .50 .68 .40

nary PATE, at only 78 compared to 427. Though the POWERSET PATE .94 .58 .70 .29
student model only trains on 427 samples compared BINARY PATE .94 .62 .85 .57
to 5717 for DPSGD, Binary PATE has the benefit of
using non-private learning on this data whereas DPSGD must add noise in training which impedes
model learning. Finally, DPSGD incurs a high computational cost (which multi-label PATE does not)
due to the expensive per-example gradient computations (Subramani et al., 2020).

5.3 MULTI-LABEL CAPC

By replacing the single-label PATE in Choquette-Choo et al. (2021) with our multi-label PATE
mechanisms, we enable multi-label learning in a multi-site setting: the framework of CaPC allows
for distributed collaboration across models located at different sites. We scale the evaluation of multilabel CaPC learning to real-world datasets and models by providing a decentralized (independent)
evaluation of each answering party, enabling large models. Our multi-label CaPC experiments
replicate the setup of Choquette-Choo et al. (2021) using the source code provided.

We train 1 model per participant on separate distinct portions of the training set and then use multilabel CaPC to improve the performance of 3 participants. Details are in Supplement Section H.
Observing Table 1, we see that multi-label CaPC consistently improves the BAC, with the greatest
improvement of a considerable 5 percentage point increase on Pascal VOC and improvement across
all other metrics. These improvements are echoed in the larger and more privacy-sensitive CheXpert
and MIMIC-CXR datasets; however, we observe some performance degradation on PadChest, likely
because of the degraded vote utility compared to the original training data. As one of the main
applications for CaPC is the healthcare domain, the on-average performance improvements and
associated privacy guarantees demonstrate the utility of multi-label CaPC in a realistic use case.

Inspecting Figure 2, we see that multi-label CaPC leads to significant improvements in low-sample,
low-performance labels (e.g., labels EN and FR on CheXpert). Thus, poorer performing models
and classes can gain from the noisy aggregation of more performant teacher models through multilabel CaPC. This has potential ramifications for fairness because our experiments consistently
demonstrate that private multi-label CaPC can improve model performance on its poorer performing
subpopulations—where Suriyakumar et al. (2021) show that differentially private can hurt performance on these subpopulations. We reiterate that these subpopulations are common in many settings
such as healthcare due to imbalanced labels, e.g., rarer diseases (see Figure 6 of Appendix G).

6 CONCLUSIONS

We address the need for privacy in the multi-label setting with three new multi-label voting mechanisms. We show and prove that, while simple, our Binary voting cannot be outperformed without
strong candidate correlations. When these correlations exist, we prove new data-independent bounds
for our τ voting mechanism and theoretically analyze when Powerset voting performs better. Using
these mechanisms, we create multi-label PATE which outperforms DPSGD in the centralized private
learning setting. We further enable multi-label learning in multi-site scenarios by creating multi-label
CaPC. Our results achieve new state-of-the-art results for private learning in multi-label settings and
demonstrate a need for further exploration to lessen the gap between private methods and non-private
baselines.


-----

7 ETHICS STATEMENT

Our work aims at the protection of privacy and from this perspective, it should benefit society. More
specifically, our work aims to provide private multi-label classification in a collaborative setting and
therefore any concerns of it being integrated into applications deemed harmful to humans or the
environment are a smaller subset of those which exist in regular multi-label classification—a common
machine learning application.

It is not well understood what the value of ε should be and therefore improper usage and large values
of epsilons could leak private information. This risk can be mitigated by setting an upper threshold
for the possible values of ε which can be used. The values of ε used in our experiments (8, 10, and
20) could be considered high. However, since our work provides multi-label classification, there
are many labels that need to be released and therefore the privacy budget will naturally be higher
than for single-label classification. Our common choice of ε = 20 is empirical. We select a value,
which although is not tight, is a reasonable bound for our multi-label setting because it is in the range
of values considered by most empirical evaluations (of order 10). We also test other ε values, for
instance, show ε = 10 for the Pascal VOC dataset in Table 1 and ε = 8 for the first 5 labels from the
CheXpert dataset in Section I.3 in the Appendix.

The worst scenario in the collaborative setting and for our method is when the querying party colludes
with the privacy guardian. In this situation, the labels from each answering party are revealed.
There are many possible attacks that can take advantage of such information. One of them is model
extraction, where the number of queries answered by an answering party can be sufficient to replicate
the party’s model and steal its intellectual property. We should make sure that the privacy guardian,
which is responsible for the privacy protection, is a trusted institution, for example, a party designated
by a government.

8 REPRODUCIBILITY STATEMENT

For theoretical results, we provide clear explanations of assumptions and complete proofs are included
in the Appendix (e.g., C). We submit our code in the supplementary material. In the README.md
file we provide the main commands needed to run our code. We also describe the pointers to the
crucial parts of the code that directly reflect the implementation described in the main part of the
submission. For Pascal VOC and medical datasets used in the experiments, we provide complete data
processing steps in the code uploaded as supplementary materials.

REFERENCES

Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
_Conference on Computer and Communications Security, pp. 308–318, 2016a._

Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
_Conference on Computer and Communications Security, CCS ’16, pp. 308–318, New York, NY,_
USA, 2016b. Association for Computing Machinery. ISBN 9781450341394. doi: 10.1145/
[2976749.2978318. URL https://doi.org/10.1145/2976749.2978318.](https://doi.org/10.1145/2976749.2978318)

Rahul Agrawal, Archit Gupta, Yashoteja Prabhu, and Manik Varma. Multi-label learning with
millions of labels: Recommending advertiser bid phrases for web pages. In Proceedings of the
_22nd international conference on World Wide Web, pp. 13–24, 2013._

Emanuel Ben-Baruch, Tal Ridnik, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, and Lihi
Zelnik-Manor. Asymmetric loss for multi-label classification. In ICCV, 2021.

David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and
Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso[ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf)
[1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/1cd138d0499a68f4bb72bee04bbec2d7-Paper.pdf)


-----

Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, and Yann LeCun. Differentially[and non-differentially-private random decision trees. CoRR, abs/1410.6973, 2014. URL http:](http://arxiv.org/abs/1410.6973)
[//arxiv.org/abs/1410.6973.](http://arxiv.org/abs/1410.6973)

Matthew R Boutell, Jiebo Luo, Xipeng Shen, and Christopher M Brown. Learning multi-label scene
classification. Pattern recognition, 37(9):1757–1771, 2004.

Kay Henning Brodersen, Cheng Soon Ong, Klaas Enno Stephan, and Joachim M Buhmann. The
balanced accuracy and its posterior distribution. In 2010 20th international conference on pattern
_recognition, pp. 3121–3124. IEEE, 2010._

Mark Bun, Cynthia Dwork, Guy N. Rothblum, and Thomas Steinke. Composable and versatile
privacy via truncated cdp. In Proceedings of the 50th Annual ACM SIGACT Symposium on
_Theory of Computing, STOC 2018, pp. 74–86, New York, NY, USA, 2018. Association for_
[Computing Machinery. ISBN 9781450355599. doi: 10.1145/3188745.3188946. URL https:](https://doi.org/10.1145/3188745.3188946)
[//doi.org/10.1145/3188745.3188946.](https://doi.org/10.1145/3188745.3188946)

Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and Maria de la Iglesia-Vayá. Padchest: A
large chest x-ray image dataset with multi-label annotated reports. Medical Image Analysis,
[66:101797, Dec 2020. ISSN 1361-8415. doi: 10.1016/j.media.2020.101797. URL http:](http://dx.doi.org/10.1016/j.media.2020.101797)
[//dx.doi.org/10.1016/j.media.2020.101797.](http://dx.doi.org/10.1016/j.media.2020.101797)

Dingfan Chen, Tribhuvanesh Orekondy, and Mario Fritz. Gs-wgan: A gradient-sanitized approach
for learning differentially private generators. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
[12673–12684. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2020/file/9547ad6b651e2087bac67651aa92cd0d-Paper.pdf)
[paper/2020/file/9547ad6b651e2087bac67651aa92cd0d-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/9547ad6b651e2087bac67651aa92cd0d-Paper.pdf)

Christopher A. Choquette-Choo, Natalie Dullerud, Adam Dziedzic, Yunxiang Zhang, Somesh Jha,
Nicolas Papernot, and Xiao Wang. CaPC Learning: Confidential and Private Collaborative
[Learning. In International Conference on Learning Representations, 2021. URL https://](https://openreview.net/forum?id=h2EbJ4_wMVq)
[openreview.net/forum?id=h2EbJ4_wMVq.](https://openreview.net/forum?id=h2EbJ4_wMVq)

Amrita Roy Chowdhury, Theodoros Rekatsinas, and Somesh Jha. Data-dependent differentially
private parameter learning for directed graphical models. In Hal Daumé III and Aarti Singh
(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of
_Proceedings of Machine Learning Research, pp. 1939–1951. PMLR, 13–18 Jul 2020. URL_
[https://proceedings.mlr.press/v119/chowdhury20a.html.](https://proceedings.mlr.press/v119/chowdhury20a.html)

Joseph Paul Cohen, Mohammad Hashir, Rupert Brooks, and Hadrien Bertrand. On the limits of
cross-domain generalization in automated x-ray prediction. In Tal Arbel, Ismail Ben Ayed, Marleen
de Bruijne, Maxime Descoteaux, Herve Lombaert, and Christopher Pal (eds.), Proceedings of
_the Third Conference on Medical Imaging with Deep Learning, volume 121 of Proceedings of_
_Machine Learning Research, pp. 136–155, Montreal, QC, Canada, 06–08 Jul 2020. PMLR. URL_
[http://proceedings.mlr.press/v121/cohen20a.html.](http://proceedings.mlr.press/v121/cohen20a.html)

Graham Cormode, Cecilia M. Procopiuc, Divesh Srivastava, Entong Shen, and Ting Yu. Differentially
private spatial decompositions. 2012 IEEE 28th International Conference on Data Engineering,
pp. 20–31, 2012.

Yuval Dagan and Gil Kur. A bounded-noise mechanism for differential privacy. arXiv preprint
_arXiv:2012.03817, 2020._

J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
_(CVPR), Miami Beach, Florida, USA, June 2009._

Emily Denton, Jason Weston, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. User conditional
hashtag prediction for images. In Proceedings of the 21th ACM SIGKDD international conference
_on knowledge discovery and data mining, pp. 1731–1740, 2015._

Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In Proceedings of the forty-first
_annual ACM symposium on Theory of computing, pp. 371–380, 2009._


-----

Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in
private data analysis. In Theory of cryptography conference, pp. 265–284. Springer, 2006.

Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
_and Trends® in Theoretical Computer Science, 9(3–4):211–407, 2014._

Edith Elkind, Piotr Faliszewski, Jean-Francois Laslier, Piotr Skowron, Arkadii Slinko, and Nimrod
Talmon. What do multiwinner voting rules do? an experiment over the two-dimensional euclidean
domain. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI’17,
pp. 494–501. AAAI Press, 2017.

Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.
The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):
303–338, 2010.

Piotr Faliszewski, Arkadii M. Slinko, and Nimrod Talmon. Multiwinner voting: A new challenge for
social choice theory. In Trends in computational social choice, 2017.

Sam Fletcher and Md Zahidul Islam. Differentially private random decision forests using smooth
sensitivity. Expert Systems with Applications, 78:16–31, 2017. ISSN 0957-4174. doi: https://doi.
[org/10.1016/j.eswa.2017.01.034. URL https://www.sciencedirect.com/science/](https://www.sciencedirect.com/science/article/pii/S0957417417300428)
[article/pii/S0957417417300428.](https://www.sciencedirect.com/science/article/pii/S0957417417300428)

Craig Gentry. A fully homomorphic encryption scheme, volume 20. Stanford university Stanford,
2009.

David J Hand and Robert J Till. A simple generalisation of the area under the roc curve for multiple
class classification problems. Machine learning, 45(2):171–186, 2001.

K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
_of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, Nevada,_
USA, June 2016.

Otto Hölder. Ueber einen mittelwerthabsatz. Nachrichten von der Königl. Gesellschaft der Wis_senschaften und der Georg-Augusts-Universität zu Göttingen, 1889:38–47, 1889._

Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
_Recognition (CVPR), July 2017._

Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik
Marklund, Behzad Haghgoo, Robyn L. Ball, Katie S. Shpanskaya, Jayne Seekins, David A.
Mong, Safwan S. Halabi, Jesse K. Sandberg, Ricky Jones, David B. Larson, Curtis P. Langlotz,
Bhavik N. Patel, Matthew P. Lungren, and Andrew Y. Ng. Chexpert: A large chest radiograph
dataset with uncertainty labels and expert comparison. CoRR, abs/1901.07031, 2019. URL
[http://arxiv.org/abs/1901.07031.](http://arxiv.org/abs/1901.07031)

Alistair E. W. Johnson, Tom J. Pollard, Nathaniel R. Greenbaum, Matthew P. Lungren, Chih
ying Deng, Yifan Peng, Zhiyong Lu, Roger G. Mark, Seth J. Berkowitz, and Steven Horng.
Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. arXiv preprint
_arXiv:1901.07042, 2019._

Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential
privacy. In International conference on machine learning, pp. 1376–1385. PMLR, 2015.

Yunhui Long, Suxin Lin, Zhuolin Yang, Carl A. Gunter, and Bo Li. Scalable differentially private
generative student model via pate, 2019.

Zelun Luo, Daniel J. Wu, Ehsan Adeli, and Li Fei-Fei. Scalable differential privacy with sparse
network finetuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
_Recognition (CVPR), pp. 5059–5068, June 2021._


-----

Dahlia Malkhi, Noam Nisan, Benny Pinkas, and Yaron Sella. Fairplay-a secure two-party computation
system. In Proceedings of the 13th Conference on USENIX Security Symposium - Volume 13,
SSYM’04, pp. 20, USA, 2004. USENIX Association.

Ilya Mironov. Rényi differential privacy. In 2017 IEEE 30th Computer Security Foundations
_Symposium (CSF), pp. 263–275. IEEE, 2017._

Payman Mohassel and Yupeng Zhang. Secureml: A system for scalable privacy-preserving machine
learning. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 19–38. IEEE, 2017.

Nicolas Papernot, Martín Abadi, Úlfar Erlingsson, Ian J. Goodfellow, and Kunal Talwar. Semisupervised knowledge transfer for deep learning from private training data. In 5th International
_Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Confer-_
_ence Track Proceedings, 2017._

Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and Úlfar
Erlingsson. Scalable private learning with PATE. In 6th International Conference on Learning
_Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track_
_Proceedings, 2018._

Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis, Thierry Artieres, George Paliouras, Eric
Gaussier, Ion Androutsopoulos, Massih-Reza Amini, and Patrick Galinari. Lshtc: A benchmark
for large-scale text classification. arXiv preprint arXiv:1503.08581, 2015.

Hieu H Pham, Tung T Le, Dat Q Tran, Dat T Ngo, and Ha Q Nguyen. Interpreting chest x-rays via
cnns that exploit hierarchical disease dependencies and uncertainty labels. Neurocomputing, 437:
186–194, 2021.

Benny Pinkas, Mike Rosulek, Ni Trieu, and Avishay Yanai. Psi from paxos: Fast, malicious private set
intersection. In Annual International Conference on the Theory and Applications of Cryptographic
_Techniques, pp. 739–767. Springer, 2020._

Michael O. Rabin. How to exchange secrets with oblivious transfer. Cryptology ePrint Archive,
[Report 2005/187, 2005. https://eprint.iacr.org/2005/187.](https://eprint.iacr.org/2005/187)

Adi Shamir. How to share a secret. Communications of the ACM, 22(11):612–613, 1979.

Yanyao Shen, Hsiang-fu Yu, Sujay Sanghavi, and Inderjit Dhillon. Extreme multi-label classification
from aggregated labels. In International Conference on Machine Learning, pp. 8752–8762. PMLR,
2020.

Pranav Subramani, Nicholas Vadivelu, and Gautam Kamath. Enabling fast differentially private sgd
via just-in-time compilation and vectorization. arXiv preprint arXiv:2010.09063, 2020.

Vinith M Suriyakumar, Nicolas Papernot, Anna Goldenberg, and Marzyeh Ghassemi. Chasing your
long tails: Differentially private prediction in health care settings. In Proceedings of the 2021 ACM
_Conference on Fairness, Accountability, and Transparency, pp. 723–734, 2021._

Grigorios Tsoumakas and Ioannis Katakis. Multi-label classification: An overview. International
_Journal of Data Warehousing and Mining (IJDWM), 3(3):1–13, 2007._

Salil Vadhan. The Complexity of Differential Privacy, pp. 347–450. Springer International Publishing,
[Cham, 2017. ISBN 978-3-319-57048-8. doi: 10.1007/978-3-319-57048-8_7. URL https:](https://doi.org/10.1007/978-3-319-57048-8_7)
[//doi.org/10.1007/978-3-319-57048-8_7.](https://doi.org/10.1007/978-3-319-57048-8_7)

Stephan Van Eeden, Jonathon Leipsic, SF Paul Man, and Don D Sin. The relationship between
lung inflammation and cardiovascular disease. American journal of respiratory and critical care
_medicine, 186(1):11–16, 2012._

Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. Differentially private generative
adversarial network, 2018.


-----

Bangzhou Xin, Wei Yang, Shaowei Wang, and Liusheng Huang. Differentially private greedy decision
forest. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal
_Processing (ICASSP), pp. 2672–2676, 2019. doi: 10.1109/ICASSP.2019.8682219._

Chao-Han Huck Yang, Sabato Marco Siniscalchi, and Chin-Hui Lee. Pate-aae: Incorporating adversarial autoencoder into private aggregation of teacher ensembles for spoken command classification.
In Interspeech, 2021.

Andrew Chi-Chih Yao. How to generate and exchange secrets (extended). pp. 162–167, 1986a.

Andrew Chi-Chih Yao. How to generate and exchange secrets. In Proceedings of the IEEE Symposium
_on Foundations of Computer Science, 1986b._

Jinsung Yoon, James Jordon, and Mihaela van der Schaar. PATE-GAN: Generating synthetic data
with differential privacy guarantees. In International Conference on Learning Representations,
[2019. URL https://openreview.net/forum?id=S1zk9iRqF7.](https://openreview.net/forum?id=S1zk9iRqF7)

Xinyue Zhang, Jiahao Ding, Maoqiang Wu, Stephen T.C. Wong, Hien Van Nguyen, and Miao
Pan. Adaptive privacy preserving deep learning algorithms for medical data. In Proceedings of
_the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 1169–1178,_
January 2021.

Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene
parsing through ADE20K dataset. In Proceedings of the IEEE Conference on Computer Vision
_and Pattern Recognition (CVPR), Honolulu, Hawaii, July 2017._

Yuqing Zhu, Xiang Yu, Manmohan Chandraker, and Yu-Xiang Wang. Private-knn: Practical differential privacy for computer vision. In Proceedings of the IEEE/CVF Conference on Computer Vision
_and Pattern Recognition (CVPR), June 2020._

A OPTIMALITY OF BINARY PATE

**Intuition behind Proposition 3.1. Intuitively, a coordinate-independent function will be bounded**
below by Binary PATE because a single pair of databases (X, X _[′]) will lead to the maximum sensitivity_
of each output coordinate. If the function were coordinate-dependent, this is not guaranteed, i.e., it is
possible that some output coordinate will have a smaller sensitivity because we can only pick one
pair of databases for all coordinates simultaneously. We motivate this intuition with two examples.
Let X be our database with 2 columns, i.e., X . We will consider ∆1f . Take f (X) =
_∈R[2]_

[sin(X0), cos(X1)]. Because this function is coordinate-independent, we can pick X = [ _[−]2[π]_ _[,][ 0]]_

and X _[′]_ = [ _[π]2_ _[, π][]][ independently to achieve the worst-case sensitivity of][ 4][. However, inspection of]_

the coordinate-dependent g(X) = [sin(X0), cos(X0)] will show ∆1g = 2√2 < ∆g0 + ∆g1 = 4,

where no two databases X and X _[′]_ can ever achieve ∆1g = 4 because the minimums and maximums
of sin and cos do not occur at the same values of X.

**Lemma A.1. If f** (X) : R[n] _→R[k]_ _is coordinate-independent, there exists a pair of databases_
(X, X _[′]) with ||X −_ _X_ _[′]||1 = 1 (a change in only one row) which achieves the worst-case sensitivity_
_for each of the coordinates fi._

_Proof. We consider the i∗_ output coordinate of f . We know that fi∗(X) = fi∗([X1, . . ., Xn]) =
_fi∗([Xi|i∈Pi∗_ _, Xi|i/∈Pi∗_ []) =][ f]i∗[([[][X]i|i∈Pi∗ _[, X]i[′]|i/∈Pi∗_ [])][ by Definition][ ??][. We define the worst-case]
sensitivity for fi∗, ∆1fi∗ as in Dwork et al. (2014). Then

∆1fi∗ = (X,Xmax[′]) _||fi∗(X) −_ _fi∗(X_ _[′])||1_
_||X−X_ _[′]||1=1_

= ||fi∗([Xi|i∈Pi∗ _, Xi|i/∈Pi∗_ [])][ −] _[f]i∗[([][X]i[′]|i∈Pi∗_ _[, X]i[′]|i/∈Pi∗_ [])][||][1]
= ||fi∗([Xi|i∈Pi∗ _, Xi|i/∈Pi∗_ [])][ −] _[f]i∗[([][X]i[′]|i∈Pi∗_ _[, X][i][|][i/]∈Pi∗_ [])][||]1

Therefore we can choose the members in Pi∗(X) arbitrarily and independently to achieve the worst
case sensitivity for fi∗ while the members not in Pi∗(X) can take any value because they do not


-----

affect the output. Using the fact that the Pi’s are disjoint, we can continue this over all values of i
_∗_
from 1 to k to get the desired result.

■

**Proposition A.2. For a coordinate-independent multi-label function fmulti(X) : R[n]** _→R[k]_ _and_
_a single-output function fbinary(X) :_ _applied once for each of the k labels such that_
_R[n]_ _→R_
_fbinary,i = fmulti,i, the ℓ1 sensitivity ∆1fmulti is equal to that of binary PATE applied per label_
_and thus does not provide a better privacy guarantee. Note that fbinary,i = fmulti,i means that they_
_are the same function; we include this to make clear the comparison between the multi-label and_
_binary functions._

_Proof. Let X, X_ _[′]_ _∈R[n]_ be databases such that ||X − _X_ _[′]||1 = 1 and let fbinary(X) : R[n]_ _→_
_, fmulti(X) :_ be arbitrary coordinate-independent functions. Let f _,i and Xi represent_
_R_ _R[n]_ _→R[k]_ _·_
the i’th coordinate of the output and input respectively. As in Definition ??, let Pi(X) represent
the partition of input coordinates that determine the output coordinates fbinary,i, fmulti,i, such that
additionally ∪i[k]=1[P][i][ =][ X][. We define the sensitivity of][ f] [,][ ∆][f][ as in Dwork et al. (2014). We now]
consider the ℓp sensitivity of fmulti:

∆fmulti = max _fmulti(X)_ _fmulti(X_ _[′])_ _p_
_X,X_ _[′]_ _||_ _−_ _||_

_||X−X_ _[′]||1=1_

= max (fmulti,1(X) _fmulti,1(X_ _[′]))[p]_ + + (fmulti,k(X) _fmulti,k(X_ _[′]))[p][][ 1]p_
_X,X_ _[′]_ _−_ _· · ·_ _−_

_X_ _X_ _[′]_ 1=1
_||_ _−_ _||_  

= max (fmulti,1([X1, . . ., Xn]) _fmulti,1([X1[′]_ _[, . . ., X]n[′]_ [])][p][ +][ . . .]
_X,X_ _[′]_ _−_

_X_ _X_ _[′]_ 1=1
_||_ _−_ _||_  

+ (fmulti,k([X1, . . ., Xn]) − _fmulti,k([X1[′]_ _[, . . ., X]n[′]_ []))][p][][ 1]p

= max _fmulti,1([P1(X),_ _i=1[P][i][(][X][)])][ −]_ _[f][multi,][1][([][P][1][(][X]_ _[′][)][,][ ∪]i[k]=1[P][i][(][X]_ _[′][)])]_ _p + . . ._
_X,X_ _[′]_ _∪[k]̸_ _̸_

_X_ _X_ _[′]_ 1=1
_||_ _−_ _||_   

+ (fmulti,k(Pk(X), ∪i≠ _kPi(X)]) −_ _fmulti,k([Pk(X_ _[′]), ∪i≠_ _kPi(X_ _[′])]))[p][][ 1]p_

= max _fmulti,1([P1(X),_ _i=1[P][i][(][X][)])][ −]_ _[f][multi,][1][([][P][1][(][X]_ _[′][)][,][ ∪]i[k]=1[P][i][(][X][)])]_ _p + . . ._
_X,X_ _[′]_ _∪[k]̸_ _̸_

_X_ _X_ _[′]_ 1=1
_||_ _−_ _||_   

+ (fmulti,k([Pk(X), _i=kPi(X)])_ _fmulti,k([Pk(X_ _[′]),_ _i=kPi(X)]))[p][][ 1]p by Definition ??_
_∪_ _̸_ _−_ _∪_ _̸_

_p_

= max _fbinary,1([P1(X),_ _i=1[P][i][(][X][)])][ −]_ _[f][binary,][1][(][P][1][(][X]_ _[′][)][,][ ∪][k]i=1[P][i][(][X][)])]_ + . . .
_X,X_ _[′]_ _∪[k]̸_ _̸_

_X_ _X_ _[′]_ 1=1
_||_ _−_ _||_   

+ (fbinary,k([Pk(X), ∪i≠ _kPi(X)]) −_ _f1binary,k([Pk(X_ _[′]), ∪i≠_ _kPi(X)]))[p][][ 1]p_

= ((∆fbinary,1)[p] + + (∆fbinary,k)[p]) _p by Lemma A.1_
_· · ·_

= ∆fbinary,1 + · · · + ∆fbinary,k with p = 1


Where the third last line follows since fmulti,i = fbinary,i.

■

1

In the case where p > 1, we have that ((∆fbinary,1)[p] + · · · + (∆fbinary,k)[p]) _p ≤_ ∆fbinary,1 +

∆· · ·f + ∆binary,fbinary,k1 + · · · + ∆ by a consequence of Hölder’s inequality and thus equivalently thatfbinary,k ∆fmulti ≤

**Lemma A.3. If a function f** (X) : R[n] _→R[k]_ _is coordinate-dependent, the worst-case sensitivity is_
_not guaranteed._


-----

_Proof. We prove by counterexample. Assuming that all coordinate-dependent functions guarantee_
the worst-case sensitivity, we must find a function that does not satisfy this claim. Take the coordinatedependent function f = [sin(X0), cos(X0)], where f (X) : R →R[2] and X0 determines > 1
output. The worst-case sensitivity of each coordinate is 2 since 1 sin(X0), cos(X0) 1 and
_−_ _≤_ _≤_
therefore the worst-case sensitivity of a function with two coordinates is 2 + 2 = 4. However, since
sin[2] + cos[2] = 1, a single choice of X0 and X0[′] [gives a worst-case sensitivity for][ f][ as][ 2]√2 < 4. We

arrive at this by taking the derivative of ||[sin(X0), cos(X0)] - [sin(X0[′] [)][, cos][(][X]0[′] [)]][||][1] [and set it to 0]
to get for example X0 = _[π]4_ [and][ X]0[′] [=][ 5]4[π] [.]

■

**Proposition A.4. For a multi-label function fmulti(X) : R[n]** _→R[k]_ _to have a lower sensitivity than_
_the sum of the worst-case sensitivities for each coordinate, i.e., ∆1fmulti < Σ[k]i=1[∆][1][f][binary,i][,][ f][multi]_
_must be coordinate-dependent._

_Proof. We proceed by contradiction. Let fmulti be a coordinate-independent function. By Propo-_
sition A.2, ∆1fmulti = ∆1fbinary,1 + · · · + ∆1fbinary,k = Σ[k]i=1[∆][1][f][binary,i][ which contradicts]
∆1fmulti < Σ[k]i=1[∆][1][f][binary,i][. Therefore][ f][multi][ must be coordinate-dependent.] ■

B POWERSET VS BINARY MULTI-LABEL PATE

B.1 PRELIMINARIES

Recall that Binary PATE has 2 · k events: the presence or absence independently for each of the
_k labels. Powerset differs from Binary PATE in that it (1) has a number of events that grows_
exponentially (2[k]) in the number of labels, because each subset of the binary vector’s powerset is a
separate event, (2) returns the entire binary vector of k labels simultaneously, and thus benefits from
correlations and deteriorates with the lack thereof, and (3) only adds noise once to the entire binary
vector. Here, we analyze under what scenarios Powerset is better or worse than binary PATE.

Recall that there are two bounds for PATE. The data-independent bound is a function only of the
noise added to the vector. The data-dependent bound is a function of the probability distribution
between events, estimated using the gaps between each predicted event. These gaps are influenced by
(1) the number of labels, (2) the amount of noise added, and (3) the amount of correlation between
labels, as we will now show.

To analyze PATE, we upper bound the RDP using Theorem B.2. The main (random) variable influencing the this privacy loss is the gap, q(¯n) which we calculate using Proposition B.1 from Papernot
et al. (2018, Proposition 10). It shows how to select ideal higher order moments µ1, µ2 and ε1, ε2,
using the data-dependent value for q(¯n) (thus making the RDP bound a nonlinear function of q(¯n)
solely).
**Proposition B.1. For a GNMax aggregator Mσ, the teachers’ votes histogram ¯n = (n1, · · ·, nk),**
_and for any i[∗]_ _∈P({0, 1}[k]), where P(·) denotes the Powerset, we have_

**Pr[** _σ(D)_ = i[∗]] _q(¯n),_
_M_ _̸_ _≤_

_and_

_q(¯n) ≜_ [1] _erfc_ _n∗i_ _[−]_ _[n][i]_

2 2σ

_i=i[∗]_  

X̸

_Where this is a minimal modification of (Papernot et al., 2018, Proposition 7) to go from single-label_
_PATE to multi-label Powerset PATE._
**Theorem B.2 (From Papernot et al. (2018)). Let M be a randomized algorithm with (µ1, ε1)-RDP**
_and (µ2, ε2)-RDP guarantees and suppose that there exists a likely outcome i given a dataset_
_D and bound ˜q_ 1 such that ˜q Pr[ (D) = i]. Additionally suppose the λ _µ1 and_
_q˜_ _e[(][µ][2][−][1)][ε][2]_ [ ] _µ ≤1µ1_ 1 _µ2µ2_ 1 _µ2_ _. Then for any neighboring dataset ≥_ _M_ _̸_ _D′ of D, we have:_ _≤_
_≤_ _−_ _[·]_ _−_

 1

_Dλ(_ (D) (D[′])) (1 _q˜)_ **A(˜q, µ2, ε2)[λ][−][1]** + ˜q **B(˜q, µ1, ε1)[λ][−][1][]**
_M_ _||M_ _≤_ _λ_ 1 [log] _−_ _·_ _·_

_−_

_where A(˜q, µ2, ε2)_ = (1∆ _q˜)/_ 1 (˜qe[ε][2] ) _µ2µ−2_ 1 _and B(˜q, µ1, ε1)_ =∆ e[ε][1] _/q˜µ11−1 ._
_−_ _−_
 


-----

When the toplarge), the RDP bound can be well approximated as 3 vote counts, n1 > n2 > n3 satisfy n1 − _n2, n2 −_ _n3 ≫_ _σ (the gap is sufficiently_

_ε[powerset]Mσ_ (λ) ≤ _exp(−2λ/σ[2])/λ, where λ = (n1 −_ _n2)/4,_ (1)

which is from Papernot et al. (2018, Corollary 11). Observing Figure 9 of Appendix I, we see that
this is indeed the regime we are in for Binary PATE. Assuming Powerset PATE can attain this regime
as well, we analyze under what circumstances Powerset PATE outperforms Binary PATE. To do this,
we will analyze the respective data-dependent bounds under the expected gap.

B.2 POWERSET

We define the Powerset mechanism in Definition 9. We sort n(x)i such that ni ≥ _nj∀i > j. Denote_
_gapUsing Equation 1, we get ≜_ _n1 −_ _n2. We seek to upper bound our best-case expected privacy loss for Powerset PATE._

E[ε[powerset]Mσ (λ)] = _exp(_ _[−]2_ _[gap] σ[2][ )][dgap.]_
Z _·_

E[ε[powerset]Mσ (λ)] → 0 when gap →∞. The base case gap with O(1) probability can be found
by viewing Powerset PATE as a non-uniform balls and bins problem. We have 2[k] subsets, i.e.,
_|P({0, 1}[k])| = 2[k]. Each subset (instance of an output binary vector) is represented by a bin with_
probability Pi of having a ball land in it (be voted on by a teacher). The t teachers (Σi ¯ni = t) each
vote independently for a bin (output binary vector).

To calculate the gap, we will calculate the load of the maximally loaded bin (most voted vector) and
assume a best-case scenario for powerset PATE where each other bin has only 1 ball. Denote by τ the
load of the maximally loaded bin. Then, gap = τ − 1 in our best-case scenario. By upper-bounding
_τ_, we get an upper bound for the gap.
**Definition 9 (Mechanism for powerset PATE). Denote the powerset operator as P(·). For a sample**
_x and k classes, let fj(x)_ 0, 1 _denote the j_ _th teacher model binary vector prediction. Let_
_∈{_ _}[k]_ _−_
_ni(x) be the vote count for the i−th subset (class), i.e., ni(x) ≜_ _|{j : fj(x) = P_ ({0, 1}[k])i}|. We
_define the powerset PATE mechanism as_


_Mσ(x) ≜_ arg maxi


_ni(x) +_ (0, σ[2])
_N_


To upper-bound the max load, we use an indicator random variable Cc representing the event of c
collisions occurring in any bin. Using Markov’s inequality followed by Stirlings approximation, we
get


_Pr[_ _c_ 1] E[ _c]_
_C_ _≥_ _≤_ _C_

2[k]

_t_
=
_c_
  _i_

X


_Pi[c]_


_t_
because (
_c_



2[k]

( _[t][ ·][ e]_
_≤_ _c_ [)][c]

X


_t!_ ( _[te]_
_≤_ _≤_ _c_ [)][c][).]


As with ¯n, we sort our probability [P1, _, P2k_ ] such that P1 _P2_ _P2k_ . Because n1 _n2,_
_· · ·_ _≥_ _· · · ≥_ _≫_
we have thatparticular, we care about the regime where the max load occurs with high-probability ( P1 ≫ _Pi∀i ̸= 1. Therefore, we have that_ _i_ _≈_ _P1[c]_ [and][ Pr][[][C][c] _[≥]_ [1]][ ≤]Pr[(][ t]c[·][[e] [)]c[c]][P][ c]1 [. In]1).

_C_ _→_

[P][2][k]

_Pr[_ _c_ 1] ( _[t][ ·][ e][ ·][ P][1]_ )[c] 1
_C_ _≥_ _≤_ _c_ _→_

exp(ln( _[t][ ·][ e][ ·][ P][1]_ )[c]) 1

_c_ _→_

= _c_ ln(t _P1) + c_ _c_ ln(c) 0
_⇒_ _·_ _·_ _−_ _·_ _→_
= _c_ ln(t _P1) + c = c_ ln(c) (2)
_⇒_ _·_ _·_ _·_


-----

Specifying t and P1, we can then directly calculate the max collisions c, then the gap, and finally the
best-case expected privacy loss. To compare directly with Binary PATE, we will directly calculate P1
from the analysis below.

B.3 BINARY PATE

Here, we aim to specify a probability for success of Binary PATE p (note the lowercase, as opposed
to uppercase P for Powerset PATE). This will directly control the expected gap of Binary PATE.
Further, from it, we calculate the same gap for powerset PATE to compare the two. Performing a
similar analysis for Binary PATE, we can model each teacher’s independent label prediction as a
Bernoulli trial of probability pi, i = 1 _k. With t independent trials, we can model the gap as a_
_· · ·_
binomial distribution of probability pi. To simplify our exposition, and since this will favour the
Powerset method over Binary PATE, we assume p = pi _i. Using composition, we get an expected_
_∀_
privacy loss of


_g_

_P_ (gap = g) [4] _−_

_g [exp]_ 2 _σ[2]_

  _·_ 

_t_ _g_

_p[g](1_ _p)[1][−][g][ 4]_ _−_

_g_ _−_ _g [exp]_ 2 _σ[2]_
   _·_

_t_ 4
g(pe)[g](1 − _p)[1][−][g]_ _ge_ 2σ1[2] ! _._


_g=t_

_g=0_

X

_g=t_

_g=0_

X

_g=t_

_g=0_

X


E[εbinary] = k
_·_

= k ·

= k ·





(3)


B.4 PRIVACY ANALYSIS COMPARISON: BINARY VERSUS POWERSET PATE

From the assumptions above, we can directly calculate each Pi, for Powerset PATE. We do this
by recognizing that a single teacher’s binary vector can be modeled as a binomial distribution of
_k (the number of labels) trials and probability p. To calculate the probability of any one of the 2[c]_
possible binary vectors from the Powerset vector, we express each one as a l−hot binary vector
(l labels present, independent of their coordinate position). Each possible subset satisfying l−hot
will have equal probability of occurring (because we fixed p for all labels) Then, the maximum
probability binary vector, when k sufficiently large (k > 5, approximately) and p → 0 or p → 1 is
when l = 0 or l = k as otherwise the combinatorics leads to a probability split amongst too many
choices. This implication may appear unnatural at first; however, it well models our output label
distribution: when there are a set of label coordinates that share similarly high (or low) probabilities
of predicting presence (or absence) of that label, then their most likely binary vector is all 1’s or 0’s.

**Impact of random labelscan bucket the ranges of p present and analyze these buckets separately. Let us operate under theTo model more complex distributions where in general pi ̸= p, we**
assumption of coordinate-independence. In this case, we have the maximal variance for the binomial
distribution and thus largest gaps for Powerset and Binary PATE. In this case, we expect both methods
to perform poorly: however, because the data-independent bound for Powerset PATE is tighter than
for Binary PATE, Powerset PATE performs better, as shown in Figure 3. This gives us our first two
observations: (1) Powerset PATE has a better worst-case privacy loss (data-independent bound) and
(2) both Powerset and Binary PATE degrade to their data-independent bounds as p → 0.5.

**Impact of Correlation (Coordinate Dependence)** Even when a bucket of labels has p ≈ 1 or
_p ≈_ 0 (but not equal), we can still have a varying distribution of outcomes. In particular, with
coordinate-independence, we can directly use our binomial analysis above and find that there is a
uniform probability for all outcomes in each l−hot vector. In particular, there are factorially many
outcomes each with equal probability. This drastically degrades the gap by reducing P1. However,
when there is coordinate-dependence, e.g., the best-case when each label’s value directly implies the
rest, then we have only two outcomes for each l−hot vector. This improves the gap by reducing the
possible subsets and improving P1. We formalize this as follows: if we have some subset of d vectors
that are dependent, then we can reduce k in our above analyses to (k − _d). This gives us our third_


-----

300

binary powerset

250

200

150

100

50

Number of answered queries

0

1 2 5 10

Number of random (p=0.5) labels


Figure 3: Powerset PATE outperforms Binary PATE as more labels are generated randomly,
**i.e., p=0.5. We use 50 teachers, privacy noise σGNMax = 7, and the privacy budget ε is set to 20. If**
all votes are random, then both Binary and Powerset PATE fall back on the data independent bound.


3000

2000


1000

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||binary powerset||binary powerset||||
||||||||||
||||||||||
||||||||||
||||||||||


10


Number of labels


Figure 4: Binary PATE outperforms Powerset PATE when they both have a similarly high gap.
We use 50 teachers, privacy noise σGNMax = 7, and privacy budget ε set to 2. Here, all teachers
always output 0 for all labels.

observation: (3) coordinate dependence has a combinatoric improvement for Powerset PATE and at
_best a linear improvement for Binary PATE._


**Impact of Higher Noise Multipliers** Appealing back to the data-dependent q(¯n) calculation of
Proposition B.1, we see that Binary PATE has a gap calculated asgives q[binary][(¯]n) = erfc(2P − _t/2σ) However, for a similar gap, which is bounded for both to a 2P −_ _t/2σ for t teachers. This_

max(gap) = t, we see that Powerset PATE will have q[binary][(¯]n) = _i_ _[erfc][(][gap/][2][σ][)][, which is]_

always worse when the gaps are equal. We can see this effect in Figure 4. Further, because we
union bound across the 2[k] classes, a higher noise multiplier has a larger impact on Powerset PATE,

[P][2][k]

as shown in Figure 8. These give our fourth and fifth observations: (4) in the best case, and for any
_equal gaps, Powerset PATE has a higher privacy loss, and (5) higher noise multipliers will impact_
_Powerset PATE more for similar gaps._


-----

C RDP-GAUSSIAN MECHANISM FOR A MULTI-LABEL SETTING

_Proof._

_Dλ(_ (X) (X _[′])) = Dλ(_ _k(f_ (X), σ[2]I) _k(f_ (X _[′]), σ[2]I)_
_M_ _||M_ _N_ _||N_

1 1 _∞_ _λ_ (1 _λ)_
= _λ_ 1 [log] _σ[k][p](2π)[k]_ exp _−2σ[2][ ∥][θ][ −]_ _[f]_ [(][X][)][∥]2[2] exp _−_ 2σ −[2] _∥θ −_ _f_ (X _[′])∥2[2]_ _dθ_

_−_  Z−∞    

= 1 1 _∞_ exp _−λ ∥θ −_ _f_ (X)∥22 _[−]_ [(1][ −] _[λ][)][ ∥][θ][ −]_ _[f]_ [(][X] _[′][)][∥]2[2]_ _dθ_

_λ_ 1 [log] _σ[k][p](2π)[k]_ 2σ[2]
_−_  Z−∞   

_k[p]_

= _λ_ 1 1 [log] _σσ[k][p](2(2ππ))[k][k][ exp (][λ][ −]2σ[2][1)][λ]_ _∥f_ (X) − _f_ (X _[′])∥2[2]_ = _[λ][ ∥][f]_ [(][X][)]2[ −]σ[2][f] [(][X] _[′][)][∥]2[2]_ _≤_ _[λ]2[∆]σ[2]2[2]_

_−_  


D TOWARDS PRIVATE EXTREME MULTI-LABEL CLASSIFICATION

We face extreme multi-label classifications (Shen et al., 2020) in many real-world applications such
as semantic segmentation (Zhou et al., 2017), hash-tag suggestions for user images (Denton et al.,
2015), product categorisation (Agrawal et al., 2013) and webpage annotation (Partalas et al., 2015)
where both input size and label size are extremely large. In this section, we investigate the privacyaccuracy tradeoffs of private semantic segmentation (a common and underlying example in extreme
multi-label settings) that links each image pixel to its corresponding object class (an integer value)
with a reasonable accuracy of above 60% but an expensive privacy cost of ε ≈ 3, 000, for a relative
"small" image of size 200 × 200, or ≈ 40, 000 pixels. We conclude this section by proposing future
directions to alleviate the privacy-accuracy tradeoffs in extreme multi-label settings.

We consider MIT ADE20K semantic segmentation dataset (Zhou et al., 2017) that contains 150
objects including 35 stuff objects (e.g. sky, building) and 115 discrete objects (e.g. person, car).
The label size for each image pixel is fixed (=150). However, the number of predicted labels for
each image is the number of pixels, which varies across the dataset. To perform private semantic
segmentation, we use PATE to label each image pixel. We split the training set of MIT ADE20K
dataset into equally sized partitions for 20 teachers and train a Pyramid Pooling ResNet50-Dilated
architecture of the Cascade Segmentation Module. The test accuracy of the ensemble of teachers
(using 2000 of the test images) with respect to the PATE noise standard deviation σG between 0
and 5 varies from 67% to 47%. We observe that the level of noise must be quite small, σ < 3, or
there is a steep drop in accuracy of more than 10 percentage points. The privacy cost is too high to
provide meaningful guarantees ε, due to the small σ and large number of pixels required to be labeled.

We believe that privacy analysis in extreme multi-label settings can be tightened by exploiting the semantics of inputs. For example in the semantic Opacity
segmentation task, we can reduce the privacy costs by taking advantage
of the dependency between pixels so that instead of releasing an answer Consolidation ...
per pixel, we can release only a single label per semantic region (a grouping of pixels). Exploring label dependence, rather than assuming label
independence, may also enable tighter privacy loss analysis and improve Pneumonia
accuracy, as our analysis of Proposition 3.1 suggests. Label dependence is

Opacity

Consolidation ...

Pneumonia

prevalent in many tasks, e.g., in healthcare labels are naturally organised

Figure 5: **Example**

into tree-like hierarchies such that domain experts (e.g. doctors) perform

**hierarchical** **struc-**

observations and diagnoses conditioned upon their parent node Van Eeden

**ture of labels in Chest**

et al. (2012). Figure 5 shows an example of the label structure where the

radiography setting.

root label node corresponds to the most generic disease of Opacity, while
the leaf label node represents the most specific disease of Pneumonia Pham
et al. (2021). Pneumonia implies the presence of both Consolidation and
Opacity diseases. Thus, there exist many possible methods to optimize the answering of queries. It
may be possible to tighten the privacy loss due to the implications (or, correlations) between labels;
or, to query labels in a specific order such that the all dependent nodes (Consolidation and Opacity)
can be inferred by the agreed presence of parent nodes (Pneumonia) by the teacher ensemble.


-----

|Table 3:|: Dependency|Matrix for the firs|st 5 labels form the|e CheXper|rt dataset.|
|---|---|---|---|---|---|
||Atelectasis|Cardiomegaly|Consolidation|Edema|Effusion|
|Atelectasis||0.975|0.987|0.976|0.983|
|Cardiomegaly|0.736||0.836|0.784|0.869|
|Consolidation|0.527|0.591||0.631|0.790|
|Edema|0.625|0.665|0.758||0.822|
|Effusion|0.485|0.567|0.731|0.633||


Table 4: Exploit label dependencies for the multi-label classification.

**# OF QUERIES ANSWERED** **ACC** **BAC** **AUC** **MAP**

**ANSWER ALL LABELS** _35_ _0.84_ _0.82_ _0.82_ _0.64_
**INCRESE PRIVACY NOISE** 127 0.63 0.61 0.61 0.44
**EXPLOIT NEGATIVE DEPENDENCIES** 127 0.68 0.72 0.72 0.48

In addition to exploiting the knowledge of input and label domains, our analysis of the optimal settings
for Binary PATE shows that privacy mechanisms can be tailored to the multi-label classifications. For
example, k-fold adaptive bounds Kairouz et al. (2015) that draw tighter (≪ sublinear) privacy bounds
for homogenous privacy settings can be extended to heterogeneous ε per label and per query settings
of multi-label classification. However, it is unclear if and under what scenarios we can achieve a
tighter bound. For instance, it is possible to take the maximum ε across all queries, but if there
is a large gap k-fold adaptive composition may yield looser bounds. These settings of coordinate
dependence, high label correlations, and heterogeneous k-fold adaptive composition are interesting
for future work.

We design an experiment where the baseline method obtains answers to all the labels while the new
proposed method exploits the semantics and queries labels selectively.

We find that the positive dependencies (e.g., if disease A is present then disease B is present as well)
constitute a small fraction of the whole dataset. This is because there are many more negative than
positive examples in the CheXpert dataset, which is caused by a class imbalance, a common problem
in medical datasets. For instance, we find that if both Pneumonia and Pneumothorax are present then
Lung Opacity occurs in 83.3% of the cases. However, both Pneumonia and Pneumothorax are present
in only 0.06% of samples of the dataset. Thus, we consider negative instead of positive dependencies.
For example, if Atelectasis is absent then Consolidation is absent as well in 98.7% of the cases. After
ignoring samples for which at least one of Atelectasis or Consolidation have missing values, the
percentage of samples where both labels are negative is 83%. We obtain the negative dependencies
using the training set and generate the dependency matrix 3.

We compare multi-label PATE executed for each label vs using the semantics and querying the first
label (Atelectasis) only, followed by (1) skipping the remaining labels and setting them as negative if
the first label is negative, or (2) querying the other labels if the first label is positive. As expected,
leveraging the semantics increases the number of answered queries from 35 to 127 for the same
privacy budget ε = 8 of at the cost of lower performance (less accurate answers to the queries).
However, increasing the number of answered queries by adding more privacy noise (σ = 67.5)
causes the answered queries to be less accurate than by exploiting the label dependencies. We show a
detailed comparison in Table 4.

Note that in the above example we consider the first five labels from the CheXpert dataset. We use
the same setup as for the comparison between DPSGD and multi-label PATE I.3. The metrics are
computed on the same 127 queries (to obtain 127 answered queries for the Answer all labels we
increase its privacy budget from 8 to 26.5).

E DETAILED COMPARISON WITH RELATED WORK

The main contributions from Zhang et al. (2021) on Adaptive DPSGD are two fold: (1) adaptive
differentially private deep networks proposed by adding Gaussian noise to gradients whose scale


-----

is linearly decaying instead of being static as in DPSGD, and (2) an analysis of the privacy loss
using tCDP (truncated Concentrated Differential Privacy) (Bun et al., 2018), which is a refinement of
differential privacy and of concentrated differential privacy, instead of using Moments Accountant
(MA) (Abadi et al., 2016a) or Rény Differential Privacy (RDP) (Mironov, 2017). tCDP provides a
tighter bound on the privacy leakage compared to MA because it can leverage privacy amplification
via sub-sampling. Similarly to RDP, the analysis of Gaussian noise is particularly simple in tCDP
and the composition of the randomized mechanism is straightforward. Adaptive DPSGD is applied
to CheXpert, CIFAR-10, and MNIST datasets, where data is centralized. Our work is applicable
in this centralized setting and the distributed collaborative setting where data is not centralized to
a single database. We focus on the collaborative learning where multiple parties (e.g., hospitals)
can collaborate via private inference. Another major difference between our approach and Adaptive
DPSGD is that we do not reveal any intermediate results because of the use of cryptography, whereas
the model trained using DPSGD is also a valuable asset that is released publicly. Finally, we note
that Adaptive DPSGD only considers tuning of the model’s last layer (the head layer). This task is
much easier (it has fewer parameters and is linear) and thus incurs less privacy leakage but requires
the assumption of a relevant public dataset to train a performant feature extractor. The comparison
between our proposed multi-label PATE and Adaptive DPSGD can be found in Section I.3. The
results show that the student models from the proposed multi-label PATE outperform DPSGD.

PATE can be used in different tasks, for example, single-label classification, multi-label classification,
generative models based on GANs (discriminate fake from real images, and generate samples
privately). The original PATE framework (Papernot et al., 2017; 2018) is only applicable to a
single-label classification. We extend PATE to the multi-label classification and show how it can
be applied in a collaborative setting. Another line of work applies PATE to the training of GANs.
PATE-GAN (Yoon et al., 2019) uses an ensemble of teacher discriminators to create a student
discriminator. To ensure differential privacy, the student discriminator is trained solely on inputs
produced by the generator and labeled by the teacher discriminators. The final task is a binary
classification to discriminate fake from real images, while our task is the multi-label classification.
Yang et al. (2021) follows the same algorithm as PATE-GAN except that the generative models used
are adversarial autoencoders instead of GANs. This algorithm demonstrates promising results on
synthetic speech generation but was not demonstrated on vision tasks. GS-WGAN (Chen et al.,
2020) allows releasing a sanitized form of the sensitive data with rigorous privacy guarantees. It
reduces gradient sensitivity using the Wasserstein distance and applies gradient sanitization to ensure
differential privacy for the generator. However, it considers the task of generating differentially
private synthetic data, which is orthogonal to our multi-label classification. G-PATE (Long et al.,
2019) is also a method for generating differentially private datasets, which diverges from previous
methods such as DP-GAN (Xie et al., 2018) or PATE-GAN by creating a differentially private
aggregation mechanism that directly uses information from teacher discriminators to train a student
generator. Both, G-PATE and GS-WGAN train the discriminator(s) non-privately while only training
the generator with DP guarantees. They differ in the way the gradient is sanitized. GS-WGAN points
that G-PATE has two main limitations: (1) gradients have to be discretized using manually selected
bins, and (2) high-dimensional gradients in the PATE framework incur a high privacy cost, which
requires dimensionality reduction techniques.

Luo et al. (2021) leverage additional, public datasets to instill strong representations in large models,
which are then adapted to private datasets at a minimal privacy cost. For the medical domain datasets,
we train the models from scratch without using pre-trained models. In contrast to our large-scale
medical domain and vision datasets, their method is applied to small-scale datasets, such as CIFAR-10,
and for the standard single-label classification only.

Although there exist many differential privacy tree-based ensemble methods, including random forests,
their non-private variants fall short of the SoTA performance achieved by deep neural networks on
the vision tasks we consider. These methods are typically applied to tabular datasets (Fletcher &
Islam, 2017; Bojarski et al., 2014; Xin et al., 2019). To approach the results achieved by deep neural
networks, these works require the use of feature engineering (which incurs additional privacy loss) or
public labeled data (which is a strong assumption that we do not use). For this reason, it is unlikely
that these results will perform comparably to differentially private neural networks, so we limit our
evaluation to neural networks. We remark that for the reasons above, DPSGD ensembling of neural
networks is also not a viable approach for these tasks.


-----

F NORMS FOR CLIPPING

Using an ℓ norm bound does not achieve tighter RDP benefits because we cannot upper-bound the
_∞_
_ℓ2 norm of the mechanism’s outputs (since_ _x_ _x_ 2) as required to achieve an RDP benefit. This
_|_ _|∞_ _≤|_ _|_
is analogous to why we achieved tighter RDP privacy loss bounds compared with Private kNN (Zhu
et al., 2020): they use the ℓ1 norm to bound the ℓ2 norm from above, whereas we adapt the analysis
to directly operate over the ℓ2 norm. Note that we are required to bound the ℓ2 norm because we use
the Gaussian mechanism (that satisfies RDP for an ℓ2 norm bound).

We carried out a theoretical analysis of the bounded noise mechanism from (Dagan & Kur, 2020) for
the multi-label classification. The bounded noise in terms of ℓ -norm has theoretical properties that
_∞_
could lower the probability of an incorrect prediction for a label. However, the method had a very
limited practical application, and for standard multi-label datasets, such as Pascal VOC or CheXpert,
the method performed much worse than the binary PATE per label. We removed the bounded noise
mechanism from consideration and focused on the binary PATE per label that was performing very
well in the empirical evaluation.

G DATASETS AND MODEL ARCHITECTURES

MIMIC-CXR PadChest

60000

8000

40000 6000

4000

20000

Number of samples 2000

0 0

AT CA CO ED EF EN FR LL LO PN PX AT CA CO ED EF EM FI FR HE IN MANO PT PN PX

CheXpert PascalVOC

4000

80000

3000

60000

2000

40000

Number of samples 20000 1000

0 0

AT CA CO ED EF EN FR LL LO PN PX 1 2 3 4 5 6 7 8 9 10 11121314151617181920

Label Label


Figure 6: The label distribution for each multi-label dataset (Pascal VOC, CheXpert, MIMIC-CXR,
and PadChest).

We experiment on four multi-label datasets. First, we use the common computer vision multi-label
dataset, Pascal VOC 2012. Three other of these datasets are privacy sensitive large-scale medical
datasets that are commonly used in ML for healthcare: CheXpert (Irvin et al., 2019), MIMICCXR (Johnson et al., 2019) and PadChest (Bustos et al., 2020),. These medical datasets present a
realistic and large-scale application for multi-label CaPC.

**Pascal VOC 2012 contains 11, 540 images that are split into 5, 717 images for training and 5, 823**
images for validation (Everingham et al., 2010). There are 20 classes of object labels (with their
index in parentheses) – aeroplane (1), bicycle (2), bird (3), boat (4), bottle (5), bus (6), car (7), cat (8),
chair (9), cow (10), dining table (11), dog (12), horse (13), motorbike (14), person (15), potted plant
(16), sheep (17), sofa (18), train (19), and tv monitor (20). We use ResNet-50 model (He et al., 2016)
that was pre-trained on ImageNet (Deng et al., 2009).


-----

**Medical datasets contain chest radiographs (X-ray) images. CheXpert (Irvin et al., 2019) has**
224, 316 radiographs. MIMIC-CXR-JPG (Johnson et al., 2019) contains 377, 110, and PadChest (Bustos et al., 2020) has 160, 868. The goal in each dataset is to predict presence of pathologies.
However, there are differences between pathology labels across these three datasets. X-ray images
of CheXpert are annotated with 11 pathologies– Atelectasis, Cardiomegaly, Consolidation, Edema,
Effusion, Enlarged Cardiomediastinum, Fracture, Lung Lesion, Lung Opacity, Pneumonia and Pneumothorax. MIMIC-CXR-JPG includes 11 pathologies– Enlarged Cardiomediastinum, Cardiomegaly,
Lung Opacity, Lung Lesion, Edema, Consolidation, Pneumonia, Atelectasis, Pneumothorax, Pleural
Effusion, Pleural Other, Fracture, Support Devices. PadChest includes 15 pathologies– Atelectasis,
Cardiomegaly, Consolidation, Edema, Effusion, Emphysema, Fibrosis, Fracture, Hernia, Infiltration,
Mass, Nodule, Pleural_Thickening, Pneumonia and Pneumothorax.

The pathology and its code (in parenthesis) is as follows: Atelectasis (AT), Cardiomegaly (CA),
Consolidation (CO), Edema (ED), Effusion (EF), Emphysema (EM), Enlarged Cardiomediastinum
(EN), Fibrosis (FI), Fracture (FR), Hernia (HE), Infiltration (IN), Lung Lesion (LL), Lung Opacity
(LO), Mass (MA), Nodule (MO), Pleural_Thickening (PT), Pneumonia (PN), Pneumothorax (PX).

All datasets obtain labels from associated reports. Both CheXpert and MIMIC-CXR-JPG use the
CheXpert labelling system, which is a rule based approach. PadChest obtains reports annotated
by trained radiologists, then trains an attention-based recurrent neural network to predict on these
annotations, and labels the remaining data. On all three datasets, we train a DenseNet-121 (Huang
et al., 2017) model and filter only frontal images (AP and PA). The main difference between our
setup and the one from Cohen et al. (2020) is that we use both frontal views AP and PA, while the
cited work uses only one of the frontal views, thus either AP and PA.

The label distribution for each multi-label dataset (Pascal VOC, CheXpert, MIMIC-CXR, and
PadChest) is presented in Figure 6. The label distribution for medical datasets is more unbalanced
than for Pascal VOC, due to prevalence or rarity of certain diseases.

H EXPERIMENTAL DETAILS

Our experiments were performed on machines with Intel®Xeon®Silver 4210 processor, 128 GB of
RAM, and four NVIDIA GeForce RTX 2080 graphics cards, running Ubuntu 18.04.

We train ResNet-50 (He et al., 2016) on the Pascal VOC 2012 (Everingham et al., 2010) dataset by
minimizing multi-label soft margin loss function in 1000 epochs. As an optimiser, we use SGD with
learning rate and weight decay of 0.001 and 1e − 4, respectively. In case of 50 Pascal VOC models,
we split the 5, 717 images of the training set to 50 private training sets in order to train 50 teacher
models.

We train the medical datasets CheXpert, MIMIC-CXR, and PadChest using the standard DenseNet121 architecture trained for 100 epochs, with weight decay of 1e − 5, Adam optimizer (learning
rate=0.001) with the adam_amsgrad paremeter set. We reduce the learning rate when it plateaus,
batch size is 64. The loss type is BCE (Binary Cross Entropy) with logits, and probability threshold is
adjusted per label (where the standard value is 0.5). For 50 CheXpert models, each model is trained
on 3750 private train samples. PadChest uses 9223 private samples per each of 10 teacher models.

Regarding the metrics, balanced accuracy (BAC), area under the curve (AUC), and mean average
precision are common metrics used to asses performance of the multi-label classification (Ben-Baruch
et al., 2021). Note that because we are in a sparse-hot multi-label setting (where a sample has many
0’s - negative labels) the accuracy is not a preferred metric because a model can achieve high accuracy
by returning all 0’s; we include the accuracy metric for completeness.

**For our CaPC experiments, we sample uniformly, without replacement, data points from the**
respective training data distribution to create disjoint partitions Di of equal size for each party i. We
use 50 total parties for Pascal VOC, CheXpert, and MIMIC-CXR, as well as 10 for PadChest; this
choice depended on the sizes of the datasets and the performance of the models on the resulting
partitioned data (here, we ensured no single model dropped below 60% BAC). We use Q = 3
querying parties and sample (without replacement) at least 1000 data points from the test distribution
to form the unlabeled set for each querying party. We leave at least 1000 held-out data points for
evaluating models. We first train party i’s model on their private data Di, then simulate multi-label


-----

CaPC learning by having each querying party complete the protocol with all other parties as answering
parties. Using the new labelled data provided by multi-label CaPC, we retrain each querying party’s
model on their original data Di plus the new labelled data. We average metrics over at least 3 runs
with each using a different random seed.

I ADDITIONAL EXPERIMENTS

I.1 MULTI-LABEL PATE: BINARY VS POWERSET

We compare Binary vs Powerset multi-label PATE in Tables 5, 8, and Figure 7. For each data point,
we check how the performance of the Binary and Powerset PATE degrades as we increase the scale of
Gaussian noise σGNMax, and select the scale that preserves high values of the metrics (ACC, BAC,
AUC, MAP), with maximum performance drop by a few percentage points - we select this threshold
arbitrarily. We align both Binary and Powerset methods according to the performance metrics. For a
given values of the metrics, we find the σGNMax starting from 1 to the highest possible value that
gives performance above the chosen threshold and allows us to answer maximum number of queries.
For example, in Figure 8, we select σGNMax = 4 for Powerset PATE and σGNMax = 11 for Binary
PATE, while the values of the metrics are ACC=.95, AUC=.66, and MAP = .37. For CheXpert, we
set σGNMax = 5 for Powerset PATE and σGNMax = 20 for Binary PATE, while the values of the
metrics are ACC=.71, AUC=.68, and MAP = .45.

Table 5: CheXpert: Performance of Binary PATE vs Powerset PATE w.r.t. number of answered
queries, ACC, BAC, AUC, and mAP (as measured on the test set with the specified σGNMax. PB (ε) is
the privacy budget. We use 50 teacher models. When we limit number of labels per dataset, we select
the first k labels.

# OF QUERIES
DATASET MODE LABELS ANSWERED PB (ε) _σGNMAX_ ACC BAC AUC MAP

CHEXPERT BINARY 1 **988** 20 8 .71 .71 .71 .63
CHEXPERT POWERSET 1 **988** 20 8 .71 .71 .71 .63
CHEXPERT BINARY 2 **898** 20 18 .73 .72 .72 .62
CHEXPERT POWERSET 2 **872** 20 12 .72 .72 .72 .60
CHEXPERT BINARY 3 **399** 20 13 .75 .77 .77 .55
CHEXPERT POWERSET 3 **674** 20 10 .74 .76 .76 .55
CHEXPERT BINARY 4 **554** 20 17 .74 .75 .75 .60
CHEXPERT POWERSET 4 **323** 20 5 .76 .78 .78 .60
CHEXPERT BINARY 5 **320** 20 17 .74 .76 .76 .59
CHEXPERT BINARY 5 **932** 20 29 .70 .70 .70 .54
CHEXPERT POWERSET 5 **582** 20 10 .74 .76 .76 .54
CHEXPERT BINARY 6 **299** 20 18 .74 .75 .75 .56
CHEXPERT POWERSET 6 **392** 20 7 .74 .75 .75 .55
CHEXPERT BINARY 7 **157** 20 12 .73 .74 .74 .51
CHEXPERT POWERSET 7 **338** 20 7 .73 .72 .72 .51
CHEXPERT BINARY 8 **145** 20 13 .73 .73 .73 .51
CHEXPERT POWERSET 8 **200** 20 5 .73 .71 .71 .50
CHEXPERT BINARY 9 **159** 20 16 .71 .70 .70 .50
CHEXPERT POWERSET 9 **241** 20 6 .71 .70 .70 .50
CHEXPERT BINARY 10 **105** 20 11 .74 .72 .72 .50
CHEXPERT POWERSET 10 **146** 20 4 .73 .71 .71 .50
CHEXPERT BINARY 11 **201** 20 5 .71 .68 .68 .45
CHEXPERT POWERSET 11 **152** 20 20 .71 .68 .68 .44

I.2 POWERSET WITH τ -VOTING

We present how to incorporate τ -voting into the Powerset method. A given teacher is allowed to
set up to τ positive labels for a given query sample. This approach limits the number of positive
classes from C = 2[k] to C = _k0_ + _k1_ + + _τk_ classes. Since we have fewer classes: (1) it
_· · ·_
increases the possibility of achieving a consensus between teachers (a higher gap between max and
        
runner-up numbers of votes can be achieved), (2) the data independent bound is lower (computed as:


-----

Table 6: Pascal VOC: Performance of Binary PATE vs Powerset PATE w.r.t. number of answered
queries, ACC, BAC, AUC, and mAP as measured on the test set with the specified σGNMax. PB (ε) is
the privacy budget. We use 50 teacher models. When we limit number of labels per dataset, we select
the first k labels.

# OF QUERIES
DATASET MODE LABELS ANSWERED PB (ε) _σGNMAX_ ACC BAC AUC MAP

PASCAL VOC BINARY 1 **5464** 20 2 .98 .84 .84 .75
PASCAL VOC POWERSET 1 **5464** 20 2 .98 .84 .84 .75
PASCAL VOC BINARY 2 **5442** 20 5 .97 .74 .74 .54
PASCAL VOC POWERSET 2 **5464** 20 5 .97 .74 .74 .55
PASCAL VOC BINARY 3 **3437** 20 7 .97 .72 .72 .51
PASCAL VOC POWERSET 3 **3416** 20 7 .97 .72 .72 .51
PASCAL VOC BINARY 5 **2398** 20 8 .96 .66 .66 .39
PASCAL VOC POWERSET 5 **2040** 20 8 .96 .65 .65 .33
PASCAL VOC BINARY 8 **1543** 20 7 .96 .69 .69 .46
PASCAL VOC POWERSET 8 **1192** 20 7 .95 .68 .68 .37
PASCAL VOC BINARY 11 **1190** 20 11 .95 .66 .66 .37
PASCAL VOC POWERSET 11 **702** 20 4 .95 .66 .66 .37
PASCAL VOC BINARY 14 **888** 20 12 .95 .65 .65 .36
PASCAL VOC POWERSET 14 **417** 20 3 .95 .65 .65 .37
PASCAL VOC BINARY 15 **465** 20 8 .94 .66 .66 .36
PASCAL VOC POWERSET 15 **165** 20 3 .94 .66 .66 .36
PASCAL VOC BINARY 16 **461** 20 7 .95 .65 .65 .38
PASCAL VOC POWERSET 16 **94** 20 2 .95 .65 .65 .38
PASCAL VOC BINARY 18 **446** 20 2 .95 .65 .65 .36
PASCAL VOC POWERSET 18 **94** 20 2 .95 .65 .65 .36
PASCAL VOC BINARY 20 **427** 20 7 .95 .65 .65 .37
PASCAL VOC POWERSET 20 **78** 20 2 .95 .65 .65 .33


Pascal VOC

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|||||Mode|||
|||||Binary Powerset|||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||



10 15 20

Number of labels


CheXpert

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
||||||Mode Bina|ry||
|||||||||
||||||Pow|erset||
|||||||||
|||||||||
|||||||||
|||||||||



4 6 8

Number of labels


3500

3000

2500

2000

1500

1000

500


1000

800


600

400


200


10


Figure 7: Binary vs Powerset PATE: number of answered queries. We compare the number of
answered queries vs number of k first labels selected from the Pascal VOC and CheXpert datasets.
We keep the privacy budget ε = 20. The private multi-label classification with Binary PATE performs
better (answers more queries) than Powerset PATE for Pascal VOC. The CheXpert dataset is more
noisy (in terms of labeling, the pathologies are sparse and difficult to detect) and does not give us a
clear preference of the multi-label PATE method.

1 − (1/C)), and (3) the method has higher performance (faster run-time). On the other hand, it does
not lower the sensitivity of the Powerset mechanism because changing one of the teachers potentially
decreases the vote count from one class and increase it in another one. Thus, the sensitivity remains
2.

Intuitively, up to τ positive labels could be selected based on the confidence of the positive value for a
label, where we would choose the top τ labels with the highest confidence of being positive. However,
given that most deep multi-label models use independent predictive heads (i.e., a separate sigmoid


-----

Pascal VOC


CheXpert


Binary

|Col1|Col2|ACC|
|---|---|---|
|||AUC MAP|
||||



20 40

(Gaussian noise scale)


Powerset

ACC
AUC
MAP


20 40

(Gaussian noise scale)


Binary

|Col1|Col2|ACC|
|---|---|---|
|||AUC MAP|
||||



20 40

(Gaussian noise scale)


Powerset

ACC
AUC
MAP


20 40

(Gaussian noise scale)


1.0

0.8

0.6

0.4

0.2


0.7

0.6

0.5

0.4

0.3


Figure 8: Binary vs Powerset PATE: performance. We compare the drop in performance in terms
of accuracy (ACC), area under the curve (AUC), and mean average precision (MAP) between Binary
PATE and Powerset PATE as we increase the scale of the Gaussian noise σGNMax. We select the first
11 labels in Pascal VOC and all 11 labels from CheXpert. We keep the privacy budget ε = 20. The
private multi-label classification with PATE performs better (preserves higher values of the metrics)
using the Binary approach.

activation per output label), naively comparing these values may not lead to the best performance.
Thus, no clear notion of what the desired subset is in the case that > τ candidates are present.

For the Pascal VOC dataset, the same threshold of 0.5 probability is used per label, so the prediction
heads for each label are aligned. For the CheXpert dataset, different probability thresholds are set
per label, so the problem of deciding which of the labels should remain positive (if there are more
positive labels than the max τ of positive labels) is difficult to resolve. This would likely require
some form of domain knowledge.

For the original train set of the Pascal VOC dataset (with 5823 samples), the average number of
positive labels per example is 1.52, with 20 total labels. More detailed statistics on the train set are
presented in Table 7.


Table 7: More statistics about the positive labels per data point in the Pascal VOC dataset.


**NUMBER OF POSITIVE LABELS** **NUMBER OF TRAIN SAMPLES**

6 1

5 17

4 105

3 484

2 1677

1 3539

0 0

When the τ -voting is applied with the fixed privacy budget, we are able to answer more queries, while
the performance metrics (e.g., accuracy) remain comparable. When the initial number of labels is
set to k = 5, for τ = 2 the number of classes is 16 instead of 32 (τ = 5, which is equivalent to no
clipping) and we are able to answer 5% queries more, for τ = 1, we have only 6 classes and almost
14% more answered queries. For τ = 5 there is only 1 more class than for τ = 4 and we observe
a very small difference in the number of answered queries (namely 8). The number of answered
queries using the Binary method is still higher than with Powerset even when τ = 1 (the number of
classes is C = k + 1).


-----

Table 8: Pascal VOC: Performance of Powerset PATE with τ **-clipping w.r.t. number of answered**
queries with the specified σGNMax. The ACC, BAC, AUC, and mAP are measured on the answered
queries from the test set. PB (ε) is the privacy budget. We use 50 teacher models. When we limit
the number of labels per dataset, we select the first k labels. The τ denotes a maximum number of
positive labels that a teacher is allowed to return per data sample.

# OF QUERIES
LABELS _τ_ ANSWERED PB (ε) _σGNMAX_ ACC BAC AUC MAP

1 1 **5464** 20 2 .98 .84 .84 .75
2 1 **5464** 20 5 .97 .75 .75 .55
2 2 **5464** 20 5 .97 .74 .74 .55
3 1 **3437** 20 7 .97 .71 .71 .48
3 2 **3421** 20 7 .97 .71 .71 .49
3 3 **3416** 20 7 .97 .72 .72 .51
4 1 **2547** 20 7 .97 .68 .68 .43
4 2 **2527** 20 7 .97 .69 .69 .43
4 3 **2485** 20 7 .97 .69 .69 .43
4 4 **2485** 20 7 .97 .69 .69 .43

5 1 **2321** 20 8 .96 .65 .65 .36
5 2 **2151** 20 8 .96 .65 .65 .34
5 3 **2077** 20 8 .96 .65 .65 .34
5 4 **2048** 20 8 .96 .65 .65 .33
5 5 **2040** 20 8 .96 .65 .65 .33

8 8 **1192** 20 7 .95 .68 .68 .37
10 1 **947** 20 5 .97 .65 .65 .36
10 2 **896** 20 5 .96 .65 .65 .36
10 3 **895** 20 5 .96 .65 .65 .33
10 4 **885** 20 5 .96 .65 .65 .33
10 5 **877** 20 5 .96 .65 .65 .33
10 6 **861** 20 5 .96 .64 .64 .30
10 7 **849** 20 5 .96 .64 .64 .29
10 8 **848** 20 5 .96 .66 .66 .34
10 9 **848** 20 5 .96 .64 .64 .32
10 10 **848** 20 5 .96 .64 .64 .30

11 11 **702** 20 4 .95 .66 .66 .37
14 14 **417** 20 3 .95 .65 .65 .37
15 15 **165** 20 3 .94 .66 .66 .36
16 16 **94** 20 2 .95 .65 .65 .38
18 18 **94** 20 2 .95 .65 .65 .36

20 1 **111** 20 2 .95 .63 .63 .33
20 2 **99** 20 2 .96 .67 .67 .41
20 3 **81** 20 2 .96 .61 .61 .32
20 4 **78** 20 2 .96 .63 .63 .33
20 5 **78** 20 2 .96 .63 .63 .34
20 10 **78** 20 2 .95 .65 .65 .34
20 20 **78** 20 2 .95 .65 .65 .33


I.3 DPSGD VS PATE ON CHEXPERT

There are a few key differences between PATE and DPSGD in a direct comparison. DPSGD requires
centralization of data whereas PATE does not, while only additionally assuming some pool of public
unlabeled samples to be used for labeling by teachers and for the semi-supervised learning (e.g.,
with MixMatch). The centralization of data is a strong requirement for DPSGD which is often not
applicable to the tasks we consider (e.g., medical or financial data) because these institutions may be
blocked from centralizing data due to regulations.

The Adaptive DPSGD method from Zhang et al. (2021) does not publish the code, thus the following
is our best effort. The results presented below come from Figure 5 in Zhang et al. (2021). Similar
to their experimental setup, we use DenseNet-121 pre-trained on ImageNet and fine-tune only the
last fully connected layer while keeping all the other (convolutional) layers fixed. Across all of the
experiments, we use ε = 8 as the privacy budget. We use the whole CheXpert test set, and here report
results for δ = 10[−][4]. To evaluate the performance of the models, we use the CheXpert test set (from


-----

the valid.csv file in CheXpert-v1.0-small). As shown in the Table 9, in this setting, we outperform the
Adaptive DPSGD.

We use the following parameters to obtain our results for the Binary multi-label PATE:


sigma gnmax = 7.0
sigma threshold = 0
threshold = 0
method = multilabel
batch size = 20
learning rate = 0.001
epochs = 100
weight decay = 0
X-ray views = [’AP’, ’PA’]
Multilabel_prob_threshold = [0.53, 0.5, 0.18, 0.56, 0.56]

Table 9: DPSGD vs PATE on Chexpert for the first 5 labels.


METHOD AT CA CO ED EF AVERAGE

**NON-PRIVATE** _0.84_ _0.80_ _0.87_ _0.90_ _0.91_ _0.87_
**DPSGD** 0.56 0.53 0.66 0.56 0.62 0.58
**ADAPTIVE DPSGD** 0.75 0.73 **0.84** **0.79** 0.79 0.78
**BINARY PATE** **0.78** **0.75** **0.84** 0.76 **0.81** **0.79**

I.3.1 CDF OF GAPS


We show the Cumulative Distribution Function (CDF) for the gaps in Figure 9. We observe that the
ensemble of teachers is confident about the answers for most labels for the Binary PATE and there are
very small gaps for most queries when using Powerset PATE, which shows much lower confidence of
teachers in choosing the same (super) classes, which are created from aggregated label predictions
(the values of the labels are collected in a binary vector that constitutes a class).


|Col1|Col2|Col3|Pascal VOC|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||Mode Binary Powerset||||||
||||||||
||||||||
||||||||
||||||||
||||||||


Mode

Binary
Powerset

10 20 30 40 50

Sorted gaps


|Col1|Col2|Col3|CheXpert|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||Mode Binary Powerset||||||
||||||||
||||||||
||||||||
||||||||
||||||||


10 20 30 40 50

Sorted gaps


1.0

0.8


1.0

0.8


0.6

0.4


0.6

0.4


0.2

0.0


0.2

0.0


Figure 9: Binary vs Powerset PATE: CDF of gaps (differences between vote counts). We use 50
teacher models trained on the Pascal VOC and CheXpert datasets. These are raw gaps without adding
any noise to the vote histograms. Most of the gaps are relatively large (> 40) for the Binary PATE
per label, which shows that teachers are confident about the answers to queries. The average gap for
the Powerset method is relatively low (only 18 for Pascal VOC and 13 for CheXpert).

I.3.2 GAPS AND PERFORMANCE METRICS


With more labels for the Powerset method, we have more possible classes and the votes become more
spread out. This can be measured by the gap, which is the difference between the maximum number


-----

of votes per class and the runner-up (the number of votes for the next class with the highest number
of votes). In Figures 10 and 11, we plot the average gap (on the y-axis) for the test set across all
histograms for a given number of labels (presented on the x-axis). We compare the Powerset PATE
(denoted as Powerset) vs the Binary PATE per label (denoted as Binary). The average gap between
votes is comparable for different number of labels of Binary PATE since this method considers each
label separately and there are always only two classes. The average gap between votes decreases very
fast for Powerset because of the exponential growth of number of classes with more labels considered.
Additionally, the performance metrics: acc (accuracy), bac (balanced accuracy), aread under the
curve (auc), and mean average precision (map), are also higher for the Binary than Powerset method
in case of CheXpert dataset and comparable for Pascal VOC.

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|||||||||||
|Binary Powerset||||||||||


Pascal VOC

50

40

avg. gap 30

Binary

20 Powerset

0.98

0.97

acc

0.96

0.95

0.80

bac 0.75

0.70

0.65

0.80

auc 0.75

0.70

0.65

0.7

0.6

map

0.5

0.4

2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0

Number of labels


Figure 10: Binary vs Powerset PATE: gaps and metrics for Pascal VOC. We use 50 teacher
models. These are raw gaps without adding any noise to the vote histograms.


I.4 GENERAL TUNING OF HYPER-PARAMETERS

We set the hyper-parameters by considering the utility and privacy of our proposed method. Based on
Lemma 3.2, we minimize the privacy budget by maximizing the σ parameter of the Gaussian noise
and minimizing the τ parameters. In Figure 1, we tune the value of the σ parameter of the Gaussian
noise based on the utility metrics: accuracy (ACC), balanced accuracy (BAC), mean average precision
(MAP). We set σ as 9, 10, and 7 for Pascal VOC, MIMIC, and CheXpert datasets, respectively. This
allows us to answer many queries with high performance in terms of ACC, BAC, and MAP.


Similarly, we tune the τ values for ℓ2 norm in Figure 12 and for ℓ1 norm in Figure 13.

Regarding the values of parameters σG, T, and σT, we follow the original work on PATE (Papernot
et al., 2017) and perform a grid search over a range of plausible values for each of the hyperparameters.


-----

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
|Binary|||||||
|Powerset|||||||


CheXpert

30

avg. gap 2520

Binary

15 Powerset

0.80

0.78

acc

0.76

0.74

0.80

0.78

bac 0.76

0.74

0.72

0.80

0.78

auc 0.76

0.74

0.72

0.70

0.65

map 0.60

0.55

2 4 6 8 10

Number of labels

Figure 11: Binary vs Powerset PATE: gaps and metrics for CheXpert. We use 50 teacher models.
These are raw gaps without adding any noise to the vote histograms.

The detailed analysis of tuning the hyper-parameters are for: τ -s in Section I.5, σG in Section I.6,
PATE’s thresholding (T, and σT ) in Section I.7, and probability thresholds in Section I.10.

I.5 TUNING τ -CLIPPING

We determine the minimum value of τ for clipping in ℓ2 and ℓ1 norms for each dataset so that the
performance as measured by accuracy, BAC, AUC, and mAP, is preserved. We present the analysis
in Figures 12 and 13.

I.6 TUNING σGNMax

We determine how much of the privacy noise, expressed as the scale of Gaussian noise σ, can be
added so that the performance as measured by accuracy, BAC, AUC, and mAP, is preserved. The
experiment is run using the binary PATE per label and presented in Figure 14.

I.7 TUNING PATE

There are three parameters to tune: the differential privacy Gaussian noise standard deviation σG, the
count threshold T, and the thresholding Gaussian noise standard deviation σT . We first disable the
thresholding (σT and T ) and tune σG to achieve a high BAC of the noisy ensemble while maximizing
the number of answered queries. In Figure 19 we tune the σG parameter from PATE for different
configurations of retraining. The goal is to maintain a high accuracy while selecting σG with high
value so that as many queries as possible are answered. After tuning σG, we grid-search σT and T .
Thresholding is useful if we want to add a relatively small amount of Gaussian noise (σG) in the
noisy max. For example, for σG = 7 and 50 teacher models trained on CheXpert, the maximum


-----

Pascal VOC



0.8

0.6

0.4

0.2

0.8

0.7

0.6

0.5

0.4

0.3

0.2


|Col1|Col2|
|---|---|
|||
|||
||ACC BAC MAP|


ACC
BAC
MAP

10 15 20

(clipping of votes)

MIMIC-CXR


CheXpert

0.7

0.6

0.5

ACC

Value of the metric0.4 BAC

MAP

0 2 4 6 8 10

(clipping of votes)


|Col1|Col2|
|---|---|
|||
|||
|||
|||
||ACC BAC|


BAC
MAP

10


(clipping of votes)


PadChest

1.0

0.8

0.6

0.4

ACC

Value of the metric0.2 BAC

MAP

0.0

0.0 2.5 5.0 7.5 10.0 12.5 15.0

(clipping of votes)


Figure 12: Value of the metric vs τ **-clipping of votes in ℓ2 norm. For a given τ**, we plot accuracy
(ACC), balanced accuracy (BAC), and mean average precision (mAP).

number of labels answered without thresholding is 737 (67 queries) with a BAC above 0.69. With
thresholding, e.g., at T = 50 and σT = 30, we can answer an average of 756 labels, up to 834. A
well tuned threshold can also help reduce σG, which is the major influencing factor on the final BA
of the noisy ensemble. Note that tuning the threshold benefits from a confident ensemble: here, we
find the ensemble is often confident with a positive-negative vote difference of 40 of max 49. Also
note that the BAC can be higher on easier queries.


I.8 COMPARE DIFFERENT METHODS

In Figure 15, we directly compare the Binary PATE (denoted as PATE), τ -PATE, and clipping in ℓ2
and ℓ1 norms. The error regions are for the three querying parties.


I.9 VARYING PRIVACY BUDGET ϵ

In Table 12, we show the performance of the retrained models when using CaPC with various privacy
budgets ε. In Figure 16 we show the detailed per label change in BAC for the retraining with privacy
budget ε = 10. We observe an increase in average BA by around 0.03 after retraining when the
privacy budget is set to ε = 10. We present detailed analysis of the Binary PATE performance in
Table 10 on the Pascal VOC dataset when selecting the privacy budget ε in the range from 1 to 20.

We also compare the performance after re-training with and without the (confidence) thresholding
mechanism used in PATE (represented by σT and threshold T parameters). We observe that in case
of the medical datasets, there can be a slightly higher increase of the metrics (accuracy, BAC, AUC,
mAP), when we do not perform the thresholding. For example, for the CheXpert dataset, when
thresholding is not used, the improvement is higher by about one percentage point across all metrics
when compared to the option with thresholding. For PadChest, such increase is for AUC and mAP, for
BAC we see a drop by one percentage point, and there is no difference in terms of accuracy (remains
at the level of about 0.86). This requires further investigation. Intuitively, we observe that the metrics


-----

Pascal VOC

0.8

0.6

0.4

ACC

Value of the metric0.2 BAC

MAP

0 5 10 15 20

(clipping of votes)


CheXpert

0.7

0.6

0.5

ACC

Value of the metric0.4 BAC

MAP

0 2 4 6 8 10

(clipping of votes)


MIMIC-CXR

0.8

0.7

0.6

0.5

0.4 ACC

Value of the metric0.3 BAC

MAP

0.2

0 2 4 6 8 10

(clipping of votes)


PadChest

1.0

0.8

0.6

0.4

ACC

Value of the metric0.2 BAC

MAP

0.0

0.0 2.5 5.0 7.5 10.0 12.5 15.0

(clipping of votes)


Figure 13: Value of the metric vs τ **-clipping of votes in ℓ1 norm. For a given τ**, we plot accuracy
(ACC), balanced accuracy (BAC), and mean average precision (mAP).






Pascal VOC CheXPert MIMIC-CXR PadChest

1.0

0.8

0.6

0.4

Value of metric

ACC

0.2 BAC

MAP

0.0

0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25

(Gaussian noise scale) (Gaussian noise scale) (Gaussian noise scale) (Gaussian noise scale)


Figure 14: Value of the metric versus noise standard deviation σ. For a given σ, we plot accuracy
(ACC), balanced accuracy (BAC), and mean average precision (mAP).

can increase substantially with thresholding for the labels (pathologies) that are easy to classify.
However, the most difficult to predict pathologies are left without answer and no improvement is
made on them. On the other hand, without thresholding, we have to answer all labels and correct
predictions on the hardest labels can produce substantial improvement in the classification of these
hard labels.


I.10 TUNING PROBABILITY THRESHOLD

We tune the global (applied to all labels) probability threshold γ that determines if a given probability
denotes positive (P > γ), or a negative (P ≤ _γ) vote. The best global γ for the 50 models trained on_
the CheXpert dataset is 0.32 with the balanced accuracy of 0.702 and the µ value of 0.45, where µ is
the average probability of predictions after applying the element-wise sigmoid function to the logits.


-----

800

600


400

200

|PATE t-PATE L2|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|L1|||||||
||||||||
||||||||
||||||||
||||||||


PATE
t-PATE
L2
L1


0 5 10 15 20 25

(Gaussian noise scale)

Figure 15: Compare methods: number of answered queries vs σ - using the Pascal VOC dataset,
with τ1 = 3.4 (set for ℓ1−norm clipping, and τ2 = 1.8 set for τ -PATE and ℓ2−norm clipping. The
red vertical line denotes the selected value of σ.


1.0

0.9


0.8

0.7


0.6

0.5



els with the lowest BAC and retrain them with CaPC. The average metrics BAC, AUC, and mAP

|Before CaPC Mean Before CaPC Ensemble Mean Ensemble|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||Before CaPC Mean Before CaPC Ensemble Mean Ensemble||||||||||||||
|After CaPC Mean After CaPC||After CaPC Mean After CaPC||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|1 2 in thr ds 21,|g w esh p 0.|3 4 ith old er 23,|5 6 pri p lab 0.46|7 va er el , 0|8 cy la u .7,|9 bu be si 0.|1 L dg l ng 2,|0 1 ab et fo 0.3|1 1 el ε r the 2.|2 13 = 1 Ch v N|14 1 0 fo eX ali ext|5 1 r th per dati ,|6 17 e P t. on we|18 1 asca se sel|9 2 l V We t ect|


_._


-----

Table 10: Pascal VOC with 20 labels: Performance of Binary PATE for different values of ε
w.r.t. number of answered queries, ACC, BAC, AUC, and mAP as measured on the test set with the
specified σGNMax. PB (ε) is the privacy budget. We use 50 teacher models. When we limit number of
labels per dataset, we select the first k labels. We set σGNMax = 7

QUERIES
PB (ε) ANSWERED ACC BAC AUC MAP

1 **0** -  -  -  - 
2 **6** .86 .62 .62 .44
3 **13** .93 .67 .67 .53
4 **22** .93 .64 .64 .44
5 **31** .95 .63 .63 .39
6 **40** .95 .67 .67 .45
7 **64** .95 .64 .64 .35
8 **81** .95 .66 .66 .40
9 **101** .95 .60 .60 .28
10 **113** .96 .63 .63 .30
11 **135** .96 .64 .64 .33
12 **165** .96 .65 .65 .35
13 **199** .96 .63 .63 .32
14 **217** .96 .64 .64 .35
15 **239** .96 .63 .63 .32
16 **272** .96 .63 .63 .31
17 **306** .96 .63 .63 .30
18 **332** .96 .63 .63 .31
19 **362** .96 .63 .63 .30
20 **403** .96 .63 .63 .30

Table 11: Performance of multi-label CaPC w.r.t. ACC, BAC, AUC, and mAP, on Pascal VOC
(PA), CheXpert (CX), MIMIC (MC), and PadChest (PC). (-) denotes N/A. PB (ε) is the privacy
budget.

# OF
DATASET MODELS STATE PB (ε) ACC BAC AUC MAP

1 INITIAL -  .97 .85 .97 .85
50 BEFORE CAPC -  .93 .02 .59 .01 .88 .01 .54 .01

PA _±_ _±_ _±_ _±_

50 AFTER CAPC 10 **.94±.01** .62±.01 .88±.01 .54±.01
50 AFTER CAPC 20 **.94±.01** **.64±.01** **.89±.01** **.55±.01**

1 INITIAL -  .79 .78 .86 .72
CX 50 BEFORE CAPC -  .77±.06 .66±.02 .75±.02 .58±.02
50 AFTER CAPC 20 .76±.07 **.69±.01** **.77±.01** **.59±.01**

1 INITIAL -  .90 .74 .84 .51
MC 50 BEFORE CAPC -  .84±.07 .63±.03 .78±.03 .43±.02
50 AFTER CAPC 20 **.85±.05** **.64±.01** **.79±.01** **.45±.03**

1 INITIAL -  .86 .79 .90 .37
PC 10 BEFORE CAPC -  .90±.01 .64±.01 .79±.01 .16±.01
10 AFTER CAPC 20 .88±.01 .64±.01 .75±.01 .14±.01


before retraining are 0.63, 0.70, 0.53, and after retraining, we observe a significant improvement to
0.68, 0.75, 0.58, respectively. Thus, the value for each metric increases by around 0.05. We find the
biggest performance improvement for these weakest models after learning from better teachers and
retraining. We present detailed results per label and for BA as well as AUC metrics in Figure 18.

I.11 CROSS-DOMAIN RETRAINING

In real-world healthcare scenarios, there are often rare diseases that may be difficult to accurately
model with machine learning. Even coalitions of hospitals sharing similar data distributions of the
same domain may not see benefits, due to poor aggregate performance. However, cross-domain
collaboration through multi-label CaPC can help improve performance in these cases. The hospital


-----

Table 12: Performance of multi-label CaPC with τ **-PATE w.r.t. ACC, BAC, AUC, and mAP, on**
Pascal VOC (PA), CheXpert (CX), MIMIC (MC), and PadChest (PC). (-) denotes N/A. PB (ε) is
the privacy budget. T (Y/N) in the table refers to the PATE thresholding i.e. corresponding to the
parameters T and σT being used (Y) or not used (N). Note that the probability thresholding per layer
was used in these experiments (as described in Section I.10).

# OF
DATASET MODELS STATE T(Y/N) PB (ε) ACC BAC AUC MAP

1 INITIAL -  -  .88 .97 .91
PA 50 BEFORE CAPC -  -  .93±.01 .59±.01 .89±.01 .54±.02
50 AFTER CAPC N 20 **.94±.01 .60±.01 .89±.01 .54±.02**

1 INITIAL -  -  .79 .78 .86 .72
50 BEFORE CAPC -  -  .75 .02 .69 .01 .77 .01 .59 .01

CX _±_ _±_ _±_ _±_

50 AFTER CAPC Y 20 .73±.01 .69±.01 .76±.01 .59±.01
50 AFTER CAPC N 20 .74±.01 .70±.01 .77±.01 .59±.01

1 INITIAL -  -  .90 .74 .84 .51
MC 50 BEFORE CAPC -  -  .84±.07 .63±.03 .78±.03 .43±.02
50 AFTER CAPC Y 20 .84±.02 .64±.04 .77±.02 .44±.01

1 INITIAL -  -  .86 .79 .90 .37
10 BEFORE CAPC -  -  .82 .01 .64 .01 .79 .01 .17 .01

PC _±_ _±_ _±_ _±_

10 AFTER CAPC Y 20 **.86±.04 .61±.01 .71±.03 .14±.02**
10 AFTER CAPC N 20 **.86±.02 .60±.01 .72±.02 .15±.03**


annotation discrepancies may pose barriers: here, we simulate this by the different X-ray image labels
between PadChest and CheXpert (see Supplement Section G).

To overcome this, we take the union of labels between the medical datasets and follow the experimental
setup of Cohen et al. (2020). We observe poor performing models on the PadChest dataset, with a
low average performance of BAC = 0.57. Because of this, the benefits of multi-label CaPC within
this coalition of hospitals are limited, since the ensemble is only marginally better. However, if this
group of hospitals collaborated with another from a different but related domain, here represented by
CheXpert, they may be able to see additional benefits. The models trained on this dataset achieve
a higher BA (particularly on the first 5 shared pathologies as presented in Figure 17). Thus, the
ensemble of all models engage in multi-label CaPC and the models trained on CheXpert act as
answering parties to provide labels for querying parties from PadChest. Using a σG = 9, T = 50, and
_σT = 30, we observe a higher BAC on all of those 5 pathologies, where we do not see a significant_
decrease on the other pathologies. Thus, multi-label CaPC provided benefits for the PadChest models
in this cross-domain scenario.


-----

1.0

0.9


0.8

0.7


0.6

0.5


|Before CaPC Mean Before CaPC PC Ensemble Mean PC Ensemble|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||CX Ensemble Mean CX Ensemble After CaPC CX Mean After CaPC on CX|||||||||||
|||||||||||||||||
|||||||||||||||||
|||||||||||||||||
|AT 7: Cro their pe e of 50 ulti-lab|CA CO ss-domai rformance CheXpert el PATE u|ED n ret on P mod sing|rai ad els the|EF ning Ches (CX Che|EM with t test se Ensem Xpert e|FI L CaPC t again ble), nsemb|F a . s a l|R bel We t the nd fi e (af|HE tra ens nall ter|in em y r Ca|IN 10 ble etra PC|MA mod of th in th on C|NO els on Pa ese mode e 10 Pad X).|PT PN dChest (P ls (PC Ens Chest mo|PX C) a embl dels v|

|Before CaPC Mean Before CaP Ensemble Mean Ensemble After CaPC Mean After CaPC|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
|AT 18 s: be tr|CA CO : Using Balanc lled by aining b|ED EF L CaPC to ed Accura the ensem y around|EN F abel imp cy (B ble o 0.05|R ro A f a on|LL L ve th C) an ll the aver|O PN P e weak d AUC. other m age.|

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
|AT Da a gi ed o|CA she ve n C|CO d li n m he|ED nes r odel Xper|EF EN FR Label epresent m using add t. All met|LL LO ean value itional Ch rics are im|PN P s of t eXpe prov|


0.90

1.0 Before CaPC Mean Before CaPC

0.85

Ensemble Mean Ensemble

0.9 After CaPC Mean After CaPC 0.80

0.75

0.8

0.70

AUC

0.7 0.65

0.60

Balanced accuracy

0.6

0.55

0.5 0.50

0.45

AT CA CO ED EF EN FR LL LO PN PX AT CA CO ED EF EN FR LL LO PN PX

Label Label

Figure 18: Using CaPC to improve the weakest models. Dashed lines represent mean values of the
_metrics: Balanced Accuracy (BAC) and AUC. We retrain a given model using additional CheXpert_
data labelled by the ensemble of all the other models trained on CheXpert. All metrics are improved
after retraining by around 0.05 on average.


-----

0.8

0.7

0.6


0.5

0.4


0.70

0.68

0.66

0.64

0.62

0.60

Balanced accuracy

0.58

0.56

0.54

0 20 40 60 80

Sigma for Gaussian noise

0.70

0.68

0.66

0.64

0.62

0.60

Balanced accuracy

0.58

0.56

0.54

0 20 40 60 80

Sigma for Gaussian noise

Figure 19: The analysis of the balanced accuracy (y-axis) of votes from the ensemble as we increase
the (sigma of the Gaussian) noise (x-axis) from the binary multi-label PATE. We search for variance
of the noise σG that can preserve high BAC for the ensemble. Left: 50 CheXpert models with train
with train on CheXpert and test on PadChest test set,and test on CheXpert, σG ≤ 9.0 preserves more than 0.69 of BAC. σG 9.0 preserves more than 0.75 of BAC. Middle: 50 CheXpert models
**RightBAC.** : 10 PadChest models with train and test on PadChest, ≤ σG ≤ 5.0 preserves more than 0.75 of


-----

J THE CAPC PROTOCOL

The CaPC protocol is as follows:

1. Private inference is run for each answering party.

(a) Compute logits. A querying party (or, student) sends an encrypted query (unlabeled x)
to each answering party (or, teacher). The teachers run the private inference and upon
completion return encrypted logits: Enc(r).

(b) Secret sharing between the answering and querying parties. Each answering party generates
a random vector ˆr and subtracts it from the vector of logits r. Then, each answering party
sends the encrypted result: Enc(r − _rˆ) to the querying party who decrypts the message to_
obtain (r − _rˆ)._

(c) One-hot encoding of the logits. The answering parties provide ˆr and the querying party
provides (r _−_ _rˆ) to run Yao’s garbled circuit protocol (Yao, 1986b). They compute the argmax_
of the logits and transform them into one-hot representations. Each one-hot vector is split
into shares s and ˆs. The s share is returned to the querying party and the ˆs share is sent to
the Privacy Guardian (PG). The PG is a common facilitator in these setups, i.e., a trusted
third-party, and is honest-but-curious. The sum of shares s and ˆs is the one-hot encoding of
the predicted label.

2. DP mechanism. This step follows PATE to provide privacy for each teacher’s data. In PATE,
Gaussian noise is added to the argmax. In CaPC, Gaussian noise is added to the sum of ˆs shares
by the PG, after being collected from each of the answering parties.

3. Noisy argmax to compute the final label. The PG and the querying party run Yao’s garbled circuit
to sum the noisy ˆs values with the s values. The result is a noisy histogram representing the
number of votes for each class. The argmax of this determines which class received the maximum
number of votes. This class is returned to the querying party as the final predicted label.

In CaPC learning Choquette-Choo et al. (2021), the main tool for confidentiality is secure multi-party
computation (MPC) (Yao, 1986a) and homomorphic encryption. MPC is used when distrusting
parties want to jointly evaluate a function on their input without revealing anything beyond the output.
The MPC protocols can be divided into two main classes: (1) generic protocols, which compute
any function with the Malkhi et al. (2004) security goal, and (2) specialized protocols, which are
used to compute selected functions, for instance, the private set intersection (Pinkas et al., 2020)
or secure machine learning (Mohassel & Zhang, 2017). Even though the specialized protocols are
less general, they are more efficient in terms of the execution time. Protocols in both categories use
similar cryptographic building blocks, including (fully) homomorphic encryption (Gentry, 2009),
secret sharing (Shamir, 1979), oblivious transfer (Rabin, 2005), garbled circuits (Yao, 1986a). Private
inference can be computed in many ways and is a component of the CaPC protocol that directly
scales with improvements in these techniques: CaPC used the HE-transformer library that uses both
MPC as well as the homomorphic encryption but the private inference library can be immediately
swapped with others.


-----

**Algorithm 1 CaPC protocol**
**Input: Querying Party QP. x - query (unlabeled data item) from QP. Answering Parties AP1,...,APn**
with ML models fi(x) = y, where i 1, ..., n, and y is the vector of logits. Privacy Guardian PG.
_∈{_ _}_
**Output: label ℓ** for query x (QP receives ℓ for its x).

1: QP encrypts x to Enc(x).
2: QP sends Enc(x) to AP1,...,APn.
3: for all APi do
4: _Enc(r)i_ _fi(Enc(x))_

5: Generate random ← ˆri

6: _Enc(ri_ _rˆi)_ _Enc(ri)_ _rˆi_ _▷_ Enabled by Homomorphic Encryption

7: Send Enc −(ri _←rˆi) to QP_ _−_
_−_

8: for all APi and QP do _▷_ 2PC (Secure 2-Party Computation) between each AP and the QP

9: QP decrypts ri _rˆi_ _Decrypt(Enc(ri_ _rˆi))_

11:10: APQP sendsi sends r ˆri −i 2PC −rˆi to 2PC ← _−_

12: 2PC: ri (ri _rˆi) + (ˆri)_

13: 2PC: one ← _hot −i_ _argmax(ri)_

14: 2PC: ˆsi + − s ← _one ← −_ _hoti_ _▷_ Secret Sharing

15: APi receives ˆsi from 2PC

16: QP receives si from 2PC

17: for all APi do
18: Send ˆsi to PG.

19: QP: S _i=1_ _[s][i]_
_←_ [P][n]

20: QP sends S to 2PC
21: PG: _S[ˆ] ←_ [P]i[n]=1 _s[ˆ]i_

22: PG: Generate Gaussian noise ˆn _N_ (0, σG)
_←_

23: PG sends _S[ˆ] and ˆn to 2PC_

24: 2PC: _h[ˆ] ←_ _S + S[ˆ] + ˆn_ _▷_ Noisy histogram _h[ˆ]_

25: 2PC: ℓ _←_ _argmax(h[ˆ])_

26: 2PC sends ℓ to QP


-----

K ALGORITHMS FOR THE MULTI-LABEL CLASSIFICATION

**Algorithm 2 Multi-label classification with τ** -clipping in ℓ2-norm and with the Confident GNMax.
**Input: Data point x, clipping threshold τ2, Gaussian noise scale σG, Gaussian noise scale for**
Confident GNMax σT, n teachers, each with model fj(x) ∈{0, 1}[k], where j ∈ [n]
**Output: Aggregated vector V with Vj = 1 if returned label/feature present, otherwise Vj = 0.**

1: for all teachers j [n] do
_∈_ _τ2_
2: _vj ←_ min(1, _∥fj_ (x)∥2 [)][f][j][(][x][)] _▷τ_ -clipping in ℓ1 norm

3: V [1] = _j=1_ _[v][j]_ _▷_ Number of positive votes per label

4: V [0] = n − _V_ [1]

5: for all labels[P][n] _i_ [k] do
_∈_
6:7: **if maxVi ={V⊥i[0][, V]i[ 1][}][ +][ N]** [(0][, σ][T][ )][ < T][ then] _▷_ Confident-GNMax

8: **else**

11:10:9: _VifVii V[0][1]_ _i[←][←][1]_ _[> V][V][V][ 0][ 1]ii_ _i[ 0][+][+][then][ N][ N]_ [(0][(0][, σ][, σ][G][G][)][)] _▷_ Add Gaussian noise for privacy protection▷ Decide on the output vote

12: _Vi = 1_

13: **else**

14: _Vi = 0_

**Algorithm 3 Multi-label classification with τ** -clipping in ℓ1-norm from Private kNN by Zhu et al.
(2020).
**Input: Data point x, clipping threshold τ1, Gaussian noise scale σG, n teachers, each with model**
_fj(x)_ 0, 1, where j [n].
_∈{_ _}[k]_ _∈_
**Output: Aggregated vector V ∈{0, 1}[k]** with Vi = 1 if returned label present, otherwise Vi = 0,
where i ∈ [k].

1: for all teachers j [n] do
_∈_ _τ1_
2: _vj ←_ min(1, _∥fj_ (x)∥1 [)][f][j][(][x][)] _▷τ_ -clipping in ℓ1 norm

3: V [1] = _j=1_ _[v][j]_ _▷_ Number of positive votes per label

4: V [0] = n − _V_ [1]

5: V [0] [P]V [0][n] + (0, σG) _▷_ Add Gaussian noise for privacy protection
_←_ _N_

6: V [1] _V_ [1] + (0, σG)
_←_ _N_
7: for all labels i ∈ [k] do
8: **if Vi[1]** _[> V]i[ 0]_ **[then]** _▷_ Decide on the output vote

9: _Vi = 1_

10: **else**

11: _Vi = 0_


-----

L DATA-DEPENDENT PRIVACY ANALYSIS

Our binary voting mechanism does leverage both the smooth sensitivity and propose-test-release
methods. For example, the Confident GNMax (proposed by Papernot et al. (2018)) is a form of the
propose-test-release method described in Section 3.2 in (Vadhan, 2017).

First explored by Cormode et al. (2012), differential privacy guarantees can be data-dependent.
These guarantees can lead to better utility with a long history, with several works using them (Cormode
et al., 2012; Papernot et al., 2017; 2018; Chowdhury et al., 2020). One main caveat is that the released
epsilon score must now also be noised because it is itself a function of the data. Papernot et al.
(2018) provide a way to do this via the smooth sensitivity (Section B in Appendix). Indeed our
data-dependent differential privacy guarantees are formally proven and are based on the following
intuition. Take the exponential mechanism which gives a uniform privacy guarantee. For instance,
when the top score and the second score are very close, applying the exponential mechanism to
this data there is a nearly uniform chance of picking either coordinate. However, for some inputs,
the utility can be very strong—when the top score is much higher than the second score, then this
mechanism is exponentially more likely to pick the top score than the second. This does not require
local sensitivity of stability based methods, but is rather derived from the likelihood of picking either
coordinate for this mechanism given the gap in the scores.


-----

