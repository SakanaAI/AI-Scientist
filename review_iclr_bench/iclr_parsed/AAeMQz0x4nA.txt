# LEARNING EXPLICIT CREDIT ASSIGNMENT FOR MULTI-AGENT JOINT Q-LEARNING

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Multi-agent joint Q-learning based on Centralized Training with Decentralized
Execution (CTDE) has become an effective technique for multi-agent cooperation. During centralized training, these methods are essentially addressing the
multi-agent credit assignment problem. However, most of the existing methods
_implicitly learn the credit assignment just by ensuring that the joint Q-value satis-_
fies the Bellman optimality equation. In contrast, we formulate an explicit credit
assignment problem where each agent gives its suggestion about how to weight
individual Q-values to explicitly maximize the joint Q-value, besides guaranteeing the Bellman optimality of the joint Q-value. In this way, we can conduct credit
assignment among multiple agents and along the time horizon. Theoretically, we
give a gradient ascent solution for this problem. Empirically, we instantiate the
core idea with deep neural networks and propose Explicit Credit Assignment joint
Q-learning (ECAQ) to facilitate multi-agent cooperation in complex problems.
Extensive experiments justify that ECAQ achieves interpretable credit assignment
and superior performance compared to several advanced baselines.

1 INTRODUCTION

Many real-world problems such as robot swarm control can be naturally modeled as cooperative
multi-agent systems where each agent can only observe parts of the systemsâ€™ state and all agents
share the same global reward. Recently, the IGM-based multi-agent joint Q-learning has become
an effective technique to solve such problems, where IGM (i.e., Individual-Global-Max) means the
consistency between individual and joint greedy action selections.

During training, most IGM-based methods (Rashid et al., 2018; Sunehag et al., 2018; Wang et al.,
2020b; Yang et al., 2020a;b) are essentially addressing the multi-agent credit assignment problem
as pointed out by Yang et al. (2020a); Zhou et al. (2020). Specifically, they try to learn an assignment function f parameterized by w to align the joint Q-value Qtotal shared by all agents with
the individual Q-values Qi belonging to agent i, i.e., Qtotal = f (Q1, ..., QN ; w). Typically, these
methods mainly apply temporal difference learning (TD-learning) to extract the parameter w, which
represents a specific credit assignment. However, TD-learning does not explicitly optimize the credit
assignment among multiple agents at a given timestep [1], and the existing methods are often called
the implicit multi-agent credit assignment as mentioned by Zhou et al. (2020); Wang et al. (2020a);
Naderializadeh et al. (2020); Li et al. (2021b;a). Besides, the extracted parameter w is usually lack
of interpretability in terms of multi-agent credit assignment. Finally, the performance may be poor
due to unsuitable credit assignment (Zhou et al., 2020; Yang et al., 2020a).

In contrast, conducting explicit multi-agent credit assignment could distribute the global reward to
each agent based on its contribution to the agent group, thus it may substantially facilitate policy
optimization and promote learning performance as pointed out by many previous methods (Proper
& Tumer, 2012; Tumer & Agogino, 2007; Wang et al., 2020c). More importantly, it can figure
out which agent is critical according to the assigned credits, so as to achieve better interpretability,
which makes up for the defect that deep neural networks are unexplainable. Inspired by these, we
investigate explicit multi-agent credit assignment for the IGM-based joint Q-learning methods.

1In general, TD-learning is considered to explicitly assign the credit along the time horizon, namely, distributing the future credit (i.e., the delayed reward) to previous timesteps.


-----

Our contributions are three-fold. First, we propose a criterion to measure an assignment function so
that we can find better assignments by explicitly optimizing this criterion. Specifically, a good assignment function should be helpful for agent cooperation to maximize the reward, so we define the
criterion as the maximization of Qtotal (i.e., the expected long-term cumulative reward) [2]. Second,
we introduce an exact solution to optimize the defined criterion. Theoretically, our solution can find
the optimal Qtotal with mild conditions. Empirically, we approximate the core idea with deep neural
networks and propose Explicit Credit Assignment joint Q-learning (ECAQ) to facilitate multi-agent
cooperation in complex scenarios. Third, we evaluate ECAQ on several challenging tasks. The
results demonstrate that ECAQ achieves interpretable credit assignment and superior performance
compared to advanced baselines.

2 BACKGROUND

**DEC-POMDP.** We consider a fully cooperative multi-agent setting that can be formulated as
DEC-POMDP (Bernstein et al., 2002). It is formally defined as a tuple âŸ¨N, S, A, T, R, O, Z, Î³âŸ©,
wherejoint action N is the number of agents;, and Ai is the set of local action S is the set of state; that agent i A can take; = A1 Ã— T ( ...s[â€²]| Ã—s, a A) :N represents the set of S Ã— A Ã— S â†’ [0, 1]
represents the state transition function; R : S Ã— A â†’ R is the reward function; O = [O1, ..., ON ] is
the set of joint observation controlled by the observation function Z : S Ã— A â†’ **_O; and Î³ âˆˆ_** [0, 1]
is the discount factor.

In a given state s, each agent i generates an action ai based on its observation oi. The joint action
**_a = âŸ¨ai, aâˆ’iâŸ©_** results in a new state s[â€²] (i.e., the state of the next timestep) and a global reward r,
where a _i is the joint action of teammates of agent i. The agent aims at learning a policy Ï€i(ai_ _oi)_
_âˆ’_ _|_
that can maximize Eoiâˆ¼Zi,aiâˆ¼Ï€i [G] where G is the discount return defined as G = _t=0_ _[Î³][t][r][t][ and]_
_H is the time horizon. In partially observable scenarios, the action is typically generated based on_
the entire observation-action history Ï„i, i.e., Ï€i(ai _Ï„i), rather than the current observation oi._
_|_ [P][H]

**Multi-agent Joint Q-learning.** The multi-agent joint Q-learning is a notable approach to solve
DEC-POMDP problems. The idea is to coordinate all agents by the joint Q-value Qjoint(Ï„ _, a)_
where Ï„ = âŸ¨Ï„i, Ï„âˆ’iâŸ© is the joint history of all agents, then the best joint action can be derived
by a[âˆ—] = argmaxa Qjoint(Ï„ _, a). In practice, the true but unknown joint Q-value Qjoint(Ï„_ _, a)_
_is approximated by Qtotal(Ï„_ _, a), which in turn is implemented using a deep neural network_
_Qtotal(Ï„_ _, a; w) parameterized by w. Typically, Qtotal(Ï„_ _, a; w) is optimized by minimizing the_
following TD-loss with temporal difference learning (TD-learning):
_L(w)_ = E(Ï„ _,a,r,Ï„ â€²)âˆ¼D[(r + Î³ maxa[â€²][ Q][total][(][Ï„][ â€²][,][ a][â€²][;][ w][âˆ’][)][ âˆ’]_ _[Q][total][(][Ï„]_ _[,][ a][;][ w][))][2][]]_ (1)

where D is the replay buffer containing recent experience tuples (Ï„ _, a, r, Ï„_ ), and Qtotal(Ï„ _, a; w[âˆ’])_

_[â€²]_
is the target network whose parameter w[âˆ’] is periodically updated by copying w.

However, vanilla joint Q-learning has some disadvantages. First, the scalability is poor for largescale agents because it needs to search the whole joint action space to find the optimal one.
Second, the agent cannot interact with the environment based on its own information Ï„i, since
_i_
the optimal action also relies on the teammatesâ€™ information Ï„âˆ’i, namely, a[âˆ—]i _â†readoutâˆ’âˆ’âˆ’âˆ’_ **_a[âˆ—]_** =

argmaxa Qtotal[âˆ—] [(][âŸ¨][Ï„][i][,][ Ï„][âˆ’][i][âŸ©][,][ a][;][ w][)][.]

To remedy these disadvantages, the Centralized Training with Decentralized Execution (CTDE)
paradigm is applied (Lowe et al., 2017; Foerster et al., 2018; Sunehag et al., 2018; Rashid et al.,
2018). During centralized training, agents are granted access to other agentsâ€™ information Ï„ _i (and_
_âˆ’_
possibly the global state s if available) to estimate the joint Q-value Qtotal(Ï„ _, a; w) in a stationary_
way, while during decentralized execution, the agent makes decision independently based on individual Q-value ai = argmaxai Qi(Ï„i, ai; Î¸i) where Î¸i is the policy parameter of agent i. In order
to achieve effective value-based CTDE, it is critical to ensure the consistency between individual
and joint greedy action selections, which induces the Individual-Global-Max (IGM) principle (Son
et al., 2019):
_âŸ¨_ argmaxa1 Q1(Ï„1, a1), ..., argmaxaN QN (Ï„N _, aN_ ) âŸ© = argmaxa Qjoint(Ï„ _, a)_ (2)

2We notice that several previous methods (Zhou et al., 2020; Wang et al., 2020e) also take the maximization
of Qtotal as a target or measurement for good credit assignment. The differences are discussed in Section 3.


-----

The IGM principle is very effective to train large-scale agents because it has a linear (rather than
exponential) search space for the optimal joint action (compared to the vanilla joint Q-learning).
Recently, QTRAN (Son et al., 2019) proposes a sufficient and necessary condition for IGM:

Î£[N]i=1[Î±][i][Q][i][(][Ï„][i][, a][i][)][ âˆ’] _[Q][joint][(][Ï„]_ _[,][ a][) +][ V][joint][(][Ï„]_ [) =] 0 **_a = [argmaxai Qi(Ï„i, ai)][N]i=1_** (3)
â‰¥ 0 otherwise

where Î±i > 0, and Vjoint(Ï„ ) = maxa Qjoint(Ï„ _, a)âˆ’Î£[N]i=1[Î±][i][ max][a]i_ _[Q][i][(][Ï„][i][, a][i][)][ could be interpreted]_
as a baseline function to correct for the discrepancy between the optimal joint Q-value and the
weighted summation of the optimal individual Q-values. Note that Equation (2) and (3) are defined
under a specific state (equally, under a specific joint history Ï„ ) because it is hard to satisfy IGM for
all states. Nevertheless, if we could always find a corresponding Î±i satisfying Equation (3) for each
possible Ï„, we say the task itself is factorizable (Son et al., 2019).

3 RELATED WORK

**IGM-based Credit Assignment.** During centralized training, the IGM-based joint Q-learning
methods are essentially addressing the multi-agent credit assignment problem (Yang et al., 2020a;
Zhou et al., 2020), namely, aligning the joint Q-value Qtotal with individual Q-values Qi:
_Qtotal(Ï„_ _, a; w) = f_ ([Qi(Ï„i, ai; Î¸i)][N]i=1[;][ w][)][. The major difference lies in the detailed implemen-]
tation of the credit assignment function f . For example, VDN (Sunehag et al., 2018) proposes a
simple additivity assignment function Qtotal(Ï„ _, a) = Î£[N]i=1[Q][i][(][Ï„][i][, a][i][;][ Î¸][i][)][, and it works pretty well.]_
QMIX (Rashid et al., 2018) applies a nonlinear assignment function to increase representation expressiveness, but with the constraint of monotonic improvement _i,_ _[âˆ‚Q]âˆ‚Q[total]i(Ï„i[(],a[Ï„]i[,][a];Î¸[;]i[w])_ [)]

IGM. Recent methods such as WQMIX (Rashid et al., 2020), QTRAN (Son et al., 2019), QPLEX âˆ€ _[â‰¥]_ [0][ to satisfy the]
(Wang et al., 2020b), Qatten (Yang et al., 2020b) and QPD (Yang et al., 2020a) propose more sophisticated assignment functions to enhance the representation expressiveness, e.g., the multi-head
attention function (Yang et al., 2020b) and the duplex dueling function (Wang et al., 2020b).

**Policy-based Credit Assignment.** A well-known method is COMA (Foerster et al., 2018), which
conducts credit assignment by counterfactual baseline. We argue that maximizing Qtotal is one of
the effective ways to learn a good credit assignment function. For example, LICA (Zhou et al., 2020)
and DOP (Wang et al., 2020e) take this as the target or measurement of good credit assignment. The
key differences between our ECAQ and these methods are two-fold: first, ECAQ is a Q-learning
method, while LICA and DOP are actor-critic methods; second, ECAQ explicitly optimizes a credit
assignment criterion, while LICA and DOP learn the decomposed credit assignment implicitly.

**Other Methods.** There are other types of multi-agent credit assignment methods (Proper &
Tumer, 2012; Tumer & Agogino, 2007; Wang et al., 2020c; Zhang et al., 2020; Zhou et al., 2021)
and multi-agent cooperation methods (Nguyen et al., 2018; Zhang et al., 2020; Mahajan et al., 2019;
Wang et al., 2020d). However, they are beyond the scope of this paper. Due to space limitation, we
provide a brief review for these methods in the Appendix.

4 OUR METHOD

Explicit credit assignment is important for achieving better performance and interpretability, but
most IGM-based methods only implicitly learn the credit assignment function, resulting in noninterpretable assignment and possibly poor performance (Zhou et al., 2020; Yang et al., 2020a). In
this section, we investigate explicit credit assignment for the IGM-based joint Q-learning. Specifically, Section 4.1 defines the considered problem; Section 4.2 proposes an exact solution to assign
credit among multiple agents at a given timestep/state; Section 4.3 approximates the exact solution
to handle complex problems; Section 4.4 combines TD-learning with our solution, so the integrated
approach can conduct credit assignment among multiple agents and between different timesteps.

4.1 PROBLEM FORMULATION

In this paper, the true but unknown joint Q-value Qjoint(Ï„ _, a) is approximated by Qtotal(Ï„_ _, a),_
which in turn is implemented using a deep neural network Qtotal(Ï„ _, a; w) parameterized by w. Us-_


-----

ing these notations, the considered multi-agent credit assignment function is formulated as follows:

_Qtotal(Ï„_ _, a) = Î£[N]i=1[Î±][i][(][Ï„][i][)][Q][i][(][Ï„][i][, a][i][;][ Î¸][i][) +][ b][(][Ï„]_ [)][ â‰¥] _[Q][joint][(][Ï„]_ _[,][ a][)]_ (4)

where Î±i(Ï„i) > 0 is the weight of Qi, and b(Ï„ ) := Vjoint(Ï„ ). We choose this formulation due to two
important reasons. First, it is a sufficient and necessary condition for IGM under some conditions
as demonstrated by Equation (3), so it has good fitting ability theoretically (Son et al., 2019; Wang
et al., 2020b). Second, it allows us to intuitively interpret the weight Î±i(Ï„i) as the importance of Qi,
which can be analyzed to understand the concrete credit assignment (please see the experiments).

There are infinite possible assignment solutions satisfying Equation (4). In order to find the best one,
we can define a criterion to justify how good an assignment function is, then explicitly optimize such
a criterion. We call this kind of methods the explicit multi-agent credit assignment (MACA).

Recall that the ultimate goal of MACA is to boost learning performance, which is measured by the
maximization of Qtotal (i.e., the expected long-term system-level rewards). Thus, we propose an
_explicit multi-agent credit assignment criterion as follows:_

_{Î±i[âˆ—][(][Ï„][i][)][, Î¸]i[âˆ—][}]_ = argmaxÎ±i(Ï„i),Î¸i _Qtotal(Ï„_ _, a)_ (5)
_{_ _}_

s.t. _Qtotal(Ï„_ _, a)_ = Î£[N]i=1[Î±][i][(][Ï„][i][)][Q][i][(][Ï„][i][, a][i][;][ Î¸][i][) +][ b][(][Ï„] [)][ and][ Î±][i][(][Ï„][i][)][ >][ 0][ and][ Î£]i[N]=1[Î±][i][(][Ï„][i][) = 1]

The additional constraint Î£[N]i=1[Î±][i][(][Ï„][i][) = 1][ makes sure that][ Î±][i][(][Ï„][i][)][ is bounded, so we cannot maximize]
_Qtotal by simply using an infinite Î±i(Ï„i)._

4.2 AN EXACT SOLUTION FOR MACA

In this section, we introduce an exact solution of Equation (5) under the single-state/timestep setting.
This will guide us to design the approximated solution for multi-state problems in Section 4.3. In
the following, we omit unnecessary notations (i.e., Ï„ and [Ï„i][N]i=1 [due to single state) for simplicity.]

Specifically, Equation (5) is a two-objective (i.e., Î±[âˆ—] = [Î±i[âˆ—][]]i[N]=1 [and][ Î¸][âˆ—] [= [][Î¸]i[âˆ—][]]i[N]=1[) optimization]
problem. The Generalized Expectation Maximization (GEM) (Fessler & Hero, 1994) is a popular
technique to solve such problems. We adopt this idea and propose a two-stage optimization method.
At the initial weighting stage, each agent i proposes an initial weighting vector Î±i = [Î±i[1][, ..., Î±]i[N] []]
that assigns Î±i[l] [weights (i.e., credits) to][ Q][l][ to show its preference about how much contribution]
agent l has made to the agent team. Here, Î±i = [Î±i[l][]][N]l=1 [is agent][ i][â€™s estimation of the optimal]
**_Î±[âˆ—]_** = [Î±i[âˆ—][]]i[N]=1[. At the][ optimization stage][, each agent][ i][ iteratively optimizes its weighting vector][ Î±][i]
and its policy parameter Î¸i as follows:

_Î±i[l]_ _[â†]_ _[Î±]i[l]_ [+][ Î²][1 1]N [Î£]j[N]=1[(][Î±]j[l] _[âˆ’]_ _[Î±]i[l][)]_ (6)

_Î¸i_ Î£[N]j=1[Î±]i[j][Î¸][j][ +][ Î²][2][âˆ‡][Q]i _[Q][total][(][a][)][âˆ‡][Î¸]i_ _[Q][i][(][a][i][;][ Î¸][i][)]_ (7)
_â†_

where Î²1 and Î²2 are the learning rates. Equation (6) guarantees that all agents eventually reach
consistent weighting vectors based on the difference of them. Equation (7) makes sure that Qtotal
can be maximized by gradient ascent given the weighting vectors. Interlacing the above updates
could be seen as a kind of GEM algorithms, and the advantages are two-fold (Blondin & Hale,
2020a;b). First, this â€œdecentralized negotiationâ€ will be more robust to the weighting disagreement
among agents compared to a centralized weighting mechanism. Second, it can guarantee that the
convergent values [Î±i[l] _âˆ—, Î¸iâˆ—[]]i[N]=1_ [are optimal under mild assumptions as shown by Proposition 1.]

**Proposition 1. Under assumption that the individual Q-value functions [Qi][N]i=1** _[were continuously]_
_differentiable and convex, interlacing Equation (6) and (7) enough times will result in convergent_

[Î±i[l] _âˆ—, Î¸iâˆ—[]]i[N]=1_ _[where][ Î±]i[l]_ _âˆ—_ = Î±jl _âˆ—_ = Î±lâˆ—[, and the joint Q-value][ Q][total][(][a][) = Î£]i[N]=1[Î±]i[âˆ—][Q][i][(][a][i][;][ Î¸]i[âˆ—][)][ will]
_converge to the optimal value Q[âˆ—]total[.]_

_Proof. First, Olfati-Saber et al. (2007) have proved that updating Equation (6) enough times will_
make all agents reach the same convergent weighting vector, i.e., Î±i[l] [=][ Î±]j[l] [. Second, assuming all]

[Qi][N]i=1 [were continuously differentiable and convex (this is a common assumption, and it is true if]
the Q-values are linear in â€œfeaturesâ€ as pointed out by Silver et al. (2014)), it is easy to prove that
_Qtotal will also be convex given a specific weighting vector; therefore, gradient ascent in Equation_


-----

[ğ‘„ğ‘–(ğ‰, ğ‘ğ‘–)]ğ‘ğ‘–=1, ğ‘(ğ‰) TD-loss ğ‘„ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™(ğ‰, ğ’‚) ECA-loss [ğ›¼ğ‘–âˆ—(ğœğ‘–)]ğ‘ğ‘–=1

eluğ‘Š2 |Â·| relurelu ğ‘  [ğ‘„ğ‘–(ğ‰, ğ‘ğ‘–)]ğ‘ğ‘–=1, ğ‘(ğ‰) [ğ›¼ğ‘–âˆ—(ğœğ‘–)]ğ‘ğ‘–=1 ğ›¼ğ‘™âˆ— = ğ‘[1] ğ‘ğ‘–=1ğ›¼ğ‘–ğ‘™

ğ‘Š1 |Â·| relu ğ‘  Transformation Consistency

[ğ‘„ğ‘–(ğœğ‘–, ğ‘ğ‘–)]ğ‘ğ‘–=1 Network Network ğœ¶ğ‘–(ğœğ‘–)=[ğ›¼ğ‘–1, â€¦, ğ›¼ğ‘–ğ‘ ]KL-lossğœ¶ğ‘—(ğœğ‘—) ğ‘œğ‘—

Q-value head      Weighting head

ğ‘„ğ‘–(ğœğ‘–, ğ‘ğ‘–) ğœ¶ğ‘–(ğœğ‘–)=[ğ›¼ğ‘–1, â€¦, ğ›¼ğ‘–ğ‘] â€¦ â€¦ MLP

MLP MLP ğ‘„1 ğœ¶1 ğ‘„ğ‘ ğœ¶ğ‘ GRU+ MLP+

+ MLP

â„ğ‘– GRU â„ğ‘–â€² Agent â€¦ Agent MLP

Network 1 Network N

MLP

L2-loss

ğ‘œğ‘– ğ‘œ1 ğ‘œğ‘ ğ‘œğ‘– ğ‘œğ‘– VAE

(a) (b) (c)


Figure 1: (a) The structures of Agent Network and Transformation Network. (b) The overall ECAQ
architecture. (c) The Consistency Network structure.

(7) can always find the maximum point Q[âˆ—]total [with proper learning rate (since the first term does]
not affect the gradient direction in expectation). Finally, the convergence theory of Expectation
Maximization (Wu, 1983; Xu & Jordan, 1996) tells us that interlacing Equation (6) and (7) will not
affect the final convergence properties, so we will eventually find the corresponding [Î±i[âˆ—][, Î¸]i[âˆ—][]]i[N]=1[.]

4.3 APPROXIMATING THE EXACT SOLUTION

The real-world problems often consist of multiple states, but the above solution can hardly learn a
set of [Î±i[âˆ—][, Î¸]i[âˆ—][]]i[N]=1 [that is optimal for all states because the optimal][ [][Î±]i[âˆ—][, Î¸]i[âˆ—][]]i[N]=1 [is state-dependent. In]
general, a policy with strong fitting ability can alleviate this problem, so we approximate the exact
solution using deep neural networks (DNN) as the function approximator.

**Overall Design. Our DNN-based method is called ECAQ, which consists of three sub-networks as**
shown in Figure 1. The Agent Network first applies a GRU to encode the local observation oi into
the history Ï„i. Then, it uses the Q-value head and the weighting head to generate the individual Qvalue Qi(Ï„i, ai; Î¸i) and the weighting vector Î±i(Ï„i; Î¸i) = [Î±i[1][, ..., Î±]i[N] []][, respectively. The weighting]
head adopts a Softmax activation function to guarantee Î£[N]l=1[Î±]i[l] [= 1][.]

The Consistency Network is proposed to optimize the weighting vector. It takes as input the original
weighting vectors from all agents, and optimizes them to the same converged value [Î±i[âˆ—][(][Ï„][i][;][ Î¸][i][)]]i[N]=1
(i.e., mimicking the effect of Equation (6)). The details will be introduced in the following section.

The Transformation Network is proposed to increase ECAQâ€™s representation expressiveness. As previous methods, it uses the joint history Ï„ (or the state s if available) to generate a two-layer hypernetwork (Ha et al., 2017), then transforms individual Q-value [Qi(Ï„i, ai; Î¸i)][N]i=1 [to][ [][Q][i][(][Ï„] _[, a][i][;][ Î¸][i][, w][)]]i[N]=1_
using this hypernetwork. Besides, it also generates a baseline function b(Ï„ ; w).

Finally, we form the joint Q-value as Qtotal(Ï„ _, a; w) = Î£[N]i=1[Î±]i[âˆ—][(][Ï„][i][;][ Î¸][i][)][Q][i][(][Ï„]_ _[, a][i][;][ Î¸][i][, w][) +][ b][(][Ï„]_ [;][ w][)][.]
Afterwards, we can optimize the policy parameter to the converged value [Î¸i[âˆ—][]]i[N]=1 [(i.e., mimicking]
the effect of Equation (7)). The details will be introduced in the following section.

**Optimizing the Weighting Vector. As mentioned before, the Consistency Network is proposed to**
optimize the weighting vectors [Î±i][N]i=1 [to the same converged value. The main difficulty is how]
to apply DNN to achieve this goal. Here, we adopt the variational inference technique (Mao et al.,
2020b) with the following key idea: assuming that the optimal weighting vector is Î±[âˆ—] = [Î±i[âˆ—][]]i[N]=1[, but]
it is unknown; each agent i would like to infer Î±[âˆ—] based on oi by p(Î±[âˆ—]|oi); if all agentsâ€™ weighting
vectors [Î±i][N]i=1 [could really converge to the same][ Î±][âˆ—][, we would achieve our goal.]

In practice, directly computing p(Î±[âˆ—] _oi) =_ _pp(o(oi_ _iÎ±|Î±[âˆ—][âˆ—])p)p(Î±(Î±[âˆ—][âˆ—])d)Î±[âˆ—]_ [is quite difficult, so we approximate]
_|_ _|_
_p(Î±[âˆ—]_ _oi) using another tractable distributionR q(Î±[âˆ—]_ _oi) by minimizing the KL-divergence between_
_|_ _|_


-----

them, namely, min KL(q(Î±[âˆ—] _oi)_ _p(Î±[âˆ—]_ _oi)), which equals to:_
_|_ _||_ _|_

max Eq(Î±âˆ—|oi) log p(oi|Î±[âˆ—]) âˆ’ _KL(q(Î±[âˆ—]|oi)||p(Î±[âˆ—]))_ (8)

Equation (8) can be modeled by a variational autoencoder (VAE), which is the main part of the
Consistency Network. The encoder of this VAE learns a mapping q(Î±i|oi; Î¸i) from oi to Î±i, and the
decoder learns a mapping p(oi|Î±i; Î¸i) from Î±i back to _oi. The loss function to train this VAE is:_

_L[vae]i_ (Î¸i) = L2(oi, _oi; Î¸i) + KL(q(Î±i_ _oi; Î¸i)_ _p(Î±[âˆ—]))_ (9)
_|_ _||_

b b

where the first term represents the reconstruction error of observations/states, and minimizing this
error makes sure that the weighting vector b **_Î±i(Ï„i; Î¸i) is state-dependent so as to better handle real-_**
world problems consisting of multiple states; the second term ensures that the learned distribution
_q(Î±i_ _oi; Î¸i) is similar to the true prior distribution p(Î±[âˆ—]). However, the true prior p(Î±[âˆ—]) in Equation_
_|_
(9) is unknown. One could assume that p(Î±[âˆ—]) follows a unit Gaussian distribution as previous
methods, but it cannot be true for all states. In practice, we find that other agentsâ€™ weighting vector
_q(Î±j_ _oj; Î¸j) is a good surrogate for p(Î±[âˆ—]), namely, we approximate Equation (9) by:_
_|_

_L[vae]i_ (Î¸i) _L2(oi,_ _oi; Î¸i) + [1]_ _j=1[KL][(][q][(][Î±][i][|][o][i][;][ Î¸][i][)][||][q][(][Î±][j][|][o][j][;][ Î¸][j][))]_ (10)
_â‰ˆ_ _N_ [Î£][N]

Provably, Proposition 2 shows that Equation (10) has the same effect as Equation (6) under single- b
state setting. Intuitively, this approximation punishes any pair of agents âŸ¨i, jâŸ© with inconsistent
weighting vectors, namely, with a large KL(q(Î±i _oi; Î¸i)_ _q(Î±j_ _oj; Î¸j)). Therefore, it is helpful for_
_|_ _||_ _|_
converging to the same weighting vector. In practice, the above optimization cannot be iterated
infinitely, so we make sure that the weighting vectors are eventually consistent by averaging them
_Î±l[âˆ—]_ _[â‰ˆ]_ _N[1]_ [Î£]i[N]=1[Î±]i[l][.]

**Proposition 2. Under single-state setting, Equation (10) has the same effect as Equation (6).**

_Proof. For single-state, some common assumptions hold: 1) there is no need to recover observation,_
so the first term of Equation (10) is removed; 2) q(Î±i) and q(Î±j) are Gaussians with equal stan
dard deviation; therefore, minimizing _N1_ [Î£]j[N]=1[KL][(][q][i][||][q][j][) =] _N1_ [Î£]j[N]=1 [log][ Ïƒ]Ïƒ[j]i [+][ Ïƒ]i[2][+(]2[Âµ]Ïƒ[i][âˆ’]j[2] _[Âµ][j]_ [)][2] _âˆ’_ 2[1] [=]

1

_N_ [Î£]j[N]=1[(][Âµ][i][(][Î±][i][)][âˆ’][Âµ][j][(][Î±][j][))][2][ has the same effect as minimizing][ 1]N [Î£]j[N]=1[(][Î±][j][ âˆ’][Î±][i][)][ in Equation (6).]

**Optimizing the Policy Parameter. In practice, we share policy parameters among agents as the**
previous methods (e.g., VDN, QMIX and QTRAN). Besides accelerating convergence, the special
advantage is that the first term of Equation (7) can be simplified as Î£[N]j=1[Î±]i[j][Î¸][j][ =][ Î¸][i][, therefore]
Equation (7) can be rewritten as:

_Î¸i â†_ _Î¸i + Î²2âˆ‡Qi_ _Qtotal(Ï„_ _, a; w)âˆ‡Î¸i_ _Qi(Ï„i, ai; Î¸i)_ (11)

The loss function of Equation (11) is L[eca]i (Î¸i) = _Qtotal. We call it Explicit Credit Assignment_
_âˆ’_
loss (ECA-loss) because it explicitly maximizes Qtotal (i.e., the criterion of good credit assignment).

4.4 PUTTING IT ALL TOGETHER

The exact solution proposed in Section 4.2 is summarized by Equation (6) and (7). In Section 4.3,
ECAQ adopts the VAE-loss (i.e., Equation (10)) and ECA-loss (i.e., Equation (11)) to approximate Equation (6) and (7), respectively. Nevertheless, VAE-loss and ECA-loss are mainly used to
optimize the credit assignment among multiple agents at a given timestep/state. For the sequential decision-making problems consisting of multiple timesteps/states, it is also critical to do credit
assignment along the time horizon (i.e., assigning the delayed reward to previous timesteps). Therefore, ECAQ also minimizes the following TD-loss:

_L[td](w) = E(Ï„_ _,a,r,Ï„ â€²)âˆ¼D[(r + Î³ maxa[â€²][ Q][total][(][Ï„][ â€²][,][ a][â€²][;][ w][âˆ’][)][ âˆ’]_ _[Q][total][(][Ï„]_ _[,][ a][;][ w][))][2][]]_ (12)

It ensures the Bellman optimality of Qtotal, namely, Qtotal can approximate the true but unknown
_Qjoint very closely. Putting it all together, the total loss to train ECAQ is:_

_L(w, Î¸i) = L[td](w) + Î·(Î£[N]i=1[L][vae]i_ (Î¸i) + Î£[N]i=1[L][eca]i (Î¸i)) (13)

where Î· is the hyperparameter to balance the credit assignment among multiple agents and between
different timesteps. The detailed training algorithm is provided in the Appendix.


-----

100

80

60

40

20

0

80

70

60

50

40

30

20

10


100

80

60

40

20

20

15

10


80

60

40

test win rate (%)

20


0 20 40 episode 60 80 100

|ECAQ (ours) QTRAN COMA QMIX VDN IDQN QPLEX|Col2|
|---|---|


ECAQ (ours)
QTRAN
COMA
QMIX
VDN
IDQN
QPLEX

(a) The easy 2s3z map.


0 20 40 episode 60 80 100

|ECAQ (ours) QTRAN COMA QMIX VDN IDQN QPLEX|Col2|
|---|---|


ECAQ (ours)
QTRAN
COMA
QMIX
VDN
IDQN
QPLEX

(b) The easy 1c3s5z map.


0 20 40 episode 60 80 100

|ECAQ (ours) QTRAN COMA QMIX VDN IDQN QPLEX|Col2|
|---|---|


ECAQ (ours)
QTRAN
COMA
QMIX
VDN
IDQN
QPLEX

(c) The hard 2c vs 64zg map.


80

60

40

test win rate (%)

20

|ECAQ (ours) QTRAN COMA QMIX VDN IDQN QPLEX|Col2|
|---|---|

|ECAQ (ours) QTRAN COMA QMIX VDN IDQN QPLEX|Col2|
|---|---|

|ECAQ (ours) QTRAN COMA QMIX VDN IDQN QPLEX|Col2|
|---|---|


0 20 40 episode 60 80 100

ECAQ (ours)
QTRAN
COMA
QMIX
VDN
IDQN
QPLEX

(d) The hard 5m vs 6m map.


0 50 episode100 150 200

ECAQ (ours)
QTRAN
COMA
QMIX
VDN
IDQN
QPLEX

(e) Super hard 3s5z vs 3s6z.


0 25 50 75 episode100 125 150 175 200

ECAQ (ours)
QTRAN
COMA
QMIX
VDN
IDQN
QPLEX

(f) The super hard MMM2 map.


Figure 2: The test win rate on different StarCraft II maps. ECAQ achieves the best performance on
four maps (i.e., the easy 1c3s5z, the hard 2c vs 64zg, and the super hard 3s5z vs 3s6z and MMM2),
and performs as good as QPLEX on two maps (i.e., the easy 2s3z and the hard 5m vs 6m).

5 EXPERIMENT


**Environment. To guarantee a fair comparison, the decentralized StarCraft II micromanagement**
problem (Samvelyan et al., 2019) is used since it is usually considered as the official testbed for the
IGM-based methods. Besides, previous works show that StarCraft II is suitable for credit assignment
study (Zhou et al., 2020; Yang et al., 2020a; Wang et al., 2020e; Foerster et al., 2018). Specifically,
six maps with different configurations (e.g., easy, hard, and super hard settings; homogeneous and
heterogeneous agents) are used to guarantee that ECAQ does not overfit to one specific scenario.

We also evaluate ECAQ on the cooperative navigation problem (Lowe et al., 2017; Mordatch &
Abbeel, 2018), which is a simple yet popular multi-agent environment. Specifically, there are N
agents and N landmarks on a 10-by-10 2D plane. The agents are controlled by our methods, and
they try to cover all landmarks. The observation is the relative positions and velocities of other
agents and landmarks. The action is the velocity of agents. The reward is the negative distance of
any agent to each landmark. We test three scenarios where N = 4, 6 and 10, respectively.

**Baseline. Since we study the explicit credit assignment for the IGM-based joint Q-learning, here we**
mainly compare with these IGM-based methods. Specifically, we consider the most relevant VDN
(Sunehag et al., 2018), QMIX (Rashid et al., 2018) and QTRAN (Son et al., 2019). We also adopt
COMA (Foerster et al., 2018) and IDQN (Tampuu et al., 2017). COMA applies a counterfactual
baseline to do credit assignment; in contrast, there is no credit assignment in IDQN. The advanced
methods like QPLEX (Wang et al., 2020b), WQMIX (Rashid et al., 2020), LICA (Zhou et al., 2020),
DOP (Wang et al., 2020e) and Qatten (Yang et al., 2020b) are also reported.

**Implementation. Since most baselines are officially provided in the PyMARL framework** [3], we
also use this framework to implement ECAQ and to conduct the experiments. We do not modify any
of the default configurations/hyperparameters of PyMARL to guarantee a fair comparison. For the
special hyperparameter Î· of ECAQ, we decrease it from 1.0 to 0.05 gradually as training goes on.

5.1 MAIN RESULT


**Result of StarCraft II. The average test win rates of five independent runs are shown in Figure 2. As**
can be observed, ECAQ achieves the best performance on four maps (i.e., the easy 1c3s5z, the hard
2c vs 64zg, and the super hard 3s5z vs 3s6z and MMM2), and performs as good as QPLEX on two
maps (i.e., the easy 2s3z and the hard 5m vs 6m). Notably, the performance of COMA is unstable:
it works well in some scenarios but it is even worse than IDQN in other scenarios. These results
demonstrate that a good credit assignment is very necessary for consistent multi-agent cooperation.
In order to compare ECAQ with advanced methods, we show the results on hard and super hard maps

3https://github.com/oxwhirl/pymarl.


-----

Table 1: The test win rate on the hard and super hard StarCraft II maps.


**Map** **Qatten** **QPLEX** **WQMIX** **LICA** **DOP** **ECAQ (ours)**

hard 2c vs 64zg 66 55 67 68 **84** **85**
hard 5m vs 6m **72** **70** 60 60 63 **73**
super hard 3s5z vs 3s6z **17** **12** 6 0 0 **15**
super hard MMM2 **79** **72** 23 **84** 50 **80**

average test win rate 58.5 52.25 39.0 53.0 49.25 **63.25**

in Table 1. As can be seen, ECAQâ€™s performance is better than or as good as many advanced methods
(e.g., Qatten, QPLEX and WQMIX) in specific maps, while its average performance is the best,
although ECAQ does not involve advanced DNN architectures like duplex dueling or multi-head
attention critic. This is because ECAQ can directly learn a good credit assignment that maximizes
the long-term rewards, which is highly positive for the test win rate. For example, in the super hard
3s5z vs 3s6z scenario, ECAQ assigns a high credit to allyâ€™s Zealots because the learned policy is
that allyâ€™s Zealots hold enemyâ€™s Zealots and attack enemyâ€™s Stalkers at the same time.


**Result of Cooperative Navigation. The average rewards**
of ten independent runs are shown in Figure 3. It can
be observed that VDN outperforms QMIX in cooperative
navigation, which is in contrast to the results of StarCraft
II where QMIX is better than VDN. This highlights the relationship among method performance, method complexity and task complexity: complex methods do not always
get better performance in simple tasks. Nevertheless, as
can be seen, ECAQ obtains more rewards than other baselines in most scenarios. It seems that the complexity of
the evaluated tasks does not influence ECAQ too much.

5.2 FURTHER ANALYSIS


-160

-140

-120

-100

-80

-60

average rewards

-40

-20

0

4_vs_4 6_vs_6 10_vs_10

IDQN VDN QMIX ECAQ (ours)


Figure 3: The average rewards in cooperative navigation. Lower bar is better.


**Ablation Study. The average test win rate is shown in Figure 4, where TD, ECA and VAE represent**
the three loss functions defined before (namely, TD+ECA+VAE stands for ECAQ). Surprisingly, it
can be observed that neither TD+ECA nor TD+VAE performs well, and they are somehow worse
than simply applying a single TD-loss. The reason may be that 1) TD+ECA cannot reach consistent
weighting vectors due to the lack of VAE-loss, so the optimization of ECA-loss may be incorrect; 2)
TD+VAE does not optimize the credit assignment at all due to the lack of ECA-loss. Therefore, both
TD+ECA and TD+VAE deteriorate the solution. These results assert a conclusion that both VAEloss and ECA-loss are necessary for ECAQâ€™s good performance. We also find the same conclusion
in the matrix games, and the details are shown in the Appendix.


20

15

10


80

60

40

20


80

60

40

20

|TD + ECA + VAE TD + VAE TD + ECA TD|Col2|
|---|---|

|TD + ECA + VAE TD + VAE TD + ECA TD|Col2|
|---|---|

|TD + ECA + VAE TD + VAE TD + ECA TD|Col2|
|---|---|


20 40timestpe (x 10,000)60 80 100

TD + ECA + VAE
TD + VAE
TD + ECA
TD

(a) The hard 2c vs 64zg.


0 25 50 75timestpe (x 10,000)100 125 150 175 200

TD + ECA + VAE
TD + VAE
TD + ECA
TD

(b) The super hard 3s5z vs 3s6z.


0 25 50 75timestpe (x 10,000)100 125 150 175 200

TD + ECA + VAE
TD + VAE
TD + ECA
TD

(c) The super hard MMM2.


Figure 4: The ablation results for (super) hard scenarios. Other scenarios are shown in the Appendix.

**Credit Assignment Study. To make a clear analysis, we should make the environment as simple**
as possible, so we adopt the matrix games in this study. Firstly, we use the asymmetric monotonic
game G as shown in Table 2(a). There are two agents in G and each agent i has three actions aiA,
_aiB, and aiC. The asymmetricity means that different agents have different impact on the payoff._
Specifically, the column agent (i.e., agent 2) has larger impact compared to the row agent (i.e., agent
1) in game G, since the payoff changes by two units in column while by one unit in row. We train
ECAQ through a full exploration conducted over 20,000 steps as QTRAN (Son et al., 2019). We


-----

Table 2: The asymmetric monotonic game and the converged results on the game.


(a) Payoff of the matrix game G.


(b) Q1, Q2 and Qtotal learned by ECAQ under setting #1.

|a a 2 1|a 2A|a 2B|a 2C|
|---|---|---|---|
|a 1A|7|5|3|
|a 1B|6|4|2|
|a 1C|5|3|1|

|Q Q 2 1|4.828(a ) 2A|2.107(a ) 2B|-0.611(a ) 2C|
|---|---|---|---|
|6.639(a ) 1A|7.004|5.005|3.007|
|2.854(a ) 1B|6.000|4.001|2.003|
|-0.927(a ) 1C|4.997|2.998|1.000|


(c) The converged values. ECAQ has learned the optimal policy (i.e., a1A and a2A).

Setting _Î±1[âˆ—]_ _Î±2[âˆ—]_ _Q[âˆ—]1_ _Q[âˆ—]2_ _Q[âˆ—]total_

#1: G 0.26523 0.73477 6.639(a1A) 4.828(a2A) 7.004
#2: G[T] 0.72562 0.27438 4.755(a1A) 6.435(a2A) 6.982
#3: G âˆ— 10 5.83e-08 1.00e+00 55.900(a1A) 33.236(a2A) 63.738

Table 3: The symmetric non-monotonic game and the converged results on the game.


(a) Payoff of the matrix game.


(b) The converged values in different settings.

Setting _Î±1[âˆ—]_ _Î±2[âˆ—]_ _Q[âˆ—]total_

#1: full exploration 0.55601 0.44399 -3.568
#2: high probability for a1A 1.35e-07 9.99e-01 0.666
#3: high probability for a2A 9.99e-01 9.09e-08 0.151

|a a 2 1|a 2A|a 2B|a 2C|
|---|---|---|---|
|a 1A|8|-12|-12|
|a 1B|-12|0|0|
|a 1C|-12|0|0|


check how different payoff settings will affect the credit assignment. As shown in Table 2(c), ECAQ
assigns a large weight to agent 2 in game G (i.e., Î±2[âˆ—]
larger impact on the payoff. In contrast, when we transpose the payoff of[â‰ˆ] [0][.][735][ in setting #1) because agent 2 has] G, denoted by G[T], the
weight of agent 2 will be small (i.e., Î±2[âˆ—]
payoff in G[T] . We further increase the payoff of[â‰ˆ] [0][.][274] G[ in setting #2) since agent 2 is less influential on the] by ten times, denoted by G âˆ— 10, and the weight
of agent 2 changes to a very large value (i.e., Î±2[âˆ—]
changes by twenty units (rather than the original two units) in column, and the absolute impact of[â‰ˆ] [1][.][0][ in setting #3). This is because the payoff]
agent 2 becomes much larger. These analyses have demonstrated the good credit assignment ability
of ECAQ. Consequently, ECAQ can easily find the optimal decentralized policy (i.e., a1A and a2A)
in all games as observed from Table 2(c). Finally, comparing Table 2(b) with 2(a), it can be seen
that ECAQ fits the payoff matrix very accurately, which asserts the good fitting ability of ECAQ.

Secondly, we use the symmetric non-monotonic game as shown in Table 3(a). This game is proposed
by QTRAN (Son et al., 2019). We check how different exploration settings will affect the credit
assignment. As shown in Table 3(b), the full exploration (i.e., setting #1 as QTRAN) will result in
almost random credit assignment values (i.e., Î±i[âˆ—]
is symmetric and the exploration is full, so there is no difference between the two agents. In contrast,[â‰ˆ] [0][.][5][). This is expected because the payoff matrix]
when we make one agent more stable (e.g., raising the probability of a1A in setting #2), ECAQ will
assign a large weight to focus on the other agent (e.g., Î±2[âˆ—]
other agent is more random, and it has greater impact on the obtained reward. The results of setting[â‰ˆ] [1][.][0][ in setting #2). The reason is that the]
#3 are similar to these of setting #2, and both settings can find the optimal policy as shown in the
Appendix. Overall, these analyses have demonstrated the good credit assignment ability of ECAQ.

6 CONCLUSION

Multi-agent credit assignment has long been a fundamental issue for multi-agent cooperation. This
paper presented the explicit multi-agent credit assignment for the IGM-based joint Q-learning, which
not only ensures the Bellman optimality of Qtotal to do credit assignment along the time horizon, but
also optimizes a criterion to do credit assignment among different agents explicitly. We instantiate
this idea with deep neural networks and propose ECAQ to facilitate multi-agent cooperation in more
realistic scenarios. Extensive experiments justify the superior performance of ECAQ. Furthermore,
the detailed analyses show that ECAQ has really learned interpretable credit assignment values. To
our best knowledge, the explicit credit assignment is complementary yet novel to the existing IGMbased studies. We believe that it is basic for building effective learning-based multi-agent systems.


-----

REFERENCES

Adrian Agogino and Kagan Turner. Multi-agent reward analysis for learning in noisy domains. In
_Proceedings of the fourth international joint conference on Autonomous agents and multiagent_
_systems, pp. 81â€“88, 2005._

Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of
decentralized control of markov decision processes. Mathematics of operations research, 27(4):
819â€“840, 2002.

Maude J Blondin and Matthew Hale. An algorithm for multi-objective multi-agent optimization. In
_2020 American Control Conference (ACC), pp. 1489â€“1494, 2020a. doi: 10.23919/ACC45564._
2020.9148017.

Maude J Blondin and MT Hale. A decentralized multi-objective optimization algorithm. arXiv
_preprint arXiv:2010.04781, 2020b._

J.A. Fessler and A.O. Hero. Space-alternating generalized expectation-maximization algorithm.
_IEEE Transactions on Signal Processing, 42(10):2664â€“2677, 1994. doi: 10.1109/78.324732._

Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial
_Intelligence, pp. 7219â€“7226, 2018._

John J Grefenstette. Lamarckian learning in multi-agent environments. Technical report, NAVY
CENTER FOR APPLIED RESEARCH IN ARTIFICIAL INTELLIGENCE WASHINGTON DC,
1995.

David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. In Proceedings of the International
_Conference on Learning Representations, 2017._

Kenneth E. Kinnear (ed.). Advances in Genetic Programming. MIT Press, Cambridge, MA, USA,
1994. ISBN 0262111888.

Jiahui Li, Kun Kuang, Baoxiang Wang, Furui Liu, Long Chen, Fei Wu, and Jun Xiao. Shapley
counterfactual credits for multi-agent reinforcement learning. In Feida Zhu, Beng Chin Ooi, and
Chunyan Miao (eds.), KDD â€™21: The 27th ACM SIGKDD Conference on Knowledge Discovery
_and Data Mining, Virtual Event, Singapore, August 14-18, 2021, pp. 934â€“942. ACM, 2021a. doi:_
[10.1145/3447548.3467420. URL https://doi.org/10.1145/3447548.3467420.](https://doi.org/10.1145/3447548.3467420)

Wenhao Li, Xiangfeng Wang, Bo Jin, Dijun Luo, and Hongyuan Zha. Structured cooperative reinforcement learning with time-varying composite action space. IEEE Transactions on Pattern
_Analysis and Machine Intelligence, 2021b._

Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multiagent actor-critic for mixed cooperative-competitive environments. In Advances in neural infor_mation processing systems, pp. 6379â€“6390, 2017._

Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. MAVEN: multiagent variational exploration. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dâ€™AlchÂ´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu_ral Information Processing Systems 32:_ _Annual Conference on Neural Information Pro-_
_cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp._
[7611â€“7622, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/](https://proceedings.neurips.cc/paper/2019/hash/f816dc0acface7498e10496222e9db10-Abstract.html)
[f816dc0acface7498e10496222e9db10-Abstract.html.](https://proceedings.neurips.cc/paper/2019/hash/f816dc0acface7498e10496222e9db10-Abstract.html)

Patrick Mannion, Sam Devlin, Jim Duggan, and Enda Howley. Multi-agent credit assignment in
stochastic resource management games. The Knowledge Engineering Review, 32, 2017.

Hangyu Mao, Zhibo Gong, and Zhen Xiao. Reward design in cooperative multi-agent reinforcement
learning for packet routing. arXiv preprint arXiv:2003.03433, 2020a.

Hangyu Mao, Wulong Liu, Jianye Hao, Jun Luo, Dong Li, Zhengchao Zhang, Jun Wang, and Zhen
Xiao. Neighborhood cognition consistent multi-agent reinforcement learning. In Proceedings of
_the AAAI Conference on Artificial Intelligence, volume 34, pp. 7219â€“7226, 2020b._


-----

Maja J Mataric. Learning to behave socially. In Third international conference on simulation of
_adaptive behavior, volume 617, pp. 453â€“462. Citeseer, 1994._

Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent
populations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.

Navid Naderializadeh, Fan H Hung, Sean Soleyman, and Deepak Khosla. Graph convolutional value
decomposition in multi-agent reinforcement learning. arXiv preprint arXiv:2010.04740, 2020.

Duc Thien Nguyen, Akshat Kumar, and Hoong Chuin Lau. Credit assignment for collective multiagent rl with global rewards. In Advances in Neural Information Processing Systems, pp. 8102â€“
8113, 2018.

Reza Olfati-Saber, J. Alex Fax, and Richard M. Murray. Consensus and cooperation in networked
multi-agent systems. Proceedings of the IEEE, 95(1):215â€“233, 2007. doi: 10.1109/JPROC.2006.
887293.

Scott Proper and Kagan Tumer. Modeling difference rewards for multiagent learning. In Pro_ceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems -_
_Volume 3, AAMAS â€™12, pp. 1397â€“1398, Richland, SC, 2012. International Foundation for Au-_
tonomous Agents and Multiagent Systems. ISBN 0981738133.

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4292â€“4301, 2018.

Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding
monotonic value function factorisation for deep multi-agent reinforcement learning. Advances in
_Neural Information Processing Systems, 33, 2020._

Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas
Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. In Proceedings of the 18th International Conference on
_Autonomous Agents and MultiAgent Systems, pp. 2186â€“2188, 2019._

Lloyd S. Shapley. A value for n-person games. In The Shapley Value, pp. 31â€“40. Cambridge
[University Press, oct 1988. doi: 10.1017/cbo9780511528446.003. URL https://doi.org/](https://doi.org/10.1017%2Fcbo9780511528446.003)
[10.1017%2Fcbo9780511528446.003.](https://doi.org/10.1017%2Fcbo9780511528446.003)

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International Conference on Machine Learning, pp.
387â€“395, 2014.

Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In Interna_tional Conference on Machine Learning, pp. 5887â€“5896, 2019._

Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th
_International Conference on Autonomous Agents and MultiAgent Systems, pp. 2085â€“2087. Inter-_
national Foundation for Autonomous Agents and Multiagent Systems, 2018.

Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan
Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning.
_PloS one, 12(4):e0172395, 2017._

Kagan Tumer and Adrian Agogino. Distributed agent-based air traffic flow management. In Pro_ceedings of the 6th international joint conference on Autonomous agents and multiagent systems,_
pp. 1â€“8, 2007.

Jianhao Wang, Zhizhou Ren, Beining Han, Jianing Ye, and Chongjie Zhang. Towards understanding linear value decomposition in cooperative multi-agent q-learning. _arXiv preprint_
_arXiv:2006.00587, 2020a._


-----

Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agent q-learning. arXiv preprint arXiv:2008.01062, 2020b.

Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yunjie Gu. Shapley q-value: A local reward
approach to solve global reward games. In Proceedings of the AAAI Conference on Artificial
_Intelligence, volume 34, pp. 7285â€“7292, 2020c._

Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. ROMA: Multi-agent reinforcement
learning with emergent roles. In Hal DaumÂ´e III and Aarti Singh (eds.), Proceedings of the 37th
_International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning_
_Research, pp. 9876â€“9886. PMLR, 13â€“18 Jul 2020d._ [URL http://proceedings.mlr.](http://proceedings.mlr.press/v119/wang20f.html)
[press/v119/wang20f.html.](http://proceedings.mlr.press/v119/wang20f.html)

Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Dop: Off-policy
multi-agent decomposed policy gradients. In International Conference on Learning Representa_tions, 2020e._

C. F. Jeff Wu. On the Convergence Properties of the EM Algorithm. The Annals of Statistics, 11(1):
[95 â€“ 103, 1983. doi: 10.1214/aos/1176346060. URL https://doi.org/10.1214/aos/](https://doi.org/10.1214/aos/1176346060)
[1176346060.](https://doi.org/10.1214/aos/1176346060)

Lei Xu and Michael I. Jordan. On convergence properties of the em algorithm for gaussian mixtures.
_Neural Computation, 8(1):129â€“151, 1996. doi: 10.1162/neco.1996.8.1.129._

Yaodong Yang, Jianye Hao, Guangyong Chen, Hongyao Tang, Yingfeng Chen, Yujing Hu, Changjie
Fan, and Zhongyu Wei. Q-value path decomposition for deep multiagent reinforcement learning.
_arXiv preprint arXiv:2002.03950, 2020a._

Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao
Tang. Qatten: A general framework for cooperative multiagent reinforcement learning. arXiv
_preprint arXiv:2002.03939, 2020b._

Tianjun Zhang, Huazhe Xu, Xiaolong Wang, Yi Wu, Kurt Keutzer, Joseph E Gonzalez, and Yuandong Tian. Multi-agent collaboration via reward attribution decomposition. _arXiv preprint_
_arXiv:2010.08531, 2020._

Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung. Learning implicit credit assignment for cooperative multi-agent reinforcement learning. In Advances in Neural Information
_Processing Systems, 2020._

Tianze Zhou, Fubiao Zhang, Kun Shao, Kai Li, Wenhan Huang, Jun Luo, Weixun Wang, Yaodong
Yang, Hangyu Mao, Bin Wang, et al. Cooperative multi-agent transfer learning with leveladaptive credit assignment. arXiv preprint arXiv:2106.00517, 2021.


-----

A THE TRAINING ALGORITHM

ECAQ adopts the Centralized Training with Decentralized Execution (CTDE) paradigm. The training algorithm for ECAQ is shown in Algorithm 1.


**Algorithm 1: Training Algorithm for ECAQ**
**Input: Randomly initialized Î¸i and w for the policy networks (i.e., individual Q-value functions**
_Qi(Ï„i, ai; Î¸i)) and the mixing critic (i.e., the joint Q-value function Qtotal(Ï„_ _, a; w))._

**Output: Converged individual Q-value Q[âˆ—]i** [(][Ï„][i][, a][i][;][ Î¸]i[âˆ—][)][ for future decentralized execution.]

**1 Qtotal(Ï„** _, a; w[âˆ’])_ _Qtotal(Ï„_ _, a; w);_
_â†_

**2 while not terminated do**

**3** The agents interact with the environment based on Qi(Ï„i, ai; Î¸i), and put the experience
tuples (s, Ï„ _, a, r, Ï„_ **_[â€²]) into replay buffer D;_**

**4** Sample batch size of tuples (s, Ï„ _, a, r, Ï„_ **_[â€²]) from replay buffer D;_**

**5** **for each tuple (s, Ï„** _, a, r, Ï„_ **_[â€²]) do_**

**6** Each agent i extracts its observation oi;

**7** The Agent Network generates individual Q-values Qi(Ï„i, ai; Î¸i) and weighting vector
**_Î±i(Ï„i; Î¸i) = [Î±i[1][, ..., Î±]i[N]_** []][ based on][ o][i][, and further generates the decoded observation]
_oË†i based on Î±i(Ï„i);_

**8** Calculate the VAE-loss as
_L[vae]i_ (Î¸i) â‰ˆ _L2(oi,_ _oi; Î¸i) +_ _N[1]_ [Î£]j[N]=1[KL][(][q][(][Î±][i][|][o][i][;][ Î¸][i][)][||][q][(][Î±][j][|][o][j][;][ Î¸][j][))][;]

**9** The Consistency Network takes as input [Î±i][N]i=1 [and outputs the converged]

[Î±[âˆ—]i [(][Ï„][i][;][ Î¸][i][)]]i[N]=1[;] b

**10** The Transformation Network takes as input [Qi(Ï„i, ai; Î¸i)][N]i=1 [and state][ s][, then]
generates [Qi(Ï„ _, ai; Î¸i, w)][N]i=1_ [and][ b][(][Ï„] [;][ w][)][;]

**11** Calculate the joint Q-value as
_Qtotal(Ï„_ _, a; w) = Î£[N]i=1[Î±]i[âˆ—][(][Ï„][i][;][ Î¸][i][)][Q][i][(][Ï„]_ _[, a][i][;][ Î¸][i][, w][) +][ b][(][Ï„]_ [;][ w][)][;]

**12** Calculate the ECA-loss as L[eca](w) = _Qtotal(Ï„_ _, a; w);_
_âˆ’_

**13** Calculate the TD-loss as
_L[td](w) = (r + Î³ maxaâ€² Qtotal(Ï„_ _[â€²], a[â€²]; w[âˆ’]) âˆ’_ _Qtotal(Ï„_ _, a; w))[2];_

**14** Calculate the total loss as L(w, Î¸i) = L[td](w) + uÎ£[N]i=1[L][vae]i (Î¸i) + vÎ£[N]i=1[L][eca]i (Î¸i);

**15** Train the network parameters [Î¸i][N]i=1 [and][ w][ by back-propagation based on the total loss;]

**16** **end**

**17** # In practice, the above for iteration is processed in a mini-batch manner;

**18** **if at target update interval then**

**19** Update the target mixing critic by Qtotal(Ï„ _, a; w[âˆ’])_ _Qtotal(Ï„_ _, a; w);_
_â†_

**20** **end**

**21 end**

B CREDIT ASSIGNMENT ANALYSIS

B.1 CREDIT ASSIGNMENT ANALYSIS FOR COOPERATIVE NAVIGATION


We analyze the training behaviors of the weighting value (i.e., credit assignment value) Î±i (i =
1, 2, 3) using the 3 vs 3 cooperative navigation. We first analyze the dynamics of Î±1 = _N1_ [Î£]i[N]=1[Î±]i[1]

and agent iâ€™s estimation Î±i[1] [(][i][ = 1][,][ 2][,][ 3][), which are shown by the four solid lines in Figure 5(a).]
As shown by the black, green and blue lines, although the values are very different at the beginning
of training, different Î±i[1] [(][i][ = 1][,][ 2][,][ 3][) tend to be consistent as training goes on. We then analyze]
whether the converged Î±i (i = 1, 2, 3) is meaningful. Figure 5(b) shows the learned policies by
the red arrows, and the credit assignment values (which can be estimated from Figure 5(a)) are as
follows : Î±1 0.47 since agent A1 is far from the landmark and its policy has the largest influence
on the reward; â‰ˆ Î±2 0.35 and Î±3 0.18 since agent A2 and A3 are near the landmark and their
policies have little influence on the reward. The results are consistent with these shown in the main â‰ˆ _â‰ˆ_
paper: the most influential agent usually corresponds to the largest assignment value. These analyses
indicate that the assignment values Î±i (i = 1, 2, 3) are meaningful to identify critical agents.


-----

0.7

0.6

0.5

0.4

alpha_1^1

0.3 alpha_2^1

alpha_3^1

0.2

alpha_1

0.1 alpha_2

alpha_3

0.0

0 2000 4000 6000 8000 10000 12000 14000

training step


(a) The training dynamic of Î±i and agent iâ€™s estimation Î±i[1] [of][ Î±][1][.]

**A3**

**A2**

**A1**


(b) Î±i is meaningful for MACA. Ai represents agent i.

Figure 5: The credit assignment analysis using the 3 vs 3 cooperative navigation task.

Table 4: The payoff matrix of the one-step game and the converged results on the game.


(a) Payoff of the matrix game.


(b) Q1, Q2 and Qtotal learned by ECAQ under setting #3.


_a1_ _a2_ **_a2A_** _a2B_ _a2C_

**_a1A_** **8** -12 -12

_a1B_ -12 0 0

_a1C_ -12 0 0


_Q1_ _Q2_ **-0.014(a2A)** -2.497(a2B) -3.062(a2C )

**1.674(a1A)** **0.1509** 0.1505 0.1505

-5.407(a1B) -6.9395 -6.9395 -6.9300

-6.487(a1C ) -8.0108 -8.0108 -8.0109


(c) The converged values for different settings. Note that both #2 and #3 can find the optimal policy.

Setting _Î±1[âˆ—]_ _Î±2[âˆ—]_ _Q[âˆ—]1_ _Q[âˆ—]2_ _Q[âˆ—]total_

#1: fully exploration 0.56 0.44 0.0403(a1C ) 0.6296(a2C ) -3.568
#2: high probability for a1A 1.35e-07 9.99e-01 0.4702(a1A) 1.7616(a2A) 0.666
#3: high probability for a2A 9.99e-01 9.09e-08 1.6739(a1A) -0.0140(a2A) 0.151
#4: training only by TD-loss 9.99e-01 2.58e-06 0.4889(a1A) -0.8592(a2B) -0.695


-----

B.2 CREDIT ASSIGNMENT ANALYSIS FOR MATRIX GAME

We also adopt a matrix game (Son et al., 2019) to check the credit assignment ability of ECAQ.
As shown in Table 4(a), there are two agents and each agent i has three actions aiA, aiB and aiC.
We check how different settings will affect the credit assignment. As shown in Table 4(c), the
fully exploration (i.e., setting #1 as QTRAN (Son et al., 2019)) will result in almost random credit
assignment value (i.e., Î±i[âˆ—]
exploration is full, so there is no difference between the two agents. In contrast, when we make one[â‰ˆ] [0][.][5][). This is expected because the payoff matrix is symmetric and the]
agent more stable (e.g., raising the probability of a1A in setting #2), ECAQ will assign large weights
to focus on the other agent (e.g., Î±2[âˆ—]
random, and it has greater impact on the obtained reward. The results of setting #3 are similar to[â‰ˆ] [1][.][0][ in setting #2). The reason is that the other agent is more]
these of setting #2, and both settings can find the optimal decentralized policy (i.e., a1A and a2A) as
observed. Comparing the results shown in Table 4(b) with these shown in (Son et al., 2019; Wang
et al., 2020b), it can be seen that ECAQ fits the optimal payoff more accurately than VDN, QMIX
and Qatten. Overall, these analyses have demonstrated the good credit assignment ability and fitting
ability of ECAQ.


C MORE ABLATION STUDY

We conduct ablation study on the easy maps (e.g., 2s3z), finding that there is no significant performance difference for different ablation models as shown by Figure 6(a). The reason is that the maps
are too easy, and all methods can get good results. Therefore, we focus on doing ablation study on
the hard and super hard maps. As shown in the main paper, neither TD+ECA nor TD+VAE performs
well, and they are somehow worse than simply applying a single TD-loss in the hard 2c vs 64zg,
super hard 3s5z vs 3s6z and super hard MMM2 maps. However, for the hard 5m vs 6m scenarios,
TD+ECA, TD+VAE and TD have almost similar performance as shown by Figure 6(b), but they are
worse than TD+ECA+VAC (i.e., ECAQ). In the future, we will do more ablation study on different
maps and draw more detailed conclusions when the computing resources are available.


100 TD + ECA + VAE

TD + VAE

80 TD + ECA

TD

60

40

test win rate (%)

20

0

0 20 40 60 80 100

timestpe (x 10,000)


TD + ECA + VAE

80 TD + VAE

TD + ECA
TD

60

40

test win rate (%)

20

0

0 25 50 75 100 125 150 175 200

timestpe (x 10,000)


(a) The easy 2s3z map.


(b) The hard 5m vs 6m map.


Figure 6: The ablation results on StarCraft II.

We also conduct ablation study on the matrix game shown in Table 4(a). The payoff matrix is
symmetric, so there will be no difference between the two agents under full exploration. Therefore,
we raise the probability of a2A (i.e., setting #3 in Table 4(c)), and ECAQ can find the optimal
decentralized policy (i.e., a1A and a2A) as observed. In contrast, if we train ECAQ only by TD-loss
with the same exploration (i.e., setting #4), agent 2 can just find a non-optimal policy a2B, which
asserts the necessity of both VAE-loss and ECA-loss.


D RELATED WORK

D.1 MULTI-AGENT CREDIT ASSIGNMENT APPROACH


In this section, we provide a brief review about the multi-agent credit assignment approaches that
are closely related to the proposed ECAQ.


-----

**Explicit Credit Assignment.** In general, the explicit methods attribute agent contributions that
are at least locally optimal (Kinnear, 1994). A notable approach is to assess an action by calculating
_difference reward against a certain reward baseline (Tumer & Agogino, 2007; Agogino & Turner,_
2005; Proper & Tumer, 2012). The key idea is that the true contribution of agent i can be approximated by the difference between rewards induced by a and [ac, a _i], where ac is a counterfactual_
_âˆ’_
action (i.e., not the true action agent i has taken). For example, Agogino & Turner (2005) and
Tumer & Agogino (2007) adopt specific ac to calculate the difference reward or CLEAN reward
to do credit assignment. COMA (Foerster et al., 2018) and SQDDPG (Wang et al., 2020c) extends
this idea from reward difference to Q-value difference, and calculates the counterfactual baseline or
Shapley Q-value to do credit assignment. Specifically, COMA (Foerster et al., 2018) uses a centralized critic to estimate the counterfactual advantage of an action. SQDDPG (Wang et al., 2020c)
applies the Shapley-value framework (Shapley, 1988) to do credit assignment based on an agentâ€™s
_marginal contribution as it is sequentially added to possible agent groups. However, the explicit_
mentioned here is different from the explicit mentioned by ECAQ.

**The IGM-based Approach. The representative methods are VDN (Sunehag et al., 2018), QMIX**
(Rashid et al., 2018), QTRAN (Son et al., 2019), Qatten (Yang et al., 2020b), QPLEX (Wang et al.,
2020b), etc. We have reviewed these methods in the main paper. They are often called the implicit
credit assignment methods as noted by Zhou et al. (2020). In contrast, ECAQ applies explicit credit
assignment training signal to optimize the IGM-based approaches.

**The Attribution Approach. The key idea is that split the final reward into two kinds of rewards:**
self-reward and attributed reward; and the attributed reward is assumed to be redistributed by other
agents based on the agentâ€™s contribution. Methods following this idea are (Nguyen et al., 2018;
Zhang et al., 2020; Mao et al., 2020a), and they can also be seen as explicit credit assignment.

**Other Approach. There are many other multi-agent credit assignment methods that can be hardly**
classified clearly, for example, the implicit credit assignment (Zhou et al., 2020), the social reward
credit assignment (Mataric, 1994) and others (Mannion et al., 2017; Grefenstette, 1995).

D.2 MULTI-AGENT COOPERATION APPROACH

Please note that we aim at giving a complementary thought for the multi-agent credit assignment
problem rather than beating all methods (with hyperparameter tuning), so we implement ECAQ
based on the basic QMIX instead of the advanced approaches like Qatten (Yang et al., 2020b) and
QPLEX (Wang et al., 2020b). Therefore, we do not intend to give a detailed review for all recent
deep MARL methods. Nevertheless, there are many topics to facilitate multi-agent cooperations,
for example, the role-based methods (Mahajan et al., 2019), the coordinative exploration methods
(Wang et al., 2020d), the graph-based methods (Blondin & Hale, 2020a;b; Mao et al., 2020b) and so
on. However, these methods are beyond the scope of this paper.

E IMPLEMENTATION DETAILS OF ECAQ

As mentioned in the main paper, we use the up-to-date PyMARL framework [4] to conduct the experiments. We do not modify any of the default configurations of PyMARL to guarantee a fair
comparison. We do not tune the hyperparameters of ECAQ too much: the weights of VAE-loss and
ECA-loss are directly set equal, and they decrease from 1.0 to 0.05 gradually as training goes on; all
of the other hyperparameters (e.g., exploration and activation functions) keep the same as these of
PyMARL. The code is available on the submission system. Therefore, the main experimental results
can be easily reproduced. In addition, we train our methods on the 64-core CPU machine with a
total memory of 128G. The training time is 4 hours to 10 hours for different StarCraft II maps. This
is not too computing heavy for MARL applications.

4https://github.com/oxwhirl/pymarl. The code licensed under the Apache License v2.0, so we can use it
freely for research purpose.


-----

F LIMITATIONS OF ECAQ

ECAQ is implemented based on deep neural networks, so it faces the â€œblack box problemâ€ where the
behaviors of individual agents may not be interpretable from the perspective of human. Fortunately,
ECAQ adopts an explicit manner to do multi-agent credit assignment, and the learned weighting
vectors are interpretable to some extend.

ECAQ focuses on the fully cooperative setting, so the goal is set as the maximization of the longterm shared global reward. As a result, the multi-agent credit assignment (MACA) may raise ethical
issues when the optimal joint actions require sacrificing certain agents. Certainly, we can define the
â€œfair-criterionâ€ for MACA, then optimize this criterion to guarantee fairness between agents, and
this is our future work.

ECAQ is an IGM-based method. The main limitation is that it cannot fit the non-monotonic payoff perfectly, as shown in Table 4(b). The reason is that both the Transformation Network and the
weighting vector Î±i are nonnegative. However, as mentioned before, we aim at giving a complementary thought for the multi-agent credit assignment problem rather than beating all methods, so
we implement ECAQ by an easy-to-explain joint Q-value function. If we implemented ECAQ based
on more advanced approaches like Qatten, QTRAN++ or QPLEX, it will be much easier to fit the
non-monotonic payoff.


-----

