# NASI: LABEL- AND DATA-AGNOSTIC NEURAL ARCHITECTURE SEARCH AT INITIALIZATION

**Yao Shu, Shaofeng Cai, Zhongxiang Dai, Beng Chin Ooi & Bryan Kian Hsiang Low**
Department of Computer Science, National University of Singapore
_{shuyao,shaofeng,daizhongxiang,ooibc,lowkh}@comp.nus.edu.sg_

ABSTRACT

Recent years have witnessed a surging interest in Neural Architecture Search (NAS).
Various algorithms have been proposed to improve the search efficiency and the
search effectiveness of NAS, i.e., to reduce the search cost and improve the generalization performance of the selected architectures, respectively. However, the search
efficiency of these algorithms is severely limited by the need for model training
during the search process. To overcome this limitation, we propose a novel NAS algorithm called NAS at Initialization (NASI) that exploits the capability of a Neural
_Tangent Kernel in being able to characterize the performance of candidate archi-_
tectures at initialization, hence allowing model training to be completely avoided
to boost the search efficiency. Besides the improved search efficiency, NASI also
achieves competitive search effectiveness on various datasets like CIFAR-10/100
and ImageNet. Further, NASI is shown to be label- and data-agnostic under mild
conditions, which guarantees the transferability of the architectures selected by our
NASI over different datasets.

1 INTRODUCTION

The past decade has witnessed the wide success of deep neural networks (DNNs) in computer
vision and natural language processing. These DNNs, e.g., VGG (Simonyan & Zisserman, 2015),
ResNet (He et al., 2016), and MobileNet (Howard et al., 2017), are typically handcrafted by human
experts with considerable trials and errors. The human efforts devoting to the design of these DNNs
are, however, not affordable nor scalable due to an increasing demand of customizing DNNs for
different tasks. To reduce such human efforts, Neural Architecture Search (NAS) (Zoph & Le, 2017)
has recently been introduced to automate the design of DNNs. As summarized in (Elsken et al., 2019),
NAS conventionally consists of a search space, a search algorithm, and a performance evaluation.
Specifically, the search algorithm aims to select the best-performing neural architecture from the
search space based on its evaluated performance via the performance evaluation. In the literature,
various search algorithms (Luo et al., 2018; Zoph et al., 2018; Real et al., 2019) have been proposed
to search for architectures with comparable or even better performance than the handcrafted ones.

However, these NAS algorithms are inefficient due to the requirement of model training for numerous
candidate architectures during the search process. To improve the search inefficiency, one-shot NAS
algorithms (Pham et al., 2018; Dong & Yang, 2019; Liu et al., 2019; Xie et al., 2019) have trained
a single one-shot architecture and then evaluated the performance of candidate architectures with
model parameters inherited from this fine-tuned one-shot architecture. So, these algorithms can
considerably reduce the cost of model training in NAS, but still require the training of the one-shot
architecture. This naturally begs the question whether NAS is realizable at initialization such that
_model training can be completely avoided during the search process? To the best of our knowledge,_
only a few empirical efforts to date have been devoted to developing NAS algorithms without model
training (Mellor et al., 2021; Park et al., 2020; Abdelfattah et al., 2021; Chen et al., 2021).

This paper presents a novel NAS algorithm called NAS at Initialization (NASI) that can completely
avoid model training to boost search efficiency. To achieve this, NASI exploits the capability of
a Neural Tangent Kernel (NTK) (Jacot et al., 2018; Lee et al., 2019a) in being able to formally
characterize the performance of infinite-wide DNNs at initialization, hence allowing the performance
of candidate architectures to be estimated and realizing NAS at initialization. Specifically, given


-----

the estimated performance of candidate architectures by NTK, NAS can be reformulated into an
optimization problem without model training (Sec. 3.1). However, NTK is prohibitively costly to
evaluate. Fortunately, we can approximate it[1] using a similar form to gradient flow (Wang et al.,
2020) (Sec. 3.2). This results in a reformulated NAS problem that can be solved efficiently by a
gradient-based algorithm via additional relaxation using Gumbel-Softmax (Jang et al., 2017) (Sec.
3.3). Interestingly, NASI is even shown to be label- and data-agnostic under mild conditions, which
thus implies the transferability of the architectures selected by NASI over different datasets (Sec. 4).

We will firstly empirically demonstrate the improved search efficiency and the competitive search
effectiveness achieved by NASI in NAS-Bench-1Shot1 (Zela et al., 2020b) (Sec. 5.1). Compared
with other NAS algorithms, NASI incurs the smallest search cost while preserving the competitive
performance of its selected architectures. Meanwhile, the architectures selected by NASI from the
DARTS (Liu et al., 2019) search space over CIFAR-10 consistently enjoy the competitive or even
outperformed performance when evaluated on different benchmark datasets, e.g., CIFAR-10/100
and ImageNet (Sec. 5.2), indicating the guaranteed transferability of architectures selected by our
NASI. In Sec. 5.3, NASI is further demonstrated to be able to select well-performing architectures
on CIFAR-10 even with randomly generated labels or data, which strongly supports the label- and
_data-agnostic search and therefore the guaranteed transferability achieved by our NASI._

2 RELATED WORKS AND BACKGROUND

2.1 NEURAL ARCHITECTURE SEARCH

A growing body of NAS algorithms have been proposed in the literature (Zoph & Le, 2017; Liu
et al., 2018; Luo et al., 2018; Zoph et al., 2018; Real et al., 2019) to automate the design of
neural architectures. However, scaling existing NAS algorithms to large datasets is notoriously hard.
Recently, attention has thus been shifted to improving the search efficiency of NAS without sacrificing
the generalization performance of its selected architectures. In particular, a one-shot architecture is
introduced by Pham et al. (2018) to share model parameters among candidate architectures, thereby
reducing the cost of model training substantially. Recent works (Chen et al., 2019; Dong & Yang,
2019; Liu et al., 2019; Xie et al., 2019; Chen & Hsieh, 2020; Chu et al., 2020) along this line have
further formulated NAS as a continuous and differentiable optimization problem to yield efficient
gradient-based solutions. These one-shot NAS algorithms have achieved considerable improvement
in search efficiency. However, the model training of the one-shot architecture is still needed.

A number of algorithms are recently proposed to estimate the performance of candidate architectures
without model training. For example, Mellor et al. (2021) have explored the correlation between the
divergence of linear maps induced by data points at initialization and the performance of candidate
architectures heuristically. Meanwhile, Park et al. (2020) have approximated the performance of
candidate architectures by the performance of their corresponding Neural Network Gaussian Process
(NNGP) with only initialized model parameters, which is yet computationally costly. Abdelfattah et al.
(2021) have investigated several training-free proxies to rank candidate architectures in the search
space, while Chen et al. (2021) intuitively adopt theoretical aspects in deep networks (e.g., NTK (Jacot
et al., 2018) and linear regions of deep networks (Raghu et al., 2017)) to select architectures with a
good trade-off between its trainability and expressivity. Our NASI significantly advances this line
of work in (a) providing theoretically grounded performance estimation by NTK (compared with
(Mellor et al., 2021; Abdelfattah et al., 2021; Chen et al., 2021)), (b) guaranteeing the transferability
of its selected architectures with its provable label- and data-agnostic search under mild conditions
(compared with (Mellor et al., 2021; Park et al., 2020; Abdelfattah et al., 2021; Chen et al., 2021))) and
(c) achieving SOTA performance in a large search space over various benchmark datasets (compared
with (Mellor et al., 2021; Park et al., 2020; Abdelfattah et al., 2021)). Based on our results, Shu et al.
(2022) recently have provided further improvement on existing training-free NAS algorithms.

2.2 NEURAL TANGENT KERNEL (NTK)

Let a dataset ( _,_ ) denote a pair comprising a set of m n0-dimensional vectors of input features
_X_ _Y_ _X_
and a vector Y ∈ R[mn][×][1] concatenating the m n-dimensional vectors of output values, respectively.

1More precisely, we approximate the trace norm of NTK.


-----

Let a DNN be parameterized bybreviated to ft) of the predicted values of θt ∈ R[p] at time. Jacot et al. (2018) have revealed that the training t and output a vector f (X ; θt) ∈ R[mn][×][1] (ab_Y_
dynamics of DNNs with gradient descent can be characterized by an NTK. Formally, define the NTK
**Θt(X** _, X_ ) ∈ R[mn][×][mn] (abbreviated to Θt) as

**Θt(X** _, X_ ) ≜ _∇θt_ **_f_** (X ; θt) ∇θt **_f_** (X ; θt)[⊤] _._ (1)
Given a loss function Lt at time t and a learning rate η, the training dynamics of the DNN can then
be characterized as
_tft =_ _η Θt(_ _,_ ) **_ft_** _t,_ _t_ _t =_ _η_ **_ft_** _t_ **[Θ][t][(][X]** _[,][ X]_ [)][ ∇][f]t _[L][t]_ _[.]_ (2)
_∇_ _−_ _X_ _X_ _∇_ _L_ _∇_ _L_ _−_ _∇_ _L[⊤]_
Interestingly, as proven in (Jacot et al., 2018), the NTK stays asymptotically constant during the
course of training as the width of DNNs goes to infinity. NTK at initialization (i.e., Θ0) can thus
characterize the training dynamics and also the performance of infinite-width DNNs.

Lee et al. (2019a) have further revealed that, for DNNs with over-parameterization, the aforementioned training dynamics can be governed by their first-order Taylor expansion (or linearization) at
initialization. In particular, define

**_f_** [lin](x; θt) ≜ **_f_** (x; θ0) + ∇θ0 **_f_** (x; θ0)[⊤](θt − **_θ0)_** (3)
for all x . Then, f (x; θt) and f [lin](x; θt) share similar training dynamics over time, as described
_∈X_
formally in Appendix A.2. Besides, following the definition of NTK in (1), this linearization f [lin]
achieves a constant NTK over time.

Given the mean squared error (MSE) loss defined as Lt ≜ _m[−][1]_ _∥Y −_ **_f_** (X ; θt)∥2[2] [and the constant]
NTK Θt = Θ0, the loss dynamics in (2) above can be analyzed in a closed form while applying
gradient descent with learning rate η (Arora et al., 2019):
_Lt = m[−][1][ P][mn]i=1[(1][ −]_ _[ηλ][i][)][2][t][(][u]i[⊤][Y][)][2][,]_ (4)

where Θ0 = _i=1_ _[λ][i][(][Θ][0][)][u][i][u]i[⊤][, and][ λ][i][(][Θ][0][)][ and][ u][i][ denote the][ i][-th largest eigenvalue and the]_
corresponding eigenvector of Θ0, respectively.

[P][mn]

3 NEURAL ARCHITECTURE SEARCH AT INITIALIZATION

3.1 REFORMULATING NAS VIA NTK


Given a loss function L and the model parameters θ(A) of architecture A, we denote the training
and validation loss as Ltrain and Lval, respectively. NAS is conventionally formulated as a bi-level
optimization problem (Liu et al., 2019):
minA Lval(θ[∗](A); A) (5)

s.t. θ[∗]( ) ≜ arg minθ( ) train(θ( ); ) .
_A_ _A_ _L_ _A_ _A_

Notably, model training is required to evaluate the validation performance of each candidate architecture in (5). The search efficiency of NAS algorithms (Real et al., 2019; Zoph et al., 2018) based on (5)
is thus severely limited by the cost of model training for each candidate architecture. Though recent
works (Pham et al., 2018) have considerably reduced this training cost by introducing a one-shot
architecture for model parameter sharing, such a one-shot architecture still requires model training
and hence incurs training cost.

To completely avoid this training cost, we exploit the capability of a NTK in characterizing the performance of DNNs at initialization. Specifically, Sec. 2.2 has revealed that the training dynamics of
an over-parameterized DNN can be governed by its linearization at initialization. With the MSE loss,
the training dynamics of such linearization are further determined by its constant NTK. Therefore,
the training dynamics and hence the performance of a DNN can be characterized by the constant
NTK of its linearization. However, this constant NTK is computationally costly to evaluate. To this
end, we instead characterize the training dynamics (i.e., MSE) of DNNs in Proposition 1 using the
trace norm of NTK at initialization, which fortunately can be efficiently approximated. For simplicity,
we use this MSE loss in our analysis. Other widely adopted loss functions (e.g., cross entropy with
softmax) can also be applied, as supported in our experiments. Note that throughout this paper, the
parameterization and initialization of DNNs follow that of Jacot et al. (2018). For a L-layer DNN,
we denote the output dimension of its hidden layers and the last layer as n1 = = nL 1 = k and
_· · ·_ _−_
_nL = n, respectively._


-----

**Proposition 1.size** = m, a given Suppose that L-layer neural architecture ∥x∥2 ≤ 1 for all x ∈X and outputs Y ∈ f[0t _, 1][0[mn], 1]for a given dataset[mn]_ _as predicted labels of (X_ _, Y) of_
_with the corresponding MSE loss |X|_ _Lt, λmin(Θ0) > A 0 for the given NTK ∈_ **Θ0 w.r.t. ft at initialization, Y**
_and gradient descent (or gradient flow) is applied with learning rate η < λ[−]max[1]_ [(][Θ][0][)][. Then, for any]
_t ≥_ 0, there exists a constant c0 > 0 such that as k →∞,

_t_ _mn[2](1_ _ηλ(Θ0))[q]_ + ϵ (6)
_L_ _≤_ _−_

_with probability arbitrarily close to 1 where q is set to 2t if t < 0.5, and 1 otherwise, λ(Θ0) ≜_

(mn)[−][1][ P][mn]i=1 _[λ][i][(][Θ][0][)][, and][ ϵ][ ≜]_ [2][c][0] _n/(mk)_ 1 + c0 1/k _._
p  p 

Its proof is in Appendix A.3. Proposition 1 implies that NAS can be realizable at initialization.
Specifically, given a fixed and sufficiently large training budget t, in order to select the best-performing
architecture, we can simply minimize the upper bound of Lt in (6) over all the candidate architectures
in the search space. Since both theoretical (Mohri et al., 2018) and empirical (Hardt et al., 2016)
justifications in the literature have shown that training and validation loss are generally highly related,
we simply use Lt to approximate Lval. Hence, (5) can be reformulated as

min _mn[2](1_ _ηλ(Θ0(_ ))) + ϵ s.t. λ(Θ0( )) < η[−][1]. (7)
_A_ _−_ _A_ _A_

Note that the constraint in (7) is derived from the condition η < λ[−]max[1] [(][Θ][0][(][A][))][ in Proposition 1, and]
_η and ϵ are typically constants[2]_ during the search process. Following the definition of trace norm, (7)
can be further reduced into

maxA ∥Θ0(A)∥tr s.t. ∥Θ0(A)∥tr < mnη[−][1] _._ (8)

Notably, Θ0( ) only relies on the initialization of . So, no model training is required in optimizing
_A_ _A_
(8), which achieves our objective of realizing NAS at initialization.

Furthermore, (8) suggests an interesting interpretation of NAS: NAS intends to select architectures
with a good trade-off between their model complexity and the optimization behavior in their model
training. Particularly, architectures containing more model parameters will usually achieve a larger
_∥Θ0(A)∥tr according to the definition in (1), which hence provides an alternative to measuring the_
complexity of architectures. So, maximizing ∥Θ0(A)∥tr leads to architectures with large complexity
and therefore strong representation power. On the other hand, the complexity of the selected architectures is limited by the constraint in (8) to ensure a well-behaved optimization with a large learning
rate η in their model training. By combining these two effects, the optimization of (8) naturally trades
off between the complexity of the selected architectures and the optimization behavior in their model
training for the best performance. Appendix C.1 will validate such trade-off. Interestingly, Chen et al.
(2021) have revealed a similar insight of NAS to us.

3.2 APPROXIMATING THE TRACE NORM OF NTK

The optimization of our reformulated NAS in (8) requires the evaluation of ∥Θ0(A)∥tr for each
architecture A in the search space, which can be obtained by

_∥Θ0(A)∥tr =_ **_x∈X_** _∇θ0(A)f_ (x, θ0(A)) F _[,]_ (9)

where F denotes the Frobenius norm. However, the Frobenius norm of the Jacobian matrix in
(9) is costly to evaluate. So, we propose to approximate this term. Specifically, given a ∥· ∥ [P] [2] _γ-Lipschitz_
continuous loss function[3] **_x (i.e.,_** **_f_** **_x_** 2 _γ for all x_ ),
_L_ _∥∇_ _L_ _∥_ _≤_ _∈X_

_γ[−][1]_ _∇θ0(A)Lx_ 2 [=][ γ][−][1] _∇f_ _Lx[⊤]_ _[∇]θ0(A)[f]_ [(][x][,][ θ][0][(][A][))] 2 _[≤]_ _∇θ0(A)f_ (x, θ0(A)) F _[.]_ (10)

The Frobenius norm _∇θ0(A)f_ (x, θ0(A)) F [can thus be approximated efficiently using its lower]
bound in (10) through automatic differentiation (Baydin et al., 2017).

Meanwhile, the evaluation of ∥Θ0(A)∥tr in (9) requires iterating over the entire dataset of size m,
which thus incurs O(m) time. Fortunately, this incurred time can be reduced by parallelization over

2The same learning rate is shared among all candidate architectures for their model training during the search
process in conventional NAS algorithms such as (Liu et al., 2019).
3Notably, the γ-Lipschitz continuity is well-satisfied by widely adopted loss functions (see Appendix A.3).


-----

mini-batches. Let the set Xj denote the input feature vectors of the j-th randomly sampled mini-batch
of size _j_ = b. By combining (9) and (10),
_|X_ _|_

2

_∥Θ0(A)∥tr ≥_ _γ[−][1][ P]x∈X_ _∇θ0(A)Lx_ 2 _[≥]_ _[bγ][−][1][ P]j[m]=1[/][b]_ _b[−][1][ P]x∈Xj_ _[∇][θ]0[(][A][)][L][x]_ 2 _[,]_ (11)

where the last inequality follows from Jensen’s inequality. Note that[2] (11) provides an approximation
of ∥Θ0(A)∥tr incurring O(m/b) time because the gradients within a mini-batch can be evaluated
in parallel. In practice, we further approximate the summation over m/b mini-batches in (11) by
using only one single uniformly randomly sampled mini-batch _j. Formally, under the definition of_
_X_
_∥Θ[e]_ 0(A)∥tr ≜ _∥b[−][1][ P]x∈Xj_ 0[(][A][)][L][x][∥][2]2 [, our final approximation of][ ∥][Θ][0][(][A][)][∥][tr] [becomes]

_[∇][θ]_

_∥Θ0(A)∥tr ≈_ _mγ[−][1]∥Θ[e]_ 0(A)∥tr . (12)

This final approximation incurs only O(1) time and can also characterize the performance of neural
architectures effectively, as demonstrated in our experiments. Interestingly, a similar form to (12),
called gradient flow (Wang et al., 2020), has also been applied in network pruning at initialization.

3.3 OPTIMIZATION AND SEARCH ALGORITHM

The approximation of ∥Θ0(A)∥tr in Sec. 3.2 engenders an efficient optimization of our reformulated
NAS in (8): Firstly, we apply a penalty method to transform (8) into an unconstrained optimization
problem. Given a penalty coefficient µ and an exterior penalty function F (x) ≜ max(0, x) with a
pre-defined constant ν ≜ _γnη[−][1], and a randomly sampled mini-batch Xj, by replacing ∥Θ0(A)∥tr_
with the approximation in (12), our reformulated NAS problem (8) can be transformed into

maxA _∥Θ[e]_ 0(A)∥tr − _µF_ (∥Θ[e] 0(A)∥tr − _ν)_ _._ (13)
h i

Interestingly, (13) implies that the complexity of the final selected architectures is limited by not only
the constraint ν (discussed in Sec. 3.1) but also the penalty coefficient µ: For a fixed constant ν, a
larger µ imposes a stricter limitation on the complexity of architectures (i.e., ∥Θ[e] 0(A)∥tr < ν) in the
optimization of (13).

The optimization of (13) in the discrete search space, however, is intractable. So, we apply certain
optimization tricks to simplify it: Following that of Liu et al. (2019); Xie et al. (2019), we represent
the search space as a one-shot architecture such that candidate architectures in the search space can
be represented as the sub-graphs of this one-shot architecture. Next, instead of optimizing (13), we
introduce a distribution pα( ) (parameterized by α) over the candidate architectures in this search
_A_
space like that in (Zoph & Le, 2017; Pham et al., 2018; Xie et al., 2019) to optimize the expected
performance of architectures sampled from pα( ):
_A_

maxα EA∼pα(A) [R(A)] s.t. R(A) ≜ _∥Θ[e]_ 0(A)∥tr − _µF_ (∥Θ[e] 0(A)∥tr − _ν) ._ (14)

Then, we apply Gumbel-Softmax (Jang et al., 2017; Maddison et al., 2017) to relax the optimization
of (14) to be continuous and differentiable using the reparameterization trick. Specifically, for a given
**_α, to sample an architecture A, we simply need to sample g from p(g) = Gumbel(0, 1) and then_**
determine A using α and g (more details in Appendix B.3). Consequently, (14) can be transformed
into
maxα Eg∼p(g) [R(A(α, g))] . (15)

After that, we approximate (15) based on its first-order Taylor expansion at initialization such that it
can be optimized efficiently through a gradient-based algorithm. In particular, given the first-order
approximation within the ξ-neighborhood of initialization α0 (i.e., ∥∆∥2 ≤ _ξ):_

Eg _p(g) [R(_ (α0 + ∆, g))] Eg _p(g)_ _R(_ (α0, g)) + **_α0_** _R(_ (α0, g))[⊤]∆ _,_ (16)
_∼_ _A_ _≈_ _∼_ _A_ _∇_ _A_

the maximum of (16) is achieved when  


**_α0_** _R(_ (α0, g))
_∇_ _A_
Eg _p(g)[_ **_α0_** _R(_ (α0, g))]
_∼_ _∇_ _A_


∆[∗] = arg max Eg _p(g)_ **_α0_** _R(_ (α0, g))[⊤]∆ = ξ Eg _p(g)_
_∥∆∥2≤ξ_ _∼_ _∇_ _A_ _∼_
 


(17)


-----

**Algorithm 1 NAS at Initialization (NASI)**

1: Input: dataset D ≜ (X _, Y), batch size b, steps T_, penalty coefficient µ, constraint constant ν, initialized model parameters θ0 for one-shot architecture and distribution pα0 (A) with initialization
**_α0 = 0, set ξ = 1_**

2: for step t = 1, . . ., T do
3: Sample data _t_ of size b

5:4: Evaluate gradientSample gt ∼ Dp( ∼Dg) = Gt = Gumbel ∇α0 _R(0(A, 1t)) and determine sampled architecture with data Dt_ _At based on α0, gt_

6: end for
7: Estimate ∆[∗] with (18) and get α[∗] = α0 + ∆[∗]

8: Select architecture = arg max _pα∗_ ( )
_A[∗]_ _A_ _A_


|1e3Search Space 1 1e3Search Space 2 1e5Search Space 3 8 ½ = 1:00 ½ = 1:00 ½ = 1:00 6 4 4 2 2|Col2|
|---|---|
|½ = 1:00|½ = 1:00|

|1e3Search Space 2 1e5Search Space 3 4 ½ = 0:95 ½ = 0:94 3 2 1|Col2|
|---|---|
|½ = 0:95|½ = 0:94|


0.5 1.0 1.5


2.5 5.0 7.5


1e3 Search Space 1

½ = 0:94


1e3


1e3

Approx w/ True Labels


1e5


1e3


1e4

Approx w/ True Data


1e5


(a) Label-agnostic (b) Data-agnostic

Figure 1: Comparison of the approximated ∥Θ0(A)∥tr following (12) using the three search spaces
of NAS-Bench-1Shot1 on CIFAR-10 (a) between random vs. true labels, and (b) between random
vs. true data. Each pair (x, y) in the plots denotes the approximation for one candidate architecture in
the search space with true vs. random labels (or data), respectively. The trends of these approximations
are further illustrated by the lines in orange. In addition, Pearson correlation coefficient ρ of the
approximations with random vs. true labels (or data) is given in the corner.


The closed-form solution in (17) follows from the definition of dual norm and requires only a one-step
optimization, i.e., without the iterative update of ∆. Similar one-step optimizations have also been
adopted in (Goodfellow et al., 2015; Wang et al., 2020).

Unfortunately, the expectation in (17) makes the evaluation of ∆[∗] intractable. Monte Carlo sampling
is thus applied to estimate ∆[∗] efficiently: Given T sequentially sampled g (i.e., g1, . . ., gT ) and let
_Gi ≜_ _∇α0_ _R(A(α0, gi)), ∆[∗]_ can be approximated as


∆[∗]
_≈_ _T[ξ]_


_Gt_ (18)

max( _G1_ 2, . . ., _Gt_ 2) _[.]_
_∥_ _∥_ _∥_ _∥_


_t=1_


Note that inspired by AMSGrad (Reddi et al., 2018), the expectation Eg∼p(g)[∇α0 _R(A(α0, g))]_ 2
in (17) has been approximated using max( _G1_ 2, . . ., _Gt_ 2) in (18) based on a sample of g at time
_∥_ _∥_ _∥_ _∥_
_t. Interestingly, this approximation is non-decreasing in t and therefore achieves a similar effect of_
learning rate decay, which may lead to a better-behaved optimization of ∆[∗]. With the optimal ∆[∗]

and α[∗] = α0 + ∆[∗], the final architecture can then be selected as ≜ arg max _pα∗_ ( ), which
_A[∗]_ _A_ _A_
finally completes our NAS at Initialization (NASI) algorithm detailed in Algorithm 1. Interestingly,
this simple and efficient solution in (18) can already allow us to select architectures with competitive
performances, as shown in our experiments (Sec. 5).

4 LABEL- AND DATA-AGNOSTIC SEARCH OF NASI


Besides the improved search efficiency by completely avoiding model training during search, NASI
can even guarantee the transferability of its selected architectures with its provable label- and dataagnostic search under mild conditions in Sec. 4.1 and Sec. 4.2, respectively. That is, under this
provable label- and data-agnostic search, the final selected architectures on a proxy dataset using
NASI are also likely to be selected and hence guaranteed to perform well on the target datasets.
So, the transferability of the architectures selected via such label- and data-agnostic search can be
naturally guaranteed, which will also be validated in Sec. 5 empirically.


-----

4.1 LABEL-AGNOSTIC SEARCH

Our reformulated NAS problem (8) has explicitly revealed that it can be optimized without the need
of the labels from a dataset. Though our approximation of ∥Θ0(A)∥tr in (12) seemingly depends
on the labels of a dataset, (12) can, however, be derived using random labels. This is because the
Lipschitz continuity assumption on the loss function required by (10), which is necessary for the
derivation of (12), remains satisfied when random labels are used. So, the approximation in (12)
(and hence our optimization objective (13) that is based on this approximation) is label-agnostic,
which hence justifies the label-agnostic nature of NASI. Interestingly, NAS algorithm achieving a
similar label-agnostic search has also been developed in (Liu et al., 2020), which further implies the
reasonableness of such label-agnostic search.

The label-agnostic approximation of ∥Θ0(A)∥tr is demonstrated in Fig. 1a using the three search
spaces of NAS-Bench-1Shot1 with randomly selected labels. According to Fig. 1a, the large Pearson
correlation coefficient (i.e., ρ ≈ 1) implies a strong correlation between the approximations with
random vs. true labels, which consequently validates the label-agnostic approximation of ∥Θ0(A)∥tr.
Overall, these empirical observations have verified that the approximation of ∥Θ0(A)∥tr and hence
NASI based on the optimization over this approximation are label-agnostic, which will be further
validated empirically in Sec. 5.3.

4.2 DATA-AGNOSTIC SEARCH

Besides being label-agnostic, NASI is also guaranteed to be data-agnostic. To justify this, we prove
in Proposition 2 (following from the notations in Sec. 3.1) below that ∥Θ0(A)∥tr is data-agnostic
under mild conditions.
**Proposition 2.size |X| = m, a given Suppose that L-layer neural architecture x ∈** R[n][0] _and ∥x∥2 ≤ A is randomly initialized, and the1 for all x ∈X given a dataset γ (-LipschitzX_ _, Y) of_
_continuous nonlinearity σ satisfies |σ(x)| ≤|x|. Then, for any two data distributions P_ (x) and
_Q(x), denote Z ≜_ _P_ (x) _Q(x)_ dx, as n1, . . ., nL 1 _sequentially,_
_∥_ _−_ _∥_ _−_ _→∞_
R

(mn)[−][1] _∥Θ0(A; P_ )∥tr −∥Θ0(A; Q)∥tr _≤_ _n[−]0_ [1][ZD][(][γ][)]

_with probability arbitrarily close to 1. D(γ) is set to L if γ = 1, and (1 −_ _γ[2][L])/(1 −_ _γ[2]) otherwise._

Its proof is in Appendix A.4. Proposition 2 reveals that for any neural architecture A, ∥Θ0(A)∥tr is
data-agnostic if either one of the following conditions is satisfied: (a) Different datasets achieve a
small Z or (b) the input dimension n0 is large. Interestingly, these two conditions required by the
data-agnostic ∥Θ0(A)∥tr can be well-satisfied in practice. Firstly, we always have Z < 2 according
to the property of probability distributions. Moreover, many real-world datasets are indeed of high
dimensions such as ∼10[3] for CIFAR-10 (Krizhevsky et al., 2009) and ∼10[5] for COCO (Lin et al.,
2014). Since ∥Θ0(A)∥tr under such mild conditions is data-agnostic, NASI using ∥Θ0(A)∥tr as the
optimization objective in (8) is also data-agnostic.

While ∥Θ0(A)∥tr is costly to evaluate, we demonstrate in Fig. 1b that the approximated ∥Θ0(A)∥tr
in (12) is also data-agnostic by using random data generated with the standard Gaussian distribution.
Similar to the results using true vs. random labels in Fig. 1a, the approximated ∥Θ0(A)∥tr using
random vs. true data are also highly correlated by achieving a large Pearson correlation coefficient
(i.e., ρ > 0.9). Interestingly, the correlation coefficient here is slightly smaller than the one of
label-agnostic approximations in Fig. 1a, which implies that our approximated ∥Θ0(A)∥tr is more
agnostic to the labels than data. Based on these results, the approximated ∥Θ0(A)∥tr is guaranteed to
be data-agnostic. So, NASI based on the optimization over such a data-agnostic approximation is
also data-agnostic, which will be further validated empirically in Sec. 5.3.

5 EXPERIMENTS

5.1 SEARCH IN NAS-BENCH-1SHOT1

We firstly validate the search efficiency and effectiveness of our NASI in the three search spaces of
NAS-Bench-1Shot1 (Zela et al., 2020b) on CIFAR-10. As these three search spaces are relatively


-----

Search Space 1 Search Space 2 Search Space 3

0.14 0.14 0.14

0.12 0.12 0.12

0.10 0.10 0.10

Test Error (%)

0.08 0.08 0.08

0.06 0.06 0.06

0 5 10 15 20 0 5 10 15 20 0 5 10 15 20

Search Budget (epoch)

Random PC-DARTS ENAS DARTS GDAS NASI


Figure 2: Comparison of search efficiency (search budget in x-axis) and effectiveness (test error
evaluated on CIFAR-10 in y-axis) between NASI and other NAS algorithms in the three search spaces
of NAS-Bench-1Shot1. The test error for each algorithm is reported with the mean and standard error
after ten independent searches.

Table 1: Performance comparison among state-of-the-art (SOTA) image classifiers on CIFAR-10/100.
The performance of the final architectures selected by NASI is reported with the mean and standard
deviation of five independent evaluations. The search costs are evaluated on a single Nvidia 1080Ti.

**Test Error (%)** **Params (M)** **Search Cost**
**Architecture** **Search Method**
**(GPU Hours)**

**C10** **C100** **C10** **C100**

DenseNet-BC (Huang et al., 2017) 3.46[∗] 17.18[∗] 25.6 25.6 -  manual

NASNet-A (Zoph et al., 2018) 2.65 -  3.3 -  48000 RL
AmoebaNet-A (Real et al., 2019) 3.34±0.06 18.93[†] 3.2 3.1 75600 evolution
PNAS (Liu et al., 2018) 3.41±0.09 19.53[∗] 3.2 3.2 5400 SMBO
ENAS (Pham et al., 2018) 2.89 19.43[∗] 4.6 4.6 12 RL
NAONet (Luo et al., 2018) 3.53 -  3.1 -  9.6 NAO


DARTS (2nd) (Liu et al., 2019) 2.76±0.09 17.54[†] 3.3 3.4 24 gradient
GDAS (Dong & Yang, 2019) 2.93 18.38 3.4 3.4 7.2 gradient
NASP (Yao et al., 2020) 2.83±0.09 -  3.3 -  2.4 gradient
P-DARTS (Chen et al., 2019) 2.50 -  3.4 -  7.2 gradient
DARTS- (avg) (Chu et al., 2020) 2.59±0.08 17.51±0.25 3.5 3.3 9.6 gradient
SDARTS-ADV (Chen & Hsieh, 2020) 2.61±0.02 -  3.3 -  31.2 gradient
R-DARTS (L2) (Zela et al., 2020a) 2.95±0.21 18.01±0.26 -  -  38.4 gradient

TE-NAS[♯] (Chen et al., 2021) 2.83±0.06 17.42±0.56 3.8 3.9 1.2 training-free

NASI-FIX 2.79±0.07 16.12±0.38 3.9 4.0 **0.24** training-free
NASI-ADA 2.90±0.13 16.84±0.40 3.7 3.8 **0.24** training-free

_† Reported by Dong & Yang (2019) with their experimental settings._
_∗_ Obtained by training corresponding architectures without cutout (Devries & Taylor, 2017) augmentation.
_♯_ Evaluated using our experimental settings in Appendix B.4.

small, a lower penalty coefficient µ and a larger constraint ν (i.e., µ=1 and ν=1000) are adopted
to encourage the selection of high-complexity architectures in the optimization of (13). Here, ν is
determined adaptively as shown in Appendix B.1.

Figure 2 shows the results comparing the efficiency and effectiveness between NASI with a oneepoch search budget and other NAS algorithms with a maximum search budget of 20 epochs to allow
sufficient model training during their search process. Figure 2 reveals that among all these three
search spaces, NASI consistently selects architectures of better generalization performance than other
NAS algorithms with a search budget of only one epoch. Impressively, the architectures selected by
our one-epoch NASI can also achieve performances that are comparable to the best-performing NAS
algorithms with 19× more search budget. Above all, NASI guarantees its benefits of improving the
search efficiency of NAS algorithms considerably without sacrificing the generalization performance
of its selected architectures.

5.2 SEARCH IN THE DARTS SEARCH SPACE

We then compare NASI with other NAS algorithms in a more complex search space than NAS-Bench1Shot1, i.e., the DARTS (Liu et al., 2019) search space (detailed in Appendix B.2). Here, NASI


-----

Table 2: Performance comparison of the architectures selected by NASI with random vs. true
labels/data on CIFAR-10. The standard method denotes the search with the true labels and data of
CIFAR-10 and each test error in the table is reported with the mean and standard deviation of five
independent searches.

**NAS-Bench-1Shot1**
**Method** **DARTS**

**S1** **S2** **S3**

Standard 7.3±1.1 7.2±0.4 7.2±0.6 2.95±0.13
Random Label 6.8±0.3 7.0±0.4 7.5±1.4 2.90±0.12
Random Data 6.6±0.2 7.5±0.7 7.3±0.9 2.97±0.10

selects the architecture with a search budget of T =100, batch size of b=64 and µ=2. Besides, two
different methods are applied to determine the constraint ν during the search process: the adaptive
determination with an initial value of 500 and the fixed determination with a value of 100. The final
selected architectures with adaptive and fixed ν are, respectively, called NASI-ADA and NASI-FIX
(visualized in Appendix C.4), which are then evaluated on CIFAR-10/100 (Krizhevsky et al., 2009)
and ImageNet (Deng et al., 2009) following Appendix B.4.

Table 1 summarizes the generalization performance of the final architectures selected by various
NAS algorithms on CIFAR-10/100. Compared with popular training-based NAS algorithms, NASI
achieves a substantial improvement in search efficiency and maintains a competitive generalization
performance. Even when compared with other training-free NAS algorithm (i.e., TE-NAS), NASI is
also able to select competitive or even outperformed architectures with a smaller search cost. Besides,
NASI-FIX achieves the smallest test error on CIFAR-100, which also demonstrates the transferability
of the architectures selected by NASI over different datasets. Consistent results on ImageNet can be
found in Appendix C.5.

5.3 LABEL- AND DATA-AGNOSTIC SEARCH

To further validate the label- and data-agnostic search achieved by our NASI as discussed in Sec. 4,
we compare the generalization performance of the final architectures selected by NASI using random
labels and data on CIFAR-10. The random labels are randomly selected from all possible categories
while the random data is i.i.d. sampled from the standard Gaussian distribution. Both NAS-Bench1Shot1 and the DARTS search space are applied in this performance comparison where the same
search and training settings in Sec. 5.1 and Sec. 5.2 are adopted.

Table 2 summarizes the performance comparison. Interestingly, among all these four search spaces,
comparable generalization performances are obtained on CIFAR-10 for both the architectures selected
with random labels (or data) and the ones selected with true labels and data. These results hence
confirm the label- and data-agnostic search achieved by NASI, which therefore also further validates
the transferability of the architectures selected by NASI over different datasets.

6 CONCLUSION

This paper describes a novel NAS algorithm called NASI that exploits the capability of NTK for
estimating the performance of candidate architectures at initialization. Consequently, NASI can
completely avoid model training during the search process to achieve higher search efficiency than
existing NAS algorithms. NASI can also achieve competitive generalization performance across
different search spaces and benchmark datasets. Interestingly, NASI is guaranteed to be label- and
data-agnostic under mild conditions, which therefore implies the transferability of the final selected
architectures by NASI over different datasets. With all these advantages, NASI can thus be adopted to
select well-performing architectures for unsupervised tasks and larger-scale datasets efficiently, which
to date remains challenging to other training-based NAS algorithms. Furthermore, NASI can also be
integrated into other training-based one-shot NAS algorithms to improve their search efficiency while
preserving the search effectiveness of these training-based algorithms.


-----

REPRODUCIBILITY STATEMENT

To guarantee the reproducibility of the theoretical analysis in this paper, we have provided complete
proof of our propositions and also the justification of certain assumptions in Appendix A. Moreover,
we have conducted adequate ablation studies to further investigate the impacts of these assumptions as
well as the approximations used in our method on the final search results in Appendix C.6. Meanwhile,
to guarantee the reproducibility of the empirical results in this paper, we have provided our codes in
the supplementary materials and detailed experimental settings in Appendix B.

ACKNOWLEDGMENTS

This research is supported by A*STAR under its RIE2020 Advanced Manufacturing and Engineering
(AME) Programmatic Funds (Award A20H6b0151).

REFERENCES

Mohamed S. Abdelfattah, Abhinav Mehrotra, Lukasz Dudziak, and Nicholas D. Lane. Zero-cost
proxies for lightweight NAS. In Proc. ICLR, 2021.

Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Proc. ICML,
pp. 322–332, 2019.

Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic differentiation in machine learning: A survey. Journal of Machine Learning Research,
18:153:1–153:43, 2017.

Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on target task
and hardware. In Proc. ICLR, 2019.

Wuyang Chen, Xinyu Gong, and Zhangyang Wang. Neural architecture search on imagenet in four
gpu hours: A theoretically inspired perspective. In Proc. ICLR, 2021.

Xiangning Chen and Cho-Jui Hsieh. Stabilizing differentiable architecture search via perturbationbased regularization. In Proc. ICML, pp. 1554–1565, 2020.

Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridging
the depth gap between search and evaluation. In Proc. ICCV, pp. 1294–1303, 2019.

Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an
alternative to the CIFAR datasets. arXiv:1707.08819, 2017.

Xiangxiang Chu, Xiaoxing Wang, Bo Zhang, Shun Lu, Xiaolin Wei, and Junchi Yan. DARTS-:
Robustly stepping out of performance collapse without indicators. arXiv:2009.01027, 2020.

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical
image database. In Proc. CVPR, 2009.

Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv:1708.04552, 2017.

Xuanyi Dong and Yi Yang. Searching for a robust neural architecture in four GPU hours. In Proc.
_CVPR, pp. 1761–1770, 2019._

Xuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture
search. In Proc. ICLR, 2020.

Simon S. Du, Xiyu Zhai, Barnabas P´ oczos, and Aarti Singh. Gradient descent provably optimizes´
over-parameterized neural networks. In Proc. ICLR, 2019.

Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. Journal
_of Machine Learning Research, 20:55:1–55:21, 2019._


-----

Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In Proc. ICLR, 2015.

Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In Proc. ICML, pp. 1225–1234, 2016.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on ImageNet classification. In Proc. ICCV, pp. 1026–1034, 2015.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proc. CVPR, pp. 770–778, 2016.

Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. MobileNets: Efficient convolutional neural networks for
mobile vision applications. arXiv:1704.04861, 2017.

Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In Proc. CVPR, pp. 2261–2269, 2017.

Arthur Jacot, Clement Hongler, and Franck Gabriel. Neural Tangent Kernel: Convergence and´
generalization in neural networks. In Proc. NeurIPS, pp. 8580–8589, 2018.

Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with Gumbel-Softmax. In
_Proc. ICLR, 2017._

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.

Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Proc. NeurIPS, pp. 8570–8581, 2019a.

Namhoon Lee, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Snip: Single-shot network pruning
based on connection sensitivity. In Proc. ICLR, 2019b.

Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollar, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In´ _Proc. ECCV,_
pp. 740–755, 2014.

Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan L.
Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Proc. ECCV,
pp. 19–35, 2018.

Chenxi Liu, Piotr Dollar, Kaiming He, Ross B. Girshick, Alan L. Yuille, and Saining Xie. Are labels´
necessary for neural architecture search? In Proc. ECCV, pp. 798–813, 2020.

Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In
_Proc. ICLR, 2019._

Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization.
In Proc. NeurIPS, pp. 7827–7838, 2018.

Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. ShuffleNet V2: Practical guidelines for
efficient CNN architecture design. In Proc. ECCV, pp. 122–138, 2018.

Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network
acoustic models. In Proc. ICML, pp. 3, 2013.

Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The Concrete distribution: A continuous
relaxation of discrete random variables. In Proc. ICLR, 2017.

Joseph Mellor, Jack Turner, Amos J. Storkey, and Elliot J. Crowley. Neural architecture search
without training. In Proc. ICML, 2021.


-----

Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
press, 2018.

Daniel S. Park, Jaehoon Lee, Daiyi Peng, Yuan Cao, and Jascha Sohl-Dickstein. Towards NNGPguided neural architecture search. arXiv:2011.06006, 2020.

Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efficient neural architecture
search via parameter sharing. In Proc. ICML, pp. 4092–4101, 2018.

Maithra Raghu, Ben Poole, Jon M. Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the
expressive power of deep neural networks. In Proc. ICML, pp. 2847–2854, 2017.

Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized evolution for image
classifier architecture search. In Proc. AAAI, pp. 4780–4789, 2019.

Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In
_Proc. ICLR, 2018._

Yao Shu, Wei Wang, and Shaofeng Cai. Understanding architectures learnt by cell-based neural
architecture search. In Proc. ICLR, 2020.

Yao Shu, Zhongxiang Dai, Zhaoxuan Wu, and Bryan Kian Hsiang Low. Unifying and boosting
gradient-based training-free neural architecture search. arXiv:2201.09785, 2022.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In Proc. ICLR, 2015.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
_Proc. CVPR, pp. 1–9, 2015._

Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and
Quoc V. Le. MnasNet: Platform-aware neural architecture search for mobile. In Proc. CVPR, pp.
2820–2828, 2019.

Hidenori Tanaka, Daniel Kunin, Daniel L. Yamins, and Surya Ganguli. Pruning neural networks
without any data by iteratively conserving synaptic flow. In Proc. NeurIPS, 2020.

Chaoqi Wang, Guodong Zhang, and Roger B. Grosse. Picking winning tickets before training by
preserving gradient flow. In Proc. ICLR, 2020.

Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: Stochastic neural architecture search.
In Proc. ICLR, 2019.

Jingjing Xu, Liang Zhao, Junyang Lin, Rundong Gao, Xu Sun, and Hongxia Yang. KNAS: green
neural architecture search. In Proc. ICML, pp. 11613–11625, 2021.

Quanming Yao, Ju Xu, Wei-Wei Tu, and Zhanxing Zhu. Efficient neural architecture search via
proximal iterations. In Proc. AAAI, pp. 6664–6671, 2020.

Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter.
Understanding and robustifying differentiable architecture search. In Proc. ICLR, 2020a.

Arber Zela, Julien Siems, and Frank Hutter. NAS-Bench-1Shot1: Benchmarking and dissecting
one-shot neural architecture search. In Proc. ICLR, 2020b.

Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In Proc. ICLR,
2017.

Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable architectures
for scalable image recognition. In Proc. CVPR, pp. 8697–8710, 2018.


-----

APPENDIX A THEOREMS AND PROOFS

We firstly introduce the theorems proposed by Jacot et al. (2018); Lee et al. (2019a), which reveal
the capability of NTK in characterizing the training dynamics of infinite-width DNNs. Note that the
these theorems follow the parameterization and initialization of DNNs in (Jacot et al., 2018). Our
analysis based on these theorems thus also needs to follow such parameterization and initialization of
DNNs.

A.1 NEURAL TANGENT KERNEL AT INITIALIZATION

Jacot et al. (2018) have validated that the outputs of infinite-width DNNs at initialization tends to
Gaussian process (Theorem 1) and have further revealed the deterministic limit of NTK at initialization
(Theorem 2). We denote InL as a nL × nL matrix with all elements being 1.

**Theorem 1 (Jacot et al. (2018)). For a network of depth L at initialization, with a Lipschitz nonlinear-**
_ity σ, and in the limit as n1,_ _, nL_ 1 _sequentially, the output functions fθ,i for i = 1,_ _, nL,_
_· · ·_ _−_ _→∞_ _· · ·_
_tend (in law) to iid centered Gaussian processes of covariance Σ[(][L][)], where Σ[(][L][)]_ _is defined recursively_
_by_

Σ[(1)](x, x[′]) = [1] **_x[⊤]x[′]_** + β[2] _,_

_n0_

Σ[(][L][)](x, x[′]) = Eg∼N (0,Σ(L−1)) [σ(g(x))σ(g(x[′]))] + β[2]

_such that the expectation is with respect to a centered Gaussian process f with covariance Σ[(][L][−][1)]._

**Theorem 2 (Jacot et al. (2018)). For a network of depth L at initialization, with a Lipschitz nonlin-**
_earityto a deterministic limiting kernel: σ, and in the limit as n1, · · ·, nL−1 →∞_ _sequentially, the NTK Θ[(][L][)]_ _converges in probability_

**Θ[(][L][)]** **Θ[(][L][)]** _L_ _[.]_
_→_ _∞_ _[⊗]_ [I][n]

_Kernel Θ[(][L][)]_
_∞_ [:][ R][n][0][×][n][0][ →] [R][ is defined recursively by]

**Θ[(1)]**
_∞_ [(][x][,][ x][′][) = Σ][(1)][(][x][,][ x][′][)][,]

**Θ[(][L][)]** (x, x[′]) Σ[˙] [(][L][)](x, x[′]) + Σ[(][L][)](x, x[′]),
_∞_ [(][x][,][ x][′][) =][ Θ][(]∞[L][−][1)]

_where_

Σ˙ [(][L][)](x, x[′]) = Eg∼N (0,Σ(L−1)) [ ˙σ(g(x)) ˙σ(g(x[′]))]

_such that the expectation is with respect to a centered Gaussian process f with covariance Σ[(][L][−][1)]_
_and ˙σ denotes the derivative of σ._

A.2 TRAINING DYNAMICS OF INFINITE-WIDTH NEURAL NETWORKS

Given λmin(Θ) as the minimal eigenvalue of NTK Θ and define ηcritical ≜ 2(λmin(Θ)+λmax(Θ))[−][1],
Lee et al. (2019a) have characterized the training dynamics of infinite-wide neural networks as below.

**Theorem 3 (Lee et al. (2019a)). Let n1 =** = nL 1 = k and assume λmin(Θ) > 0. Applying
_· · ·_ _−_
_gradient descent with learning rate η < ηcritical (or gradient flow), for every x ∈_ R[n][0] _with ∥x∥≤_ 1,
_with probability arbitrarily close to 1 over random initialization,_

supt 0 **_ft −_** **_ft[lin]_** 2 [=][ O][(][k][−] 2[1] ) _as k →∞_ _._
_≥_

**Remark.** For the case of L = 2, Du et al. (2019) have revealed that if any two input vectors
of a dataset are not parallel, then λmin(Θ) > 0 holds, which fortunately can be well-satisfied for
most real-world datasets. Though the training dynamics of DNNs only tend to be governed by their
linearization at initialization when the infinite width is satisfied as revealed in Theorem 3, empirical
results in (Lee et al., 2019a) suggest that such linearization can also govern the training dynamics of
practical over-parameterized DNNs.


-----

A.3 PROOF OF PROPOSITION 1

With aforementioned theorems, especially Theorem 3, our Proposition 1 can be proved as below with
an introduced lemma (Lemma 1).
**Lemma 1. Let loss function L (f** (X ; θt), Y), abbreviated to L(ft), be γ-Lipschitz continuous within
_the domain V. Under the condition in Theorem 3, there exists a constant c0 > 0 such that as k →∞,_
(ft) (ft[lin][)] 2
_L_ _−L_ _[≤]_ _[c]√[0]k[γ]_

_with probability arbitrarily close to 1._

_Proof. Since_ (ft) is γ-Lipschitz continuous, for any v, u, we have
_L_ _∈V_

(v) (u) 2 _γ_ **_v_** **_u_** 2 (19)
_∥L_ _−L_ _∥_ _≤_ _∥_ _−_ _∥_

following the definition of Lipschitz continuity. Besides, under the condition in Theorem 3, Theorem 3
reveals that there exists a constant c0 such that as k →∞,
**_ft_** **_ft[lin]_** 2 (20)
_−_ _[≤]_ _√[c][0]k_

with probability arbitrarily close to 1. Combining (19) and (20), we hence can finish the proof by
(ft) (ft[lin][)] 2 **_ft_** **_ft[lin]_** 2 as k _._ (21)
_L_ _−L_ _[≤]_ _[γ]_ _−_ _[≤]_ _[c]√[0]k[γ]_ _→∞_

**Remark. The γ-Lipschitz continuity based on ∥· ∥2 is commonly satisfied for widely adopted loss**
functions. For example, given 1-dimensional MSE L = m[−][1][ P][m]i [(][x][i][ −] _[y][i][)][2][, let][ x][i][, y][i][ ∈]_ [[0][,][ 1]][ denote]
the prediction and label respectively, the Lipschitz of MSE with respect to x ≜ (x1, _, xm)[⊤]_ is 2.
_n_ _· · ·_
Meanwhile, given the n-class Cross Entropy with Softmax L = −m[−][1][ P][m]i _j_ _[y][i,j][ log(][p][i,j][)][ as the]_
loss function, let pi,j (0, 1) and yi,j 0, 1 denote the prediction and label correspondingly, with
_∈_ _∈{_ _}_ P

_j_ _[y][i,j][ = 1][ and][ p][i,j][ ≜]_ [exp(][x][i,j][)][/][ P]j [exp(][x][i,j][)][ for input][ x][i,j][ ∈] [R][, the Lipschitz of Cross Entropy]

with Softmax with respect toP **_x ≜_** (x1,1, · · ·, xi,j, · · ·, xm,n)[⊤] is then 1.

**Proof of Proposition 1.** Note that the linearization f [lin](x; θt) achieves a constant NTK Θt = Θ0
because its gradient with respect to θt (i.e., ∇θ0 **_f_** (x; θ0)) stays constant over time. Given MSE
as the loss function, according to the loss decomposition (4) in Sec. 2.2, the training dynamics of
**_f_** [lin](x; θt) can then be analyzed in a closed form:

_mn_

(ft[lin][) = 1] (1 _ηλi)[2][t](u[⊤]i_ (22)
_L_ _m_ _i=1_ _−_ _[Y][)][2][,]_

X

where Θ0 = _i=1_ _[λ][i][(][Θ][0][)][u][i][u]i[⊤][, and][ λ][i][(][Θ][0][)][ and][ u][i][ denote the][ i][-th largest eigenvalue and the]_
corresponding eigenvector of Θ0, respectively. With η < λmax(Θ0)[−][1] and λmin(Θ0) > 0, ηλi(Θ0)
is then under the constraint that

[P][mn]

0 < ηλi(Θ0) < [λ][max][(][Θ][0][)] (23)

_λmax(Θ0)_

_[⇒]_ [0][ < ηλ][i][(][Θ][0][)][ <][ 1][ .]

Hence, for the case of t ≥ 0.5, with 0 < 1 − _ηλi(Θ0) < 1 and λ(Θ0) ≜_ (mn)[−][1][ P][mn]i=1 _[λ][i][(][Θ][0][)][,]_

_mn_ _mn_

(1 _ηλi(Θ0))[2][t]_ (1 _ηλi(Θ0))_
_i=1_ _−_ _≤_ _i=1_ _−_ (24)

X X

= mn(1 _ηλ(Θ0)) ._
_−_

Further, given 0 _t < 0.5, the scalar function y = x[2][t]_ is concave for any x R 0. Following from
_≤_ _∈_ _≥_
the Jensen’s inequality on this concave function,

_mn_ _mn_ 2t

1

(1 _ηλi(Θ0))[2][t]_ _mn_ (1 _ηλi(Θ0))_
_i=1_ _−_ _≤_ " _mn_ _i=1_ _−_ # (25)

X X

= mn(1 _ηλ(Θ0))[2][t]_ _._
_−_


-----

With bounded labels Y ∈ [0, 1][mn] and the unit norm of eigenvectors ∥ui∥2 = 1,

(u[⊤]i 2[∥Y∥][2]2 2 (26)

_[Y][)][2][ ≤∥][u][i][∥][2]_ _[≤∥Y∥][2]_ _[≤]_ _[mn .]_

By introducing (24), (25) and (26) into (22), the training loss L(ft[lin][)][ can then be bounded by]

_mn_

(ft[lin][)][ ≤] [(][mn][)][ ·][ 1] (1 _λi(Θ0))[2][t]_
_L_ _m_ _i=1_ _−_ (27)

X

_mn[2](1_ _ηλ(Θ0))[q]_ _,_
_≤_ _−_

where q is set to be 2t if 0 ≤ _t < 0.5, and 1 otherwise._

Following from Theorem 3, while applying gradient descent (or gradient flow) with learning rate
_η < λ[−]max[1]_ _[< η][critical]_ [≜] [2(][λ][min] [+][ λ][max][)][−][1][, for all][ x][ ∈X][ with][ ∥][x][∥≤] [1][, there exists a constant][ c][0]
such that as k →∞,
**_ft_** **_ft[lin]_** 2 (28)
_−_ _[≤]_ _√[c][0]k_

_mn_

with probability arbitrarily close to 1. Hence, ft[lin] _c0_ 1/k, 1 + c0 1/k with ft
_∈_ _−_ _mn_ _∈_

[0, 1][mn]. Within the extended domain ft _c0_ 1/k, 1 +h _c0_ p1/k for the MSE loss function,p i
_∈_ _−_
h p p i

**_ft_** 2 = [2]
_∥∇_ _L∥_ _m_

(29)

_[∥Y −]_ **_[f][t][∥][2]_**

2 _n/m_ 1 + c0 1/k _._
_≤_
p  p 


Hence, the MSE within this extended domain is 2

Combining with Lemma 1,


1/k -Lipschitz continuous.



_n/m_ 1 + c0



(ft) (ft[lin][) + 2][c][0]
_L_ _≤L_


_n/(mk)_ 1 + c0



1/k


(30)
1/k _,_



_mn[2](1_ _ηλ)[q]_ + 2c0
_≤_ _−_


_n/(mk)_ 1 + c0



which concludes the proof by denoting Lt ≜ _L(ft)._

**Remark.as data pre-processing in practice. ∥x∥2 ≤** 1 can be well-satisfied for the normalized dataset, which is conventionally adopted

A.4 PROOF OF PROPOSITION 2

According to Theorem 2, Θ0 ≜ **Θ[(][L][)]** is determined by both Σ[(][L][)](x, x) and Σ[˙] [(][L][)](x, x) of all
**_x ∈X_** . We hence introduce Lemma 2 below regarding Σ[(][L][)](x, x) and Σ[˙] [(][L][)](x, x) to ease the proof
of Proposition 2. Particularly, we set β = 0 for the β in Theorem 1 and Theorem 2 throughout our
analysis. Note that this condition is usually satisfied by training DNNs without bias.

**Lemma 2. For a network of depth L at initialization, with the γ-Lipschitz continuous nonlinearity σ**
_satisfying |σ(x)| ≤|x| for all x ∈_ R, given the input features x ∈X _,_

Σ[(][L][)](x, x) ≤ _n[−]0_ [1][x][⊤][x][,] Σ˙ [(][L][)](x, x) ≤ _γ[2]_ _,_

_where Σ[(][L][)](x, x) and_ Σ[˙] [(][L][)](x, x) are defined in Theorem 1 and Theorem 2, respectively, and β = 0.

_Proof. Following from the definition in Theorem 1,_

Σ[(][L][)](x, x) = Eg∼N (0,Σ(L−1)) [σ(g(x))σ(g(x))]

(a)
_≤_ Eg∼N (0,Σ(L−1)) _g[2](x)_ (31)

(b)
= Σ[(][L][−][1)](x, x),  


-----

in which (a) follows from |σ(x)| ≤|x| and (b) results from the variance of g ∼N (0, Σ[(][L][−][1)]). By
following (31) from layer L to 1, we get

Σ[(][L][)](x, x) ≤ Σ[(1)](x, x) (=a) n[−]0 [1][x][⊤][x][,] (32)

where (a) is defined in Theorem 1.

Similarly, following from the definition in Theorem 2,


Σ˙ [(][L][)](x, x) = Eg∼N (0,Σ(L−1)) [ ˙σ(g(x)) ˙σ(g(x))]

(a)
_≤_ Eg∼N (0,Σ(L−1)) _γ[2][]_

(b)
= γ[2] _,_ 


(33)


in which (a) results from the γ-Lipschitz continuity of nonlinearity and (b) follows from the expectation of a constant.

**Proof of Proposition 2.** Notably, Theorem 2 reveals that in the limit as n1, _, nL_ 1
_· · ·_ _−_ _→∞_
sequentially, Θ[(][L][)] _→_ **Θ[(]∞[L][)]** _[⊗]_ [I]nL [with probability arbitrarily close to 1 over random initializations.]
We therefore only need to focus on this deterministic limiting kernel to simplify our analysis, i.e., let
**Θ0 = Θ[(][L][)]** _nL_ [. Particularly, given][ m][ input vectors][ x][ ∼] _[P]_ [(][x][)][ with covariance matrix][ Σ]P [and a]
_∞_ _[⊗]_ [I]
_L-layer neural architecture of n-dimensional output, For γ ̸= 1, we have_


(mn)[−][1] **Θ0** tr = (mn)[−][1] **Θ[(][L][)]** _L_ _[∥][tr]_
_∥_ _∥_ _∥_ _∞_ _[⊗]_ [I][n]

(a)
= (mn)[−][1]∥Θ[(]∞[L][)][∥][tr][∥][I][n]L _[∥][tr]_

(b)
= m[−][1] **Θ[(][L][)]**
_∥_ _∞_ _[∥][tr]_

(c)
= Ex _P (x)_ **Θ[(][L][)]**
_∼_ _∞_ [(][x][,][ x][)]
h i


(34)


(d)
= Ex _P (x)_ **Θ[(][L][−][1)](x, x) Σ[˙]** [(][L][)](x, x) + Σ[(][L][)](x, x)
_∼_ _∞_

(e) h i
Ex _P (x)_ _γ[2]Θ[(][L][−][1)](x, x) + n[−]0_ [1][x][⊤][x]
_≤_ _∼_ _∞_

(f ) h i
= Ex∼P (x) _γ[2]Θ[(]∞[L][−][1)](x, x)_ + n[−]0 [1]

_[∥][Σ][P][ ∥][tr][,]_
h i

in which (a) derives from the property of Kronecker product, (b) follows from the notation nL = n
and (c) results from the property of expectation and trace norm. In addition, (d) follows from the
definition of Θ[(][L][)]
_∞_ [(][x][,][ x][)][ in Theorem 2 and (e) is derived by introducing Lemma 2 into (d). Finally,]
(f) follows from the expectation and variance of P (x). By following (c-f) from layer L to 1, we can
get

(mn)[−][1] **Θ0** tr _γ[2(][L][−][1)]Ex_ _P (x)_ Σ[(1)](x, x) +
_∥_ _∥_ _≤_ _∼_
h i

_n[−]0_ [1] 1 _γ[2(][L][−][1)][  ]1_ _γ[2][][−][1]_ (35)

_[∥][Σ][P][ ∥][tr]_ _−_ _−_


= n[−]0 [1] 1 _γ[2][L][  ]1_ _γ[2][][−][1]_ _._

_[∥][Σ][P][ ∥][tr]_ _−_ _−_
 

Notably, (35) is derived under the condition that γ ̸= 1. For the case of γ = 1, similarly, we can get

(mn)[−][1]∥Θ0∥tr ≤ _n[−]0_ [1][L][∥][Σ][P][ ∥][tr][ .] (36)

Note that with **_x_** 2 1, we have
_∥_ _∥_ _≤_

ΣP tr = Ex _P (x)_ **_x[⊤]x_** Ex _P (x) [x][⊤]_ Ex _P (x) [x]_
_∥_ _∥_ _∼_ _−_ _∼_ _∼_

Ex _P (x)_ x[⊤]x (37)
_≤_ _∼_

1 .

 

_≤_


-----

Given any two different distributions P (x) and Q(x), define Z = _∥P_ (x) − _Q(x)∥_ dx,[4] we can
construct a special probability density function _P_ (x) = Z _[−][1]_ _P_ (x) _Q(x)_ to further employee
_∥_ R − _∥_
the bounds in (35) and (36). Specifically, for γ ̸= 1, by combining (35) and (37), we can get

[e]

[(][a][)]
(mn)[−][1] _∥Θ0(P_ )∥tr −∥Θ0(Q)∥tr = **Θ[(]∞[L][)][(][x][,][ x][)]** _−_ Ex∼Q(x) **Θ[(]∞[L][)][(][x][,][ x][)]**

h i h i

(b)

[E][x][∼]P[P]([ (]x[x])[)] _Q(x)_ **Θ[(][L][)]**

_≤_ _∥_ _−_ _∥_ _∞_ [(][x][,][ x][)][ d][x] (38)
Z

(c)
_≤_ _n[−]0_ [1][Z][∥][Σ][P][ ∥][tr] 1 − _γ[2][L][  ]1 −_ _γ[2][][−][1]_

(d)

 

_≤_ _n[−]0_ [1][Z] 1 − _γ[2][L][  ]1 −_ _γ[2][][−][1]_ _,_

in which (a) follows from (c) in (34), (b) results from Cauchy Schwarz inequality and (c) is obtained 
by introducing inequality (35) and distribution _P_ (x). Finally, (d) is based on (37).

Similarly, in the case of γ = 1, we can get

[e]

(mn)[−][1] _∥Θ0(P_ )∥tr −∥Θ0(Q)∥tr _≤_ _n[−]0_ [1][ZL,] (39)

which concludes the proof.

**Remark. Note that ReLU is widely adopted as the nonlinearity in neural networks, which satisfies**
the Lipschitz continuity with γ = 1 and the inequality |σ(x)| ≤|x|. Notably, many other ReLU-type
nonlinearities (e.g., Leaky ReLU (Maas et al., 2013) and PReLU (He et al., 2015)) can also satisfy
these two conditions.

APPENDIX B EXPERIMENTAL SETTINGS

B.1 DETERMINATION OF CONSTRAINT ν AND PENALTY COEFFICIENT µ

As demonstrated in Sec. 3.1, constraint ν (derived from ∥Θ0(A)∥tr < mnη[−][1] in (8)) introduces a
trade-off between the complexity of final selected architectures and the optimization behavior in their
model training. This constraint ν hence is of great importance to our NASI algorithm. Though ν
has already been pre-defined as ν ≜ _γnη[−][1]_ in Sec. 3.3, we still tend to take it as a hyper-parameter
to be determined for the selection of best-performing architectures in practice. Specifically, in this
paper, two methods are adopted to determine ν during the search progress: The fixed and adaptive
method shown as below. Notably, the final architectures selected by NASI via the fixed and adaptive
method are called NASI-FIX and NASI-ADA, respectively. Note that our experiments in the main
text suggest that both methods can select architectures with competitive generalization performance
over different tasks.

**The fixed determination.** We initialize and fix ν with ν0 during the whole search process. Hence,
_ν0 is required to provide a good trade-off between the complexity of architectures and their optimiza-_
tion behavior in the search space. Intuitively, the expectation of architecture complexity in the search
space can help to select architectures with medium complexity and hence implicitly achieve a good
trade-off between the complexity of architectures and their optimization behavior. Specifically, we
randomly sample N = 50 architectures in the search space (i.e., 1, _,_ _N_ ), and then determine
_A_ _· · ·_ _A_
_ν0 before the search process by_


_ν = ν0 = N_ _[−][1]_ _∥Θ[e]_ 0(Ai)∥tr . (40)

_i_

X

Note that we can further enlarge ν0 in practice to encourage the selection of architectures with larger
complexity.

4We abuse this integration to ease notations.


-----

**The adaptive determination.** We initialize ν with a relatively large ν0 and then adaptively update
it with the expected ∥Θ[e] 0(A)∥tr of sampled architectures during the search process. Specifically, with
sampled architectures A1, · · ·, At in the history, ν at time t (i.e., νt) during the search process is
given by

_t−1_

_νt = t[−][1]_ _ν0 +_ _τ_ =1 _∥Θ[e]_ 0(Aτ )∥tr! _._ (41)

X

We apply a relatively large ν0 to ensure a loose constraint on the complexity of architectures in
the first several steps of the search process. Note that the adaptive determination provides a more
accurate approximation of the expected complexity of architectures in the search space than the fixed
determination method if more architectures are sampled to update νt, i.e., t > N .

Note that (13) in Sec. 3.3 further reveals the limitation on the complexity of final selected architectures
by the penalty coefficient µ. Particularly, µ=0 indicates no limitation on the complexity of architectures. Following from the introduced trade-off between the complexity of final selected architectures
and the optimization behavior in their model training by the constraint ν, for a search space with
relatively larger-complexity architectures, a larger penalty coefficient µ (i.e., µ = 2) is preferred to
search for architectures with relatively smaller complexity to ensure a well-behaved optimization
with a larger learning rate η. On the contrary, for a search space with relatively smaller-complexity
architectures, a lower penalty coefficient µ (i.e., µ=1) is adopted to ensure the complexity and hence
the representation power of the final selected architectures. Appendix C.6 provides further ablation
study on the constraint ν and the penalty coefficient µ.

B.2 THE DARTS SEARCH SPACE

Following from the DARTS (Liu et al., 2019) search space, candidate architecture in our search space
comprise a stack of L cells, and each cell can be represented as a directed acyclic graph (DAG) of
_N nodes denoted by {x0, x1, . . ., xN_ _−1}. To select the best-performing architectures, we instead_
need to select their corresponding cells, including the normal and reduction cell. Specifically, x0
and x1 denote the input nodes, which are the output of two preceding cells, and the output xN of a
cell is the concatenation of all intermediate nodes from x2 to xN 1. Following that of SNAS (Xie
_−_
et al., 2019), by introducing the distribution of architecture in the search space (i.e., pα( ) in (14)),
_A_
each intermediate nodes xi (2 ≤ _i ≤_ _N −_ 1) denotes the output of a single sampled operation
_oi_ _pi(o) with_ _o_ _[p][i][(][o][) = 1][ given a single sampled input][ x][j][ ∼]_ _[p][i][(][x][)][ with][ j][ ∈]_ [0][,][ · · ·][, i][ −] [1][ and]
_∼i_ 1 _∈O_

_−_
_j=0_ _[p][i][(][x][j][) = 1][, where][ O][ is a predefined operation set. After the search process, only operations]_
achieving the top-2 largest[P] _pi(o) and inputs achieving the top-2 largest pi(x) for node xi are retained._
P
Each intermediate node xj hence connects to two preceding nodes with the corresponding selected
operations.

Following from DARTS, the candidate operation set O includes following operations: 3 × 3 max
pooling, 3 _×_ 3 avg pooling, identity, 3 _×_ 3 separable conv, 5 _×_ 5 separable conv, 3 _×_ 3 dilated separable
conv, 5 × 5 dilated separable conv. Note that our search space has been modified slightly based on the
standard DARTS (Liu et al., 2019) search space: (a) Operation zero is removed from the candidate
operation set in our search space since it can never been selected in the standard DARTS search
space, and (b) the inputs of each intermediate node are selected independently from the selection
of operations, while DARTS attempts to select the inputs of intermediate nodes by selecting their
coupling operations with the largest weights. Notably, following from DARTS, we need to search
for two different cells: A normal cell and a reduction cell. Besides, a max-pooling operation in
between normal and reduction cell is applied to down-sampling intermediate features during the
search process inspired by NAS-Bench-1Shot1 (Zela et al., 2020b).

B.3 SAMPLING ARCHITECTURE WITH GUMBEL-SOFTMAX

Notably, aforementioned pi(o) and pi(x) for the node xi follow the categorical distributions. To
relax the optimization of (14) with such categorical distributions to be continuous and differentiable,
Gumbel-Softmax (Jang et al., 2017; Maddison et al., 2017) is applied. Specifically, supposing pi(o)
and pi(x) are parameterized by αi,j[o] [and][ α]i,k[x] [respectively, with Straight-Through (ST) Gumbel]
Estimator (Jang et al., 2017), we can sample the single operation and input for the node xi (2 ≤ _i ≤_


-----

_N −_ 1) during the search process by

_j[∗]_ = arg max
_j_

_k[∗]_ = arg max


exp((αi,j[o] [+][ g]i,j[o] [)][/τ] [)]

_j_ [exp((][α]i,j[o] [+][ g]i,j[o] [)][/τ] [)]

(42)

exp((αi,k[x] [+][ g]i,k[x] [)][/τ] [)]

_k_ [exp((][α]i,k[x] [+][ g]i,k[x] [)][/τ] [)][,]


where gi,j[o] [and][ g]i,k[x] [are sampled from Gumbel(0, 1) and][ τ][ denotes the softmax temperature, which]
is conventionally set to be 1 in our experiments. Note that in (42), j ∈ [0, · · ·, |O| − 1] and
_k ∈_ [0, · · ·, i − 1], which correspond to the |O| operations in the operation set O and the i candidate
inputs for the node xi, respectively. Notably, the gradient through ST can be approximated by
its continuous counterpart as suggested in (Jang et al., 2017), thus allowing the continuous and
differentiable optimization of (14). By sampling the discrete operation and input with (42) for each
nodes in the cells, the final sampled architecture can hence be determined.

B.4 EVALUATION ON CIFAR-10/100 AND IMAGENET

**Evaluation on CIFAR-10/100.** Following DARTS (Liu et al., 2019), the final selected architectures
consist of 20 searched cells: 18 of them are identical normal cell and 2 of them are identical reduction
cell. An auxiliary tower with weight 0.4 is located at 13-th cell of the final selected architectures
and the number of initial channels is set to be 36. The final selected architecture are then trained via
stochastic gradient descent (SGD) of 600 epochs with a learning rate cosine scheduled from 0.025
to 0, momentum 0.9, weight decay 3 × 10[−][4] and batch size 96 on a single Nvidia 2080Ti GPU.
Cutout (Devries & Taylor, 2017), and ScheduledDropPath linearly increased from 0 to 0.2 are also
employed for regularization purpose.

**Evaluation on ImageNet.** We evaluate the transferability of the selected architectures from CIFAR10 to ImageNet. The architecture comprises of 14 cells (12 normal cells and 2 reduction cells). To
evaluate in the mobile setting (under 600M multiply-add operations), the number of initial channels
is set to 46. We adopt conventional training enhancements (Liu et al., 2019; Chen et al., 2019;
Chen & Hsieh, 2020) include an auxiliary tower loss of weight 0.4 and label smoothing. Following
P-DARTS (Chen et al., 2019) and SDARTS-ADV (Chen & Hsieh, 2020), we train the model from
scratch for 250 epochs with a batch size of 1024 on 8 Nvidia 2080Ti GPUs. The learning rate
is warmed up to 0.5 for the first 5 epoch and then decreased to zero linearly. We adopt the SGD
optimizer with 0.9 momentum and a weight decay of 3 × 10[−][5] for the model training on ImageNet.

APPENDIX C MORE RESULTS

C.1 TRADE-OFF BETWEEN MODEL COMPLEXITY AND OPTIMIZATION BEHAVIOUR

In this section, we empirically validate the existence of trade-off between model complexity of
selected architectures and the optimization behavior in their model training for our reformulated NAS
detailed in Sec. 3.1.

**The trade-off in NAS-Bench-1Shot1.** Specifically, Figure 3 illustrates the relation between test
error and approximated ∥Θ0(A)∥tr of candidate architectures in the three search spaces of NASBench-1Shot1. Note that with the increasing of the approximated ∥Θ0(A)∥tr, the test error decreases
rapidly to a minimum and then increase gradually, which implies that architectures only with certain
_∥Θ0(A)∥tr (or with desirable complexity instead of the largest complexity) can achieve the best_
generalization performance. These results hence validate the existence of trade-off between the
model complexity and the generalization performance of selected architectures. Note that such
trade-off usually results from different optimization behavior in the model training of those selected
architectures as demonstrated in the following experiments.

**The trade-off in the DARTS search space.** We then illustrate the optimization behavior of the final
selected architecture from the DARTS search space with different constraint and penalty coefficient in
Figure 4 to further confirm the existence of such trade-off in our reformulated NAS (8). Specifically,


-----

1e−1 [Search Space 1]

2.0 2.0 [1e−1][ Search Space 2] 2.0 [1e−1][ Search Space 3]

1.5 1.5 1.5

1.0 1.0 1.0

Test Error (%)

0.5 0.5 0.5

0 2 4 0 2 4 0 2 4

1e2 1e2 1e2

Approximated Trace Norm of NTK


Figure 3: The relation between test error and approximated ∥Θ0(A)∥tr of candidate architectures
in the three search spaces of NAS-Bench-1Shot1 over CIFAR-10. Note that x-axis denotes the
approximated ∥Θ0(A)∥tr, which is averaged over the architectures grouped in the same bin based on
their approximated ∥Θ0(A)∥tr. Correspondingly, y-axis denotes the averaged test error with standard
deviation (scaled by 0.5) of these grouped architectures. In addition, the red lines demonstrate the
smallest test errors achieved by candidate architectures in these three search spaces.

12.5 º = 100; p = 4:27 12.5 ¹ = 2; p = 4:27


7.5

5.0

2.5

|Col1|Col2|
|---|---|
|¹ = 1; p = 5:27 ¹ = 2; p = 4:27 ¹ = 3; p = 4:13||


200 400 600

Epoch


15.0

º = 010; p = 2:26

12.5 º = 100; p = 4:27

º = 200; p = 4:49

10.0

7.5

Test Error (%)

5.0

2.5

0 200 400 600

Epoch


(a) Constraint ν (b) Penalty coefficient µ

Figure 4: The optimization behavior (test error on CIFAR-10 in model training) of the final selected
architectures with different constraint ν and penalty coefficient µ. The model parameter (MB) denoted
by p of each architecture is given in the top-right corner.


we apply NASI with the fixed determination of ν to select architectures in the the DARTS search
space over CIFAR-10, where ν0 in the fixed determination method introduced in Appendix B.1 is
modified manually for the comparison. Besides, a search budget of T = 100 and batch size of b = 64
are adopted. These final selected architectures are then trained on CIFAR-10 following Appendix 5.2.

Note that, in Figure 4(a), with the increasing of ν, the final selected architectures by NASI contain
more parameters and hence achieve larger complexity. Meanwhile, the final selected architecture with
_ν=10 enjoys a faster convergence rate in the first 50 epochs, but a poorer generalization performance_
than the one with ν= 200. Interestingly, the final selected architecture with ν=100 realizes the
fastest convergence and the best generalization performance by achieving a proper complexity of final
selected architectures. These results validate the existence and also the importance of the trade-off
between the complexity of final selected architectures and the optimization behavior in their model
training. However, the trade-off introduced by the penalty coefficient µ shown in Figure 4(b) is hard to
be observed, which indicates that the constraint ν is of greater importance than the penalty coefficient
_µ in terms of the trade-off between the complexity of architectures and their optimization behavior._
Interestingly, Figure 4(b) can still reveal the slower convergence rate and poorer generalization
performance caused by the selection of architecture with relatively larger complexity, i.e., µ=1.

C.2 COMPARISON TO OTHER TRAINING-FREE METRICS


We compare our methods with other training-free NAS methods using both the Spearman correlation
and the Kendall’s tau between the training-free metrics and the test accuracy on CIFAR-10 in the
three search spaces of NAS-Bench-1Shot1. We adopt one uniformly randomly sampled mini-batch
data to evaluate these two correlations and apply the same implementations of these training-free


-----

Table 3: Comparison of the Spearman correlation and the Kendall’s tau for various training-free
metrics in the three search spaces of NAS-Bench-1Shot1 on CIFAR-10.

**Spearman Correlation** **Kendall’s Tau**
**Methods**

**S1** **S2** **S3** **S1** **S2** **S3**

SNIP (Lee et al., 2019b) _−0.49_ _−0.62_ _−0.79_ _−0.39_ _−0.49_ _−0.63_
GraSP (Wang et al., 2020) 0.41 0.54 0.17 0.33 0.42 0.15
SynFlow (Tanaka et al., 2020) _−0.52_ _−0.45_ _−0.53_ _−0.42_ _−0.40_ _−0.47_
NASWOT (Mellor et al., 2021) 0.21 0.32 0.54 0.16 0.24 0.44

NASI (conditioned) **0.62** **0.74** **0.76** **0.44** **0.53** **0.53**

Table 4: The comparison among state-of-the-art (SOTA) NAS algorithms on NAS-Bench-201. The
performance of the final architectures selected by NASI is reported with the mean and standard
deviation of four independent trials. The search costs are evaluated on a single Nvidia 1080Ti.

**Test Accuracy (%)** **Search Cost**
**Architecture** **Search Method**
**(GPU Sec.)**

**C10** **C100** **IN-16-120**

ResNet (He et al., 2016) 93.97 70.86 43.63 -  - 


ENAS (Pham et al., 2018) 54.30 15.61 16.32 13315 RL
DARTS (1st) (Liu et al., 2019) 54.30 15.61 16.32 10890 gradient
DARTS (2nd) (Liu et al., 2019) 54.30 15.61 16.32 29902 gradient
GDAS (Dong & Yang, 2019) 93.61±0.09 70.70±0.30 41.84±0.90 28926 gradient

NASWOT (N=10) (Mellor et al., 2021) 92.44±1.13 68.62±2.04 41.31±4.11 3 training-free
NASWOT (N=100) (Mellor et al., 2021) 92.81±0.99 69.48±1.70 43.10±3.16 30 training-free
NASWOT (N=1000) (Mellor et al., 2021) 92.96±0.81 69.98±1.22 44.44±2.10 306 training-free
TE-NAS (Chen et al., 2021) 93.90±0.47 71.24±0.56 42.38±0.46 1558 training-free
KNAS (Xu et al., 2021) 93.05 68.91 34.11 4200 training-free

NASI (T ) 93.08±0.24 69.51±0.59 40.87±0.85 30 training-free
NASI (4T ) 93.55±0.10 71.20±0.14 44.84±1.41 120 training-free

NAS methods in (Abdelfattah et al., 2021) for the comparison. Table 3 summarizes the comparison,
where the results of our metric are reported under the constraint in (8). Interestingly, our metric
generally achieves a higher positive correlation than other training-free metrics, which confirms the
reasonableness and also the effectiveness of our training-free metric.

C.3 SEARCH IN NAS-BENCH-201

To further justify the improved search efficiency and competitive search effectiveness of our NASI
algorithm, we also compare it with other state-of-the-art NAS algorithms in NAS-Bench-201 (Dong &
Yang, 2020) on CIFAR-10/100 (C10/100) and ImageNet-16-200 (IN-16-200) [5]. Table 4 summarizes
the comparison. Note that the baselines in Table 4 are obtained from TE-NAS (Chen et al., 2021)
paper. Notably, compared with training-based NAS algorithms, our NASI algorithm can achieve
significantly improved search efficiency while maintaining a competitive or even outperforming
test performance. Furthermore, our NASI algorithm is shown to be able to enjoy both improved
search efficiency and effectiveness when compared with most other training-free baselines. Although
TE-NAS, as the best-performing training-free NAS algorithm on both CIFAR-10/100, achieves a
relatively improved test accuracy than our NASI (T ), our NASI with a search budget of T = 30s is
50× more efficient than TE-NAS and is able to achieve compelling test performance on all the three
datasets. Moreover, by providing a larger search budget, our NASI algorithm (i.e., NASI (4T )) can in
fact achieve comparable (on CIFAR-10/100) or even better (on ImageNet-16-120) search results with
12× lower search cost compared with TE-NAS.

C.4 ARCHITECTURES SELECTED BY NASI

The final selected cells in the DARTS search space (i.e., NASI-FIX and NASI-ADA used in Sec. 5.2)
are illustrated in Figure 5, where different operations are denoted with different colors for clarification.

5ImageNet-16-200 is a down-sampled variant of ImageNet (ImageNet16×16) (Chrabaszcz et al., 2017)


-----

Dil-3 Dil-3 Sep-3 Dil-5 Sep-3 Sep-3 Sep-5 Max Max Sep-5 Sep-3 Sep-3 Sep-3

x_2 x_3 x_4 x_5 x_2 x_3 x_4 x_5

x_6 x_6

x_0 x_1

Dil-3 Dil-3 Sep-3 Dil-5 Sep-3 Sep-3

Dil-5

x_2 x_3 x_4 x_5

x_6

x_0 x_1

Max Max Sep-5 Sep-3 Sep-3 Sep-3

Sep-3 Sep-3

x_2 x_3 x_4 x_5

x_6


(a) NASI-FIX (Normal) (b) NASI-FIX (Reduction)

x_0 x_1 x_0 x_1

Sep-5 Dil-5 Dil-3 Dil-3 Sep-3 Sep-3 Sep-5 Max Max Sep-5 Sep-5 Sep-3 Sep-3

x_2 x_3 x_4 x_5 x_2 x_3 x_4 x_5

x_6 x_6

x_0 x_1

Max Max Sep-5 Sep-5 Sep-3 Sep-3

Skip Sep-3

x_2 x_3 x_4 x_5

x_6

x_0 x_1

Sep-5 Dil-5 Dil-3 Dil-3 Sep-3 Sep-3

Avg

x_2 x_3 x_4 x_5

x_6


(c) NASI-ADA (Normal) (d) NASI-ADA (Reduction)

Figure 5: The final selected normal and reduction cells of NASI-FIX and NASI-ADA in the reduced
DARTS search space on CIFAR-10. Note that x0, x1 denote the input nodes, x2, x3, x4, x5 denote
intermediate nodes and x6 denotes the output node as introduced in Appendix B.2.


-----

Table 5: Performance comparison among SOTA image classifiers on ImageNet. Note that architectures followed by C10 are transferred from the CIFAR-10 dataset, while architectures followed by
ImageNet are directly selected on ImageNet.

**Test Error (%)** **Params** + **Search Cost**
**Architecture** _×_
**(M)** **(M)** **(GPU Days)**
**Top-1** **Top-5**

Inception-v1 (Szegedy et al., 2015) 30.1 10.1 6.6 1448 - 
MobileNet (Howard et al., 2017) 29.4 10.5 4.2 569 - 
ShuffleNet 2×(v2) (Ma et al., 2018) 25.1 7.6 7.4 591 - 

NASNet-A (Zoph et al., 2018) 26.0 8.4 5.3 564 2000
AmoebaNet-A (Real et al., 2019) 25.5 8.0 5.1 555 3150
PNAS (Liu et al., 2018) 25.8 8.1 5.1 588 225
MnasNet-92 (Tan et al., 2019) 25.2 8.0 4.4 388 - 

DARTS (Liu et al., 2019) 26.7 8.7 4.7 574 4.0
SNAS (mild) (Xie et al., 2019) 27.3 9.2 4.3 522 1.5
GDAS (Dong & Yang, 2019) 26.0 8.5 5.3 581 0.21
ProxylessNAS (Cai et al., 2019) 24.9 7.5 7.1 465 8.3
P-DARTS (Chen et al., 2019) 24.4 7.4 4.9 557 0.3
DARTS- (Chu et al., 2020) 23.8 7.0 4.5 467 4.5
SDARTS-ADV (Chen & Hsieh, 2020) 25.2 7.8 5.4 594 1.3

TE-NAS (C10) (Chen et al., 2021) 26.2 8.3 5.0 -  0.05
TE-NAS (ImageNet) (Chen et al., 2021) 24.5 7.5 5.4 -  0.17

NASI-FIX (C10) 24.3 7.3 5.2 585 **0.01**
NASI-FIX (ImageNet) 24.4 7.4 5.5 615 **0.01**
NASI-ADA (C10) 25.0 7.8 4.9 559 **0.01**
NASI-ADA (ImageNet) 24.8 7.5 5.2 585 **0.01**

Interestingly, according to the definitions and findings in (Shu et al., 2020), these final selected cells by
NASI are relatively deeper and shallower than the ones selected by other NAS algorithms and hence
may achieve worse generalization performance. Nonetheless, in our experiments, the architectures
constructed with these cells (i.e., NASI-FIX and NASI-ADA) achieve competitive or even better
generalization performance than the ones selected by other NAS algorithms as shown in Table 1 and
Table 5. A possible explanation is that our NASI algorithm provides a good trade-off between the
complexity of architectures and the optimization in their model training, while other NAS algorithms
implicitly prefer architectures with smaller complexity and faster convergence rate as revealed in (Shu
et al., 2020). Note that the final selected cells in NASI-FIX and NASI-ADA are of great similarity
to each other, which hence implies that the adaptive determination and the fixed determination of
constraint ν share similar effects on the selection of final architectures.

C.5 EVALUATION ON IMAGENET

We also evaluate the performance of the final architectures selected by NASI on ImageNet and
summarize the results in Table 5. Notably, NASI-FIX and NASI-ADA outperform the expertdesigned architecture ShuffleNet 2×(v2), NAS-based architecture MnasNet-92 and DARTS by a
large margin, and are even competitive with best-performing one-shot NAS algorithm DARTS-.
Notably, while achieving better generalization performance than TE-NAS (ImageNet), NASI (C10)
is even shown to be more efficient by directly transferring the architectures selected on CIFAR-10
to ImageNet based on its provable transferability in Sec. 4. Meanwhile, by directly searching on
ImageNet, NASI-ADA (ImageNet) is able to achieve further improved performance over NASI-ADA
(C10) while the performance of NASI-FIX (ImageNet) and NASI-FIX (C10) are quite similar.[6]
Above all, these results on ImageNet further confirm the good transferability of the architectures
selected by NASI to larger-scale datasets.

6In order to maintain the same initial channels between NASI-FIX (ImageNet) and NASI-FIX (C10), the
multiply-add operations of NASI-FIX (ImageNet) has to be larger than 600M by a small margin.


-----

Table 6: Search with varying architecture widths N in the three search spaces of NAS-Bench-1Shot1.

**Spaces** _N = 2_ _N = 4_ _N = 8_ _N = 16_ _N = 32_

S1 7.0±0.6 8.3±1.8 8.0±2.2 7.3±1.5 6.5±0.2
S2 7.3±0.7 8.0±1.5 7.3±0.6 7.3±0.4 7.0±0.2
S3 7.6±0.8 7.7±1.1 6.8±0.1 6.8±0.3 6.4±0.2

Table 7: Pearson correlation between our NTK trace norm approximation and the exact NTK trace
norm in the three search spaces of NAS-Bench-1Shot1.

**Mini-batch**
**Spaces** **Sum**

_b = 4_ _b = 8_ _b = 16_ _b = 32_ _b = 64_ _b = 128_

S1 0.74 0.80 0.84 0.83 0.85 0.86 0.88
S2 0.82 0.87 0.90 0.89 0.91 0.91 0.92
S3 0.66 0.74 0.81 0.79 0.82 0.84 0.87

C.6 ABLATION STUDY AND ANALYSIS

**The impacts of architecture width.** As our theoretically grounded performance estimation of
neural architectures relies on an infinite width assumption (i.e., n →∞ in Proposition 1), we
investigate the impacts of varying architecture width on the generalization performance of final
selected architectures by our NASI. We adopt the same search settings in Sec. 5.1 but with a varying
architecture width (i.e., a varying number of initial channels) on CIFAR-10 for the three search spaces
of NAS-Bench-1Shot1. Table 6 shows results of the effectiveness of our NASI in NAS-Bench-1Shot1
with varying architecture widths N : NASI consistently selects well-performing architectures and a
larger width (N = 32) enjoys better search results. Hence, the infinite width assumption for NTK
does not cause any empirical issues for our NASI.

**The effectiveness of NTK trace norm approximations.** Since the trace norm of NTK is costly
to evaluate, we have provided our approximation to it in Sec. 3.2. In this section, we empirically
validate the effectiveness of our NTK trace norm approximation. Specifically, we evaluate the Pearson
correlation between our approximations (including the approximations using the sum of sample
gradient norm in the first inequality of (11) and the approximations using mini-batch gradient norm
in (12)) and the exact NTK trace norm under varying batch size in the three search spaces of NASBench-1Shot1. The results are summarized in Table 7. The results confirm that our approximation
is reasonably good to estimate the exact NTK trace norm of different architectures by achieving a
high Pearson correlation between our approximations and the exact NTK trace norm. Interestingly, a
larger batch size of mini-batch gradient norm generally achieves a better approximation, and the sum
of sample gradient norm achieves the best approximation, which can be explained by the possible
approximation errors we introduced when deriving our (11) and (12).

**The impacts of batch size.** Since we only adopt a mini-batch to approximate the NTK trace norm
shown in Sec. 3.2, we further examine the impacts of varying batch size on the search results in the
three search spaces of NAS-Bench-1Shot1 in this section. Table 8 summarizes the search results.
Interestingly, the results show that our approximation under varying batch sizes achieves comparable
search results, further confirming the effectiveness of our approximations in select well-performing
architectures.

**The impacts of constraint ν and penalty coefficient µ.** Based on the analysis in Sec. 3.3 and the
results in Appendix C.1, the choice of constraint ν and penalty coefficient µ is thus non-trivial to
select best-performing architectures since they trade off the complexity of final selected architectures
and the optimization in their model training. In this section, we demonstrate their impacts on the
generalization performance of the finally selected architectures and also the effectiveness of our fixed
determination on the constraint ν in detail. Notably, we adopt the same settings (including the search
and training settings) as those in Appendix C.1 on the DARTS search space, where ν0 in the fixed
determination method introduced in Appendix B.1 is modified manually for the comparison.


-----

Table 8: Search with varying batch sizes b in the three search spaces of NAS-Bench-1Shot1.

**Spaces** _b = 8_ _b = 16_ _b = 32_ _b = 64_ _b = 128_

S1 6.8±0.3 6.7±0.2 6.7±0.1 6.0±0.3 7.0±0.3
S2 6.9±0.2 7.3±0.4 7.1±0.2 7.2±0.3 6.8±0.2
S3 7.0±0.3 6.6±0.2 6.7±0.2 6.6±0.1 7.1±0.2

|10|50|100 º|150|200|
|---|---|---|---|---|
|10.11|3.23|3.23|3.09|3.39|
|4.07|3.27|2.70|3.25|3.48|
|4.07|3.16|2.78|3.19|3.20|
|4.07|3.16|2.91|2.94|3.36|


10.11 3.23 3.23 3.09 3.39

4.07 3.27 2.70 3.25 3.48

4.07 3.16 2.78 3.19 3.20

4.07 3.16 2.91 2.94 3.36


Test Error (%)

2.70 3.30 3.90 4.50


3.5

3.0

0 2 4

¹

4

Test Error (%)

3

1 2

º 1e2


(a) Joint impacts (b) Individual impacts

Figure 6: The impacts of constraint ν and penalty coefficient µ on the generalization performance of
the final selected architectures by NASI: (a) their joint impacts, and (b) their individual impacts.


Figure 6 summarizes their impacts into two parts: the joint impacts in Figure 6(a) and the individual
impacts in Figure 6(b). Notably, in both plots, with the increasing of ν or µ, the test error of the final
selected architecture by NASI gradually decreases to a minimal and then increase steadily, hence
further validating the existence of the trade-off introduced by ν and µ. Moreover, the final selected
architectures with ν=100 significantly outperform other selected architectures. This result thus
supports the effectiveness of our fixed determination method for ν in NASI. Interestingly, Figure 6(b)
reveals a less obvious decreasing trend of test error for µ compared with ν, which hence further
implies the greater importance of constraint ν than penalty coefficient µ in terms of the generalization
performance of the final selected architectures. Therefore, in our experiments, we conventionally set
penalty coefficient µ=1 for small-complexity search spaces and µ=2 for large-complexity search
spaces as introduced in Appendix B.1.


-----

