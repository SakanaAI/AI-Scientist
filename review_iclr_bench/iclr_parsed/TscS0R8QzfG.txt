# PDAML: A PSEUDO DOMAIN ADAPTATION PARADIGM FOR SUBJECT-INDEPENDENT EEG-BASED EMOTION RECOGNITION

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Domain adaptation (DA) and domain generalization (DG) methods have been
successfully adopted to alleviate the domain shift problem caused by the subject variability of EEG signals in subject-independent affective brain-computer
interfaces (aBCIs). Usually, the DA methods give relatively promising results
than the DG methods but require additional computation resources each time a
new subject comes. In this paper, we first propose a new paradigm called Pseudo
_Domain Adaptation (PDA), which is more suitable for subject-independent aB-_
CIs. Then we propose the pseudo domain adaptation via meta-learning (PDAML)
based on PDA. The PDAML consists of a feature extractor, a classifier, and a sumdecomposable structure called domain shift governor. We prove that a network
with a sum-decomposable structure can compute the divergence between different domains effectively in theory. By taking advantage of the adversarial learning
and meta-learning, the governor helps PDAML quickly generalize to a new domain using the target data through a few self-adaptation steps in the test phase.
Experimental results on the public aBICs dataset demonstrate that our proposed
method not only avoids the additional computation resources of the DA methods
but also reaches a similar generalization performance of the state-of-the-art DA
methods.

1 INTRODUCTION

Affective brain-computer interfaces (aBCIs), which focus on developing machines to recognize human emotion automatically and provide more humanized interaction, have attracted widespread
attention from academics and industries (M¨uhl et al. (2014); Shanechi (2019)). Various studies have
demonstrated that the electroencephalography (EEG) signal is especially reliable to recognize human emotion in the subject-dependent emotion model, in which the training and test data are from
one subject (Jenke et al. (2014); Alarc˜ao & Fonseca (2019)). However, due to the non-stationary nature of EEG signal and structural variability between different subjects, subject-independent model
based on the assumption of Independent Identity Distribution(i.i.d.) usually shows bad generalization performance in real aBCIs applications, which is called the problem of Domain Shift (Sugiyama
et al. (2007); Samek et al. (2013); Sussillo et al. (2016); Zheng & Lu (2016); Li et al. (2018d)).

Domain adaptation (DA) is one of the promising ways to solve this problem. DA uses data from
both source and target domain to promote the adaption performance. One of the most sufficient
studied DA is mapping the two distributions to one common feature space where they have the
same marginal distribution. Though DA has demonstrated significant success in subject-independent
EEG-based emotion recognition (Zheng & Lu (2016); Li et al. (2018b;c); Luo et al. (2018)), the additional computation and time to apply the DA methods is an exasperating problem in real-world
scenarios and causes poor user experience. As a consequence, the concept of domain generalization (DG) arises in situations where multiple source domains can be accessed but unlabeled target
samples are not available. The DG methods have also been successfully adopted to build the subjectindependent emotion model (Ma et al. (2019b;a)). However, since there is no prior information about
the target domain during training, it’s challenging for DG to perform as promising as DA.


-----

A compromise solution is Fast Domain Adaptation (FDA), which trains the main model in advance
and uses a small amount of test samples to adjust efficiently. Even though FDA methods can avoid
consuming time cost by adaptation, most of them need to storage both source and target domains in
the test phase (Chai et al. (2017); Zhao et al. (2021)), which needs extra storage space and causes
poor portability. However, in real-world applications, whether a EEG-based affective model can
adapt to different subjects quickly as well as keep its portability does matter.

In this paper, we propose a new paradigm called Pseudo Domain Adaptation(PDA) for subjectindependent EEG-based emotion recognition. Compared to typical FDA, only target domain is
required in the test phase of PDA, which makes PDA capable to output the prediction more quickly
than DA and FDA. From the perspective of the real-world application, PDA is more suited to build
subject-independent affective model.

To implement PDA, we first prove that with the sum-decomposable structure, a network is equivalent
to domain discrepancy metrics in traditional DA methods like MMD or H-divergence. Based on our
theory, we propose a method named Pseudo Domain Adaptation via Meta Learning(PDAML). The
PDAML consists of a feature extractor, a classifier, and a sum-decomposable structure called domain
shift governor. By taking advantage of the adversarial learning and meta-learning, the governor helps
PDAML quickly generalize to a new domain using the target data through a few self-adaptive steps
in the test phase.

The main contributions of this paper can be summarized as follows:

-  We propose a more suitable paradigm to build subject-independent affective model.

-  We prove that a sum-decomposable network is equivalent to domain discriminator for representing any type of domain discrepancy in theory.

-  We propose PDAML that is portable and can fast adapt to different subjects in EEG-based
emotion recognition.

-  We conduct extensive experiments on the SEED dataset[1], which is a public available EEGbased affective dataset. The experimental results demonstrate that our method has better
performance than DG methods. From the perspective of time and storage cost, the PDAML
performs as well as DG methods.

2 RELATED WORK

aBCIs have received considerable attention very recent. M¨uhl et al. presented the definition of
aBCIs (M¨uhl et al. (2014)) by introducing the affective factors into traditional brain-computer interfaces (BCIs) (Zander & Jatzev (2011)). More applied works focus on studying EEG-based emotion
recognition. Zheng and Lu recruited 15 subjects to watch 15 selected Chinese movie clips to elicit
three emotions: happy, neutral and sad. They developed a public emotion dataset called SEED by
recording the EEG signals of the subjects (Zheng & Lu (2015)). Based on the SEED dataset, researchers have made great progress in developing EEG-based emotion recognition model, especially
for subject-dependent model.

Due to the non-stationary nature of EEG signal and structural variability between different subjects,
it is hard to develop subject-independent EEG-based emotion recognition model by directly using
typical machine learning approaches. Researchers have focused on applying DA and DG methods to
subject-independent EEG-based emotion recognition. Typical DA methods are discrepancy-based,
and they alleviate the domain shift problem by minimizing traditional metrics, such as Maximum
Mean Discrepancy (MMD) (Pan et al. (2011); Long et al. (2017); Wang et al. (2018)), KullbackLeibler (KL) divergence (Zhuang et al. (2015)), and H-divergence (Bendavid et al. (2010)), between
the different domains. Zheng and Lu adopted transfer component analysis (TCA) (Pan et al. (2011)),
which minimizes MMD (Gretton et al. (2007)) between two domains by constructing kernel matrix,
and successfully built personalized EEG-based emotion models (Zheng & Lu (2016)).

Recently, adversarial DA methods have made great successes in different fields (Ganin & Lempitsky
(2015); Tzeng et al. (2017); Shen et al. (2018)). The basic idea of the adversarial training is similar
to generative adversarial networks (GANs) (Goodfellow et al. (2014)), which play an adversarial

1http://bcmi.sjtu.edu.cn/∼seed/index.html


-----

game to make the generated distribution approximate to the real distribution. After the adversarial
training, the data distribution of the target domain is similar to the source domain, and the domain
shift is diminished. Researchers have successfully adopted adversarial DA methods to aBCIs. Li et
_al. (Li et al. (2018b)) adopted domain-adversarial neural networks (DANN) (Ganin & Lempitsky_
(2015)) to EEG-based emotion recognition and improved the recognition accuracies of subjectindependent models. And they (Li et al. (2018c)) also achieved significant performance in subjectindependent vigilance estimation by implementing DANN and adversarial discriminative domain
adaptation (ADDA) (Tzeng et al. (2017)). Luo et al. proposed Wasserstein GAN (Arjovsky et al.
(2017)) adversarial domain adaptation (WGANDA) and successfully adopted it to build subjectindependent emotion recognition models (Luo et al. (2018)).

In real-world aBCIs applications, each subject can be viewed as an individual domain. DA methods,
which require high additional computation resources for each new domain, hinder the development
of aBCIs from lab to real scenarios. DG methods, which can be utilized by data manipulation,
representation learning, or meta-learning (Wang et al. (2021)), aim to generalize to unseeing target domains without additional data collection from target domains. Researchers have focused on
adopting DG methods to aBCIs very recently. Ma et al. generalized the structure of DANN into DG
and proposed an adversarial structure called Domain Residual Network (DResNet). They adopted
DResNet to subject-independent EEG-based vigilance estimation and emotion recognition (Ma et al.
(2019b;a)). The experimental result demonstrated that the DG method could improve the generalization ability without data collection from the target domains. However, the DA methods usually
give relatively promising results than DG methods in aBCIs applications. As a compromise way,
FDA was adopted in aBCIs. Zhao et al. proposed a Plug-and-Play Domain Adaptation (PPDA)
method to fast adapt the model in EEG-based emotion recognition (Zhao et al. (2021)).

Meta-learning, also known as learning to learn, has received a resurgence in interest recently with
applications, one of which is domain generalization. Meta-learning aims to learn episodes sampled
from the related tasks (Finn et al. (2017)). Meta-Learning for Domain Generalization (MLDG) first
introduces meta-learning strategy to DG (Li et al. (2018a)), then MetaReg (Balaji et al. (2018))
and Feature-Critic (Li et al. (2019)) are subsequently proposed to enhance the model’s generalizing
ability by introducing an auxiliary loss in the training. Compared to most previous DG work that
designs a specific model, meta-learning-based DG methods focus on model agnostic training strategy by exposing the model to domain shift in the training phase. To the best of our knowledge, we
are the first to introduce meta-learning based methods to aBCIs tasks.

3 THEORY

The motivation of PDAML is using a simple network to compute domain shift, taking only the target domain as input. Traditional DA methods usually taking two domains as input when comparing
the target domain with a specific source domain. Thus they need extra storage space for source
data and sophisticated methods (e.g. GAN) to represent domain shift in the test phase, which is
time-consuming and storage-consuming. Either or both of these problems obstruct the practical application of DA method in EEG-based diagnosis. However, in multi-source setting, we will show
that minimizing the discrepancy between all pairwise domains is equivalent to minimizing the discrepancy between each domain and an implicit domain. Additionally, we will prove that a network
with what is termed sum-decomposition form can represent any domain shift metrics in theory.

3.1 PROBLEM SETUP

Now we give a formal definition of the problem. Let X denote input EEG data space and Y denote
output space. We define a domain D to be the joint distribution PXY on X × Y. This distribution
changes for many reasons and we assume it follows distribution P. Domains cannot be observed
directly. What we observe are samples {Di} of domains where each Di denotes a set of {Xi, Yi}.

The inconsistency of domains may cause poor generalization ability. One way to handle this is to
use a functional T mapping each domain into another while reducing the discrepancy between new
domains. Typically, a divergence loss function d(·, ·) is selected, which takes the marginal or joint
distribution into consideration. And the final T is chosen by minimizing Equation (1),

_Tda = arg min_ _d(T (Di), T (Dj))._ (1)
_T_


-----

Since such functional Tda can transfer domains into a common feature space, the model trained on
_T (D) will not suffer domain shift problem. This method is called alignment._

3.2 SHIFT-FREE DOMAIN

In the multi-source DA or DG setting, an easy way to introduce DA methods is simultaneously
minimizing divergence between every pairwise source domains Equation (2),

_Tmda = arg min_ _d(T (Di), T (Dj))._ (2)
_T_ _Di,Dj_ _DS_

X∈

In the ideal situation, the final discrepancy _Di,Dj_ _DS_ _[d][(][T][ (][D][i][)][,][ T][ (][D][j][))][ limits to zero. All do-]_

_∈_
mains will be the same. We define that final domain as shift-free domain.

**Definition 1. A shift-free domain usf is any T[P] (Di) in the limit that** _i≠_ _j_ _[d][(][T][ (][D][i][)][,][ T][ (][D][j][))][ →]_ [0][.]

**Theorem 1. in the limit, optimizing total discrepancy** _Di,Dj_ _DS_ _[d][(][T][ (][D][i][)][,][ T][ (][D][j][))][ is equivalent]_
_∈_

[P]

_to optimizing a loss function_ _Di∈DS_ _i[)][, where][ L][T][ (][D]i[)][ :=][ d][(][T][ (][D][i][)][, u][sf]_ [)][.]

_[L][T][ (][D]_ [P]

As shown in figure 1, aligning all the pairwise domains simultaneously is equivalent to aligning

[P]

all single domains with the shift-free domain. As long as we derive the shift-free domain, we can
construct a network to compute the shift of the target domain and use it to govern the pseudo domain
adaptation. Next, we will show how to construct the network.

3.3 SUM-DECOMPOSITION


A key requirement for a function to represent domain discrepancy is the permutation-invariant
constraints. That is to say, the order of the source domain data should be irrelevant to the output. This property has been well studied in previous works (Zaheer et al. (2017); Qi et al. (2017)).
Usually, the summation is introduced to enforce permutation-invariance. This form is termed sum_decomposition. And the definition of sum-decomposable is shown in Definition 2._

**Definition 2. A function g is sum-decomposable via R[N]** _if there exist function ψ : R →_ R[N] _and_
_ρ : R[N]_ _→_ R, such that g(X) = ρ _x∈X_ _[ψ][(][x][)]_ _._

**Theorem 2. An continues map F : P R[M]** R is permutation invariant if and only if it is continuously
_→_
_sum-decomposable via R[M]_ _._

**Proposition 1. Traditional domain discrepancy like MMD or H-divergence can be computed equiv-**
_alently using a sum-decomposable function._

The proof of Theorem 2 can be found in previous study (Wagstaff et al. (2019)). By adding a summation layer or averaging layer, one can easily enforce permutation-invariant property. Theorem 2
suggests that sum-decomposable network via a latent space with sufficient dimension should suffice
to model any permutation-invariant function, including d( _, usf_ ) occurs in Theorem 1.

_·_

4 METHODOLOGY

(a) (b)

Figure 1: The sketch map of the total discrepancy between pairwise domains and discrepancy between each domain and the shift-free domain.


-----

G M G
R L R LDS
L P L

Summation

Governor

X LSTM MLP

Data Classifier

Feature Extractor


Figure 2: The working flow of PDAML. When a new domain comes, the feature extractor first
changes the data to feature. Then the governor judges its discrepancy and computes loss LDS. Under
the governing of G, the feature extractor makes quick self-adaptation. After several adaptation steps,
the data X will be passed to a new feature extractor and then forward propagate to the classifier.

4.1 OVERVIEW

As an implementation of the theory, our method PDAML couples a sum-decomposable domain shift
governor with the task network. As illustrated in Figure 2, the task network is decomposed into a
feature extractor F (θ) and a classifier C(φ). Domain shift governor G(ω) takes the feature from
_F_ (θ) to analyze the discrepancy between the current domain and the shift-free domain. When we use
this network to classify some data of the target domain, G will first propagate forward to compute
the domain shift, and then automatically propagate backward to fine-tune the feature extractor F . It
seems like a smart governor who knows how to modify the network according to its performance on
producing shift-free features. Under the governing of G, the entire network can generalize to any
unseen domains via pseudo domain adaptation. In the rest of this section, we will introduce how to
design a domain shift governor, and then introduce our training strategy.

4.2 DESIGNING THE GOVERNOR

Since the governor needs to be permutation-invariant, an easy way to satisfy this requirement is
to insert a summation layer into a neural network, as what Feature-Critic did. The Feature-Critic
network simply adds a summation layer at the end of a multi-layer perceptron (MLP), omitting
the outside map ρ in Definition 2. Hence it may not represent a domain-shift governor perfectly.
Besides, we found in the experiment that introducing adversarial in the network can promote its
capability. Specifically, we insert two Gradient Reversal Layers (GRL) before and after a twolayer MLP and add another layer after it.

The motivation for adding GRL into the network comes from traditional methods representing domains discrepancy. between two domains, like MMD or adversarial-based methods. Those methods
all have one thing in common, that we should use the ’biggest’ difference between two domains to
represent the divergence.

To enforce our network the same capability of simulating domain shift like these traditional methods,
we define a new divergence similar to MMD and h-divergence, which we term Maximum Mean
Norm Discrepancy(MMND).
**Definition 3. The MMND between two domains Di, Dj is defined as,**

_MMND(Di, Dj) := maxf_ Ex∈Di _f_ (x) − Ex∈Dj _f_ (x) 2 _[,]_ (3)
_∈T_

_where f is a function mapping x into a vector space._

According to Theorem(1), we can choose an implicit domain as the shift-free domain to avoid direct
domain comparison. Intuitively, the implicit domain should be chosen to make the total divergence
as small as possible, such that the optimization will be fast. So our proposed objective function
contains both minimization and maximization as equation (4) shows,


max min (4)
_DXi∈DS_ _f_ _∈T_ _µ∈Vlatent_ _[||][E][x][∈][D][i]_ _[f]_ [(][x][)][ −] _[µ][||][2][ .]_


_Lds =_


-----

In domain shift governor, f is the inner map ψ in Equation (2), which is represented by the first
several layers of the network. The summation layer computes the mean value of f (x). Finally, the
difference’s norm will be computed by the last layer of the domain shift governor. As for maximization and minimization, we adopt Goodfellow et al. (2014) method that adding Gradient Reversal
Layers in the governor network. The GRL works as an identity map during forwarding propagation
but reverses the gradient direction in backward. Compared with Domain Adaptation Regularizer in
DANN, our proposed domain shift governor has one more minimization task. Thus we need two
GRL in total. As Figure 2 shows, the left GRL fools the governor to find out the biggest difference
of domains, and the right GRL corrects the feature extractor to generate uniform features of different
domains. This two-GRL structure could be unstable in experiments. To handle this, we freeze part
of the network between two GRL when the iteration reaches a threshold.

4.3 TRAINING THE NETWORK

The next step is to learn all the parameters. To guarantee the generalization ability of the governor,
we introduce the meta-learning strategy. The algorithm can be divided into training the governor
and training the network. They are represented in Algorithm 1 and Algorithm 2, respectively.

**Training the governor. To enhance the model’s generalization ability, we not only optimize the**
governor’s output LDS to reduce domain shift but also couple it with the generalization process via
meta-learning. First, all accessible domains are randomly split into meta-train domains Dtr and
meta-test domains (also known as meta valid domains Dval). We use the governor to optimize the
feature extractor F (θ) on meta-train domains, then evaluate the promotion of F (θ[′]) on meta-test
domains.

Let ℓ denote the classification loss function. After θ updates to θ[′], the loss function will change from
_ℓ(x[(][i][)], y[(][i][)]; θ) to ℓ(x[(][i][)], y[(][i][)]; θ[′]). Following Li et al.’s work Li et al. (2019), we construct the meta_
loss functionin Equation (5),


tanh _ℓ(x[(][i][)], y[(][i][)]; θ[′]) −_ _ℓ(x[(][i][)], y[(][i][)]; θ)_ _._ (5)
(xi,yi) _Dval_

X∈  


_Lmeta =_


Finally, the total loss for updating ω is shown in Equation (6),

_DS(θ, φ, ω; Dtr) + λ_ _meta(θ[′], φ, ω; Dval)._ (6)
_L_ _L_

Here λ is a hyper-parameter, θ, φ, ω are parameters of F, C, G. By optimizing Equation(6), G(ω)
finally comes to its real update.

**Training the entire network. Unlike MetaReg or Feature-Critic that first trains the auxiliary net-**
work and then trains the task network, we propose to train our governor and task network alternately. Because we need G still working even in the test phase, which means we must keep updating
_G during the training of other parts of the network. Besides, to fully implement the principle of_
meta-learning, we use the MAML framework to train the network, instead of directly optimizing
it. We view the governor and classification as two tasks, then use episode training to update their
parameters.

Still, the domains are split into Dtr and Dval. G uses data from Dtr to compute the domain shift loss
_LDS(θ, ω; Dtr) and classification loss Lce[(][tr][)][, which is further used to update the feature extractor]_
_F_ (θ). With the updated parameters, F (θ[′]) take data from Dval and the task network computes a
corresponding _ce_ . The total loss for optimizing F (θ) and C(φ) is as follows,
_L[(][val][)]_

_λLDS(θ, ω; Dtr) + L[(]ce[tr][)][(][θ, φ][) +][ L]ce[(][val][)](θ[′], φ)._ (7)


-----

**Algorithm 2 Training the entire network**

**Input: D, θ, φ, ω**
**Output: θ, φ, ω**

Random Split D
**Meta train:**
_DS(θ, ω; Dtr)_ _G(F_ (X))
_L_ _←_
_L[(]ce[tr][)][(][θ, φ][)][ ←]_ _[ℓ][(][C][(][F]_ [(][X]tr[))][, Y]tr[)]

_θ[′]_ _θ_ _α_ _θ_ _DS(θ, ω; Dtr)_
_←_ _−_ _∇_ _L_
**Meta validate:**
_ce_ (θ[′], φ) _ℓ_ (C(F (Xval)), Yval)
_L[(][val][)]_ _←_

**Meta optimization:**
update θ, φ, ω using λLDS + Lce[(][val][)] + L[(]ce[tr][)]


**Algorithm 1 Training the governor**

**Input: D, θ, φ, ω**
**Output: ω**

Random Split D :
( _tra,_ _val)_
_D_ _D_ _←D_
**Meta train:**
_DS(θ, ω; Dtr)_ _G(F_ (X))
_L_ _←_
_θ[′]_ _θ_ _α_ _θ_ _DS(θ, ω; Dtr)_
_←_ _−_ _∇_ _L_
**Meta validate:**
_meta(θ, θ[′], φ; Dval)_ Equation (5)
_L_ _←_
**Meta optimization:**
update ω using LDS + λLmeta


**Analysis in the view of meta-learning. The way we introduce PDAML is in the view of alignment-**
based domain adaptation. However, this method can also be explained in the view of meta-learning.
According to Li et al. (2018a), meta-learning can be seen as coupling different tasks by making their
gradients in the same direction. In our setting, the governor is a task artificially set to be coupled with
the ”domain generalization” process’. When the model accesses a new domain, the optimization for
the model to adapt to this new domain is in the same direction as the optimization promoted by the
governor.

5 EXPERIMENTS

**Datasets. We evaluate our proposed PDAML method on the SEED dataset (Zheng & Lu (2015)).**
The SEED dataset contains the EEG signals of 15 subjects. They were recruited to watch 15 wellprepared video clips that can elicit exactly one of the three kinds of emotion: happy, neutral, and
sad. The criteria of film clip selection ensure that each clip is well-edited to create coherent emotion
eliciting and maximize emotional meanings. The signals were sampled at the rate of 1000 Hz with
ESI NeuroScan System from a 62-electrode headset.

**Feature Extraction.** We extract the same feature following the existing studies (Zheng & Lu
(2015)). Since the SEED dataset has been preprocessed, we could directly extract the feature. Differential entropy (DE) feature (Duan et al. (2013)) has been extracted. Previous works have shown
that the DE feature of EEG signals is efficient for EEG-based emotion recognition (Zheng & Lu
(2015); Zheng et al. (2019); Yang et al. (2018)). Shi et al. have demonstrated that the value of DE is
equal to the logarithmic spectral energy for a fixed-length EEG sequence in a certain band (Shi et al.
(2013)). So we firstly use Short Time Fourier Transform (STFT) with a 1-s-long non-overlapping
Hanning window to extract the spectral energy of EEG signal from five frequency bands: δ: 1-3 Hz,
_θ: 4-7 Hz, α: 8-13 Hz, β: 14-30 Hz, and γ: 31-50 Hz. Then we can calculate the DE feature. Taking_
account into the dynamic characteristics of EEG-based emotion recognition tasks, we employ the
linear dynamic system (LDS) approach to filter the DE feature. The dimension of each sample is
310 (62 channels × 5 frequency bands). Since the EEG data are time series, we re-sampling the
feature with a time-step of 15 and a 1-s-long overlapping, and each subject has 3184 samples.

**Implementation Details. Following the previous work of Plug-and-play, we adopt a leave-one-**
subject-out strategy to study the generalization ability of PDAML. In each iteration, we select one
subject as the target and use the other 14 subjects to train our model. In the test phase, we choose
the prediction results after 10 steps of self-adaptation. The feature extractor of PDAML is an LSTM
with 2 layers, output dimension 256, and time step 15. The classifier is an MLP with 2 layers,
hidden size 100. We use Adam optimizer with learning rate 2e-4, weight decay 1e-4 for both the
task network and governor. λ is set to be 0.1. First, we pre-train the task network to reach more than
85% accuracy in the training set. Then we use PDAML training the domain shift governor as well
as the task network at the same time. The threshold for freezing the part of the governor inside GRL
is 40, the max iteration is 200.


-----

6 RESULT AND DISCUSSION

To evaluate our method, we adopt the leave-one-subject-out evaluation scheme and compare
PDAML with various DA and DG methods on the SEED dataset. The results, including mean
accuracy (avg.) and standard deviations (std.), are reported in Table 1. In comparison with the baseline of aggregating the data from all source domains and directly using a Support Vector Machine
(SVM) to train a single model, all these methods show great improvement of the accuracy by at least
13%. Among them, PDAML outperforms all DG methods. When compared with the DA methods,
PDAML still achieves the considerable results. Only WGANDA and PPDA have slightly higher
accuracy than PDAML. But WGANDA needs all of the source domains and PPDA needs part of
them to apply adaptation, which cannot compare PPDA’s fast generalization ability.

|Methods|TYPE|Avg. Std.|
|---|---|---|
|SVM (Zheng & Lu (2016))|Baseline|0.567 0.163|
|DICA (Ma et al. (2019b)) DResNet (Ma et al. (2019b)) PPDA NC (Zhao et al. (2021)) MLDG (Li et al. (2018a)) Feature-Critic (Li et al. (2019))|DG|0.694 0.078 0.853 0.080 0.854 0.071 0.795 0.120 0.806 0.120|
|TCA (Zheng & Lu (2016)) TPT (Zheng & Lu (2016)) DANN (Li et al. (2018b)) DAN (Li et al. (2018b)) WGANDA (Luo et al. (2018))|DA|0.640 0.146 0.752 0.128 0.792 0.131 0.838 0.086 0.871 0.071|
|PPDA ((Zhao et al. (2021))|Fast DA|0.867 0.071|
|PDAML (Ours)|Pseudo DA|0.864 0.094|



Table 1: Results on the SEED dataset.

6.1 COMPARED WITH META-LEARNING BASED DG

It should be noticed that as our method is an implementation of meta-learning strategy, its accuracy
outperforms MLDG’s and Feature-Critic’s by 6.9% and 5.8%, respectively. Results demonstrate that
a method like MLDG that directly adopts episode training to generalize the model is not competent
to overcome the subject variability in EEG-based emotion recognition. Nevertheless, Feature Critic
uses a sum-decomposable MLP to simulate domain shift in the training phase, but still does not
improve the result significantly. It implies that applying domain shift governor in the test phase
is useful. As a PDA method, our PDAML predicts any target set with only several steps of selfadaptation, combining the advantage of both the DA and DG.

6.2 VISUALIZATION

The domain shift governor is designed to perform a similar way to a conventional domain discriminator that helps feature extractor generate domain-invariant features. As shown in Figure 3, with the
trained governor, the feature extractor can reduce the domain shift between data from each subject.
Data with the same emotion label from different domains share a similar distribution in the common
space. And this distribution is shift free domain.

6.3 FAST ADAPTATION

To evaluate the domain shift governor, we record the accuracy of the self-adaptation in the test
phase. As shown in Figure 4, we see that PDAML can adapt to the a new domain stably, achieving
good performance in just a few self-adapt steps. The left one is the adaptation performance on the
target domain during the training phase. Each self-adaptation requires no extra information except
the input data for prediction. Good performance is achieved within only a few self-adaptation steps.


-----

(a) original domain (b) shift-free domain

Figure 3: Visualization of the data using t-SNE. Different colors stand for different subjects. Different shapes stand for different emotion labels.

The right one is the comparison between the governor with and without GRL. We can see that the
introduce of the adversarial strategy makes the domain shift governor more stable and efficient,
otherwise there may be fluctuating or decrease.

(a) (b)

Figure 4: The test accuracy of self-adaptation. (a) Results of standard PDAML; (b) Results when
GRLs are removed.

7 CONCLUSION

In this work, we propose a PDA paradigm, in which no source domain is required in the test phase,
for building subject-independent EEG-based affective model. As an implementation of PDA, we
propose PDAML method with a sum-decomposable domain shift governor to make PDA. By taking advantage of adversarial learning and meta-learning, our PDAML generalizes to a new domain within only a few self-adaptive steps. Experimental results on the SEED dataset demonstrate
that PDAML outperforms the DG methods and converges quickly, which is more suit for building
subject-independent affective model than typical DA, DG and FDA methods.


-----

REFERENCES

S. M. Alarc˜ao and M. J. Fonseca. Emotions recognition using EEG signals: A survey. IEEE Trans.
_Affective Computing, 10(3):374–393, July 2019._

Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein GAN. _arXiv preprint_
_arXiv:1701.07875, 2017._

Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. MetaReg: towards domain generalization using meta-regularization. In Proceedings of the 32nd International Conference on
_Neural Information Processing Systems, NIPS’18, pp. 1006–1016, Red Hook, NY, USA, Decem-_
ber 2018. Curran Associates Inc.

Shai Bendavid, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine Learning, 79(1):151–175,
2010.

Xin Chai, Qisong Wang, Yongping Zhao, Yongqiang Li, Dan Liu, Xin Liu, and Ou Bai. A
Fast, Efficient Domain Adaptation Technique for Cross-Domain Electroencephalography(EEG)Based Emotion Recognition. Sensors, 17(5):1014, May 2017. doi: 10.3390/s17051014. URL
[https://www.mdpi.com/1424-8220/17/5/1014. Number: 5 Publisher: Multidisci-](https://www.mdpi.com/1424-8220/17/5/1014)
plinary Digital Publishing Institute.

Ruo-Nan Duan, Jia-Yi Zhu, and Bao-Liang Lu. Differential entropy feature for EEG-based emotion
classification. In International IEEE/EMBS Conference on Neural Engineering (NER), pp. 81–84.
IEEE, 2013.

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In International Conference on Machine Learning, pp. 1126–1135.
PMLR, July 2017. [URL http://proceedings.mlr.press/v70/finn17a.html.](http://proceedings.mlr.press/v70/finn17a.html)
ISSN: 2640-3498.

Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
_ICML’15, volume 37, pp. 1180–1189, 2015._

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS’14, pp. 2672–2680,
2014.

Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Sch¨olkopf, and Alex J Smola. A
kernel method for the two-sample-problem. In NIPS’07, pp. 513–520, 2007.

R. Jenke, A. Peer, and M. Buss. Feature extraction and selection for emotion recognition from eeg.
_IEEE Trans. Affective Computing, 5(3):327–339, July 2014._

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to Generalize: MetaLearning for Domain Generalization. Proceedings of the AAAI Conference on Artificial Intelli_[gence, 32(1), April 2018a. ISSN 2374-3468. URL https://ojs.aaai.org/index.php/](https://ojs.aaai.org/index.php/AAAI/article/view/11596)_
[AAAI/article/view/11596. Number: 1.](https://ojs.aaai.org/index.php/AAAI/article/view/11596)

He Li, Yi-Ming Jin, Wei-Long Zheng, and Bao-Liang Lu. Cross-subject emotion recognition using
deep adaptation networks. In Neural Information Processing, pp. 403–413, 2018b.

He Li, Wei-Long Zheng, and Bao-Liang Lu. Multimodal vigilance estimation with adversarial
domain adaptation networks. In IJCNN, pp. 1–6, 07 2018c.

Xiang Li, Dawei Song, Peng Zhang, Yazhou Zhang, Yuexian Hou, and Bin Hu. Exploring EEG
features in cross-subject emotion recognition. Frontiers in Neuroscience, 12:162, 03 2018d. doi:
10.3389/fnins.2018.00162.

Yiying Li, Yongxin Yang, Wei Zhou, and Timothy Hospedales. Feature-critic networks for heterogeneous domain generalization. In Proceedings of the 36th International Conference on Machine
_Learning, volume 97, pp. 3915–3924. PMLR, 09–15 Jun 2019._


-----

Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint
adaptation networks. In ICML’17, pp. 2208–2217, 2017.

Yun Luo, Si-Yang Zhang, Wei-Long Zheng, and Bao-Liang Lu. WGAN domain adaptation for
EEG-based emotion recognition. In Neural Information Processing, pp. 275–286, 2018.

Bo-Qun Ma, He Li, Yun Luo, and Bao-Liang Lu. Depersonalized cross-subject vigilance estimation with adversarial domain generalization. In 2019 International Joint Conference on Neural
_Networks (IJCNN), pp. 1–8, 2019a. doi: 10.1109/IJCNN.2019.8852347._

Bo-Qun Ma, He Li, Wei-Long Zheng, and Bao-Liang Lu. Reducing the subject variability of eeg
signals with adversarial domain generalization. In Neural Information Processing, pp. 30–42,
Cham, 2019b. Springer International Publishing.

Christian M¨uhl, Brendan Allison, Anton Nijholt, and Guillaume Chanel. A survey of affective brain
computer interfaces: principles, state-of-the-art, and challenges. Brain-Computer Interfaces, 1
(2):66–84, 2014.

Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer
component analysis. IEEE Trans. Neural Networks, 22(2):199–210, 2011.

Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. PointNet++: Deep Hierarchical
Feature Learning on Point Sets in a Metric Space. In Advances in Neural Information Process_[ing Systems, volume 30. Curran Associates, Inc., 2017. URL https://papers.nips.cc/](https://papers.nips.cc/paper/2017/hash/d8bf84be3800d12f74d8b05e9b89836f-Abstract.html)_
[paper/2017/hash/d8bf84be3800d12f74d8b05e9b89836f-Abstract.html.](https://papers.nips.cc/paper/2017/hash/d8bf84be3800d12f74d8b05e9b89836f-Abstract.html)

Wojciech Samek, Frank C. Meinecke, and Klaus-Robert Muller. Transferring subspaces between
subjects in brain–computer interfacing. IEEE Trans. Biomedical Engineering, 60(8):2289–2298,
2013.

Maryam Shanechi. Brain–machine interfaces from motor to mood. Nature Neuroscience, 22:1554–
1564, 10 2019. doi: 10.1038/s41593-019-0488-y.

Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein distance guided representation
learning for domain adaptation. In AAAI’18, pp. 4058–4065, 2018.

Li-Chen Shi, Ying-Ying Jiao, and Bao-Liang Lu. Differential entropy feature for EEG-based vigilance estimation. International IEEE/EMBS Conference on Neural Engineering (NER), 2013
(2013):6627–6630, 2013.

Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert M Aˇ[˜] zller. Covariate shift adaptation by
importance weighted cross validation. JMLR, 8:985–1005, 2007.

David Sussillo, Sergey D Stavisky, Jonathan C Kao, Stephen I Ryu, and Krishna V Shenoy. Making
brain–machine interfaces robust to future neural variability. Nature Communications, 7(1):13749,
2016.

Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In CVPR’17, pp. 7167–7176, 2017.

Edward Wagstaff, Fabian Fuchs, Martin Engelcke, Ingmar Posner, and Michael A. Osborne. On
the Limitations of Representing Functions on Sets. In International Conference on Machine
_[Learning, pp. 6487–6494. PMLR, May 2019. URL http://proceedings.mlr.press/](http://proceedings.mlr.press/v97/wagstaff19a.html)_
[v97/wagstaff19a.html. ISSN: 2640-3498.](http://proceedings.mlr.press/v97/wagstaff19a.html)

Jindong Wang, Wenjie Feng, Yiqiang Chen, Han Yu, Meiyu Huang, and Philip S Yu. Visual domain
adaptation with manifold embedded distribution alignment. In ACMMM’18, pp. 402–410, 2018.

Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, and Tao Qin. Generalizing to unseen
domains: A survey on domain generalization. In Proceedings of the Thirtieth International Joint
_Conference on Artificial Intelligence, IJCAI-21, pp. 4627–4635, 8 2021. doi: 10.24963/ijcai._
2021/628.


-----

Yimin Yang, Q M Jonathan Wu, Wei-long Zheng, and Bao-liang Lu. EEG-based emotion recognition using hierarchical network with subnetwork nodes. IEEE Transactions on Cognitive and
_Developmental Systems, 10(2):408–419, 2018._

Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnab´as P´oczos, Ruslan R. Salakhutdinov,
[and Alexander J. Smola. Deep Sets. January 2017. URL https://openreview.net/](https://openreview.net/forum?id=HJbcbv-_WH)
[forum?id=HJbcbv-_WH.](https://openreview.net/forum?id=HJbcbv-_WH)

T. O. Zander and S Jatzev. Context-aware brain-computer interfaces: exploring the information
space of user, technical system and environment. Journal of Neural Engineering, 9(1):016003,
2011.

Li-Ming Zhao, Xu Yan, and Bao-Liang Lu. Plug-and-Play Domain Adaptation for Cross-Subject
EEG-based Emotion Recognition. Proceedings of the AAAI Conference on Artificial Intelligence,
35(1):863–870, 2021. [URL https://ojs.aaai.org/index.php/AAAI/article/](https://ojs.aaai.org/index.php/AAAI/article/view/16169)
[view/16169.](https://ojs.aaai.org/index.php/AAAI/article/view/16169)

Wei-Long Zheng and Bao-Liang Lu. Investigating critical frequency bands and channels for EEGbased emotion recognition with deep neural networks. IEEE Trans. AMD, 7(3):162–175, 2015.

Wei-Long Zheng and Bao-Liang Lu. Personalizing EEG-based affective models with transfer learning. In IJCAI’16, pp. 2732–2738, 2016.

Wei-Long Zheng, Jia-Yi Zhu, and Bao-Liang Lu. Identifying stable patterns over time for emotion
recognition from eeg. IEEE Trans. Affective Computing, 10(3):417–429, July 2019. ISSN 23719850. doi: 10.1109/TAFFC.2017.2712143.

Fuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno Jialin Pan, and Qing He. Supervised representation
learning: transfer learning with deep autoencoders. In IJCAI’15, pp. 4119–4125, 2015.

A APPENDIX

You may include other additional sections here.


-----

