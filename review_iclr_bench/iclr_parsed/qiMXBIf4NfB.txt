# HOW DOES UNLABELED DATA IMPROVE GENERALIZA## TION IN SELF-TRAINING? A ONE-HIDDEN-LAYER THE- ORETICAL ANALYSIS


**Shuai Zhang**
Rensselaer Polytechnic Institute
Troy, NY, USA 12180
zhangs29@rpi.edu

**Sijia Liu**
Michigan State University
East Lansing, MI, USA 48824
MIT-IBM Watson AI Lab, IBM Research
liusiji5@msu.edu

**Jinjun Xiong**
University at Buffalo
Buffalo NY, USA 14260
jinjun@buffalo.edu


**Meng Wang**
Rensselaer Polytechnic Institute
Troy, NY, USA 12180
wangm7@rpi.edu

**Pin-Yu Chen**
IBM Research
Yorktown Heights, NY, USA 10562
Pin-Yu.Chen@ibm.com


ABSTRACT

Self-training, a semi-supervised learning algorithm, leverages a large amount of
unlabeled data to improve learning when the labeled data are limited. Despite empirical successes, its theoretical characterization remains elusive. To the best of
our knowledge, this work establishes the first theoretical analysis for the known
iterative self-training paradigm and proves the benefits of unlabeled data in both
training convergence and generalization ability. To make our theoretical analysis
feasible, we focus on the case of one-hidden-layer neural networks. However,
theoretical understanding of iterative self-training is non-trivial even for a shallow neural network. One of the key challenges is that existing neural network
landscape analysis built upon supervised learning no longer holds in the (semisupervised) self-training paradigm. We address this challenge and prove that iterative self-training converges linearly with both convergence rate and generalization
accuracy improved in the order of 1/‚àöM, where M is the number of unlabeled

samples. Experiments from shallow neural networks to deep neural networks are
also provided to justify the correctness of our established theoretical insights on
self-training.

1 INTRODUCTION

Self-training (Scudder, 1965; Yarowsky, 1995; Lee et al., 2013; Han et al., 2019), one of the most
powerful semi-supervised learning (SemiSL) algorithms, augments a limited number of labeled data
with unlabeled data so as to achieve improved generalization performance on test data, compared
with the model trained by supervised learning using the labeled data only. Self-training has shown
empirical success in diversified applications such as few-shot image classification (Su et al., 2020;
Xie et al., 2020; Chen et al., 2020a; Yalniz et al., 2019; Zoph et al., 2020), objective detection
(Rosenberg et al., 2005), robustness-aware model training against adversarial attacks (Carmon et al.,
2019), continual lifelong learning (Lee et al., 2019), and natural language processing (He et al.,
2019; Kahn et al., 2020). The terminology ‚Äúself-training‚Äù has been used to describe various SemiSL


-----

algorithms in the literature, while this paper is centered on the commonly-used iterative self-training
method in particular. In this setup, an initial teacher model (learned from the labeled data) is applied
to the unlabeled data to generate pseudo labels. One then trains a student model by minimizing the
weighted empirical risk of both the labeled and unlabeled data. The student model is then used as
the new teacher to update the pseudo labels of the unlabeled data. This process is repeated multiple
times to improve the eventual student model. We refer readers to Section 2 for algorithmic details.

Despite the empirical achievement of self-training methods with neural networks, the theoretical
justification of such success is very limited, even in the field of SemiSL. The majority of the theoretical results on general SemiSL are limited to linear networks (Chen et al., 2020b; Raghunathan
et al., 2020; Oymak & Gulcu, 2020; Oneto et al., 2011). The authors in (Balcan & Blum, 2010)
show that unlabeled data can improve the generalization bound if the unlabeled data distribution and
target model are compatible. For instance, the unlabeled data need to be well-chosen such that the
target function for labeled data can separate the unlabeled data clusters, which, however, may not be
able to be verified ahead. Moreover, (Rigollet, 2007; Singh et al., 2008) proves that unlabeled data
can improve the convergence rate and generalization error under a similar clustering assumption,
where the data contains clusters that have homogeneous labels. A recent work by Wei et al. (2020)
analyzes SemiSL on nonlinear neural networks and proves that an infinite number of unlabeled data
can improve the generalization compared with training with labeled data only. However, Wei et al.
(2020) considers single shot rather than iterative SemiSL, and the training problem aims to minimize
the consistency regularization rather than the risk function in the conventional self-training method
(Lee et al., 2013). Moreover, Wei et al. (2020) directly analyzes the global optimum of the nonconvex training problem without any discussion about how to achieve the global optimum. To the best
of our knowledge, there exists no analytical characterization of how the unlabeled data affect the
generalization of the learned model by iterative self-training on nonlinear neural networks.

**Contributions. This paper provides the first theoretical** 7
study of iterative self-training on nonlinear neural networks. Focusing on one-hidden-layer neural networks,
this paper provides a quantitative analysis of the generalization performance of iterative self-training as a
function of the number of labeled and unlabeled sam- 6
ples. Specifically, our contributions include Improvement (%)

**1.** **Quantitative justification of generalization im-**
**provement by unlabeled data.** Assuming the exis- 100K 200K 300K 400K 500K
tence of a ground-truth model with weights W _[‚àó]_ that Number of unlabeled data

7

6.5

6

Test accuracy

Improvement (%)

5.5

100K 200K 300K 400K 500K

Number of unlabeled data

maps the features to the corresponding labels, we prove
that the learned model via iterative self-training moves

Figure 1: The trend of test accuracy

closer to W _[‚àó]_ as the number M of unlabeled data in- improvement (%) on CIFAR-10 by selfcreases, indicating a better testing performance. Specif
training on CIFAR-10 (labeled) with dif
ically, we prove that the Frobenius distance to W _[‚àó],_ ferent amount of unlabeled data from 80
which is approximately linear in the generalization error, decreases in the order of 1/‚àöM . As an exam- Million Tiny Images matches our theoret-ical prediction.

ple, Figure 1 shows that the proposed theoretical bound
matches the empirical self-training performance versus the number of unlabeled data for image classification; see details in Section 4.2.

**2. Analytical justification of iterative self-training over single shot alternative. We prove that**
the student models returned by the iterative self-training method converges linearly to a model close
to W _[‚àó], with the rate improvement in the order of 1/‚àöM_ .

**3. Sample complexity analysis of labeled and unlabeled data for learning a proper model.**
We quantify the impact of labeled and unlabeled data on the generalization of the learned model.
In particular, we prove that the sample complexity of labeled data can be reduced compared with
supervised learning.

1.1 RELATED WORKS


**Semi-supervised learning. Besides self-training, many recent SemiSL algorithms exploit either**
consistency regularization or entropy minimization. Consistency regularization is based on the as

-----

sumption that the learned model will return same or similar output when the input is perturbed
(Laine & Aila, 2016; Bachman et al., 2014; Sajjadi et al., 2016; Tarvainen & Valpola, 2017; Reed
et al., 2015). (Grandvalet & Bengio, 2005) claims that the unlabeled data are more informative if the
pseudo labels of the unlabeled data have lower entropy. Therefore, a line of works (Grandvalet &
Bengio, 2005; Miyato et al., 2018) adds a regularization term that minimizes the entropy of the outputs of the unlabeled data. In addition, hybrid algorithms that unify both the above regularizations
have been developed like (Berthelot et al., 2019a;b; Sohn et al., 2020).

**Domain adaptation. Domain adaptation exploits abundant data in the source domain to learn a**
model for the target domain, where only limited training data are available (Liebelt & Schmid,
2010; Vazquez et al., 2013; Zhang et al., 2013; Long et al., 2015; Tzeng et al., 2014). Source and
target domain are related but different. Unsupervised domain adaptation (Ganin & Lempitsky, 2015;
Ganin et al., 2016; Gong et al., 2013; Bousmalis et al., 2016), where training data in target domain
are unlabeled, is similar to SemiSL, and self-training methods have been used for analysis (Zou
et al., 2018; Tang et al., 2012; French et al., 2018). However, self-training and unsupervised domain
adaptation are fundamentally different. The former learns a model for the domain where there is
limited labeled data, with the help of a large number of unlabeled data from a different domain. The
latter learns a model for the domain where the training data are unlabeled, with the help of sufficient
_labeled data from a different domain._

**Generalization analysis of supervised learning. In theory, the testing error is upper bounded by the**
training error plus the generalization gap between training and testing. These two quantities are often
analyzed separately and cannot be proved to be small simultaneously for deep neural networks. For
example, neural tangent kernel (NTK) method (Jacot et al., 2018; Du et al., 2018; Lee et al., 2018)
shows the training error can be zero, and the Rademacher complexity in (Bartlett & Mendelson,
2002) bounds the generalization gap (Arora et al., 2019a). For one-hidden-layer neural networks
(Safran & Shamir, 2018), the testing error can be proved to be zero under mild conditions. One
common assumption is that the input data belongs to the Gaussian distribution (Zhong et al., 2017;
Ge et al., 2018; Kalai et al., 2008; Bakshi et al., 2019; Zhang et al., 2016; Brutzkus & Globerson,
2017; Li & Yuan, 2017; Soltanolkotabi et al., 2018). Another line of approaches (Brutzkus et al.,
2018; Li & Liang, 2018; Wang et al., 2019) consider linearly separable data.

The rest of this paper is organized as follows. Section 2 introduces the problem formulation and
self-training algorithm. Major results are summarized in Section 3, and empirical evaluations are
presented in Section 4. Section 5 concludes the whole paper. All the proofs are in the Appendix.

2 FORMALIZING SELF-TRAINING: NOTATION, FORMULATION, AND
ALGORITHM

**Problem formulation.** Given N labeled data sampled from distribution Pl, denoted by D =
_{xn, yn}n[N]=1[, and][ M][ unlabeled data drawn from distribution][ P][u][, denoted by][ e]D = {xm}m[M]=1[. The]_

aim is to find a neural network model g(W ), where W denotes the trainable weights, that minimizes
the testing error on data sampled from Pl.

e

**Table 1: Iterative Self-Training**

(S1) Initialize iteration ‚Ñì = 0 and obtain a model W [(][‚Ñì][)] as the teacher using labeled data D
only;
(S2) Use the teacher model to obtain pseudo labels _ym of unlabeled data in_ _D;_
(S3) Train the neural network by minimizing (1) via T -step mini-batch gradient descent
method using disjoint subsets _t_ _t=0_ [and][ {][ e]t _t=0 e_ [of][ e]. Let W [(][‚Ñì][+1)] denote the obtained[e]
_{D_ _}[T][ ‚àí][1]_ _D_ _}[T][ ‚àí][1]_ _D_
student model;
(S4) Use W [(][‚Ñì][+1)] as the current teacher model. Let ‚Ñì _‚Üê_ _‚Ñì_ + 1 and go back to step (S2);


**Iterative self-training. In each iteration, given the current teacher predictor g(W** [(][‚Ñì][)]), the pseudolabels for the unlabeled data in _D are computed as Àúym = g(W_ [(][‚Ñì][)]; **_xm). The method then minimizes_**
the weighted empirical risk _f[ÀÜ]D,D[(][W][ )][ of both labeled and unlabeled data through stochastic gradient]_

[e] e
e


-----

descent, where

_N_ _M_

_Œª_ 2 _Œª_ 2
_fÀÜD,D[(][W][ ) =]_ 2N _n=1_ _yn ‚àí_ _g(W ; xn)_ + 2M _m=1_ _ym ‚àí_ _g(W ;_ **_xm)_** _,_ (1)
e X    e X   

and Œª + _Œª = 1. The learned student model g(W_ [(][‚Ñì][+1)]) is used as the teacher model in the next iter-e e
ation. The initial model g(W [(0)]) is learned from labeled data. The formal algorithm is summarized
as in Table 1.

[e]

**Model and assumptions. This paper considers regression[1], where g is a one-hidden-layer fully**
connected neural network equipped with K neurons. Namely, given the input x ‚àà R[d] and weights
**_W = [w1, w2, ¬∑ ¬∑ ¬∑, wK] ‚àà_** R[d][√ó][K], we have

_K_

_g(W ; x) := [1]_ _œÜ(wj[T]_ **_[x][)][,]_** (2)

_K_

_j=1_

X

where œÜ is the ReLU activation function[2], and œÜ(z) = max{z, 0} for any input z ‚àà R. Here, we fix
the top layer weights as 1 for simplicity, and the equivalence of such a simplification is discussed in
Appendix K.

Moreover, we assume an unknown ground-truth model with weights W _[‚àó]_ that maps all the features to
the corresponding labels drawn from Pl, i.e., y = g(W ; x), where (x, y) _Pl. The generalization_

_[‚àó]_ _‚àº_
function (GF) with respect to g(W ) is defined as

2 2
_I_ _g(W )_ = E(x,y)‚àºPl _y ‚àí_ _g(W ; x)_ = E(x,y)‚àºPl _g(W_ _[‚àó]; x) ‚àí_ _g(W ; x)_ _._ (3)

By definition  _I_ _g(W_ _[‚àó])_ is zero. Clearly,  **_W_** _[‚àó]is not unique because any column permutation of _ 
**_W_** _[‚àó], which corresponds to permuting neurons, represents the same function as W_ _[‚àó]_ and minimizes
  
GF in (3) too. To simplify the representation, we follow the convention and abuse the notation that
the distance from W to W _[‚àó], denoted by ‚à•W ‚àí_ **_W_** _[‚àó]‚à•F, means the smallest distance from W to_
any permutation of W _[‚àó]. Additionally, some important notations are summarized in Table 2._

We assume the inputs of both the labeled and unlabeled data belong to the zero mean Gaussian
distribution, i.e., x (0, Œ¥[2]Id), and **_x_** (0, _Œ¥[Àú][2]Id). The Gaussian assumption is motivated_
_‚àºN_ _‚àºN_
by the data whitening (LeCun et al., 2012) and batch normalization techniques (Ioffe & Szegedy,
2015) that are commonly used in practice to improve learning performance. Moreover, training onee
hidden-layer neural network with multiple neurons is NP-Complete (Blum & Rivest, 1992) without
any assumption.

**The focus of this paper. This paper will analyze three aspects about self-training: (1) the gener-**
alization performance of W [(][L][)], the returned model by self-training after L iterations, measured by
_‚à•W_ [(][L][)] _‚àí_ **_W_** _[‚àó]‚à•F_ [3]; (2) the influence of parameter Œª in (1) on the training performance; and (3) the
impact of unlabeled data on the training and generalization performance.

Table 2: Some Important Notations

|D = {x n, y n}N n=1|Labeled dataset with N number of samples;|
|---|---|
|De = {x em}M m=1|Unlabeled dataset with M number of samples;|
|d|Dimension of the input x orx; e|
|K|Number of neurons in the hidden layer;|
|Œ∫|Conditional number (the ratio of the largest and smallest singular values) of W‚àó;|
|W (‚Ñì)|Model returned by self-training after ‚Ñìiterations; W (0) is the initial model;|
|W‚àó|Weights of the ground truth model;|
|[ŒªÀÜ] W|W [ŒªÀÜ] = ŒªÀÜW ‚àó+ (1 ‚àíŒªÀÜ)W (0);|



1The results can be extended to binary classification with a cross-entropy loss function. Please see
Appendix-I.

2Because ReLU is non-linear and non-smooth, (1) is non-convex and non-smooth, which poses analytical
challenges. The results can be easily extended to smooth functions with bounded gradients, e.g., Sigmoid.

3We use this metric because I _g(W )_ is shown to be linear in ‚à•W [(][L][)] _‚àí_ **_W_** _[‚àó]‚à•F numerically when W_ [(][L][)]

is close to W _[‚àó], see Figure 4._

  


-----

3 THEORETICAL RESULTS

**Beyond supervised learning:** **Challenge of self-training.** The existing theoretical works
such as (Zhong et al., 2017; Zhang et al., 2020a;b;c) verify that for one-hidden-layer neural networks, if only labeled data are available, and x are drawn from the standard Gaussian distribution, then supervised learning by minimizing (1) with Œª = 1 can return a
model with ground-truth weights W _[‚àó]_ (up to column permutation), as long as the number of labeled data N is at least N _[‚àó], which depends on Œ∫, K and d._ In contrast, this
paper focuses on the low labeled-data regime when N is less than N _[‚àó]._ Specifically,


_N_ _[‚àó]/4 < N ‚â§_ _N_ _[‚àó]._ (4)

Intuitively, if N < N _[‚àó], the landscape of the empir-_
ical risk of the labeled data becomes highly nonconvex, even in a neighborhood of W _[‚àó], thus, the_
existing analyses for supervised learning do not
hold in this region. With additional unlabeled data,
the landscape of the weighted empirical risk becomes smoother near W _[‚àó]. Moreover, as M in-_
creases, and starting from a nearby initialization,
the returned model W [(][L][)] by iterative self-training
can converge to a local minimum that is closer to
**_W_** _[‚àó]_ (see illustration in Figure 2).


Objective function


without unlabeled data

Objective function

with unlabeled data

Local minima

Generalization

function

ùëæùëæ

ùëæùëæ[(0)]


ùëæùëæ[‚àó]


Figure 2: Adding unlabeled data in the empirical risk function drives its local minimum
closer to W _[‚àó], which minimizes the generaliza-_
tion function.


Compared with supervised learning, the formal analyses of self-training need to handle new technical challenges from two aspects. First, the existing analyses of supervised learning exploit the fact
that the GF and the empirical risk have the same minimizer, i.e., W _[‚àó]. This property does not hold_
for self-training as W _[‚àó]_ no longer minimizes the weighted empirical risk in (1). Second, the iterative
manner of self-training complicates the analyses. Specifically, the empirical risk in each iteration is
different and depends on the model trained in the previous iteration through the pseudo labels.

In what follows, we provide theoretical insights and the formal theorems. Some important quantities
_ŒªÀÜ and ¬µ are defined below_

_ŒªŒ¥[2]_ _ŒªŒ¥[2]_ + _ŒªŒ¥[Àú][2]_
_ŒªÀÜ :=_ and _¬µ = ¬µ(Œ¥,_ _Œ¥[Àú]) :=_ _,_ (5)

_ŒªŒ¥[2]_ + _ŒªŒ¥[Àú][2][,]_ s _ŒªœÅ(Œ¥) +_ _ŒªœÅ(Œ¥[Àú])_

[e]

where œÅ is a positive function defined in (73). _Œª[ÀÜ] is an increasing function of Œª. Also, from Lemma_
11 (in Appendix), œÅ(Œ¥) is in the order of[e] _Œ¥[2]_ when Œ¥ ‚â§ 1 for ReLU activation functions. Thus,[e] _¬µ is a_
fixed constant, denoted by ¬µ[‚àó], for all Œ¥, _Œ¥[Àú] ‚â§_ 1. When Œ¥ and _Œ¥[Àú] are large, ¬µ increases as they increase._

The formal definition of N _[‚àó]_ in (4) is c(Œ∫)¬µ[‚àó][2]K [3]d log q, where c(Œ∫) is some polynomial function of
_Œ∫ and can be viewed as constant._

3.1 INFORMAL KEY THEORETICAL FINDINGS

To the best of our knowledge, Theorems 1 and ùúüùúüùüéùüé ùìîùìîùüèùüè = 1 + ùí™ùí™ ùëÄùëÄ »â [‚Ñ∞]ùêæùêæ[0]
2 provide the first theoretical characterization of ùëæùëæ[(2)] **. . .**
iterative self-training on nonlinear neural net- ùëæùëæ[(1)]
works. Before formally presenting them, we
summarize the highlights as follows.


**1. Linear convergence of the learned models.**
The learned models converge linearly to a model
close to W _[‚àó]. Thus, the iterative approach re-_
turns a model with better generalization than that
by the single-shot method. Moreover, the convergence rate is a constant term plus a term in
the order of 1/‚àöM (see ‚àÜ1 in Figure 3), indi
cating a faster convergence with more unlabeled
data.


1

ùúüùúüùüéùüé ùìîùìîùüèùüè = 1 + ùí™ùí™ ùëÄùëÄ »â [‚Ñ∞]ùêæùêæ[0]

ùëæùëæ[(2)] **. . .**

ùëæùëæ[(ùêøùêø)]

ùëæùëæ[(1)]

ùëæùëæ[(0)] ùëæùëæ[[‡∑°]ùúÜùúÜ] ùëæùëæ[‚àó]

ùúüùúüùüèùüè = 1 + ùí™ùí™ ùëÄùëÄ1 »â [Œî]ùêæùêæ[0]
ùìîùìîùüéùüé = 1 ‚àíùúÜùúÜ»â ùëæùëæ[ÃÇ] [‚àó] ‚àíùëæùëæ[(0)]

Figure 3: Illustration of the (1) ground truth W _[‚àó],_
**_W(2) iterations[(][L][)], and (4) { WW[[ÀÜ][(]Œª[‚Ñì]][)] = ÀÜ}‚Ñì[L]=0ŒªW[, (3) convergent point][‚àó]_** + (1 ‚àí _Œª[ÀÜ])W_ [(0)].


-----

**2. Returning a model with guaranteed generalization in the low labeled-data regime. Even**
when the number of labeled data is much less than the required sample complexity to obtain W _[‚àó]_
in supervised learning, we prove that with the help of unlabeled data, the iterative self-training can
return a model in the neighborhood of W [[ÀÜ]Œª], where W [Œª[ÀÜ]] is in the line segment of W (0) (ŒªÀÜ = 0)
and ground truth W _[‚àó]_ (Œª[ÀÜ] = 1). Moreover, _Œª[ÀÜ] is upper bounded by_ _N/N_ _[‚àó]. Thus W_ [(][L][)] moves

closer to W _[‚àó]_ as N increases (E0 in Figure 3), indicating a better generalization performance withp
more labeled data.

**3. Guaranteed generalization improvement by unlabeled data. The distance between W** [(][L][)]

_Œª]_
andM, W W[(][[ÀÜ][L][)] (moves closer toE1 in Figure 3) scales in the order of W [[ÀÜ]Œª] and thus W ‚àó, indicating an improved generalization performance 1/‚àöM . With a larger number of unlabeled data

(Theorem 1). When N is close to N _[‚àó]_ but still smaller as defined in (12), both W [(][L][)] and W [[ÀÜ]Œª]
converge to W _[‚àó], and thus the learned model achieves zero generalization error (Theorem 2)._

3.2 FORMAL THEORY IN LOW LABELED-DATA REGIME

_Takeaways of Theorem 1: Theorem 1 characterizes the convergence rate of the proposed algorithm_
and the accuracy of the learned model W [(][L][)] in a low labeled-data regime. Specifically, the iterates
converge linearly, and the learned model is close to W [[ÀÜ]Œª] and guaranteed to outperform the initial
model W [(0)].
**Theorem 1. Suppose the initialization W** [(0)] _and the number of labeled data satisfy_

**_W_** _F_ 1
**_W_** [(0)] **_W_** _F_ _p[‚àí][1]_ _‚à•_ _[‚àó]‚à•_ _with_ _p_ _,_ (6)
_‚à•_ _‚àí_ _[‚àó]‚à•_ _‚â§_ _¬∑_ _c(Œ∫)¬µ[2]K_ [3][/][2] _‚àà_ 2 _[,][ 1]_

2   

1
_and_ max _K [, p][ ‚àí]_ [2]¬µ[p]‚àö[ ‚àí]K[1] _¬∑ N_ _[‚àó]_ _‚â§_ _N ‚â§_ _N_ _[‚àó]._ (7)
n o

_If the value of_ _Œª[ÀÜ] in (5) and unlabeled data amount M satisfy_

1 _N_
max _K [, p][ ‚àí]_ [2]¬µ[p]‚àö[ ‚àí]K[1] _‚â§_ _Œª[ÀÜ] ‚â§_ min _N_ _[‚àó]_ _[, p][ + 2]¬µ[p]‚àö[ ‚àí]K[1]_ _,_ (8)

_and_ _M ‚â•n_ (2p ‚àí 1)[‚àí][2]c(Œ∫o)¬µ[2][ ]1 ‚àí _Œª[ÀÜ]_ 2Kn[r]3d log q. o (9)
_Then, when the number T of SGD iterations is large enough in each loop ‚Ñì, with probability at least_

1 ‚àí _q[‚àí][d], the iterates {W_ [(][‚Ñì][)]}‚Ñì[L]=0 _[converge to][ W][ [ÀÜ]Œª] as_

_L_

_‚à•W_ [(][L][)] _‚àí_ **_W_** [[ÀÜ]Œª]‚à•F ‚â§ 1 + Œò _¬µ(1‚àö‚àíMŒªÀÜ)_ _¬∑_ _Œª[ÀÜ]_ _¬∑ ‚à•W_ [(0)] _‚àí_ **_W_** [[ÀÜ]Œª]‚à•2 + 1 + Œò _¬µ(1‚àö‚àíMŒªÀÜ)_ _¬∑ ‚à•W_ _[‚àó]_ _‚àí_ **_W_** [[ÀÜ]Œª]‚à•F,
   []     [] (10)

_where W_ [[ÀÜ]Œª] = ÀÜŒªW _[‚àó]_ + (1 ‚àí _Œª[ÀÜ])W_ [(0)]. Typically, when the iteration number L is sufficient large, we
_have_

_¬µ(1_ _Œª)_
_‚à•W_ [(][L][)] _‚àí_ **_W_** _[‚àó]‚à•F ‚â§_ 1 + Œò _‚àö ‚àíM[ÀÜ]_ _¬∑ 2(1 ‚àí_ _Œª[ÀÜ]) ¬∑ ‚à•W_ _[‚àó]_ _‚àí_ **_W_** [(0)]‚à•F . (11)
   []

The accuracy of the learned model W [(][L][)] with respect to W _[‚àó]_ is characterized as (10), and the
learning model is better than initial model as in (11) if the following conditions hold. First, the
weights Œª in (1) are properly chosen as in (8). Second, the number of unlabeled data is sufficiently
large as in (9).

**Selection of Œª in self-training algorithms. When** _Œª[ÀÜ] increases, the required number of unlabeled_
data is reduced from (9), and the convergence point W [(][L][)] becomes closer to W _[‚àó]_ from (11), which
indicates a smaller generalization error. Thus, a large _Œª[ÀÜ] within its feasible range (8) is desirable._
When the initial model W [(0)] is closer to W _[‚àó]_ (corresponding to a larger p), and the number of
labeled data N increases, the upper bound in (8) increases, and thus, one can select a larger _Œª[ÀÜ]._

**The initial model W** [(0)]. The tensor initialization from (Zhong et al., 2017) can return a W [(0)] that
satisfies (6) when the number of labeled data is N = p[2]N _[‚àó]_ (see Lemma 3 in Appendix). Combining
with the requirement in (7), Theorem 1 applies to the case that N is at least N _[‚àó]/4._


-----

3.3 FORMAL THEORY OF ACHIEVING ZERO GENERALIZATION ERROR

_Takeaways of Theorem 2: Theorem 2 indicates the model returned by the proposed algorithm con-_
verges linearly to the ground truth W _[‚àó]. Thus the distance between the learned model and the ground_
truth can be arbitrarily small with the ability to achieve zero generalization error. The required sample complexity is reduced by a constant factor compared with supervised learning.
**Theorem 2. Consider the number of unlabeled data satisfies**

2

1 ‚àí 1/(¬µ‚àöK) _¬∑ N ‚àó_ _‚â§_ _N ‚â§_ _N ‚àó,_ (12)

_we choose_ _Œª[ÀÜ] such that_   
1 ‚àí 1/(¬µ‚àöK) ‚â§ _Œª[ÀÜ] ‚â§_ _N/N_ _[‚àó]._ (13)

_Suppose the initial model W_ [(0)] _and the number of unlabeled datap_ _M satisfy_


**_W_** _F_
**_W_** [(0)] **_W_** _F_ _‚à•_ _[‚àó]‚à•_ _and_ _M_ _c(Œ∫)¬µ[2](1_ _Œª)[2]K_ [3]d log q, (14)
_‚à•_ _‚àí_ _[‚àó]‚à•_ _‚â§_ _c(Œ∫)¬µ[2]K_ [3][/][2] _‚â•_ _‚àí_ [ÀÜ]

_the iterates {W_ [(][‚Ñì][)]}‚Ñì[L]=0 _[converge to the ground truth][ W][ ‚àó]_ _[as follows,]_

_‚à•W_ [(][L][)] _‚àí_ **_W_** _[‚àó]‚à•F ‚â§_ 1 + _[c]‚àö[(][Œ∫]N[)ÀÜ]Œª_ + _[c][(][Œ∫][)(1]‚àöM[ ‚àí]_ _Œª[ÀÜ])_ _¬∑ ¬µ‚àöK(1 ‚àí_ _Œª[ÀÜ])_ _L_ _¬∑ ‚à•W_ [(0)] _‚àí_ **_W_** _[‚àó]‚à•F ._ (15)
h   i

The models W [(][‚Ñì][)]‚Äôs converge linearly to the ground truth W _[‚àó]_ as (15) when the number of labeled
data satisfies (12). In contrast, supervised learning requires at least N _[‚àó]_ labeled samples to estimate
**_W_** _[‚àó]_ accurately without unlabeled data, which suggests self-training at least saves a constant fraction
of labeled data.


3.4 THE MAIN PROOF IDEA

Our proof builds upon and extends one recent line of works on supervised learning such as (Zhong
et al., 2017; Zhang et al., 2020b;c; 2021). The standard framework of these works is first to show
that the generalization function I(g(W )) in (3) is locally convex near W _[‚àó], which is its global_
minimizer. Then, when M = 0 and N is sufficiently large, the empirical risk function using labeled
data only can approximate I(g(W )) well in the neighborhood of W _[‚àó]. Thus, if initialized in this_
local convex region, the iterations, returned by applying gradient descent approach on the empirical
risk function, converge to W _[‚àó]_ linearly.

The technical challenge here is that in self-training, when unlabeled data are paired with pseudo
labels, W _[‚àó]_ is no longer a global minimizer of the empirical risk _f[ÀÜ]_ _,_ [in (1), and][ ÀÜ]f _,_ [does not]
_D_ _D_ _D_ _D_
approach I(g(W )) even when M and N increase to infinity. Our new idea is to design a population
e e
risk function f (W ; Œª[ÀÜ]) in (17) (see appendix), which is a lower bound of _f[ÀÜ]_ _,_ [when][ M][ and][ N][ are]
_D_ _D_

infinity. f (W ; Œª[ÀÜ]) is locally convex around its minimizer W [[ÀÜ]Œª], and W [Œª[ÀÜ]] approaches e **_W ‚àó_** as ÀÜŒª
increases. Then we show the iterates generated by _f[ÀÜ]_ _,_ [stay close to][ f] [(][W][ ; ÀÜ]Œª), and the returned
_D_ _D_

model W [(][L][)] is close to W [[ÀÜ]Œª]. New technical tools are developed to bound the distance between the

e

functions _f[ÀÜ]_ _,_ [and][ f] [(][W][ ; ÀÜ]Œª).
_D_ eD

4 EMPIRICAL RESULTS

4.1 SYNTHETIC DATA EXPERIMENTS


We generate a ground-truth neural network with the width K = 10. Each entry of W _[‚àó]_ is uniformly
selected from [‚àí2.5, 2.5]. The input of labeled data xn are generated from Gaussian distribution
_N_ (0, Id) independently, and the corresponding label yn is generated through (2) using W _[‚àó]. The_
unlabeled data **_xm are generated from N_** (0, _Œ¥[2]Id) independently with_ _Œ¥ = 1 except in Figure 7._
_d is set as 50 except in Figure 9. The value of Œª is selected as_ _N/(2Kd) except in Figure 8._

We consider one-hidden-layer except in Figure 4. The initial teacher model e [e] [e] **_W_** [(0)] in self-training

p

is randomly selected from {W |‚à•W ‚àí **_W_** _[‚àó]‚à•F /‚à•W_ _[‚àó]‚à•F ‚â§_ 0.5} to reduce the computation. In


-----

each iteration, the maximum number of SGD steps T is 10. Self-training terminates if ‚à•W [(][‚Ñì][+1)] _‚àí_
**_Wcurves are averaged over[(][‚Ñì][)]‚à•F /‚à•W_** [(][‚Ñì][)]‚à•F ‚â§ 10 1000[‚àí][4] or reaching independent trials, and the regions in lower transparency indicate 1000 iterations. In Figures 5 to 8, all the points on the
the corresponding one-standard-deviation error bars. Our empirical observations are summarized
below.

**(a) GF (testing performance) proportional to ‚à•W ‚àí** **_W_** _[‚àó]‚à•F . Figure 4 illustrates the GF in (3)_
against the distance to the ground truth W _[‚àó]. To visualize results for different networks together,_
GF is normalized in [0, 1], divided by its largest value for each network architecture. All the results
are averaged over 100 independent choice of W . One can see that for one-hidden-layer neural
networks, in a large region near W _[‚àó], GF is almost linear in ‚à•W ‚àí_ **_W_** _[‚àó]‚à•F . When the number of_
hidden layers increases, this region decreases, but the linear dependence still holds locally. This is
an empirical justification of using ‚à•W ‚àí **_W_** _[‚àó]‚à•F to evaluate the GF and, thus, the testing error in_
Theorems 1 and 2.

**(b) ‚à•W** [(][L][)] _‚àí_ **_W_** _[‚àó]‚à•F as a linear function of 1/‚àöM_ **. Figure 5 shows the relative error ‚à•W** [(][L][)] _‚àí_

**_W_** _[‚àó]‚à•F /‚à•W_ _[‚àó]‚à•F when the number of unlabeled data and labeled data changes. One can see that the_
relative error decreases when either M or N increases. Additionally, the dash-dotted lines represent
the best fitting of the linear functions of 1/‚àöM using the least square method. Therefore, the relative

error is indeed a linear function of 1/‚àöM, as predicted by our results in (11) and (15).


1

0.8

0.6

0.4 1-hidden-layer

5-hidden-layer

0.2 10-hidden-layer

50-hidden-layer

0

0 0.1 0.2 0.3 0.4 0.5


N = 250

0.99955 N = 300

N = 350

0.9995

0.99945

0.9994

Convergence rate0.99935

0.9993

0.0165 0.017 0.0175 0.018


4.3

4

3.5

Relative error (%)

3

200 400 600 800 1000

Number of unlabeled data (M)


Figure 4: The generalization
function against the distance to
the ground truth neural network


Figure 6: The convergence rate
with different M when N _<_
_N_ _[‚àó]._


Figure 5: The relative error
against the number of unlabeled
data.


**(c) Convergence rate as a linear function of 1/‚àöM** **. Figure 6 illustrates the convergence rate**

when M and N change. We can see that the convergence rate is a linear function of 1/‚àöM, as

predicted by our results (11) and (15). When M increases, the convergence rate is improved, and
the method converges faster.

**(d) Increase of** _Œ¥ slows down convergence. Figure 7 shows that the convergence rate becomes_
worse when the variance of the unlabeled data _Œ¥ increases from 1. When_ _Œ¥ is less than 1, the_
convergence rate almost remains the same, which is consistent with our characterization in (10) that[e]
the convergence rate is linear in ¬µ. From the discussion after (5),[e] _¬µ increases as[e]_ _Œ¥ increases from 1_
and stays constant when _Œ¥ is less than 1._

[e]

**(e) ‚à•W** [(][L][)]‚àíW _[‚àó]‚à•F /‚à•W[e][‚àó]‚à•F is improved as a linear function of_ _Œª[ÀÜ]. Figure 8 shows that the relative_
errors of W [(][L][)] with respect to W _[‚àó]_ decrease almost linearly when _Œª[ÀÜ] increases, which is consistent_
with the theoretical result in (11). Moreover, when Œª exceeds a certain threshold positively correlated
with N, the relative error increases rather than decreases. That is consistent with the analysis in (8)
that _Œª[ÀÜ] has an upper limit, and such a limit increases as N increases._

**(f) Unlabeled data reduce the sample complexity to learn W** _[‚àó]. Figure 9 depicts the phase tran-_
sition of returning W [(][L][)]. For every pair of d and N, we construct 100 independent trials, and each
the successful trials, while the block in black indicates all failures. Whentrial is said to be successful if ‚à•W [(][L][)] _‚àí_ **_W_** _[‚àó]‚à•F /‚à•W_ _[‚àó]‚à•F ‚â§_ 10[‚àí][2]. The white blocks correspond to d increases, the required
number of labeled data to learn W _[‚àó]_ is linear in d. Thus, the sample complexity bound in (12) is
order-wise optimal for d. Moreover, the phase transition line when M = 1000 is below the one
when M = 0. Therefore, with unlabeled data, the required sample complexity of N is reduced.


-----

6.9 N=200 680 680

6.5 N=240 620 620

N=280 560 560

6.1 500 500

440 440

5.7 380320 380320

Relative error (%) 260 260

5.3 200 200

0.1 0.2 0.3 0.4 0.5 20 26 32 38 44 50 20 26 32 38 44 50


1

0.995

Convergence rate 0.99

1 2 3 4 5


Figure 8: _‚à•W_ _‚à•[(][L]W[)]‚àí[‚àó]W‚à•F_ _[‚àó]‚à•F_

when _Œª[ÀÜ] and N change._


Figure 7: Convergence rate
with different _Œ¥[ÀÜ]._


Figure 9: Empirical phase transition of
the curves with (a) M = 0 and (b) M =
1000.


4.2 IMAGE CLASSIFICATION ON AUGMENTED CIFAR-10 DATASET

We evaluate self-training on the augmented CIFAR-10 dataset, which has 50K labeled data. The
unlabeled data are mined from 80 Million Tiny Images following the setup in (Carmon et al., 2019)[4],
and additional 50K images are selected for each class, which is a total of 500K images, to form the
unlabeled data. The self-training method is the same implementation as that in (Carmon et al., 2019).
_Œª and_ _Œª is selected as N/(M + N_ ) and M/(N + M ), respectively, and the algorithm stops after
200 epochs. In Figure 10, the dash lines stand for the best fitting of the linear functions of 1/‚àöM

via the least square method. One can see that the test accuracy is improved by up to 7% using

[e]

unlabeled data, and the empirical evaluations match the theoretical predictions. Figure 11 shows the
convergence rate calculated based on the first 50 epochs, and the convergence rate is almost a linear
function of 1/‚àöM, as predicted by (10).


0.962

0.96

0.958

0.956

0.954

0.952

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
||||||N=15K|


N=15K
N=30K
N=50K


5 6 7 8 9 10

10[-3]

Figure 11: The convergence rate against the
number of unlabeled data


90

85

Test accuracy (%)

80

0 100K 200K 300K 400K 500K

Number of unlabeled data (M)


Figure 10: The test accuracy against the
number of unlabeled data


5 CONCLUSION

This paper provides new theoretical insights into understanding the influence of unlabeled data in
the iterative self-training algorithm. We show that the improved generalization error and convergence rate is a linear function of 1/‚àöM, where M is the number of unlabeled data. Moreover,

compared with supervised learning, using unlabeled data reduces the required sample complexity of
labeled data for achieving zero generalization error. Future directions include generalizing the analysis to multi-layer neural networks and other semi-supervised learning problems such as domain
adaptation.


ACKNOWLEDGEMENT

This work was supported by AFOSR FA9550-20-1-0122, ARO W911NF-21-1-0255, NSF 1932196
and the Rensselaer-IBM AI Research Collaboration (http://airc.rpi.edu), part of the IBM AI Horizons Network (http://ibm.biz/AIHorizons).


4The codes are downloaded from https://github.com/yaircarmon/semisup-adv


-----

REFERENCES

Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. In Advances in neural information processing
_systems, pp. 6158‚Äì6169, 2019._

Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International
_Conference on Machine Learning, pp. 322‚Äì332. PMLR, 2019a._

Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In 36th In_ternational Conference on Machine Learning, ICML 2019, pp. 477‚Äì502. International Machine_
Learning Society (IMLS), 2019b.

Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. Advances in
_neural information processing systems, 2014._

Ainesh Bakshi, Rajesh Jayaram, and David P Woodruff. Learning two layer rectified neural networks
in polynomial time. In Conference on Learning Theory, pp. 195‚Äì268. PMLR, 2019.

Maria-Florina Balcan and Avrim Blum. A discriminative model for semi-supervised learning. Jour_nal of the ACM (JACM), 57(3):1‚Äì46, 2010._

Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463‚Äì482, 2002.

David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and
Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations, 2019a.

David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin
Raffel. Mixmatch: A holistic approach to semi-supervised learning. _arXiv preprint_
_arXiv:1905.02249, 2019b._

Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business Media, 2013.

Avrim L Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. Neural
_Networks, 5(1):117‚Äì127, 1992._

Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan.
Domain separation networks. In Proceedings of the 30th International Conference on Neural
_Information Processing Systems, pp. 343‚Äì351, 2016._

Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp.
605‚Äì614. JMLR. org, 2017.

Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns overparameterized networks that provably generalize on linearly separable data. In International
_Conference on Learning Representations, 2018._

Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
data improves adversarial robustness. Advances in Neural Information Processing Systems, 32:
11192‚Äì11203, 2019.

Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big
self-supervised models are strong semi-supervised learners. _Advances in Neural Information_
_Processing Systems, 33:22243‚Äì22255, 2020a._

Yining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma. Self-training avoids using spurious
features under domain shift. Advances in Neural Information Processing Systems, 33, 2020b.


-----

Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2018.

Geoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation. In International Conference on Learning Representations, number 6, 2018.

Haoyu Fu, Yuejie Chi, and Yingbin Liang. Guaranteed recovery of one-hidden-layer neural networks
via cross entropy. IEEE Transactions on Signal Processing, 68:3225‚Äì3235, 2020.

Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
_International conference on machine learning, pp. 1180‚Äì1189. PMLR, 2015._

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran√ßois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096‚Äì2030, 2016.

Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with land[scape design. In International Conference on Learning Representations, 2018. URL https:](https://openreview.net/forum?id=BkwHObbRZ)
[//openreview.net/forum?id=BkwHObbRZ.](https://openreview.net/forum?id=BkwHObbRZ)

Boqing Gong, Kristen Grauman, and Fei Sha. Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation. In International
_Conference on Machine Learning, pp. 222‚Äì230. PMLR, 2013._

Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Con_ference d‚Äôapprentissage CAp, pp. 281, 2005._

Jiangfan Han, Ping Luo, and Xiaogang Wang. Deep self-learning from noisy labels. In Proceedings
_of the IEEE/CVF International Conference on Computer Vision, pp. 5138‚Äì5147, 2019._

Junxian He, Jiatao Gu, Jiajun Shen, and Marc‚ÄôAurelio Ranzato. Revisiting self-training for neural
sequence generation. In International Conference on Learning Representations, 2019.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. volume 37 of Proceedings of Machine Learning Research, pp.
448‚Äì456, Lille, France, 07‚Äì09 Jul 2015. PMLR.

Arthur Jacot, Franck Gabriel, and Cl√©ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Proceedings of the 32nd International Conference on Neural
_Information Processing Systems, 2018._

Jacob Kahn, Ann Lee, and Awni Hannun. Self-training for end-to-end speech recognition. In
_ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing_
_(ICASSP), pp. 7084‚Äì7088. IEEE, 2020._

Adam Tauman Kalai, Adam R Klivans, Yishay Mansour, and Rocco A Servedio. Agnostically
learning halfspaces. SIAM Journal on Computing, 37(6):1777‚Äì1805, 2008.

Volodymyr Kuleshov, Arun Chaganty, and Percy Liang. Tensor factorization via matrix factorization. In Artificial Intelligence and Statistics, pp. 507‚Äì516, 2015.

Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint
_arXiv:1610.02242, 2016._

Yann A LeCun, L√©on Bottou, Genevieve B Orr, and Klaus-Robert M√ºller. Efficient backprop. In
_Neural networks: Tricks of the trade, pp. 9‚Äì48. Springer, 2012._

Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for
deep neural networks. In Workshop on challenges in representation learning, ICML, volume 3,
2013.

Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. In International Conference on
_Learning Representations, 2018._


-----

Kibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee. Overcoming catastrophic forgetting with unlabeled data in the wild. In Proceedings of the IEEE/CVF International Conference on Computer
_Vision, pp. 312‚Äì321, 2019._

Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157‚Äì
8166, 2018.

Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activation. In Advances in Neural Information Processing Systems, pp. 597‚Äì607. 2017.

Joerg Liebelt and Cordelia Schmid. Multi-view object class detection with a 3d geometric model.
In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp.
1688‚Äì1695. IEEE, 2010.

Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with
deep adaptation networks. In International conference on machine learning, pp. 97‚Äì105. PMLR,
2015.

Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
_analysis and machine intelligence, 41(8):1979‚Äì1993, 2018._

Luca Oneto, Davide Anguita, Alessandro Ghio, and Sandro Ridella. The impact of unlabeled patterns in rademacher complexity theory for kernel classifiers. Advances in neural information
_processing systems, 24:585‚Äì593, 2011._

Samet Oymak and Talha Cihad Gulcu. Statistical and algorithmic insights for semi-supervised
learning with self-training. arXiv preprint arXiv:2006.11006, 2020.

Samet Oymak and Mahdi Soltanolkotabi. End-to-end learning of a convolutional neural network via
deep tensor decomposition. arXiv preprint arXiv: 1805.06523, 2018.

Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding
and mitigating the tradeoff between robustness and accuracy. In International Conference on
_Machine Learning, pp. 7909‚Äì7919. PMLR, 2020._

Scott E Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew
Rabinovich. Training deep neural networks on noisy labels with bootstrapping. In ICLR (Work_shop), 2015._

Philippe Rigollet. Generalization error bounds in semi-supervised classification under the cluster
assumption. Journal of Machine Learning Research, 8(7), 2007.

Chuck Rosenberg, Martial Hebert, and Henry Schneiderman. Semi-supervised self-training of object
detection models. In Proceedings of the Seventh IEEE Workshops on Application of Computer
_Vision (WACV/MOTION‚Äô05)-Volume 1-Volume 01, pp. 29‚Äì36, 2005._

Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.
In International Conference on Machine Learning, pp. 4430‚Äì4438, 2018.

Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. Advances in neural information
_processing systems, 29:1163‚Äì1171, 2016._

Henry Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transac_tions on Information Theory, 11(3):363‚Äì371, 1965._

Aarti Singh, Robert Nowak, and Jerry Zhu. Unlabeled data: Now it helps, now it doesn‚Äôt. Advances
_in neural information processing systems, 21:1513‚Äì1520, 2008._

Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel,
Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised
learning with consistency and confidence. Advances in Neural Information Processing Systems,
33, 2020.


-----

Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
_Theory, 65(2):742‚Äì769, 2018._

Jong-Chyi Su, Subhransu Maji, and Bharath Hariharan. When does self-supervision improve fewshot learning? In European Conference on Computer Vision, pp. 645‚Äì666. Springer, 2020.

Kevin Tang, Vignesh Ramanathan, Fei-Fei Li, and Daphne Koller. Shifting weights: Adapting
object detectors from image to video. In Advances in Neural Information Processing Systems, pp.
647‚Äì655, 2012.

Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Proceedings of the 31st Interna_tional Conference on Neural Information Processing Systems, pp. 1195‚Äì1204, 2017._

Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
_mathematics, 12(4):389‚Äì434, 2012._

Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion:
Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.

David Vazquez, Antonio M Lopez, Javier Marin, Daniel Ponsa, and David Geronimo. Virtual and
real world adaptation for pedestrian detection. IEEE transactions on pattern analysis and machine
_intelligence, 36(4):797‚Äì809, 2013._

Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
_arXiv:1011.3027, 2010._

Gang Wang, Georgios B Giannakis, and Jie Chen. Learning relu networks on linearly separable
data: Algorithm, optimality, and generalization. IEEE Transactions on Signal Processing, 67(9):
2357‚Äì2370, 2019.

Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. Theoretical analysis of self-training
with deep networks on unlabeled data. In International Conference on Learning Representations,
2020.

Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student
improves imagenet classification. In Proceedings of the IEEE/CVF Conference on Computer
_Vision and Pattern Recognition, pp. 10687‚Äì10698, 2020._

I Zeki Yalniz, Herv√© J√©gou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semisupervised learning for image classification. arXiv preprint arXiv:1905.00546, 2019.

David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd
_annual meeting of the association for computational linguistics, pp. 189‚Äì196, 1995._

Kun Zhang, Bernhard Sch√∂lkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under
target and conditional shift. In International Conference on Machine Learning, pp. 819‚Äì827.
PMLR, 2013.

Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Guaranteed convergence
of training convolutional neural networks via accelerated gradient descent. In 2020 54th An_[nual Conference on Information Sciences and Systems (CISS), 2020a. URL doi:10.1109/](doi: 10.1109/CISS48834.2020.1570627111)_
[CISS48834.2020.1570627111.](doi: 10.1109/CISS48834.2020.1570627111)

Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Fast learning of graph neural
networks with guaranteed generalizability:one-hidden-layer case. In 2020 International Confer_ence on Machine Learning (ICML), 2020b._

Shuai Zhang, Meng Wang, Jinjun Xiong, Sijia Liu, and Pin-Yu Chen. Improved linear convergence
of training cnns with generalizability guarantees: A one-hidden-layer case. IEEE Transactions
_on Neural Networks and Learning Systems, 32(6):2622‚Äì2635, 2020c._


-----

Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Why lottery ticket wins? a theoretical perspective of sample complexity on pruned neural networks. In Thirty-fifth Conference
_on Neural Information Processing Systems (NeurIPS), 2021._

Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer relu
networks via gradient descent. In The 22nd International Conference on Artificial Intelligence
_and Statistics, pp. 1524‚Äì1534. PMLR, 2019._

Yuchen Zhang, Jason D. Lee, and Michael I. Jordan. L1-regularized neural networks are improperly
learnable in polynomial time. In Proceedings of The 33rd International Conference on Machine
_Learning, volume 48, pp. 993‚Äì1001, 2016._

Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. In Proceedings of the 34th International Conference
_on Machine Learning-Volume 70, pp. 4140‚Äì4149. JMLR. org, https://arxiv.org/abs/1706.03175,_
2017.

Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, and Quoc Le.
Rethinking pre-training and self-training. Advances in Neural Information Processing Systems,
33, 2020.

Yang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic segmentation via class-balanced self-training. In Proceedings of the European conference
_on computer vision (ECCV), pp. 289‚Äì305, 2018._


-----

## Appendix

A OVERVIEW OF THE PROOF TECHNIQUES

We first provide an overview of the techniques used in proving Theorems 1 and 2.

**1. Characterization of a proper population risk function. To characterize the performance of the**
iterative self-training algorithm via the stochastic gradient descent method, we need first to define
a population risk function such that the following two properties hold. First, the landscape of the
population risk function should be analyzable near {W [(][‚Ñì][)]}‚Ñì[L]=0[. Second, the distance between the]
empirical risk function in (1) and the population risk function should be bounded near {W [(][‚Ñì][)]}‚Ñì[L]=0[.]
The generalization function defined in (3), which is widely used in the supervised learning problem
with a sufficient number of samples, failed the second requirement. To this end, we turn to find a
new population risk function defined in (17), and the illustrations of the population risk function and
objection function are included in Figure 12.

**2. Local convex region of the population risk function. The purpose is to characterize the it-**
erations via the stochastic gradient descent method in the population risk function. To obtain the
local convex region of the population risk function, we first bound the Hessian of the population risk
function at its global optimal. Then, we utilize Lemma 12 in Appendix H.1 to obtain the Hessian of
the population risk function near the global optimal. The local convex region of the population risk
function is summarized in Lemma 1, and the proof of Lemma 1 is included in Appendix H.1.

**3. Bound between the population risk and empirical risk functions. After the characterization of**
the iterations via the stochastic gradient descent method in the population risk function, we need to
bound the distance between the population risk function and empirical risk function. Therefore, the
behaviors of the iterations via the stochastic gradient descent method in the empirical risk function
can be described by the ones in the population risk function and the distance between these two. The
key lemma is summarized in Lemma 2 (see Appendix H.2), and the proof is included in Appendix
H.2.

|Col1|Col2|Col3|n|Col5|
|---|---|---|---|---|
||||||
|||of ùëìÃÇ ùíüùëìùíünear ùëæ(ùëæ0,0) ùúÇùúÇùúÇ ùúÇùëìÃÇùëì ùíüùíü ùõΩùõΩ(ùëæùëæ0,1 ‚àí|ùëæùëæ(0,0))|Objective function: ùëìÃÇ ùíüùëìùíü(ùëæ)ùëæ|
||||0,ùëáùëá)ùëæùëæ[||



Figure 12: The landscapes of the objection function and population risk function.

In the following contexts, the details of the iterative self-training algorithm are included in Appendix
B. We then first provide the proof of Theorem 2 in Appendix E, which can be viewed as a special
case of Theorem 1. Then, with the preliminary knowledge from proving Theorem 2, we turn to
present the full proof of a more general statement summarized in Theorem 3 (see Appendix F),
which is related to Theorem 1. The definition and relative proofs of ¬µ and œÅ are all included in
Appendix G. The proofs of preliminary lemmas are included in Appendix H.

B ITERATIVE SELF-TRAINING ALGORITHM

In this section, we implement the details of the mini-batch stochastic gradient descent used in each
stage of the iterative self-training algorithm. After t number of iterations via mini-batch stochastic


-----

gradient descent at ‚Ñì-th stage of self-training algorithm, the learned model is denoted as W [(][‚Ñì,t][)]. One
can easily check that W [(][‚Ñì][)] in the main context is denoted as W [(][‚Ñì,][0)] in this section and the following
proofs. Last, the pseudo-code of the iterative self-training algorithm is summarized in Algorithm 1.

**Algorithm 1 Iterative Self-Training Algorithm**

**Input: labeled D = {(xn, yn)}n[N]=1[, unlabeled data][ e]D = {xm}m[M]=1[, and gradient step size][ Œ∑][;]**

**Initialization: preliminary teacher model with weights W** [(0][,][0)];

e


**Partition: randomly and independently pick data from D and** _D to form T subsets {Dt}t[T]=0[ ‚àí][1]_ [and]
_{Dt}t[T]=0[ ‚àí][1][, respectively;]_

[e]

**for ‚Ñì** = 0, 1, _, L_ 1 do

[e] _¬∑ ¬∑ ¬∑_ _‚àí_


_ym = g(W_ [(][‚Ñì,][0)]; **_xm) for m = 1, 2, ¬∑ ¬∑ ¬∑, M_**

**for t = 0, 1,** _, T_ 1 do
_¬∑ ¬∑ ¬∑_ e _‚àí_

**_W_** [(][‚Ñì,t][+1)] = W [(][‚Ñì,t][)] _‚àí_ _Œ∑ ¬∑ ‚àáf[ÀÜ]Dt,Dt_ [(][W][ (][‚Ñì,t][)][) +][ Œ≤][ ¬∑] **_W_** [(][‚Ñì,t][)] _‚àí_ **_W_** [(][‚Ñì,t][‚àí][1)][]

**end for** e  

**_W_** [(][‚Ñì][+1][,][0)] = W [(][‚Ñì,T][ )]

**end for**

C NOTATIONS

In this section, we first introduce some important notations that will be used in the following proofs,
and the notations are summarized in Table 1.

As shown in Algorithm 1, W [(][‚Ñì,t][)] denotes the learned model after t number of iterations via minibatch stochastic gradient descent at ‚Ñì-th stage of the iterative self-training algorithm. Given a student
model **_W, the pseudo label for_** **_x ‚àà_** _D[e] is generated as_

_yÀú = g(W[f] ;_ **_x)._** (16)

[f] e

Further, let W [[][p][]] = pW _[‚àó]_ + (1 ‚àí _p)W_ [(0][,][0)], we then define the population risk function as

e

2 _Œª_ 2

_f_ (W ; p) = _[Œª]2_ [E][x] _y[‚àó](p) ‚àí_ _g(W ; x)_ + 2 [E]x[e] _y[‚àó](p) ‚àí_ _g(W ;_ **_x)_** _,_ (17)

where y[‚àó](p) = g(W [[][p][]]; x) with x (0, Œ¥[2]I) and _y[‚àó]e(p) =_ _g(W_ [[][p][]]; **_x) withx_** (0, _Œ¥[Àú][2]I)._
_‚àºN_ e e _‚àºN_
When p = 1, we have W [[][p][]] = W _[‚àó]_ and y[‚àó](p) = y for data in D.

Moreover, we use œÉi to denote the i-th largest singular value of e **_W_** _[‚àó]. Then, e Œ∫ is defined as e_ _œÉ1/œÉK,_
and Œ≥ = _i=1_ _[œÉ][i][/œÉ][K][. Additionally, to avoid high dimensional tensors, the first order derivative of]_
the empirical risk function is defined in the form of vectorized W as

[Q][K] _‚àÇf_ _T_ _T_ _T_ _T_

_f_ (W ) = _, [‚àÇf]_ _,_ _, [‚àÇf]_ R[dK] (18)
_‚àá_ [ÀÜ] _‚àÇw1_ _‚àÇw2_ _¬∑ ¬∑ ¬∑_ _‚àÇwK_ _‚àà_
h i

with W = [w1, w2, ¬∑ ¬∑ ¬∑, wK] ‚àà R[d][√ó][K]. Therefore, the second order derivative of the empirical risk function is in R[dk][√ó][dk]. Similar to (18), the high order derivatives of the population risk
functions are defined based on vectorized W as well. In addition, without special descriptions,
**_Œ± = [Œ±[T]1_** _[,][ Œ±]2[T]_ _[,][ ¬∑ ¬∑ ¬∑][,][ Œ±]K[T]_ []][T][ stands for any unit vector that in][ R][dK][ with][ Œ±][j][ ‚àà] [R][d][. Therefore, we have]

_[K]_ _‚àÇf[ÀÜ]_ 2

_f_ 2 = max _f_ **_Œ±_** 2 = max **_Œ±[T]j_** _._ (19)
_‚à•‚àá[2][ ÀÜ]‚à•_ **_Œ±_** _‚à•_ **_Œ±_** _‚àÇwj_

_[‚à•][Œ±][T][ ‚àá][2][ ÀÜ]_  Xj=1 


-----

Finally, since we focus on order-wise analysis, some constant numbers will be ignored in the majority of the steps. In particular, we use h1(z) ‚â≥ (or ‚â≤, ‚âÇ)h2(z) to denote there exists some positive
constant C such that h1(z) ‚â• (or ‚â§, =)C ¬∑ h2(z) when z ‚àà R is sufficiently large.

Table 3: Some Important Notations

|D = {x n, y n}N n=1|Labeled dataset with N number of samples;|
|---|---|
|De = {x em}M m=1|Unlabeled dataset with M number of samples;|
|Dt = {x n, y n}N n=t 1|a subset of D with N t number of labeled data;|
|Det = {x em}M m=t 1|a subset of De with M t number of unlabeled data;|
|d|Dimension of the input x orx; e|
|K|Number of neurons in the hidden layer;|
|W‚àó|Weights of the ground truth model;|
|W [p]|W [p] = pW ‚àó+ (1 ‚àíp)W (0,0);|
|W (‚Ñì,t)|Model returned by iterative self-training after t step mini-batch stochastic gradient de- scent at stage ‚Ñì; W (0,0) is the initial model;|
|fÀÜ ( or fÀÜ) D, De|The empirical risk function defined in (1);|
|f(W ; p)|The population risk function defined in (17);|
|ŒªÀÜ|The value of ŒªŒ¥2/(ŒªŒ¥2 + ŒªeŒ¥Àú2);|
|¬µ|ŒªŒ¥2+ŒªeŒ¥Àú2 The value of ; ŒªœÅ(Œ¥)+ŒªeœÅ(Œ¥Àú)|
|œÉ i|The i-th largest singular value of W‚àó;|
|Œ∫|The value of œÉ /œÉ ; 1 K|
|Œ≥|The value of QK œÉ i/œÉ K; i=1|
|q|Some large constant in R+;|



D PRELIMINARY LEMMAS

We will first start with some preliminary lemmas. As outlined at the beginning of the supplementary
material, Lemma 1 illustrates the local convex region of the population risk function, and Lemma
2 explains the error bound between the population risk and empirical risk functions. Then, Lemma
3 describes the returned initial model W [(0][,][0)] via tensor initialization method (Zhong et al., 2017)
purely using labeled data. Next, Lemma 4 is the well known Weyl‚Äôs inequality in the matrix setting.
Moreover, Lemma 5 is the concentration theorem for independent random matrices. The definitions
of the sub-Gaussian and sub-exponential variables are summarized in Definitions 1 and 2. Lemmas
6 and 7 serve as the technical tools in bounding matrix norms under the framework of the confidence
interval.

**Lemma 1. Given any W ‚àà** R[d][√ó][K], let p satisfy

_œÉK_
_p ‚â≤_ _._ (20)

_¬µ[2]K_ **_W_** **_W_** _F_
_¬∑ ‚à•_ _‚àí_ _[‚àó]‚à•_


_Then, we have_


_ŒªœÅ(Œ¥) +_ _ŒªœÅ(Œ¥[Àú])_ _ŒªŒ¥[Àú][2])_

_f_ (W ; p)
12Œ∫[2]Œ≥K[e] [2] _‚™Ø‚àá[2]_ _‚™Ø_ [7(][ŒªŒ¥][2]K[ +][ e]


(21)


-----

**Lemma 2. Let f and** _f[ÀÜ] be the functions defined in (17) and (1), respectively. Suppose the pseudo_
_label is generated through (16) with weights_ **_W . Then, we have_**

_d log q_ _ŒªŒ¥[Àú][2]_ _d log q_

_‚à•‚àáf_ (W ) ‚àí‚àáf[ÀÜ](W )‚à•2 ‚â≤ _[ŒªŒ¥]K[2]_ _N_ _¬∑ ‚à•[f]W ‚àí_ **_W_** _[‚àó]‚à•_ + _K_ _M_ _¬∑ ‚à•W ‚àí_ **_W[f] ‚à•2_**

r r (22)

e

_ŒªŒ¥[2]_ **_W_** **_W_** [[][p][]][] + _ŒªŒ¥[Àú][2]_ **_W_** **_W_** [[][p][]][]

+ _¬∑_ _‚àí_ _¬∑_ _[‚àó]_ _‚àí_ 2

2K

 f [e]  

_with probability at least 1 ‚àí_ _q[‚àí][d]._

**Lemma 3 (Initialization, (Zhong et al., 2017)). Assuming the number of labeled data satisfies**

_N ‚â•_ _p[2]N_ _[‚àó]_ (23)

_for some large constant q and p ‚àà_ [ _K[1]_ _[,][ 1]][, the tensor initialization method, which is summarized in]_

_Appendix I, outputs W_ [(0][,][0)] _such that_

**_W_** [(0][,][0)] **_W_** _F_ _œÉK_ (24)
_‚à•_ _‚àí_ _[‚àó]‚à•_ _‚â§_ _p_ _c(Œ∫)¬µ[2]K_

_¬∑_

_with probability at least 1 ‚àí_ _q[‚àí][d]._

**Lemma 4 (Weyl‚Äôs inequality, (Bhatia, 2013)). Let B = A + E be a matrix with dimension m √ó m.**
_Let Œªi(B) and Œªi(A) be the i-th largest eigenvalues of B and A, respectively. Then, we have_

_Œªi(B)_ _Œªi(A)_ **_E_** 2, _i_ [m]. (25)
_|_ _‚àí_ _| ‚â§‚à•_ _‚à•_ _‚àÄ_ _‚àà_

**Lemma 5 ((Tropp, 2012), Theorem 1.6). Consider a finite sequence {Zk} of independent, random**
_matrices with dimensions d1_ _d2. Assume that such random matrix satisfies_
_√ó_

E(Zk) = 0 _and_ _‚à•Zk‚à•‚â§_ _R_ _almost surely._
_Define_
_Œ¥[2]_ := max E(ZkZk[‚àó][)] _,_ E(Zk[‚àó][Z][k][)] _._
n X X o


_Then for all t ‚â•_ 0, we have

_Prob_


_t[2]/2_
(d1 + d2) exp _‚àí_
_‚â§_ _Œ¥[2]_ + Rt/3



_Prob_ **_Zk_** (d1 + d2) exp _‚àí_ _._

( Xk _[‚â•]_ _[t])_ _‚â§_  _Œ¥[2]_ + Rt/3 

**Definition 1 (Definition 5.7, (Vershynin, 2010)). A random variable X is called a sub-Gaussian**
_random variable if it satisfies_
(E|X|[p])[1][/p] _‚â§_ _c1‚àöp_ (26)

_for all p ‚â•_ 1 and some constant c1 > 0. In addition, we have

Ee[s][(][X][‚àí][E][X][)] _‚â§_ _e[c][2][‚à•][X][‚à•]œà[2]_ 2 _[s][2]_ (27)

_for all s ‚àà_ R and some constant c2 > 0, where ‚à•X‚à•œÜ2 is the sub-Gaussian norm of X defined as
_X_ _œà2 = supp_ 1 p[‚àí][1][/][2](E _X_ )[1][/p].
_‚à•_ _‚à•_ _‚â•_ _|_ _|[p]_

_Moreover, a random vector X ‚àà_ R[d] _belongs to the sub-Gaussian distribution if one-dimensional_
_marginal Œ±[T]_ **_X is sub-Gaussian for any Œ± ‚àà_** R[d], and the sub-Gaussian norm of X is defined as
**_X_** _œà2 = sup_ **_Œ±_** 2=1 **_Œ±[T]_** **_X_** _œà2_ _._
_‚à•_ _‚à•_ _‚à•_ _‚à•_ _‚à•_ _‚à•_

**Definition 2 (Definition 5.13, (Vershynin, 2010)). A random variable X is called a sub-exponential**
_random variable if it satisfies_
(E|X|[p])[1][/p] _‚â§_ _c3p_ (28)

_for all p ‚â•_ 1 and some constant c3 > 0. In addition, we have

Ee[s][(][X][‚àí][E][X][)] _‚â§_ _e[c][4][‚à•][X][‚à•]œà[2]_ 1 _[s][2]_ (29)

_for s ‚â§_ 1/‚à•X‚à•œà1 and some constant c4 > 0, where ‚à•X‚à•œà1 is the sub-exponential norm of X
_defined as_ _X_ _œà1 = supp_ 1 p[‚àí][1](E _X_ )[1][/p].
_‚à•_ _‚à•_ _‚â•_ _|_ _|[p]_


**_Zk_**


-----

**Lemma 6 (Lemma 5.2, (Vershynin, 2010)). Let B(0, 1) ‚àà{Œ±** _‚à•Œ±‚à•2 = 1, Œ± ‚àà_ R[d]} denote a
_unit ball in R[d]. Then, a subset SŒæ is called a Œæ-net of B(0, 1) if every point z ‚ààB(0, 1) can be_
_approximated to within Œæ by some point Œ±_ (0, 1), i.e., **_z_** **_Œ±_** 2 _Œæ. Then the minimal_
_cardinality of a Œæ-net SŒæ satisfies_ _‚ààB_ _‚à•_ _‚àí_ _‚à•_ _‚â§_
_Œæ_ (1 + 2/Œæ)[d]. (30)
_|S_ _| ‚â§_

**Lemma 7 (Lemma 5.3, (Vershynin, 2010)). Let A be an d1 √ó d2 matrix, and let SŒæ(d) be a Œæ-net**
_of B(0, 1) in R[d]_ _for some Œæ ‚àà_ (0, 1). Then

**_A_** 2 (1 _Œæ)[‚àí][1]_ max 1 **_[AŒ±][2][|][.]_** (31)
_‚à•_ _‚à•_ _‚â§_ _‚àí_ **_Œ±1‚ààSŒæ(d1),Œ±2‚ààSŒæ(d2)_** _[|][Œ±][T]_

**Lemma 8 (Mean Value Theorem). Let U ‚äÇ** R[n][1] _be open and f : U ‚àí‚Üí_ R[n][2] _be continuously_
_differentiable, and x ‚àà_ **_U_** _, h ‚àà_ R[n][1] _vectors such that the line segment x + th, 0 ‚â§_ _t ‚â§_ 1 remains
_in U_ _. Then we have:_

1
**_f_** (x + h) ‚àí **_f_** (x) = 0 _‚àáf_ (x + th)dt _¬∑ h,_
Z 

_where ‚àáf denotes the Jacobian matrix of f_ _._

E PROOF OF THEOREM 2

With p = 1 in (17), the population risk function is reduced as


_Œª_

_f_ (W ) = _[Œª]_ **_x[(]y[e][‚àó]_** _g(W ;_ **_x)),_** (32)

2 [E][x][(][y][ ‚àí] _[g][(][W][ ;][ x][)) +]_ 2 [E][e] _‚àí_
e

where y = g(W ; x) with x (0, Œ¥[2]I) and _y[‚àó]_ = g(W ; **_x) with_** **_x_** (0, _Œ¥[Àú][2]I). In fact,_

_[‚àó]_ _‚àºN_ _[‚àó]_ e _‚àºN_
(32) can be viewed as the expectation of the empirical risk function in (1) given _ym = g(W_ _[‚àó];_ **_xm)._**
Moreover, the ground-truth model W _[‚àó]_ is the global optimal to (32) as well. Lemmas 9 and 10 e e e
are the special case of Lemmas 1 and 2 with p = 1. The proof of Theorem 2 is followed by the
e e
presentation of the two lemmas.

The main idea in proving Theorem 2 is to characterize the gradient descent term by the MVT in
Lemma 8 as shown in (36) and (37). The IVT is not directly applied in the empirical risk function because of its non-smoothness. However, the population risk functions defined in (17) and
(32), which are the expectations over the Gaussian variables, are smooth. Then, as the distance
_‚à•‚àáf_ (W ) ‚àí‚àáf (W _[‚àó])‚à•F is upper bounded by a linear function of ‚à•W ‚àí_ **_W_** _[‚àó]‚à•F as shown in (47),_
we can establish the connection between ‚à•W [(][‚Ñì,t][+1)] _‚àí_ **_W_** _[‚àó]‚à•F and ‚à•W_ [(][‚Ñì,t][)] _‚àí_ **_W_** _[‚àó]‚à•F as shown in_
(50). Finally, by mathematical induction over ‚Ñì and t, one can characterize ‚à•W [(][L,][0)] _‚àí_ **_W_** _[‚àó]‚à•F by_
_‚à•W_ [(0][,][0)] _‚àí_ **_W_** _[‚àó]‚à•F as shown in (52), which completes the whole proof._

**Lemma 9 (Lemma 1 with p = 1). Let f and** _f[ÀÜ] are the functions defined in (32) and (1), respectively._
_Then, for any W that satisfies,_
**_W_** **_W_** _F_ (33)
_‚à•_ _‚àí_ _[‚àó]‚à•_ _‚â§_ _¬µ[œÉ][2][K]K [,]_

_we have_
_ŒªœÅ(Œ¥) +_ _ŒªœÅ(Œ¥[Àú])_ _ŒªŒ¥[Àú][2])_

_f_ (W ) _._ (34)

**Lemma 10 (Lemma 2 with p = 112Œ∫[2])Œ≥K. Let[e]** [2] f and‚™Ø‚àáf[ÀÜ] be the functions defined in[2] _‚™Ø_ [7(][ŒªŒ¥][2]K[ +][ e] (32) and (1), respectively.
_Suppose the pseudo label is generated through (16) with weights_ **_W . Then, we have_**

_ŒªŒ¥2_ _d log q_ _Œ¥[2]_ _d log q_
_‚à•‚àáf_ (W ) ‚àí‚àáf[ÀÜ](W )‚à•2 ‚â≤ _K_ _N_ + [(1][ ‚àí]K[Œª][)Àú] [f]M _¬∑ ‚à•W ‚àí_ **_W_** _[‚àó]‚à•2_

r r (35)

 

+ [(1][ ‚àí] _[Œª][)Àú]Œ¥[2]_ _d log q_ + [1] **_W_** **_W_** 2

_K_ _M_ 2 _¬∑ ‚à•_ [f] ‚àí _[‚àó]‚à•_

[r] 

_with probability at least 1 ‚àí_ _q[‚àí][d]._


-----

_Proof of Theorem 2. From Algorithm 1, in the ‚Ñì-th outer loop, we have_

**_W_** [(][‚Ñì,t][+1)] =W [(][‚Ñì,t][)] _‚àí_ _Œ∑‚àáf[ÀÜ]Dt,Dt_ [(][W][ (][‚Ñì,t][)][) +][ Œ≤][(][W][ (][‚Ñì,t][)][ ‚àí] **_[W][ (][‚Ñì,t][‚àí][1)][)]_**

=W [(][‚Ñì,t][)] _Œ∑_ _f_ (W [(][‚Ñì,t][)]) + Œ≤(W [(][‚Ñì,t][)] **_W_** [(][‚Ñì,t][‚àí][1)]) (36)

e

_‚àí_ _‚àá_ _‚àí_

+ Œ∑ ¬∑ _‚àáf_ (W [(][‚Ñì,t][)]) ‚àí‚àáf[ÀÜ]Dt,Dt [(][W][ (][‚Ñì,t][)][)] _._

Since ‚àáf is a smooth function and  W _[‚àó]_ is a local (global) optimal to e _f_, then we have

_‚àáf_ (W [(][‚Ñì,t][)]) =‚àáf (W [(][‚Ñì,t][)]) ‚àí‚àáf (W _[‚àó])_

1 (37)

= 0 _‚àá[2]f_ **_W_** [(][‚Ñì,t][)] + u ¬∑ (W [(][‚Ñì,t][)] _‚àí_ **_W_** _[‚àó])_ _du ¬∑ (W_ [(][‚Ñì,t][)] _‚àí_ **_W_** _[‚àó]),_

Z  

where the last equality comes from MVT in Lemma 8. For notational convenience, we use H [(][‚Ñì,t][)]
to denote the integration as

1
**_H_** [(][‚Ñì,t][)] := 0 _‚àá[2]f_ **_W_** [(][‚Ñì,t][)] + u ¬∑ (W [(][‚Ñì,t][)] _‚àí_ **_W_** _[‚àó])_ _du._ (38)
Z  


Then, we have
**_W_** [(][‚Ñì,t][+1)] _‚àí_ **_W_** _[‚àó]_ = **_I ‚àí_** _Œ∑H_ [(][‚Ñì,t][)] _Œ≤I_ **_W_** [(][‚Ñì,t][)] _‚àí_ **_W_** _[‚àó]_
" **_W_** [(][‚Ñì,t][)] **_W_** # " **_I_** **0** # "W [(][‚Ñì,t][‚àí][1)] **_W_**

_‚àí_ _[‚àó]_ _‚àí_ _[‚àó]_

+ Œ∑ _‚àáf_ (W [(][‚Ñì,t][)]) ‚àí‚àáf[ÀÜ]Dt,Dt [(][W][ (][‚Ñì,t][)][)]

" **0**
e

Let H [(][‚Ñì,t][)] = SŒõS[T] be the eigen-decomposition of H [(][‚Ñì,t][)]. Then, we define


(39)

(40)

share the same


**_S[T]_**


**_I ‚àí_** _Œ∑Œõ + Œ≤I_ _Œ≤I_


**_A(Œ≤) :=_**


**_A(Œ≤)_**


**_S[T]_**


**_S[T]_**
# " **0**


**_I ‚àí_** _Œ∑Œõ + Œ≤I_ _Œ≤I_


Since


, we know A(Œ≤) and


**_S[T]_**


eigenvalues. Let Œ≥i[(][Œõ][)] be the i-th eigenvalue of _f_ (w[(][t][)]), then the corresponding i-th eigenvalue
_‚àá[2]_
of (40), denoted by Œ≥i[(][A][)], satisfies
b

(Œ≥i[(][A][)](Œ≤))[2] (1 _Œ∑Œ≥i[(][Œõ][)]_ + Œ≤)Œ≥i[(][A][)](Œ≤) + Œ≤ = 0. (41)
_‚àí_ _‚àí_

By simple calculation, we have

2

_‚àöŒ≤,_ if _Œ≤_ 1 _Œ∑Œ≥i[(][Œõ][)]_ _,_

_|Œ≥i[(][A][)](Œ≤)| =_ Ô£±Ô£¥Ô£≤ 21 _i_ + ‚â• Œ≤ ) + ‚àí q(1 ‚àí _Œ∑Œ≥i[(][Œõ][)]_ + Œ≤)[2] _‚àí_ 4Œ≤ _[,][ otherwise][.]_ (42)

q

Specifically, we have Ô£¥Ô£≥ [(1][ ‚àí] _[Œ∑Œ≥][(][Œõ][)]_

_Œ≥i[(][A][)](0) > Œ≥i[(][A][)](Œ≤),_ for _Œ≤_ 0, (1 _Œ∑Œ≥i[(][Œõ][)])[2][],_ (43)
_‚àÄ_ _‚àà_ _‚àí_

2

 

and Œ≥i[(][A][)] achieves the minimum Œ≥i[(][A][)][‚àó] = 1 _Œ∑Œ≥i[(][Œõ][)]_ when Œ≤ = 1 _Œ∑Œ≥i[(][Œõ][)]_ . From Lemma
_‚àí_ _‚àí_

9, for any a ‚àà R[d] with ‚à•a‚à•2 = 1, we have q  q 

1 1
**_a[T]_** _‚àáf_ (W [(][‚Ñì,t][)])a = 0 **_a[T]_** _‚àá[2]f_ **_W_** [(][‚Ñì,t][)] + u ¬∑ (W [(][‚Ñì,t][)] _‚àí_ **_W_** _[‚àó])_ **_adu ‚â§_** 0 _Œ≥max‚à•a‚à•2[2][du][ =][ Œ≥][max][,]_
Z 1   Z 1

**_a[T]_** _‚àáf_ (W [(][‚Ñì,t][)])a = 0 **_a[T]_** _‚àá[2]f_ **_W_** [(][‚Ñì,t][)] + u ¬∑ (W [(][‚Ñì,t][)] _‚àí_ **_W_** _[‚àó])_ **_adu ‚â•_** 0 _Œ≥min‚à•a‚à•2[2][du][ =][ Œ≥][min][,]_
Z   Z (44)


-----

where Œ≥max = [7(][ŒªŒ¥][2]K[+]Œª[e]Œ¥[Àú][2]), and Œ≥min = _[ŒªœÅ]12[(][Œ¥]Œ∫[)+][2]Œ≥KŒªœÅ[e]_ ([2]Œ¥[Àú][ . Therefore, we have])

_ŒªœÅ(Œ¥[Àú])_ _ŒªŒ¥[Àú][2])_
_Œ≥min[(][Œõ][)]_ [=][ ŒªœÅ][(][Œ¥][) +][ e] _,_ and _Œ≥max[(][Œõ][)]_ [= 7(][ŒªŒ¥][2][ +][ e] _._

12Œ∫[2]Œ≥K [2] _K_

Thus, we can select Œ∑ = _‚àöŒ≥max[(][Œõ][)]_ [+]1 _Œ≥min[(][Œõ][)]_ 2, and ‚à•A(Œ≤)‚à•2 can be bounded by
  q 

_ŒªœÅ(Œ¥) +_ _ŒªœÅ(Œ¥[Àú])_ _ŒªŒ¥[Àú][2])_

min _/_ 2 [7(][ŒªŒ¥][2][ +][ e]
_Œ≤_ s 12Œ∫[2]Œ≥K [2] _¬∑_ _K_

_[‚à•][A][(][Œ≤][)][‚à•][2][ ‚â§][1][ ‚àí]_

  [e]   

_¬µ(Œ¥,_ _Œ¥[Àú])_
=1 ‚àí 168Œ∫[2]Œ≥K _,_

1/2

where ¬µ(Œ¥, _Œ¥[Àú]) =_ _ŒªœÅ(Œ¥)+ŒªœÅ[e]_ (Œ¥[Àú]) . p

_ŒªŒ¥[2]+Œª[e]Œ¥[Àú][2]_

 

From Lemma 10, we have


(45)

(46)

(47)

(48)

(49)


_ŒªŒ¥2_
_f_ (W [(][‚Ñì,t][)]) _f_ (W [(][‚Ñì,t][)]) 2 =
_‚à•‚àá_ _‚àí‚àá_ [ÀÜ] _‚à•_ _K_


_ŒªŒ¥[Àú][2]_


_d log q_ _ŒªŒ¥[Àú][2]_

+
_Nt_ _K_
e

_d log q_ + [1]

_Mt_ 2

[r]


_d log q_

_Mt_


**_W_** [(][‚Ñì,t][)] **_W_** 2

_¬∑ ‚à•_ _‚àí_ _[‚àó]‚à•_


**_W_** [(][‚Ñì,][0)] **_W_** 2.

_¬∑ ‚à•_ _‚àí_ _[‚àó]‚à•_


Given Œµ > 0 and ÀúŒµ > 0 with Œµ + ÀúŒµ < 1, let

_Œ∑_ _[ŒªŒ¥][2]_
_¬∑_ _K_

r

_ŒªŒ¥[Àú][2]_
and _Œ∑_
_¬∑_ _K_
e

where we need


_d log q_ _Œµ¬µ(Œ¥,_ _Œ¥[Àú])_

,

_Nt_ _‚â§_ 168Œ∫[2]Œ≥K

_d log q_ p _Œµ¬µÀú_ (Œ¥, _Œ¥[Àú])_

_Mt_ _‚â§_ 168Œ∫[2]Œ≥K

r

p


_Nt_ _Œµ[‚àí][2]¬µ[‚àí][2][ ]_ _ŒªŒ¥[2]_ 2Œ∫2Œ≥K 3d log q,
_‚â•_ _ŒªŒ¥[2]_ + _ŒªŒ¥[Àú][2]_

(49)



and _Mt_ _ŒµÀú[‚àí][2]¬µ[‚àí][2][ ]_ _ŒªŒ¥[Àú][2]_ 2Œ∫2Œ≥K 3d log q.
_‚â•_ _ŒªŒ¥[2]_ [e]+ _ŒªŒ¥[Àú][2]_
e 

Therefore, from (46), (47) and (48), we have

[e]

**_W_** [(][‚Ñì,t][+1)] **_W_** 2
_‚à•_ _‚àí_ _[‚àó]‚à•_

_Œµ)¬µ(Œ¥,_ _Œ¥[Àú])_ _ŒªŒ¥[Àú][2]_ _d log q_
_‚â§_ 1 ‚àí [(1][ ‚àí] _[Œµ]168[ ‚àí]_ _Œ∫[Àú][2]Œ≥K_ _‚à•W_ [(][‚Ñì,t][)] _‚àí_ **_W_** _[‚àó]‚à•2 + Œ∑ ¬∑_ _K_ _Mt_ + [1]2 _¬∑ ‚à•W_ [(][‚Ñì,][0)] _‚àí_ **_W_** _[‚àó]‚à•2_
  e [r] 
p _Œµ)¬µ(Œ¥,_ _Œ¥[Àú])_ _ŒªŒ¥[Àú][2]_

_‚â§_ 1 ‚àí [(1][ ‚àí] _[Œµ]168[ ‚àí]_ _Œ∫[Àú][2]Œ≥K_ _‚à•W_ [(][‚Ñì,t][)] _‚àí_ **_W_** _[‚àó]‚à•2 + Œ∑ ¬∑_ _K_
  e _[‚à•][W][ (][‚Ñì,][0)][ ‚àí]_ **_[W][ ‚àó][‚à•][2]_** (50)
p

when M ‚â• 4d log q. By mathematical induction on (50) over t, we have

**_W_** [(][‚Ñì,t][)] **_W_** 2
_‚à•_ _‚àí_ _[‚àó]‚à•_

_Œµ)¬µ_ _t_
1 **_W_** [(][‚Ñì,][0)] **_W_** 2
_‚â§_ _‚àí_ [(1][ ‚àí]168[Œµ][ ‚àí]Œ∫[2]Œ≥K[Àú] _¬∑ ‚à•_ _‚àí_ _[‚àó]‚à•_
 

168p _Œ∫[2]Œ≥K_ _‚àöK_ _ŒªŒ¥[Àú][2]_ (51)

+

(1p ‚àí _Œµ ‚àí_ _ŒµÀú)¬µ_ _[¬∑]_ 14(ŒªŒ¥[2] + _ŒªŒ¥[Àú][2])_ _¬∑_ _K_ _[‚à•][W][ (][‚Ñì,][0)][ ‚àí]_ **_[W][ ‚àó][‚à•][2]_**

e

_Œµ)¬µ_ _t_ _Œ∫[2]Œ≥Œª[e]Œ¥[Àú][2]_
1 + **_W_** [(][‚Ñì,][0)] **_W_** 2
_‚â§_ _‚àí_ [(1][ ‚àí]168[Œµ][ ‚àí]Œ∫[2]Œ≥K[Àú]  (1 ‚àí[e] _Œµ ‚àípŒµÀú)¬µ(ŒªŒ¥[2]_ + _ŒªŒ¥[Àú][2])_  _¬∑ ‚à•_ _‚àí_ _[‚àó]‚à•_
p

[e]


-----

By mathematical induction on (51) over ‚Ñì, we have


**_W_** [(][‚Ñì,T][ )] **_W_** 2
_‚à•_ _‚àí_ _[‚àó]‚à•_

_Œµ)¬µ_ _T_
1 ‚àí [(1][ ‚àí]168[Œµ][ ‚àí]Œ∫[2]Œ≥K[Àú]
 
p


_‚Ñì_ (52)
**_W_** [(0][,][0)] **_W_** 2

_¬∑ ‚à•_ _‚àí_ _[‚àó]‚à•_



_Œ∫[2]Œ≥Œª[e]Œ¥[Àú][2]_

(1 _Œµ_ pŒµÀú)¬µ(ŒªŒ¥[2] + _ŒªŒ¥[Àú][2])_
_‚àí_ _‚àí_ [e]


F PROOF OF THEOREM 1

Instead of proving Theorem 1, we turn to prove a stronger version, as shown in Theorem 3. One can
verify that Theorem 1 is a special case of Theorem 3 by selecting _Œª[ÀÜ] in the order of p and_ _Œµ is in the_
order of (2p ‚àí 1).

The major idea in proving Theorem 3 is similar to that of Theorem 2. The first step is to characterize e
the gradient descent term on the population risk function by the MVT in Lemma 8 as shown in
(58) and (59). Then, the connection between ‚à•W [(][‚Ñì][+1][,][0)] _‚àí_ **_W_** [[][p][]]‚à•F and ‚à•W [(][‚Ñì,][0)] _‚àí_ **_W_** [[][p][]]‚à•F are
characterized in (64). Compared with proving Theorem 2, where the induction over ‚Ñì holds naturally
with large size of labeled data, the induction over ‚Ñì requires a proper value of p as shown in (69). By
induction over ‚Ñì on (64), the relative error ‚à•W [(][L,][0)] _‚àí_ **_W_** [[][p][]]‚à•F can be characterized by ‚à•W [(0][,][0)] _‚àí_
**_W_** [[][p][]]‚à•F as shown in (71).

**Theorem 3. Suppose the initialization W** [(0][,][0)] _satisfies with_

_Œµ)p_ 1
_p_ _Œª_ _‚àí_ (53)
_|_ _‚àí_ [ÀÜ]| ‚â§ [2(1][ ‚àí]¬µ‚àö[e]K

_for some constant_ _Œµ ‚àà_ (0, 1/2), where
e _ŒªÀÜ :=_ _ŒªŒ¥[2]_ _N_ [1]2 (54)

_ŒªŒ¥[2]_ + _ŒªŒ¥[Àú][2][ =]_ _Œ∫[2]Œ≥K_ [3]¬µ[2]d log q
  

_and_

[e] _ŒªŒ¥[2]_ + _ŒªŒ¥[Àú][2]_

_¬µ = ¬µ(Œ¥,_ _Œ¥[Àú]) =_ _._ (55)

_ŒªœÅ(Œ¥) +_ _ŒªœÅ(Œ¥[Àú])_

[e]

_Then, if the number of samples in_ _D further satisfies_

_M ‚â≥[e]_ _ŒµÀú[‚àí][2]Œ∫[2]Œ≥¬µ[2][ ]1 ‚àí_ _Œª[ÀÜ]_ 2K[e]3d log q, (56)

_the iterates {W_ [(][‚Ñì,t][)]}‚Ñì,t[L,T]=0 _[converge to][ W][ [][p][]][ with][ p][ satisfies]_ [ (53)][ as]

lim
_T ‚Üí‚àû_ _[‚à•][W][ (][‚Ñì,T][ )][ ‚àí]_ **_[W][ [][p][]][‚à•][2]_**

_‚â§_ 1 1 _ŒµÀú_ 1 ‚àí _p[‚àó]_ + ¬µ‚àöK (ÀÜŒª ‚àí _p[‚àó])_ _¬∑ ‚à•W_ [(0][,][0)] _‚àí_ **_W_** _[‚àó]‚à•2 +_ (1 _ŒµÀú_ _ŒµÀú)_ (57)

_‚àí_ _[¬∑]_   _‚àí_ _[¬∑ ‚à•][W][ (][‚Ñì,][0)][ ‚àí]_ **_[W][ [][p][]][‚à•][2][,]_**

_with probability at least 1 ‚àí_ _q[‚àí][d]._


_Proof of Theorem 3. From Algorithm 1, in the ‚Ñì-th outer loop, we have_

**_W_** [(][‚Ñì,t][+1)] =W [(][‚Ñì,t][)] _‚àí_ _Œ∑‚àáf[ÀÜ]Dt,Dt_ [(][W][ (][‚Ñì,t][)][) +][ Œ≤][(][W][ (][‚Ñì,t][)][ ‚àí] **_[W][ (][‚Ñì,t][‚àí][1)][)]_**

=W [(][‚Ñì,t][)] _Œ∑_ _f_ (W e[(][‚Ñì,t][)]) + Œ≤(W [(][‚Ñì,t][)] **_W_** [(][‚Ñì,t][‚àí][1)]) (58)
_‚àí_ _‚àá_ _‚àí_

+ Œ∑ ¬∑ _‚àáf_ (W [(][‚Ñì,t][)]) ‚àí‚àáf[ÀÜ]Dt,Dt [(][W][ (][‚Ñì,t][)][)]
 

Since ‚àáf is a smooth function and W [[][p][]] is a local (global) optimal to e _f_, then we have

_‚àáf_ (W [(][‚Ñì,t][)]) =‚àáf (W [(][‚Ñì,t][)]) ‚àí‚àáf (W [[][p][]])

1 (59)
= 0 _‚àá[2]f_ **_W_** [(][‚Ñì,t][)] + u ¬∑ (W [(][‚Ñì,t][)] _‚àí_ **_W_** [[][p][]]) _du ¬∑ (W_ [(][‚Ñì,t][)] _‚àí_ **_W_** [[][p][]]),
Z  


-----

where the last equality comes from Lemma 8.

Similar to the proof of Theorem 2, we have

_‚à•W_ [(][‚Ñì,t][+1)]‚àíW [[][p][]]‚à•2 ‚â§‚à•A(Œ≤)‚à•2¬∑‚à•W [(][‚Ñì,t][)]‚àíW [[][p][]]‚à•2+Œ∑¬∑‚à•‚àáf (W [(][‚Ñì,t][)])‚àí‚àáf[ÀÜ]Dt,Dt [(][W][ (][‚Ñì,t][)][)][‚à•][2][.][ (60)]

From Lemma 2, we have

e


_f_ (W [(][‚Ñì,t][)]) _f_ (W [(][‚Ñì,t][)]) 2
_‚à•‚àá_ _‚àí‚àá_ [ÀÜ] _‚à•_

_d log q_ _ŒªŒ¥[Àú][2]_

‚â≤ _[ŒªŒ¥]K[2]_ _Nt_ _¬∑ ‚à•W_ [(][‚Ñì,t][)] _‚àí_ **_W_** _[‚àó]‚à•_ + _K_

r

e


_d log q_

**_W_** [(][‚Ñì,t][)] **_W_** [(][‚Ñì,][0)] 2
_Mt_ _¬∑ ‚à•_ _‚àí_ _‚à•_


(61)

(62)

(63)


+ _ŒªŒ¥[2]_ _¬∑ (W_ [(0][,][0)] _‚àí_ **_W_** [[][p][]]) ‚àí _ŒªŒ¥[Àú][2]_ _¬∑ (W_ _[‚àó]_ _‚àí_ **_W_** [[][p][]])

_K_

e

When ‚Ñì = 0, following the similar steps from (41) to (46), we have


_f_ (W [(][‚Ñì,t][)]) _f_ (W [(][‚Ñì,t][)]) 2
_‚à•‚àá_ _‚àí‚àá_ [ÀÜ] _‚à•_

_d log q_ _ŒªŒ¥[Àú][2]_

‚â≤ _[ŒªŒ¥][2]_ **_W_** [(][‚Ñì,t][)] **_W_** [[][p][]] +

_K_ _Nt_ _¬∑ ‚à•_ _‚àí_ _‚à•_ _K_

r

e


_d log q_

**_W_** [(][‚Ñì,t][)] **_W_** [[][p][]] 2
_Mt_ _¬∑ ‚à•_ _‚àí_ _‚à•_

_d log q_

**_W_** [(0][,][0)] **_W_** [[][p][]] 2
_Mt_ _¬∑ ‚à•_ _‚àí_ _‚à•_


_d log q_ _ŒªŒ¥[Àú][2]_

_Nt_ _¬∑ ‚à•W_ _[‚àó]_ _‚àí_ **_W_** [[][p][]]‚à• + _K_
e


+ _[ŒªŒ¥][2]_


_ŒªŒ¥[2]_ _¬∑ (1 ‚àí_ _p) ‚àí_ _ŒªŒ¥[Àú][2]_ _¬∑ p_

_K_

e


**_W_** [(0][,][0)] **_W_** 2

_¬∑ ‚à•_ _‚àí_ _[‚àó]‚à•_


and


**_W_** [(][‚Ñì,t][+1)] **_W_** [[][p][]] 2
_‚à•_ _‚àí_ _‚à•_

1 _ŒµÀú_
1 _‚àí_
_‚àí_ _¬µ(Œ¥,_ _Œ¥[Àú])_ 154Œ∫[2]Œ≥K


p


**_W_** [(][‚Ñì,t][)] **_W_** [[][p][]] 2

_¬∑ ‚à•_ _‚àí_ _‚à•_


_ŒªŒ¥2(1_ _p)_
+ Œ∑ _‚àí_
_¬∑_ _K_


_ŒµŒª[e]Œ¥[Àú][2]_ _p_
+ Œ∑ [Àú] _¬∑_
_¬∑_ _K_ _¬∑_

r

Therefore, we have


_d log q_ + _ŒªŒ¥[2]_ _¬∑ (1 ‚àí_ _p) ‚àí_ _ŒªŒ¥[Àú][2]_ _¬∑ p_

_Nt_ _K_

r

e

_d log q_

**_W_** [(0][,][0)] **_W_** 2.
_Mt_ _‚à•_ _‚àí_ _[‚àó]‚à•_


**_W_** [(0][,][0)] **_W_** 2

_¬∑ ‚à•_ _‚àí_ _[‚àó]‚à•_


lim
_T ‚Üí‚àû_ _[‚à•][W][ (][‚Ñì,T][ )][ ‚àí]_ **_[W][ [][p][]][‚à•][2]_**

154Œ∫[2]Œ≥K _ŒªŒ¥2(1_ _p)_ _d log q_ _ŒªŒ¥[2]_ (1 _p)_ _ŒªŒ¥[Àú][2]_ _p_

_Œ∑_ _‚àí_ + _¬∑_ _‚àí_ _‚àí_ _¬∑_ **_W_** [(0][,][0)] **_W_** 2

_‚â§_ _[¬µ]_ 1 _ŒµÀú_ _¬∑_ _¬∑_ _K_ _Nt_ _K_ _¬∑ ‚à•_ _‚àí_ _[‚àó]‚à•_
p _‚àí_ h r e 

_ŒµŒª[e]Œ¥[Àú][2]_ _p_ _d log q_
+ [Àú] _¬∑_ **_W_** [(0][,][0)] **_W_** 2

_K_ _¬∑_ _Mt_ _¬∑ ‚à•_ _‚àí_ _[‚àó]‚à•_

r

154Œ∫[2]Œ≥K _K_ _ŒªŒ¥2(1 ‚àí_ _p)_ _d log q_ + _ŒªŒ¥[2]_ _¬∑ (1i_ _‚àí_ _p) ‚àí_ _ŒªŒ¥[Àú][2]_ _¬∑ p_

_‚â§_ _[¬µ]p_ 1 ‚àí _ŒµÀú_ _¬∑_ 14(ŒªŒ¥[2] + _ŒªŒ¥[Àú][2])_ _¬∑_ h _K_ r _Nt_ _K_ e 

_ŒµŒª[e]Œ¥[Àú][2]_ _p_ _d log q_

_¬∑ ‚à•[e]W_ [(0][,][0)] _‚àí_ **_W_** _[‚àó]‚à•2 + [Àú]_ _K_ _¬∑_ _¬∑_ _Mt_ _¬∑ ‚à•W_ [(0][,][0)] _‚àí_ **_W_** _[‚àó]‚à•2_

r

1 i

1 _p +_ _‚àöK_ (1 _p)¬µŒªÀÜ_ _p¬µ(1_ _Œª)_ **_W_** [(0][,][0)] **_W_** 2

_‚âÉ_ 1 _ŒµÀú_ _‚àí_ _¬∑_ _‚àí_ _‚àí_ _‚àí_ [ÀÜ] _¬∑ ‚à•_ _‚àí_ _[‚àó]‚à•_

_‚àí_ _ŒµpÀú_ _[¬∑]_  

+

(1 ‚àí _ŒµÀú)_ _[¬∑ ‚à•][W][ (0][,][0)][ ‚àí]_ **_[W][ ‚àó][‚à•][2]_**

= 1 1 _ŒµÀú_ 1 ‚àí _p + ¬µ‚àöK_ _Œª ‚àí_ _p_ _¬∑ ‚à•W_ [(0][,][0)] _‚àí_ **_W_** _[‚àó]‚à•2 +_ (1 _ŒµpÀú_ _ŒµÀú)_

_‚àí_ _[¬∑]_   _‚àí_ _[¬∑ ‚à•][W][ (0][,][0)][ ‚àí]_ **_[W][ ‚àó][‚à•][2][,]_** (64)

b

where _Œª[ÀÜ] =_ _ŒªŒ¥[2]_

_ŒªŒ¥[2]+Œª[e]Œ¥[Àú][2][ .]_


-----

To guarantee the convergence in the outer loop, we require

lim
_T ‚Üí‚àû_ _[‚à•][W][ (][‚Ñì,T][ )][ ‚àí]_ **_[W][ [][p][]][‚à•][2][ ‚â§‚à•][W][ (0][,][0)][ ‚àí]_** **_[W][ [][p][]][‚à•][2][ =][ p][‚à•][W][ (0][,][0)][ ‚àí]_** **_[W][ ‚àó][‚à•][2][,]_** (65)

and lim
_T ‚Üí‚àû_ _[‚à•][W][ (][‚Ñì,T][ )][ ‚àí]_ **_[W][ ‚àó][‚à•][2][ ‚â§‚à•][W][ (0][,][0)][ ‚àí]_** **_[W][ ‚àó][‚à•][2][.]_**

Since we have

_‚à•W_ [(][‚Ñì,T][ )] _‚àí_ **_W_** [[][p][]]‚à•2 ‚â§‚à•W [(][‚Ñì,T][ )] _‚àí_ **_W_** _[‚àó]‚à•2 + ‚à•W_ _[‚àó]_ _‚àí_ **_W_** [[][p][]]‚à•2 (66)

=‚à•W [(][‚Ñì,T][ )] _‚àí_ **_W_** _[‚àó]‚à•2 + (1 ‚àí_ _p) ¬∑ ‚à•W_ _[‚àó]_ _‚àí_ **_W_** [(0][,][0)]‚à•2,

it is clear that (65) holds if and only if


1

1 _p +_ _Œµp + ¬µ‚àöK_ _ŒªÀÜ_ _p_ + 1 _p_ 1. (67)
1 _ŒµÀú_ _‚àí_ _‚àí_ _‚àí_ _‚â§_
_‚àí_ _[¬∑]_  

To guarantee the iterates strictly converges to the desired point, we let

e


_ŒªÀÜ_ _p_ + 1 _p_ 1
_‚àí_ _‚àí_ _‚â§_ _‚àí_ _C[1]_



(68)

(69)


1 _p +_ _Œµp + ¬µ_ _K_
1 _ŒµÀú_ _‚àí_
_‚àí_ _[¬∑]_ 

for some larger constant C, which is equivalent to

e


_Œµ)p_ 1
_p_ _Œª_ _‚àí_
_|_ _‚àí_ [ÀÜ]| ‚â§ [2(1][ ‚àí]¬µ‚àö[e]K

To make the bound in (69) meaningful, we need


1
_p_ (70)
_‚â•_ 2(1 _Œµ)_ _[.]_

When ‚Ñì> 1, following similar steps in (64), we have ‚àí e

lim
_T ‚Üí‚àû_ _[‚à•][W][ (][‚Ñì,T][ )][ ‚àí]_ **_[W][ [][p][]][‚à•][2]_**

(71)

_‚â§_ 1 1 _ŒµÀú_ 1 ‚àí _p + ¬µ‚àöK_ (ÀÜŒª ‚àí _p)_ _¬∑ ‚à•W_ [(0][,][0)] _‚àí_ **_W_** _[‚àó]‚à•2 +_ 1 _ŒµpÀú_ _ŒµÀú_

_‚àí_ _[¬∑]_   _‚àí_ _[¬∑ ‚à•][W][ (][‚Ñì,][0)][ ‚àí]_ **_[W][ [][p][]][‚à•][2][,]_**

Given (69) holds, from (71), we have

lim
_L‚Üí‚àû,T ‚Üí‚àû_ _[‚à•][W][ (][L,T][ )][ ‚àí]_ **_[W][ [][p][]][‚à•][2]_**


_ŒªÀÜ_ _p_ **_W_** [(0][,][0)] **_W_** 2
_‚àí_ _¬∑ ‚à•_ _‚àí_ _[‚àó]‚à•_


_ŒªÀÜ_ _p_ **_W_** [(0][,][0)] **_W_** 2.
_‚àí_ _¬∑ ‚à•_ _‚àí_ _[‚àó]‚à•_



1 _p + ¬µ_
1 _Œµ_ _‚àí_
_‚àí1_ _[¬∑]_ 

1 _p + ¬µ_
1 _Œµe_ _‚àí_
_‚àí_ _[¬∑]_ 
e


(72)


G DEFINITION AND RELATIVE PROOFS OF œÅ

In this section, the formal definition of œÅ is included in Definition 3, and a corresponding claim about
_œÅ is summarized in Lemma 11. One can quickly check that the ReLU activation function satisfies_
the conditions in Lemma 11.

The major idea in proving Lemma 11 is to show Hr(Œ¥) and Jr(Œ¥) in Definition 3 are in the order of
_Œ¥[r]_ when Œ¥ is small.
**Definition 3. Let Hr(Œ¥) = Ez** (0,Œ¥2) _œÜ[‚Ä≤](œÉKz)z[r][]_ _and Jr(Œ¥) = Ez_ (0,Œ¥2) _œÜ[‚Ä≤][2](œÉKz)z[r][]. Then,_
_‚àºN_ _‚àºN_
_œÅ = œÅ(Œ¥) is defined as_
   

_œÅ(Œ¥) = min_ _J0(Œ¥) ‚àí_ _H0[2][(][Œ¥][)][ ‚àí]_ _[H]1[2][(][Œ¥][)][, J][2][(][Œ¥][)][ ‚àí]_ _[H]1[2][(][Œ¥][)][ ‚àí]_ _[H]2[2][(][Œ¥][)][, H][0][(][Œ¥][)][ ¬∑][ H][2][(][Œ¥][)][ ‚àí]_ _[H]1[2][(][Œ¥][)]_ _, (73)_
n o

_where œÉK is the minimal singular value of W_ _[‚àó]._


-----

**Lemma 11 (Order analysis of œÅ). If œÅ(Œ¥) > 0 for Œ¥ ‚àà** (0, Œæ) for some positive constant Œæ and the
_sub-gradient of œÅ(Œ¥) at 0 can be non-zero, then œÅ(Œ¥) = Œò(Œ¥[2]) when Œ¥ ‚Üí_ 0[+]. Typically, for ReLU
_activation function, ¬µ in (5) is a fixed constant for all Œ¥,_ _Œ¥[Àú] ‚â§_ 1.

_Proof of Lemma 11 . From Definition 3, we know that Hr(Œ¥) = Ez_ (0,Œ¥2)œÜ[‚Ä≤](œÉKz)z[r]. Suppose
_‚àºN_
we have Hr(Œ¥) = Œò(Œ¥[r]) and Jr(Œ¥) = Œò(Œ¥[r]), then from (73) we have


_J0(Œ¥) ‚àí_ _H0[2][(][Œ¥][)][ ‚àí]_ _[H]1[2][(][Œ¥][)][ ‚àà]_ [Œò(1)][ ‚àí] [Œò(][Œ¥][2][)][,]

_J2(Œ¥) ‚àí_ _H1[2][(][Œ¥][)][ ‚àí]_ _[H]2[2][(][Œ¥][)][ ‚àà]_ [Œò(][Œ¥][2][)][,]

_H0(Œ¥) ¬∑ H2(Œ¥) ‚àí_ _H1[2][(][Œ¥][)][ ‚àà]_ [Œò(][Œ¥][2][)][ ‚àí] [Œò(][Œ¥][4][)][.]


(74)


Because œÅ is a continuous function with œÅ(z) > 0 for some z > 0. Therefore, œÅ Ã∏= J0(Œ¥) ‚àí _H0[2][(][Œ¥][)][ ‚àí]_
_H1[2][(][Œ¥][)][ when][ Œ¥][ ‚Üí]_ [0][+][, otherwise][ œÅ][(][z][)][ <][ 0][ for any][ z >][ 0][. When][ Œ¥][ ‚Üí] [0][+][, both][ J][2][(][Œ¥][)][ ‚àí] _[H]1[2][(][Œ¥][)][ ‚àí]_
_H2[2][(][Œ¥][)][ and][ H][0][(][Œ¥][)][ ¬∑][ H][2][(][Œ¥][)][ ‚àí]_ _[H]1[2][(][Œ¥][)][ are in the order of][ Œ¥][2][, which indicates that][ ¬µ][ is a fixed constant]_
when both Œ¥ and _Œ¥[Àú] are close to 0. In addition, J2(Œ¥) ‚àí_ _H1[2][(][Œ¥][)][ ‚àí]_ _[H]2[2][(][Œ¥][)][ goes to][ +][‚àû]_ [while both]
_J0(Œ¥) ‚àí_ _H0[2][(][Œ¥][)][ ‚àí]_ _[H]1[2][(][Œ¥][)][ and][ H][0][(][Œ¥][)][ ¬∑][ H][2][(][Œ¥][)][ ‚àí]_ _[H]1[2][(][Œ¥][)][ go to][ ‚àí‚àû]_ [when][ Œ¥][ ‚Üí] [+][‚àû][. Therefore, with a]
large enough Œ¥, we have

_œÅ(Œ¥) ‚àà_ Œò(Œ¥[2]) ‚àí Œò(Œ¥[4]) or Œò(1) ‚àí Œò(Œ¥[2]), (75)

which indicates that ¬µ is a strictly decreasing function when Œ¥ and _Œ¥[Àú] are large enough._

Next, we provide the conditions that guarantee Hr(Œ¥) = Œò(Œ¥[r]) hold, and the relative proof for Jr(Œ¥)
can be derived accordingly following the similar steps as well. From Definition 3, we have


_Hr(Œ¥)_ +‚àû _z_ _r_ 1
lim = lim _œÜ[‚Ä≤](œÉKz)_ _Œ¥[2]_ _dz_
_Œ¥_ 0[+] _Œ¥[r]_ _Œ¥_ 0[+] _Œ¥_ _‚àö2œÄŒ¥ [e][‚àí]_ _[z][2]_
_‚Üí_ _‚Üí_ Z‚àí‚àû

+   

(a) _‚àû_
= lim _œÜ[‚Ä≤](œÉKŒ¥t)_ _[t][r]_
_Œ¥_ 0[+] _‚àö2œÄ [e][‚àí][t][2]_ _[dt]_
_‚Üí_ Z‚àí‚àû

0[‚àí] +‚àû

= lim _œÜ[‚Ä≤](œÉKŒ¥t)_ _[t][r]_ _œÜ[‚Ä≤](œÉKŒ¥t)_ _[t][r]_
_Œ¥‚Üí0[+]_ Z‚àí‚àû _‚àö2œÄ [e][‚àí][t][2]_ _[dt][ + lim]Œ¥‚Üí0[+]_ Z0[+] _‚àö2œÄ [e][‚àí][t][2]_ _[dt]_

0[‚àí] _t[r]_ +‚àû _t[r]_
=œÜ[‚Ä≤](0[‚àí]) Z‚àí‚àû _‚àö2œÄ [e][‚àí][t][2]_ _[dt][ +][ œÜ][‚Ä≤][(0][+][)]_ Z0[+] _‚àö2œÄ [e][‚àí][t][2]_ _[dt,]_


(76)


where equality (a) holds by letting t = _[z]Œ¥_ [. It is easy to verify that]


+‚àû _t[r]_ 0[‚àí]
Z0[+] _‚àö2œÄ [e][‚àí][t][2]_ _[dt][ = (][‚àí][1)][r]_ Z‚àí‚àû


_t[r]_

2œÄ [e][‚àí][t][2] _[dt,]_


and both are bounded for a fixed r. Thus, as long as either œÜ[‚Ä≤](0[‚àí]) or œÜ[‚Ä≤](0[+]) is non-zero, we have
_Hr(Œ¥) = Œò(Œ¥[r]) when Œ¥_ 0[+].
_‚Üí_

If œÜ has bounded gradient as |œÜ[‚Ä≤]| ‚â§ _CœÜ for some positive constant CœÜ. Then, we have_

+‚àû _z_ _r_ 1
= _œÜ[‚Ä≤](œÉKz)_ _Œ¥[2]_ _dz_
_Œ¥[r]_ _Œ¥_ _‚àö2œÄŒ¥ [e][‚àí]_ _[z][2]_
Z‚àí‚àû

+   

_[H][r][(][Œ¥][)]_ _‚àû_

= _œÜ[‚Ä≤](œÉKŒ¥t)_ _[t][r]_ (77)

_‚àö2œÄ [e][‚àí][t][2]_ _[dt]_

Z‚àí‚àû

+‚àû _t[r]_
_CœÜ_
_‚â§_ _¬∑_ _‚àö2œÄ [e][‚àí][t][2]_ _[dt]_
Z‚àí‚àû

Therefore, we have Hr(Œ¥) = (Œ¥[r]) for all Œ¥ > 0 when œÜ has bounded gradient.
_O_

Typiclly, for ReLU function, one can directly calculate that Hr(Œ¥) = Œ¥[r] for Œ¥ ‚àà R, and œÅ(Œ¥) = CŒ¥[2]
when Œ¥ ‚â§ 1 for some constant C = 0.091. Then, it is easy to check that ¬µ is a constant when
_Œ¥,_ _Œ¥[Àú] ‚â§_ 1.


-----

H PROOF OF PRELIMINARY LEMMAS

H.1 PROOF OF LEMMA 1

The eigenvalues of ‚àá[2]f (¬∑; p) at any fixed point W can be bounded in the form of (80) by Weyl‚Äôs
inequality (Lemma 4). Therefore, the primary technical challenge lies in bounding ‚à•‚àá[2]f (W ; p) ‚àí
_f_ (W [[][p][]]; p) 2, which is summarized in Lemma 12. Lemma 13 provides the exact calulation of
_‚àá[2]_ _‚à•_ 2

the lower bound of Ex _Kj=1_ **_[Œ±]j[T]_** **_[x][œÜ][‚Ä≤][(][w]j[[][p][]][T]_** **_x)_** when x belongs to Gaussian distribution with
zero mean, which is used in proving the lower bound of the Hessian matrix in (81). P 

**Lemma 12. Let f** (W ; p) be the population risk function defined in (17) with p and W satisfying
(20). Then, we have


_Œ¥[2]_
_f_ (W [[][p][]]; p) _f_ (W ; p) 2 ‚â≤ _[ŒªŒ¥][2][ + (1][ ‚àí]_ _[Œª][)Àú]_
_‚à•‚àá[2]_ _‚àí‚àá[2]_ _‚à•_ _K_


_._ (78)

_¬∑ [‚à•][W][ [][p]œÉ[]][ ‚àí]K_ **_[W][ ‚à•][2]_**


**Lemma 13 (Lemma D.6, (Zhong et al., 2017)). For any** **_wj_** _j=1_
_vector defined in (19). When the œÜ is ReLU function, we have {_ _}[K]_ _[‚àà]_ [R][d][, let][ Œ±][ ‚àà] [R][dK][ be the unit]

_[K]_ 2
min **_Œ±[T]j_** **_[x][œÜ][‚Ä≤][(][w]j[T]_** **_[x][)]_** ‚â≥ _œÅ(œÉ),_ (79)
_‚à•Œ±‚à•2=1_ [E][x][‚àºN][ (0][,œÉ][2][)] _j=1_
 X 

_where œÅ(œÉ) is defined in Definition 3._

_Proof of Lemma 1. Let Œªmax(W ) and Œªmin(W ) denote the largest and smallest eigenvalues of_
_‚àá[2]f_ (W ; p) at point W, respectively. Then, from Lemma 4, we have

_Œªmax(W )_ _Œªmax(W_ [[][p][]]) + _f_ (W ; p) _f_ (W [[][p][]]; p) 2,
_‚â§_ _‚à•‚àá[2]_ _‚àí‚àá[2]_ _‚à•_ (80)

_Œªmin(W )_ _Œªmin(W_ [[][p][]]) _f_ (W ; p) _f_ (W [[][p][]]; p) 2.
_‚â•_ _‚àí‚à•‚àá[2]_ _‚àí‚àá[2]_ _‚à•_

Then, we provide the lower bound of the Hessian matrix of the population function at W [[][p][]]. For
any Œ± ‚àà R[dK] defined in (19) with ‚à•Œ±‚à•2 = 1, we have

min
**_Œ±_** 2=1 **_[Œ±][T][ ‚àá][2][f]_** [(][W][ [][p][]][;][ p][)][Œ±]
_‚à•_ _‚à•_

_[K]_ 2 _[K]_ 2

= [1] min _ŒªEx_ **_Œ±[T]j_** **_[x][œÜ][‚Ä≤][(][w]j[[][p][]][T]_** **_x)_** + _ŒªEx_ **_Œ±[T]j_** **_xœÜ[‚Ä≤](wj[[][p][]][T]_** **_x)_**

_K_ [2] **_Œ±_** 2=1
_‚à•_ _‚à•_ _j=1_ _j=1_

h [K]X 2 [e] e X _[K][e]_ e  i 2 (81)

min **_Œ±[T]j_** **_[x][œÜ][‚Ä≤][(][w]j[[][p][]][T]_** **_x)_** + min _ŒªEx_ **_Œ±[T]j_** **_xœÜ[‚Ä≤](wj[[][p][]][T]_** **_x)_**

_‚â•_ _K[1][2]_ **_Œ±_** 2=1 _[Œª][E][x]_ **_Œ±_** 2=1

_‚à•_ _‚à•_ _j=1_ _‚à•_ _‚à•_ _j=1_
 X  e X 

_ŒªœÅ(Œ¥[Àú])_ e [e] e

_,_

_‚â•_ _[ŒªœÅ]11[(][Œ¥][) +]Œ∫[2]Œ≥K[ e]_ [2]

where the last inequality comes from Lemma 13.

Next, the upper bound can be bounded as

max
**_Œ±_** 2=1 **_[Œ±][T][ ‚àá][2][f]_** [(][W][ [][p][]][;][ p][)][Œ±]
_‚à•_ _‚à•_

= [1] max _ŒªEx_ _[K]_ **_Œ±[T]j_** **_[x][œÜ][‚Ä≤][(][w]j[[][p][]][T]_** **_x)_** 2 + _ŒªEx_ _[K]_ **_Œ±[T]j_** **_xœÜ[‚Ä≤](wj[[][p][]][T]_** **_x)_** 2

_K_ [2] **_Œ±_** 2=1 (82)
_‚à•_ _‚à•_ _j=1_ _j=1_

h  X  e X  i

_[K]_ 2 [e] _[K][e]_ e 2
max **_Œ±[T]j_** **_[x][œÜ][‚Ä≤][(][w]j[[][p][]][T]_** **_x)_** + max _ŒªEx_ **_Œ±[T]j_** **_xœÜ[‚Ä≤](wj[[][p][]][T]_** **_x)_** _._

_‚â§_ _K[1][2]_ **_Œ±_** 2=1 _[Œª][E][x]_ **_Œ±_** 2=1

_‚à•_ _‚à•_ _j=1_ _‚à•_ _‚à•_ _j=1_
 X  e X 

e [e] e


-----

2
For Ex _Kj=1_ **_[Œ±]j[T]_** **_[x][œÜ][‚Ä≤][(][w]j[[][p][]][T]_** **_x)_**, we have
 P 

_[K]_ 2
Ex **_Œ±[T]j_** **_[x][œÜ][‚Ä≤][(][w]j[[][p][]][T]_** **_x)_**

_j=1_

 X 


**_Œ±[T]j1_** **_[x][œÜ][‚Ä≤][(][w]j[[][p]1[]][T]_** **_x)Œ±[T]j2_** **_[x][œÜ][‚Ä≤][(][w]j[[][p]2[]][T]_** **_x)_**

_j1=1_ _j2=1_

X X


=Ex


ExŒ±[T]j1 **_[x][œÜ][‚Ä≤][(][w]j[[][p]1[]][T]_** **_x)Œ±[T]j2_** **_[x][œÜ][‚Ä≤][(][w]j[[][p]2[]][T]_** **_x)_**
_j2=1_

X


_j1=1_

_K_

_j1=1_

X

_K_

_j1=1_

X


Ex(Œ±[T]j1 **_[x][)][4][E][x][(][œÜ][‚Ä≤][(][w]j[[][p]1[]][T]_** **_x))[4]Ex(Œ±[T]j2_** **_[x][)][4][E][x][(][œÜ][‚Ä≤][(][w]j[[][p]2[]][T]_** **_x))[4][i][1][/][4]_**


(83)

(84)


_j2=1_


3Œ¥[2] **_Œ±j1_** 2 **_Œ±j2_** 2
_‚à•_ _‚à•_ _‚à•_ _‚à•_
_j2=1_

X


_‚â§6Œ¥[2]_


1
2 [+][ ‚à•][Œ±][j]2 _[‚à•][2]2[)]_
2 [(][‚à•][Œ±][j][1] _[‚à•][2]_


_j1=1_ _j2=1_


=6KŒ¥[2]

Therefore, we have

max
**_Œ±_** 2=1 **_[Œ±][T][ ‚àá][2][f]_** [(][W][ [][p][]][;][ p][)][Œ±]
_‚à•_ _‚à•_

_[K]_ 2 _[K]_ 2
max **_Œ±[T]j_** **_[x][œÜ][‚Ä≤][(][w]j[[][p][]][T]_** **_x)_** + max _ŒªEx_ **_Œ±[T]j_** **_xœÜ[‚Ä≤](wj[[][p][]][T]_** **_x)_**

_‚â§_ _K[1][2]_ **_Œ±_** 2=1 _[Œª][E][x]_ **_Œ±_** 2=1

_‚à•_ _‚à•_ _j=1_ _‚à•_ _‚à•_ _j=1_
 X  e X 

_ŒªŒ¥[Àú][2])_ e [e] e

_._

_‚â§_ [6(][ŒªŒ¥][2]K[ +][ e]

Then, given (20), we have


_‚à•W_ [(0][,][0)] _‚àí_ **_W_** [[][p][]]‚à•F = p‚à•W [(0][,][0)] _‚àí_ **_W_** _[‚àó]‚à•F ‚â≤_ _¬µ[œÉ][2][K]K [.]_ (85)

Combining (85) and Lemma 12, we have


_ŒªœÅ(Œ¥[Àú])_
_f_ (W ; p) _f_ (W [[][p][]]; p) 2 ‚â≤ _[ŒªœÅ][(][Œ¥][) +][ e]_ _._ (86)
_‚à•‚àá[2]_ _‚àí‚àá[2]_ _‚à•_ 132Œ∫[2]Œ≥K [2]

Therefore, (86) and (80) completes the whole proof.

H.2 PROOF OF LEMMA 2

The task of bounding of the quantity between ‚à•‚àáf[ÀÜ] ‚àí‚àáf _‚à•2 is dividing into bounding I1, I2, I3 and_
_I4 as shown in (89). I1 and I3 represent the deviation of the mean of several random variables to_
their expectation, which can be bounded through concentration inequality, i.e, Chernoff bound. I2
and I4 come from the inconsistency of the output label y and pseudo label _y in the empirical risk_
function in (1) and population risk function in (17). The major challenge lies in characterizing the
upper bound of I2 and I4 as the linear function of **_W ‚àíW_** [[][p][]] and W [[][p][]] _‚àíW_ _[‚àó] e, which is summarized_
in (96).

[f]


-----

_Proof of Lemma 2. From (1), we know that_


_N_ 1

_K_

_n=1_

X 


1

_K_




_‚àÇf[ÀÜ]_

(W ) = _[Œª]_
_‚àÇwk_ _N_


_K_

_œÜ(wj[T]_ **_[x][n][)][ ‚àí]_** _[y][n]_ **_xn + [1][ ‚àí]_** _[Œª]_

_M_

_j=1_

X 


_œÜ(wj[T]_ **_xm)_** _yn_ **_xm_**
_‚àí_
_j=1_

X 

[e] e e


_m=1_


_œÜ(wj[T]_ **_[x][n][)][ ‚àí]_** _[œÜ][(][w]j[‚àó][T]_ **_[x][n][)]_** **_xn_**



_K_ [2]N


_n=1_ _j=1_


+ [1][ ‚àí] _[Œª]_

_K_ [2]M


+ K[1][ ‚àí][2]M[Œª] _œÜ(wj[T]_ **_xm) ‚àí_** _œÜ(wj[T]_ **_xm)_** **_xm._**

_m=1_ _j=1_

X X  

(87)

[e] e [e] e

From (32), we know that

_K_ _K_

_‚àÇf[ÀÜ]_

(W ) = _[Œª]_ _œÜ(wj[T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[‚àó][T]_ **_[x][)]_** **_x + [1][ ‚àí]_** _[Œª]_ **_x_** _œÜ(wj[T]_ **_x)_** _œÜ(wj[T]_ **_x)_** **_x._**
_‚àÇwk_ _K_ [2][ E][x] _K_ [2][ E][e] _‚àí_

_j=1_ _j=1_

X   X  

(88)

[e] e [e] e

Then, from (17), we have


_‚àÇf[ÀÜ]_

(W ) = _[Œª]_
_‚àÇwk_ _K_ [2][ E][x]


_‚àÇf[ÀÜ]_

(W ) (W ; p)
_‚àÇwk_ _‚àí_ _‚àÇ[‚àÇf]wk_


_œÜ(wj[T]_ **_[x][n][)][ ‚àí]_** _[œÜ][(][w]j[‚àó][T]_ **_[x][n][)]_** **_xn ‚àí_** Ex _œÜ(wj[T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[[][p][]][T]_ **_x)_**
  


_K_ [2]N


_j=1_


_n=1_


_K_ _M_

+ K[1][ ‚àí][2]M[Œª] _j=1_ _m=1_ _œÜ(wj[T]_ **_xm) ‚àí_** _œÜ(wj[T]_ **_xm)_** **_xm ‚àí_** Ex _œÜ(wj[T]_ **_x) ‚àí_** _œÜ(wj[[][p][]][T]_ **_x)_** **_x_**

X h X   e  i

_K_ _N_

1 [e] e [e] e [e] e e

= _[Œª]_ _œÜ(wj[T]_ **_[x][n][)][ ‚àí]_** _[œÜ][(][w]j[‚àó][T]_ **_[x][n][)]_** **_xn_** Ex _œÜ(wj[T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[‚àó][T]_ **_[x][)]_** **_x_**

_K_ [2] _j=1_ _N_ _n=1_ _‚àí_ (89)

X h X       i

_K_

+ _[Œª]_ Ex _œÜ(wj[‚àó][T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[[][p][]][T]_ **_x)_** **_x_**

_K_ [2]

_j=1_

X h   i

_K_ 1 _M_

+ [1]K[ ‚àí][2][Œª] _j=1_ _M_ _m=1_ _œÜ(wj[T]_ **_xm) ‚àí_** _œÜ(wj[T]_ **_xm)_** **_xm ‚àí_** Ex _œÜ(wj[T]_ **_x) ‚àí_** _œÜ(wj[T]_ **_x)_** **_x_**

X h X   e  i

_K_

[e] e [e] e [e] e [e] e

+ [1]K[ ‚àí][2][Œª] Ex _œÜ(wj[T]_ **_x) ‚àí_** _œÜ(wj[[][p][]][T]_ (p)x) **_x_**

_j=1_

X eh  i

:=I1 + I2 + I3 + I4. e [e] e e

For any Œ±j ‚àà R[d] with ‚à•Œ±j‚à•2 ‚â§ 1, we define a random variable Z(j) = _œÜ(wj[T]_ **_[x][)][‚àí][œÜ][(][w]j[‚àó][T]_** **_[x][)]_** **_Œ±[T]j_** **_[x]_**
and Zn(j) = _œÜ(wj[T]_ **_[x][n][)]_** _[‚àí]_ _[œÜ][(][w]j[‚àó][T]_ **_[x][n][)]_** **_Œ±[T]j_** **_[x][n][ as the realization of][ Z][(][j][)] [ for][ n][ = 1][,][ 2][ ¬∑ ¬∑ ¬∑][, N]_** [. Then,]
for any p N[+], we have
_‚àà_   
E _Z_ = E _œÜ(wj[T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[‚àó][T]_ **_[x][)][|][p][ ¬∑ |][Œ±]j[T]_** **_[x][|][p][][1][/p]_**
_|_ _|[p][][1][/p]_ _|_
   (90)

E (wj **_wj[‚àó][)][T][ x][|][p][ ¬∑ |][Œ±]j[T]_** **_[x][|][p][][1][/p]_**
_‚â§_ _|_ _‚àí_


_C_ _Œ¥[2]_ **_wj_** **_wj[‚àó]_**
_‚â§_ _¬∑_ _‚à•_ _‚àí_ _[‚à•][2]_ _[¬∑][ p,]_

where C is a positive constant and the last inequality holds since x ‚àºN (0, Œ¥[2]). From Definition 2,
by Chernoff inequality, we havewe know that Z belongs to sub-exponential distribution with ‚à•Z‚à•œà1 ‚â≤ _Œ¥[2]‚à•wj ‚àí_ **_wj[‚àó][‚à•][2][. Therefore,]_**

_N_

_j_

_[‚à•][2][)][2][¬∑][Ns][2]_

P [1] _Zn(j)_ EZ(j) _< t_ 1 (91)

_N_ _‚àí_ _‚â§_ _‚àí_ _[e][‚àí][C][(][Œ¥][2][‚à•][w]e[j][Nst][‚àí][w][‚àó]_

 _nX=1_ 


-----

for some positive constant C and any s ‚àà R.


Let t = Œ¥[2] **_wj_** **_wj[‚àó]_**
_‚à•_ _‚àí_ _[‚à•][2]_


_d log q_


and s =


2

_CŒ¥[2]_ **_wj_** **_wj[‚àó]_**
_‚à•_ _‚àí_ _[‚à•][2][ ¬∑][ t][ for some large constant][ q >][ 0][, we have]_


_d log q_



[1] _Zn(j)_ EZ(j) ‚â≤ _Œ¥[2]_ **_wj_** **_wj[‚àó]_**

_N_ _n=1_ _‚àí_ _‚à•_ _‚àí_ _[‚à•][2]_ _[¬∑]_

X

with probability at least 1 ‚àí _q[‚àí][d]. From Lemma 7, we have_


(92)

(93)


_œÜ(wj[T]_ **_[x][n][)][ ‚àí]_** _[œÜ][(][w]j[‚àó][T]_ **_[x][n][)]_** **_xn ‚àí_** Ex _œÜ(wj[T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[‚àó][T]_ **_[x][)]_**
  


_n=1_


_d log q_


2Œ¥[2] **_wj_** **_wj[‚àó]_**
_‚â§_ _‚à•_ _‚àí_ _[‚à•][2]_ _[¬∑]_


with probability at least 1 ‚àí (q/5)[‚àí][d]. Since q is a large constant, we release the probability as
1 ‚àí _q[‚àí][d]_ for simplification. Similar to Z, we have


_M_

_M[1]_ _m=1_ _œÜ(wj[T]_ **_xm) ‚àí_** _œÜ(wj[T]_ **_xm)_** **_xm ‚àí_** Ex _œÜ(wj[T]_ **_x) ‚àí_** _œÜ(wj[T]_ **_x)_** **_x_**

X   e 

[e] _d log q_ e [e] e [e] e [e] e

‚â≤Œ¥[Àú][2] **_wj_** **_wj_** 2
_‚à•_ _‚àí_ _‚à•_ _¬∑_ r _M_

with probability at least 1e _q[‚àí][d]._
_‚àí_

|II ùúÉùúÉ 1 IV-A|I ùíò‚àóùíò ùëóùëó ùúÉ 1ùúÉ ùíò ùëó[ùíò ùëóùëù]ùëù|
|---|---|
||III|



Figure 13: The subspace spanned by wj[‚àó] [and][ w]j[[][p][]]


(94)


For term Ex _œÜ(wj[‚àó][T]_ **_[x][)][‚àí][œÜ][(][w]j[[][p][]][T]_** **_x)_** **_x_**, let us define the angle between wj[‚àó] [and][ w]j[[][p][]] as Œ∏1. Figure
13 shows the subspace spanned by the vectorh   i **_wj[‚àó]_** [and][ e]wj. We divide the subspace by 4 pieces, where
the gray region denotes area I, and the blue area denotes area II. Areas III and IV are the symmetries
of II and I from the origin, respectively. Hence, we have

Ex _œÜ(wj[‚àó][T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[[][p][]][T]_ **_x)_** **_x_**

=Exh area I _œÜ(wj[‚àó][T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[[][p][]][T]ix)_ **_x_** + Ex area II _œÜ(wj[‚àó][T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[[][p][]][T]_ **_x)_** **_x_**
_‚àà_ _‚àà_

+ Ex area IIIh  _œÜ(wj[‚àó][T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[[][p][]][T]_ **_xi)_** **_x_** + Ex area IVh  _œÜ(wj[‚àó][T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[[][p][]][T]_ **_xi)_** **_x_**
_‚àà_ _‚àà_

(95)

=Ex area I _œÜ(h wj[‚àó][T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[[][p][]][T]_ **_x)_** **_x_** + Ei **_x_** area II **_wj[‚àó][T]h xx_** Ex area III **_wj[[][p][]][T]_** **_xx_** i
_‚àà_ _‚àà_ _‚àí_ _‚àà_

=Ex‚àà area Ih (wj[‚àó] _j_ [)][T][ xx] + Ex‚ààarea IIi (wj[‚àó] _j_ [)][T][ xx]e e  

_[‚àí]_ **_[w][[][p][]]_** _[‚àí]_ **_[w][[][p][]]_**

= [1] (wj[‚àó] _j_ [)][T][ xx]   

2 [E][x] _[‚àí]_ **_[w][[][p][]]_**
 


-----

Therefore, we have

_Œª_

_œÜ(wj[‚àó][T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[[][p][]][T]_ **_x)_** **_x_** + [1][ ‚àí] _[Œª]_ **_x_** _œÜ(wj[T]_ **_x)_** _œÜ(wj[[][p][]][T]_ **_x)_** **_x_**
_K_ [2][ E][x] _K_ [2][ E][e] _‚àí_ 2

_Œª_ h   i h  i
= (wj[‚àó] _j_ [)][T][ xx] + [1][ ‚àí] _[Œª]_ **_x_** (wj **_wj[[][p][]][)][T] e[ xx][e]_** e e (96)

2K [2][ E][x] _[‚àí]_ **_[w][[][p][]]_** 2K [2][ E][e] _‚àí_ 2
   

_ŒªŒ¥[2]_ **_wj_** **_wj[[][p][]]_** + (1 _Œª)Œ¥[Àú][2]_ **_wj[‚àó]_** ej 2

= _¬∑_ _‚àí_ _‚àí_ _¬∑_ _[‚àí]_ **_[w][[][p][]]_** _._

2K [2]

From (93), (94) and (96), we have  e    

_f_

_[‚àÇ]_ [ÀÜ] (W ; p) (W )

_‚àÇwk_ _‚àí_ _‚àÇ[‚àÇf]wk_ 2

_K_ _N_

[1] _œÜ(wj[T]_ **_[x][n][)][ ‚àí]_** _[œÜ][(][w]j[‚àó][T]_ **_[x][n][)]_** **_xn_** Ex _œÜ(wj[T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[‚àó][T]_ **_[x][)]_** **_x_**

_‚â§_ _K[Œª][2]_ _j=1_ _N_ _n=1_ _‚àí_ 2

X X      

_K_ _M_

+ [1][ ‚àí] _[Œª]_ [1] _œÜ(wj[T]_ **_xm)_** _œÜ(wj[T]_ **_xm)_** **_xm_** Ex _œÜ(wj[T]_ **_x)_** _œÜ(wj[T]_ **_x)_** **_x_**

_K_ [2] _j=1_ _M_ _m=1_ _‚àí_ _‚àí_ _‚àí_ 2

X X   e 

_K_

_Œª_ [e] e [e] e [e] e [e] e

+ _œÜ(wj[‚àó][T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[[][p][]][T]_ **_x)_** **_x_** + [1][ ‚àí] _[Œª]_ **_x_** _œÜ(wj[T]_ **_x)_** _œÜ(wj[[][p][]][T]_ **_x)_** **_x_**

_j=1_ _K_ [2][ E][x] _K_ [2][ E][e] _‚àí_ 2 (97)

X h   i h  i

_K_ _K_

_d log q_ _d log q_ e [e] e e

_‚â§_ _K[Œª][2][ Œ¥][2]r_ _N_ _¬∑_ _j=1_ _‚à•wj ‚àí_ **_wj[‚àó][‚à•][2]_** [+ 1]K[ ‚àí][2][Œª] _¬∑_ _Œ¥[Àú][2]r_ _M_ _¬∑_ _j=1_ _‚à•wj ‚àí_ **_wj‚à•2_**

X X

_K_

1 e
+ 2K [2][ ¬∑] _j=1_ _ŒªŒ¥[2]_ _¬∑_ **_wj ‚àí_** **_wj[[][p][]]_** + _ŒªŒ¥[Àú][2]_ _¬∑_ **_wj[‚àó]_** _[‚àí]_ **_[w]j[[][p][]]_** 2

_Œª_ Xd log q   e  [e]   _d log q_
_‚â§_ _K_ [3][/][2][ Œ¥][2] _N_ _¬∑ ‚à•W ‚àí_ **_W_** _[‚àó]‚à•2 + [1]K[ ‚àí][3][/][Œª][2][ ¬∑][ Àú]Œ¥[2]_ _M_ _¬∑ ‚à•W ‚àí_ **_W[f] ‚à•2_**

r r

1
+ _ŒªŒ¥[2]_ **_W_** **_W_** [[][p][]][] + (1 _Œª)Œ¥[Àú][2]_ **_W_** **_W_** [[][p][]][]

2K [3][/][2] _¬∑_ _‚àí_ _‚àí_ _¬∑_ _[‚àó]_ _‚àí_ 2
 

with probability at least f 1 _q[‚àí][d]._
_‚àí_

In conclusion, let Œ± ‚àà R[Kd] and Œ±j ‚àà R[d] with Œ± = [Œ±[T]1 _[,][ Œ±]2[T]_ _[,][ ¬∑ ¬∑ ¬∑][,][ Œ±]K[T]_ []][T][, we have]

_‚à•‚àáf_ (W ) ‚àí‚àáf[ÀÜ](W )‚à•2 = **_Œ±[T][  ]‚àáf_** (W ) ‚àí‚àáf[ÀÜ](W )

_K_



_‚àÇf[ÀÜ]_

_‚â§_ **_Œ±[T]k_** _‚àÇwk_ (W ) ‚àí _‚àÇ[‚àÇf]wk_ (W )

_k=1_

X   

_K_

_f_

‚â≤ _[‚àÇ]_ [ÀÜ] (W ) (W )

_‚àÇwk_ _‚àí_ _‚àÇ[‚àÇf]wk_ 2

_k=1_ _[¬∑ ‚à•][Œ±][k][‚à•][2]_

X

_d log q_ _d log q_

‚â≤ _K [Œª]_ _[Œ¥][2]_ _N_ _¬∑ ‚à•W ‚àí_ **_W_** _[‚àó]‚à•2 + [1][ ‚àí]K_ _[Œª]_ _¬∑_ _Œ¥[Àú][2]_ _M_ _¬∑ ‚à•W ‚àí_ **_W[f] ‚à•2_**

r r

1
+ _ŒªŒ¥[2]_ **_W_** **_W_** [[][p][]][] + (1 _Œª)Œ¥[Àú][2]_ **_W_** **_W_** [[][p][]][]

2K _¬∑_ _‚àí_ _‚àí_ _¬∑_ _[‚àó]_ _‚àí_ 2

(98)
 f  

with probability at least 1 ‚àí _q[‚àí][d]._

H.3 PROOF OF LEMMA 12

The distance of the second order derivatives of the population risk function f (¬∑; p) at point W and
**_W_** [[][p][]] can be converted into bounding P1, P2, P3 and P4, which are defined in (101). The major


-----

idea in proving P1 is to connect the error bound to the angle between W and W [[][p][]]. Similar ideas
apply in bounding the other three items as well.

_Proof of Lemma 12. From (17), we have_

_‚àÇ[2]f_

_‚àÇwj1_ _‚àÇwj2_ (W [[][p][]]; p) = _K[Œª][2][ E][x][œÜ][‚Ä≤][(][w]j[[][p]1[]][T]_ **_x)œÜ[‚Ä≤](wj[[][p]2[]][T]_** **_x)xx[T]_** + [1]K[ ‚àí][2][Œª][ E]x[e][œÜ][‚Ä≤][(][w]j[[][p]1[]][T] **_x)œÜ[‚Ä≤](wj[[][p]2[]][T]_** **_x)xx[T]_** _,_

(99)

e e e e


_‚àÇ[2]f_
and (W ; p) = _[Œª]_ _j1_ **_[x][)][œÜ][‚Ä≤][(][w]j[T]2_** **_[x][)][xx][T][ + 1][ ‚àí]_** _[Œª]_ **_x[œÜ][‚Ä≤][(][w]j[T]1_** **_x)œÜ[‚Ä≤](wj[T]2_** **_x)xx[T]_** _,_

_‚àÇwj1_ _‚àÇwj2_ _K_ [2][ E][x][œÜ][‚Ä≤][(][w][T] _K_ [2][ E][e]

(100)

[e] [e] e e

where wj[[][p][]] is the j-th column of W [[][p][]]. Then, we have

_‚àÇ[2]f_ _‚àÇ[2]f_

_‚àÇwj1_ _‚àÇwj2_ (W _[‚àó]) ‚àí_ _‚àÇwj1_ _‚àÇwj2_ (W )


= _[Œª]_

_K_ [2][ E][x]


_œÜ[‚Ä≤](wj[[][p]1[]][T]_ **_x)œÜ[‚Ä≤](wj[[][p]2[]][T]_** **_x)_** _œÜ[‚Ä≤](wj[T]1_ **_[x][)][œÜ][‚Ä≤][(][w]j[T]2_** **_[x][)]_** **_xx[T]_**
_‚àí_
i


+ [1]K[ ‚àí][2][Œª][ E]x[e] _œÜ[‚Ä≤](wj[[][p]1[]][T]_ **_x)œÜ[‚Ä≤](wj[[][p]2[]][T]_** **_x) ‚àí_** _œÜ[‚Ä≤](wj[T]1_ **_x)œÜ[‚Ä≤](wj[T]2_** **_x)_** **_xx[T]_**

h i

= _[Œª]_ _œÜ[‚Ä≤](wj[[][p]1[]][T]_ **_x)_** _œÜ[‚Ä≤]e(wj[[][p]2[]][T]_ **_x)_** eœÜ[‚Ä≤](wj[T]2 **_[x][)]_** + œÜ[‚Ä≤](wj[T]2 **_[x][)]_** _œÜe[‚Ä≤]e(wj[[][p]1[]][T]_ **_x)_** _œÜ[‚Ä≤](wj[T]1_ **_[x][)]_** **_xx[T]_**

_K_ [2][ E][x] _‚àí_ [e] [e] _‚àí_
h      [i]

+ [1]K[ ‚àí][2][Œª][ E]x[e] _œÜ[‚Ä≤](wj[[][p]1[]][T]_ **_x)_** _œÜ[‚Ä≤](wj[[][p]2[]][T]_ **_x) ‚àí_** _œÜ[‚Ä≤](wj[T]2_ **_x)_** + œÜ[‚Ä≤](wj[T]2 **_x)_** _œÜ[‚Ä≤](wj[[][p]1[]][T]_ **_x) ‚àí_** _œÜ[‚Ä≤](wj[T]1_ **_x)_** **_xx[T]_**

h      [i]

= _[Œª]_ ExœÜ[‚Ä≤](wj[[][p]1[]][T] **_x)_** _œÜ[‚Ä≤]e(wj[[][p]2[]][T]_ **_x)_** eœÜ[‚Ä≤](wj[T]2 **_[x][)]_** **_xx[T]_** + ExœÜ[‚Ä≤](wj[T]2 **_[x][)]_** _œÜ[‚Ä≤](wj[[][p]1e[]][T]_ **_x)_** _œÜ[‚Ä≤](wj[T]1_ **_[x][)]exxe_** _[T][ i]_

_K_ [2] _‚àí_ [e] [e] _‚àí_ [e]

h      

+ [1]K[ ‚àí][2][Œª] Ex[œÜ][‚Ä≤][(][w]j[[][p]1[]][T] **_x)_** _œÜ[‚Ä≤](wj[[][p]2[]][T]_ **_x) ‚àí_** _œÜ[‚Ä≤](wj[T]2_ **_x)_** **_xx[T]_** + Ex[œÜ][‚Ä≤][(][w]j[T]2 **_x)_** _œÜ[‚Ä≤](wj[[][p]1[]][T]_ **_x) ‚àí_** _œÜ[‚Ä≤](wj[T]1_ **_x)_** **_xx[T][ i]_**

h e   e  

:= _K[Œª][2][ (][P][1][ +][ P][2][) + 1]K[ ‚àí][2][Œª][ (]e_ **_[P][3][ +][ P][4][)][.]e_** [e] e e [e] e [e] e e

(101)

For any a ‚àà R[d] with ‚à•a‚à•2 = 1, we have

**_a[T]_** **_P1a =ExœÜ[‚Ä≤](wj[[][p]1[]][T]_** **_x)_** _œÜ[‚Ä≤](wj[[][p]2[]][T]_ **_x)_** _œÜ[‚Ä≤](wj[T]2_ **_[x][)]_** (a[T] **_x)[2]_** (102)
_‚àí_

where a R[d]. Let I = œÜ[‚Ä≤](wj[[][p]1[]][T] **_x)_** _œÜ[‚Ä≤](wj[[][p]2[]][T] x)_ _œÜ[‚Ä≤](wj[T]2_ **_[x][)]_** (a[T] **_x)[2]. It is easy to verify there ex-_**
_‚àà_ _‚àí_ _¬∑_
ists a group of orthonormal vectors such that  _B = {a, b, c, a[‚ä•]4[,][ ¬∑ ¬∑ ¬∑][,][ a]d[‚ä•][}][ with][ {][a][,][ b][,][ c][}][ spans a sub-]T_

space that contains a, wj2 and wj[‚àó]2 [. Then, for any][ x][, we have a unique][ z][ =] _z1,_ _z2,_ _¬∑ ¬∑ ¬∑,_ _zd_

such that h i
**_x = z1a + z2b + z3c + ¬∑ ¬∑ ¬∑ + zda[‚ä•]d_** _[.]_
Also, since x (0, Œ¥[2]Id), we have z (0, Œ¥[2]Id). Then, we have
_‚àºN_ _‚àºN_

_I =Ez1,z2,z3_ _œÜ[‚Ä≤][ ]wj[T]2_ **_[x]_** _œÜ[‚Ä≤][ ]wj[[][p]2[]][T]_ **_x_** **_a[T]_** **_x_**
_|_ _‚àí_ _| ¬∑ |_ _|[2]_

= _œÜ[‚Ä≤][ ]wj[T]2_ **_[x]_** _œÜ[‚Ä≤][ ]wj[[][p]2[]][T]_ **_x_** **_a[T]_** **_x_** _fZ(z1, z2, z3)dz1dz2dz3,_
_|_ _‚àí_ _| ¬∑ |_ _|[2]_ _¬∑_
Z

where x = z1a + z2b + z3c and _fZ(z1, z2, z3) is probability density function of (z1, z2, z3). Next,_
we consider spherical coordinates with z1 = RcosœÜ1, z2 = RsinœÜ1sinœÜ2, z3 = RsinœÜ1cosœÜ2.
Hence,

_I =_ _œÜ[‚Ä≤][ ]wj[T]2_ **_[x]_** _œÜ[‚Ä≤][ ]wj[[][p]2[]][T]_ **_x_** _R cos œÜ1_ _fZ(R, œÜ1, œÜ2)R[2]_ sin œÜ1dRdœÜ1dœÜ2. (103)
_|_ _‚àí_ _| ¬∑ |_ _|[2]_ _¬∑_
Z
 

It is easy to verify that œÜ[‚Ä≤][ ]wj[T]2 **_[x]_** only depends on the direction of x and

_fZ(R, œÜ1, œÜ2) =_ 1 3 _z1[2]_ [+]2[z]Œ¥2[2][2][+][z]3[2] = 1 3 2Œ¥[2]

(2œÄŒ¥[2]) 2 _[e][‚àí]_ (2œÄŒ¥[2]) 2 _[e][‚àí]_ _[R][2]_


-----

only depends on R. Then, we have

_I(i2, j2)_

= _œÜ[‚Ä≤][ ]wj[T]2_ [(][x][/R][)] _œÜ[‚Ä≤][ ]wj[[][p]2[]][T]_ (x/R) _R cos œÜ1_ _fZ(R)R[2]_ sin œÜ1dRdœÜ1dœÜ2
_|_ _‚àí_ _| ¬∑ |_ _|[2]_ _¬∑_
Z ‚àû  _œÄ_ 2œÄ 

= _R[4]fz(R)dR_ cos œÜ1 sin œÜ1 _œÜ[‚Ä≤][ ]wj[T]2_ [(][x][/R][)] _œÜ[‚Ä≤][ ]wj[[][p]2[]][T]_ (x/R) _dœÜ1dœÜ2_

0 0 0 _|_ _|[2]_ _¬∑_ _¬∑ |_ _‚àí_ _|_

Z Z Z

(a) _‚àû_ _œÄ_ 2œÄ  
3Œ¥[2] _R[2]fz(R)dR_ sin œÜ1 _œÜ[‚Ä≤][ ]wj[T]2_ [(][x][/R][)] _œÜ[‚Ä≤][ ]wj[[][p]2[]][T]_ (x/R) _dœÜ1dœÜ2_
_‚â§_ _¬∑_ 0 0 0 _¬∑ |_ _‚àí_ _|_
Z Z Z

 

=3Œ¥[2] Ez1,z2,z3 _œÜ[‚Ä≤][ ]wj[T]2_ **_[x]_** _œÜ[‚Ä≤][ ]wj[[][p]2[]][T]_ **_x_**

_¬∑_ _‚àí_ _|_

3Œ¥[2] Ex _œÜ[‚Ä≤][ ]wj[T]2_ **_[x]_** _œÜ[‚Ä≤][ ]wj[[][p]2[]][T]_ **_x_** _,_ 
_‚â§_ _¬∑_ _‚àí_ _|_
(104)
 

where the inequality (a) is derived from the fact that _cosœÜ1_ 1 and
_|_ _| ‚â§_
_‚àû_ 1 _‚àû_

_R[4]_ 3 2Œ¥[2] _dR =_ 3 2Œ¥[2] )
0 (2œÄŒ¥[2]) 2 _[e][‚àí]_ _[R][2]_ 0 _‚àí_ (2[R]œÄŒ¥[3][Œ¥][2][2]) 2 _[d][(][e][‚àí]_ _[R][2]_

Z Z

_‚àû_
= _e[‚àí]_ 2[R]Œ¥[2][2] _d [R][3][Œ¥][2]_ 3 (105)

0 (2œÄŒ¥[2]) 2

Z

_‚àû_ 1
=3Œ¥[2] _R[2]_ 3 2Œ¥[2] _dR._

0 (2œÄŒ¥[2]) 2 _[e][‚àí]_ _[R][2]_

Z

Define a set 1 = **_x_** (wj[[][p]2[]][T] **_x)(wj[T]2_** **_[x][)][ <][ 0][}][. If][ x][ ‚ààA][1][, then][ w]j[[][p]2[]][T]_** **_x and wj[T]2_** **_[x][ have different]_**
_A_ _{_ _|_

thatsigns, which means the value of œÜ[‚Ä≤](wj[T]2 **_[x][)][ and][ œÜ][‚Ä≤][(][w]j[[][p]2[]][T]_** **_x) are different. This is equivalent to say_**

_|œÜ[‚Ä≤](wj[T]2_ **_[x][)][ ‚àí]_** _[œÜ][‚Ä≤][(][w]j[[][p]2[]][T]_ **_x)| =_** 10,, if if x x ‚ààA11 _._ (106)
 _‚ààA[c]_

Moreover, if x 1, then we have
_‚ààA_

**_wj[[][p]2[]][T]_** **_x_** **_wj[[][p]2[]][T]_** **_x_** **_wj[T]2_** **_[x][| ‚â§‚à•][w]j[[][p]2[]]_** 2 _[‚à•][2][ ¬∑ ‚à•][x][‚à•][2][.]_ (107)
_|_ _| ‚â§|_ _‚àí_ _[‚àí]_ **_[w][j]_**

Let us define a set A2 such that

**_wj[[][p]2[]][T]_** **_x_** _j2_ 2 _[‚à•][2]_ **_wj[[][p]2[]]_** 2 _[‚à•][2]_
_A2 =nx_ _‚à•w|_ _j[‚àó]2_ _[‚à•][2][‚à•][x]|[‚à•][2]_ _‚â§_ _[‚à•][w][‚àó]‚à•w[‚àí]j[‚àó]2[w][‚à•][2][j]_ o = nŒ∏x,wj[‚àó]2 _| cos Œ∏x,wj[p2]_ _[| ‚â§]_ _‚à•_ _‚à•w[‚àí]j[[][p]2[]][w][‚à•][2][j]_ o. (108)

Hence, we have that

Ex _œÜ[‚Ä≤](wj[T]2_ **_[x][)][ ‚àí]_** _[œÜ][‚Ä≤][(][w]j[[][p]2[]][T]_ **_x)_** =Ex _œÜ[‚Ä≤](wj[T]2_ **_[x][)][ ‚àí]_** _[œÜ][‚Ä≤][(][w]j[[][p]2[]][T]_ **_x)_**
_|_ _|[2]_ _|_ _|_

=Prob(x 1) (109)
_‚ààA_
Prob(x 2).
_‚â§_ _‚ààA_

Since x ‚àºN (0, Œ¥[2]‚à•a‚à•2[2][I][)][,][ Œ∏]x,wj[[][p]2[]] [belongs to the uniform distribution on][ [][‚àí][œÄ, œÄ][]][, we have]

_œÄ_ arccos _‚à•wj[[][p]2[]][‚àí][w][j]2_ _[‚à•][2]_
Prob(x 2) = _‚àí_ _‚à•wj[[][p]2[]][‚à•][2]_ _‚à•wj[[][p]2[]]_ _[‚àí]_ **_[w][j]2_** _[‚à•][2]_ )
_‚ààA_ _œÄ_ _‚â§_ _œÄ[1]_ [tan(][œÄ][ ‚àí] [arccos] **_wj[[][p]2[]]_**

_‚à•_ _[‚à•][2]_

= [1] _‚à•wj[[][p]2[]]_ _[‚àí]_ **_[w][j]2_** _[‚à•][2]_ ) (110)

_œÄ_ [cot(arccos] **_wj[[][p]2[]]_**

_‚à•_ _[‚à•][2]_

**_wj[[][p]2[]]_** 2 _[‚à•][2]_
_‚à•_ _[‚àí]_ **_[w][j]_** _._

_‚â§_ _œÄ[2]_ **_wj[[][p]2[]]_**

_‚à•_ _[‚à•][2]_


-----

Hence, (104) and (110) suggest that

_I_
_‚â§_ [6]œÄ[Œ¥][2]


**_wj2_** **_wj[[][p]2[]]_**
_‚à•_ _‚àí_ _[‚à•][2]_ **_a_** 2[.] (111)

_œÉK_ _¬∑ ‚à•_ _‚à•[2]_


The same bound that shown in (111) holds for P2 as well.

**_P3 and P4 satisfy (111) except for changing Œ¥[2]_** to _Œ¥[Àú][2]._

Therefore, we have

_f_ (W [[][p][]]; p) _f_ (W ; p) 2
_‚à•‚àá[2]_ _‚àí‚àá[2]_ _‚à•_

= max **_Œ±[T]_** ( _f_ (W [[][p][]]; p) _f_ (W ; p))Œ±
**_Œ±_** 2 1 _‚àá[2]_ _‚àí‚àá[2]_
_‚à•_ _‚à•_ _‚â§_

_K_ _K_

_‚àÇ[2]f_ _‚àÇ[2]f_

_‚â§_ _j1=1_ _j2=1_ _j1_  _‚àÇwj1_ _‚àÇwj2_ (W [[][p][]]; p) ‚àí _‚àÇwj1_ _‚àÇwj2_ (W ; p)Œ±j2

X X

_K_ _K_

**_[Œ±][T]_** _Œª_ **_P1 + P2_** 2 + (1 _Œª)_ **_P3 + P4_** 2 **_Œ±j1_** 2 **_Œ±j2_** 2

_‚â§_ _K[1][2]_ _‚à•_ _‚à•_ _‚àí_ _‚à•_ _‚à•_ _‚à•_ _‚à•_ _‚à•_ _‚à•_

_j1=1_ _j2=1_

X X  

_K_ _K_ **_wj[[][p]2[]]_** 2 _[‚à•][2]_

4(ŒªŒ¥[2] + (1 _Œª)Œ¥[Àú][2])_ _‚à•_ _[‚àí]_ **_[w][j]_** **_Œ±j1_** 2 **_Œ±j2_** 2

_‚â§_ _K[1][2]_ _‚àí_ _œÉK_ _‚à•_ _‚à•_ _‚à•_ _‚à•_

_j1=1_ _j2=1_

X X

_ŒªŒ¥[2]_ + (1 _Œª)Œ¥[Àú][2][]_ _,_

_‚â§_ _K[4]_ _‚àí_ _¬∑ [‚à•][W][ [][p]œÉ[]][ ‚àí]K_ **_[W][ ‚à•][2]_**

 

where Œ± ‚àà R[Kd] and Œ±j ‚àà R[d] with Œ± = [Œ±[T]1 _[,][ Œ±]2[T]_ _[,][ ¬∑ ¬∑ ¬∑][,][ Œ±]K[T]_ []][T][ .]


(112)


I INITIALIZATION VIA TENSOR METHOD

In this section, we briefly summarize the tensor initialization in (Zhong et al., 2017) by studying the
target function class as


_y = [1]_


_vj[‚àó][œÜ][(][w]j[‚àó][T]_ **_[x][)][,]_** (113)
_j=1_

X


whereWithout loss of generalization, we can assume vj[‚àó] _[‚àà]_ _[R][. Note that for ReLU function, we have] vj[‚àó]_ _[ v]j[‚àó][œÜ][(][w]j[‚àó][T]_ **_[x][) =][ sign][(][v]j[‚àó][)][œÜ][(][|][v]j[‚àó][|][w]j[‚àó][T]_** **_[x][)][.]_**
function studied in (2) is the special case of (113) when[‚àà{] v[+1]j[‚àó] _[,][= 1][ ‚àí][1][}][ for all][. Additionally, it is clear that the][ j][. In addition, Theorem 5.6]_
in (Zhong et al., 2017) show that the sign of vj[‚àó] [can be directly recovered using tensor initialization,]
which indicates the the equivalence of (2) and (113) when using tensor initialization.

We first define some high order momenta in the following way:

**_M1 = Ex{yx} ‚àà_** R[d], (114)

**_M2 = Ex_** _y_ **_x ‚äó_** **_x ‚àí_** _Œ¥[2]I_ _‚àà_ R[d][√ó][d], (115)
h   [i]

**_M3 = Ex_** _y_ **_x[‚äó][3]_** _‚àí_ **_x‚äó[e]_** _Œ¥[2]I_ _‚àà_ R[d][√ó][d][√ó][d], (116)

where Ex is the expectation over x andh z [‚äó][3] := z ‚äó **_z ‚äóz[i]. The operator_** _‚äó_ is defined as

_d2_

**_v‚äó[e]_** **_Z =_** _i=1(v ‚äó_ **_zi ‚äó_** **_zi + zi ‚äó_** **_v ‚äó_** **_zi + zi ‚äó_** **_zi ‚äó[e]v),_** (117)

X

for any vector v ‚àà R[d][1] and Z ‚àà R[d][1][√ó][d][2] .


-----

Following the same calculation formulas in the Claim 5.2 (Zhong et al., 2017), there exist some
known constants œài, i = 1, 2, 3, such that


_œà1_ **_wj[‚àó]_** _j_ _[,]_ (118)
_j=1_ _¬∑ ‚à•_ _[‚à•][2]_ _[¬∑][ w][‚àó]_

X


**_M1 =_**


_j=1_ _œà2 ¬∑ ‚à•wj[‚àó][‚à•][2]_ _[¬∑][ w]j[‚àó][w][‚àó]j_ _[T]_ _[,]_ (119)

X


**_M2 =_**


_œà3_ **_wj[‚àó]_** _j_ _,_ (120)
_j=1_ _¬∑ ‚à•_ _[‚à•][2]_ _[¬∑][ w][‚àó‚äó][3]_

X


**_M3 =_**


where w[‚àó]j [=][ w]j[‚àó][/][‚à•][w]j[‚àó][‚à•][2][ in (114)-(116) is the normalization of][ w]j[‚àó][. Therefore, we can see that the]
information of **_wj[‚àó]_** _j=1_ [are separated as the direction of][ w][j][ and the magnitude of][ w][j][ in][ M][1][,][ M][2]
and M3. _{_ _[}][K]_

_N_
**_M1, M2 and M3 can be estimated through the samples_** (xn, yn) _n=1[, and let][ c]M1,_ **_M2,_** **_M3_**
denote the corresponding estimates. First, we will decompose the rank-K tensor M3 and obtain the

**_w[‚àó]j_** _[}][K]j=1[. By applying the tensor decomposition method (Kuleshov et al., 2015) to][ c]M3, the outputs,[c]_ [c]
_{_

denoted by **_w‚àój_** [, are the estimations of][ {][s][j][w][‚àó]j _[}][K]j=1[, where][ s][j][ is an unknown sign. Second, we will]_
estimate sj, vj[‚àó] [and][ ‚à•][w]j[‚àó][‚à•][2][ through][ M][1][ and][ M][2][. Note that][ M][2][ does not contain the information of]
**_sj because s[b][2]j_** [is always][ 1][. Then, through solving the following two optimization problem:]


**_Œ±1 = arg min_** **_M1_** _œà1Œ±1,jw‚àój_ _,_
**_Œ±1‚ààR[K][ :]_** _‚àí_ _j=1_

X

_K_

**_Œ±2b = arg min_** **_M2[c]_** _œà2Œ±2,jw‚àój[b]w‚àój_ _T_
**_Œ±2‚ààR[K][ :]_** _‚àí_ _j=1_

X

The estimation of sj can be given asb [c] [b] [b]

_sÀÜj = sign(Œ±1,j/Œ±2,j)._

Also, we know that _Œ±1,j_ is the estimation of **_wj[‚àó]_**
_|_ _|_ _‚à•_ b[‚à•] [and]b

_vÀÜj = sign(Œ±1,j/sj) = sign(Œ±2,j)._

b

Thus, W [(0)] is given as

b b

sign(Œ±2,1)Œ±1,1w‚àó1[,] _,_ sign(Œ±2,K)Œ±1,Kw‚àóK

_¬∑ ¬∑ ¬∑_
h
b b [b] b b [b]

**Subroutine 1 Tensor Initialization Method**


(121)


1: Input: labeled data D = {(xn, yn)}n[N]=1[;]

2: Partition into three disjoint subsets 1, 2, 3;
_D_ _D_ _D_ _D_
3: Calculate **_M1,_** **_M2 following (114), (115) using D1, D2, respectively;_**

4: Obtain the estimate subspace **_V of_** **_M2;_**

5: Calculate **_M[c]3(V[c],_** **_V,_** **_V ) through_** 3;
_D_

6: Obtain {sj}j[K]=1 [via tensor decomposition method (Kuleshov et al., 2015) on][b] [c] [ c]M3(V, **_V,_** **_V );_**

7: Obtain **_Œ±1,[c]Œ±2 by solving optimization problem (121);[b]_** [b] [b]

8: Return: wb _j[(0)]_ = sign(Œ±2,j)Œ±1,jV **_uj and vj[(0)]_** = sign(Œ±2,j), j = 1, ..., K. [b] [b] [b]

b b

To reduce the computational complexity of tensor decomposition, one can projectb b [b] b b **_M3 to a lower-_**
dimensional tensor (Zhong et al., 2017). The idea is to first estimate the subspace spanned by
**_wj[‚àó]_** _j=1[, and let][ b]V denote the estimated subspace. Moreover, we have_ [c]
_{_ _[}][K]_

**_M3(V,_** **_V,_** **_V ) = Ex_** _y_ (V _[T]_ **_x)[‚äó][3]_** _‚àí_ (V _[T]_ **_x)‚äó[e]_** Ex(V _[T]_ **_x)(V_** _[T]_ **_x)[T][ i]_** _‚àà_ R[K][√ó][K][√ó][K], (122)
h  

[b] [b] [b] [b] [b] [b] [b]


-----

Then, one can decompose the estimate **_M3(V,_** **_V,_** **_V ) to obtain unit vectors_** **_sÀÜj_** _j=1_
_{_ _}[K]_ _[‚àà]_ [R][K][. Since]

**_w[‚àó]_** lies in the subspace V, we have V V _[T]_ **_w[‚àó]j_** [=][ w]j[‚àó][. Then,][ b]V ÀÜsj is an estimate of w[‚àó]j [. The]
initialization process is summarized in Subroutine 1.[c] [b] [b] [b]

J CLASSIFICATION PROBLEMS

The framework in this paper is extendable to binary classification problem. For binary classification
problem, the output y given input x is defined as

Prob{y = 1} = g(W _[‚àó]; x)_ (123)

with some ground truth parameter W _[‚àó]. To guarantee the output is within [0, 1], the activation_
function is often used as sigmoid. For classification, the loss function is cross-entropy, and the
objective function over labeled data D is defined as

_f_ (W ) = [1] _yn log g(W ; xn)_ (1 _yn) log(1_ _g(W ; xn))._ (124)
_D_ _N_ _‚àí_ _‚àí_ _‚àí_ _‚àí_

(xnX,yn)‚ààD

The expectation of objective function can be written as


E _f_ (W ) =E(x,y) _y log(g(W ; xn))_ (1 _y) log(1_ _g(W ; x))_
_D_ _D_ _‚àí_ _‚àí_ _‚àí_ _‚àí_

=ExE(y|x) ‚àí _y log(g(W ; xn)) ‚àí_ (1 ‚àí _y) log(1 ‚àí_ _g(W ; x))_ (125)

=Ex _‚àí_ _g(W_ _[‚àó]; x) log(g(W ; xn)) ‚àí_ (1 ‚àí _g(W_ _[‚àó]; x)) log(1 ‚àí_ _g(W ; xn))_
h i

Please note that (125) is exactly the same as (32) with Œª = 1 when the loss function is squared loss.

For cross entropy loss function, the second order derivative of (125) is calculated as

_‚àÇfD(W )_ = [1] _yn_ 1 ‚àí _yn_ _j_ **_[x][)][œÜ][‚Ä≤][(][w]k[T]_** **_[x][)][xx][T][ .]_** (126)
_‚àÇwj‚àÇwk_ _N_ [[] _g[2](W ; x) [+]_ (1 _g(W ; x))[2][ ]][ ¬∑][ œÜ][‚Ä≤][(][w][T]_

_‚àí_

when j Ã∏= k. Refer to (88) in (Fu et al., 2020) or (132) in (Zhang et al., 2020b), we have
_yn(œÜ[‚Ä≤](wj[T]_ **_[x][)][œÜ][‚Ä≤][(][w]k[T]_** **_[x][))]_** _œÜ[‚Ä≤](wj[T]_ **_[x][)][œÜ][‚Ä≤][(][w]k[T]_** **_[x][)]_**

(127)

_g[2](W ; x)_ 2 _g[2](W ; x)_ 2

_[‚â§]_ _[‚â§]_ _[K]_ [2][.]

Following similar steps in (90), from Defintion 2, we know that Œ±j[T] _‚àÇf‚àÇwDj(‚àÇWwk )_ _[Œ±][k][ belongs to the sub-]_
exponential distribution. Therefore, similiar results for objective function with cross-entropy loss
can be established as well. One can check (Fu et al., 2020) or (Zhang et al., 2020b) for details.

K ONE-HIDDEN LAYER NEURAL NETWORK WITH TOP LAYER WEIGHTS

For a general one-hidden layer neural network, the output of the neural network is defined as


_g(W, v; x) = [1]_


_vjœÜ(wj[T]_ **_[x][)][,]_** (128)
_j=1_

X


where v = [v1, v2, _, vK]_ _R[K]. Then, the target function can be defined as_
_¬∑ ¬∑ ¬∑_ _‚àà_


_y = g(W_ _, v[‚àó]; x) = [1]_

_[‚àó]_ _K_

for some unknown weights W _[‚àó]_ and v[‚àó].


_vj[‚àó][œÜ][(][w]j[‚àó][T]_ **_[x][)]_** (129)
_j=1_

X


In the following paragraphs, we will provide a short description for the equivalence of (129) and (2)
in theoretical analysis. Note that for ReLU functions, we have vjœÜ(wj[T] **_[x][) =][ sign][(][v][j][)][œÜ][(][|][v][j][|][w]j[T]_** **_[x][)][.]_**


-----

Without loss of generalization, we can assume vj, vj[‚àó]
I, we know that the sign of vj[‚àó] [can exactly estimated through tensor initialization. There, we can][‚àà{][+1][,][ ‚àí][1][}][ for all][ j][ ‚àà] [[][K][]][5][. From Appendix]
focus on analysis the neural network in the form as


_g(W ; x) = [1]_


_vj[‚àó][œÜ][(][w]j[T]_ **_[x][)][.]_** (130)
_j=1_

X


Considering the objective function in (1) and population risk function in (17), we have

_f_

_[‚àÇ]_ [ÀÜ] (W ) (W ; p)

_‚àÇwk_ _‚àí_ _‚àÇ[‚àÇf]wk_ 2

_K_ _N_

_Œª_
= _vj[‚àó]_ _œÜ(wj[T]_ **_[x][n][)][ ‚àí]_** _[œÜ][(][w]j[‚àó][T]_ **_[x][n][)]_** **_xn_** Ex _œÜ(wj[T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[[][p][]][T]_ **_x)_**

_K_ [2]N _j=1_ _n=1_ _‚àí_

X h X     


_œÜ(wj[T]_ **_xm)_** _œÜ(wj[T]_ **_xm)_** **_xm_** Ex _œÜ(wj[T]_ **_x)_** _œÜ(wj[[][p][]][T]_ **_x)_** **_x_**
_‚àí_ _‚àí_ _‚àí_
e 

[e] e [e] e [e] e e

_œÜ(wj[T]_ **_[x][n][)][ ‚àí]_** _[œÜ][(][w]j[‚àó][T]_ **_[x][n][)]_** **_xn ‚àí_** Ex _œÜ(wj[T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[[][p][]][T]_ **_x)_** **_x_**
     


+ K[1][ ‚àí][2]M [Œª] _[v]j[‚àó]_

_K_

_vj[‚àó]_
_j=1_ _¬∑|_ _[| ¬∑]_

X


_j=1_

_Œª_

_K_ [2]N


_m=1_

_N_

_n=1_

h X


_M_

+ [1][ ‚àí] _[Œª]_ _œÜ(wj[T]_ **_xm)_** _œÜ(wj[T]_ **_xm)_** **_xm_** Ex _œÜ(wj[T]_ **_x)_** _œÜ(wj[[][p][]][T]_ **_x)_** **_x_**

_K_ [2]M _m=1_ _‚àí_ _‚àí_ _‚àí_ 2

_K_ h XN  e  i

_Œª_ [e] e [e] e [e] e e

= _œÜ(wj[T]_ **_[x][n][)][ ‚àí]_** _[œÜ][(][w]j[‚àó][T]_ **_[x][n][)]_** **_xn_** Ex _œÜ(wj[T]_ **_[x][)][ ‚àí]_** _[œÜ][(][w]j[[][p][]][T]_ **_x)_** **_x_**

_j=1_ _K_ [2]N _n=1_ _‚àí_

X h X       i

_M_

+ [1][ ‚àí] _[Œª]_ _œÜ(wj[T]_ **_xm)_** _œÜ(wj[T]_ **_xm)_** **_xm_** Ex _œÜ(wj[T]_ **_x)_** _œÜ(wj[[][p][]][T]_ **_x)_** **_x_**

_K_ [2]M _m=1_ _‚àí_ _‚àí_ _‚àí_ 2[,]

h X   e  i (131)

[e] e [e] e [e] e e

which is exact the same as (89). Similar results can be derived for Lemma 12. Therefore, the
conclusions and proofs of Lemma 1 and Lemma 2 does not change at all.

Additionally, fixing the second-layer weights and only training the hidden layer is the state-of-theart practice in analyzing two-layer neural networks (Arora et al., 2019b;a; Allen-Zhu et al., 2019;
Safran & Shamir, 2018; Li & Liang, 2018; Brutzkus & Globerson, 2017; Oymak & Soltanolkotabi,
2018; Zhang et al., 2019). Additionally, as indicated in (Safran & Shamir, 2018), training a onehidden-layer neural network with all vj fixed as 1 has intractable many spurious local minima, which
indicates that training problem is not trivial.

the new ground truth weights.5To see this, one can view |vj‚àó[|][w]j[‚àó] [as the new ground truth weights, and the goal for this paper is to recover]


-----

