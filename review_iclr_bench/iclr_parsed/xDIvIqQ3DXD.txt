# On the approximation properties of recur- rent encoder-decoder architectures


**Zhong Li[‚àó]**
School of Mathematical Sciences
Peking University
```
li zhong@pku.edu.cn

```
**Qianxiao Li[‚Ä†]**
Department of Mathematics
National University of Singapore
```
qianxiao@nus.edu.sg

```

**Haotian Jiang[‚àó]**
Department of Mathematics
National University of Singapore
```
e0012663@u.nus.edu

```

Abstract

Encoder-decoder architectures have recently gained popularity in sequence
to sequence modelling, featuring in state-of-the-art models such as transformers. However, a mathematical understanding of their working principles still remains limited. In this paper, we study the approximation properties of recurrent encoder-decoder architectures. Prior work established
theoretical results for RNNs in the linear setting, where approximation
capabilities can be related to smoothness and memory of target temporal
relationships. Here, we uncover that the encoder and decoder together form
a particular ‚Äútemporal product structure‚Äù which determines the approximation efficiency. Moreover, the encoder-decoder architecture generalises
RNNs with the capability to learn time-inhomogeneous relationships. Our
results provide the theoretical understanding of approximation properties
of the recurrent encoder-decoder architecture, which precisely characterises,
in the considered setting, the types of temporal relationships that can be
efficiently learned.

1 Introduction

Encoder-decoder is an increasingly popular architecture for sequence to sequence modelling
problems (Sutskever et al., 2014; Chiu et al., 2018; Venugopalan et al., 2015). The core
of this architecture is to first encode the input sequence into a vector using the encoder
and then map the vector into the output sequence through the decoder. In particular,
such architecture forms the main component in the transformer network (Vaswani et al.,
2017), which has become a powerful method for modelling sequence to sequence relationships
(Parmar et al., 2018; Beltagy et al., 2020; Li et al., 2019).

The encoder-decoder family of structures differ significantly from direct application of recurrent neural networks (RNNs, Elman (1990)) and its generalisations (Hochreiter & Schmidhuber, 1997; Cho et al., 2014b) for processing sequences. However, both architectures can
be considered as modelling mappings between sequences, albeit with different underlying
structures. Hence, a natural but unresolved question is: how are these approaches fundamentally different? Answering this question is not only of theoretical importance but also
of practical interest. Currently, architectural selection for different time series modelling
tasks is predominantly empirical. Thus, it is desirable to develop a concrete mathematical framework to understand the key differences between separate architectures in order to
guide practitioners in a principled way.

_‚àóEqual contribution_
_‚Ä†Corresponding author_


-----

In this paper, we investigate the approximation properties of encoder-decoder architectures.
Approximation is one of the most basic and important problems for supervised learning. It
considers to what extent the model can fit a target. In particular, we prove a general approximation result in the linear setting, which characterises the types of temporal input-output
relationships that can be efficiently approximated by encoder-decoder architectures. These
results reveal that such architectures essentially generalise RNNs by lifting the requirement
of time-homogeneity (see Remark 3.2) in the target relationships. Hence, it can be used
to tackle a broader class of sequence to sequence problems. Furthermore, of particular interest is the identification of a ‚Äútemporal product structure‚Äù ‚Äî a precise property of the
target temporal relationship that highlights another intrinsic difference between recurrent
encoder-decoders and RNNs.

Our main contributions can be summarised as follows.

1. We prove a universal approximation result for recurrent encoder-decoder architectures in the linear setting, including the approximation rates.

2. We show that in the considered setting, the recurrent encoder-decoder generalises
the RNNs and can approximate time-inhomogeneous relationships, which further
adapt to additional temporal product structures in the target relationship. This
answers precisely how encoder-decoders are different from RNNs, at least in the
considered setting.

**Organisation.** In Section 2, we review the related work on encoder-decoder architectures
and general approximation theories of sequence modelling. The approximation problem is
formulated in Section 3. Our main results, their consequences and numerical illustrations
are presented in Section 4. All the proofs and numerical details are included in appendices.

**Notations.** For consistency, we adhere to the following notations. Boldfaced letters are
reserved for sequences or paths, which can be understood as functions of time. Lower case
letters can mean vectors or scalars. Matrices are denoted by capital letters. For Œ± ‚àà N, C _[Œ±]_
denotes the space of functions with continuous derivatives up to order-Œ±.

2 Related work

We first review some previous works on sequence to sequence modelling. The encoderdecoder architecture first appeared in Kalchbrenner & Blunsom (2013), where they map the
input sequence into a vector using convolutional neural networks (CNNs), and then using
a recurrent structure to map the vector to the output sequence. With the flexibility of
manipulating the underlying structure of encoder and decoder, numerous models based on
this architecture have come out thereafter. For instance, Cho et al. (2014b) used gated RNNs
as both the encoder and decoder, while in the later work (Cho et al., 2014a), they proposed
a CNN-based decoder. In Sutskever et al. (2014), they proposed a deep LSTM for both
the encoder and decoder. Bahdanau et al. (2015) first introduced the attention mechanism,
which was further developed in the well-known transformer networks (Vaswani et al., 2017).
However, most of the research on encoder-decoder architectures focused on applications. A
theoretical understanding is helpful for its further improvement and development.

From the theoretical point of view, Ye & Sung (2019) studied several theoretical properties of
CNN encoder-decoders, including expressiveness, generalisation capability and optimisation
landscape. Of particular relevance to the current work is expressiveness, which considers
the relationships that can be generated from the architecture. However, this is not approximation. Yun et al. (2020) proved the universal approximation property of transformers for
certain classes of functions, for example, permutation equivariant functions, but they did not
consider the actual dynamical properties of target relationships that affect approximation.
Dynamical proprieties such as memory, smoothness and low rank structures are essential,
because they can precisely characterise different temporal relationships and affect the approximation capabilities of models. Assuming the target generated from a hidden dynamical
system is one approach, which is widely applied (Maass et al., 2007; Sch¬®afer & Zimmermann,
2007; Doya, 1993; Funahashi & Nakamura, 1993). In contrast, a functional-based approach is


-----

recently introduced, where the target temporal relationships are generated from functionals
satisfying specific properties such as linearity, continuity, regularity and time-homogeneity
(Li et al., 2021). In Li et al. (2021), the approximation properties of linear RNN models
are studied, and the results therein show that the approximation efficiency is related to the
memory structure. In Jiang et al. (2021), similar formulations are applied to investigate
convolutional architectures, where the results suggest that targets with certain spectrum
regularity can be well approximated by dilated CNNs. Under this framework, the target
temporal relationship that can be efficiently approximated is characterised by properties
such as memory, smoothness and sparsity. This enables us to make precise mathematical comparisons between different architectures. Our results in this work reveal that the
encoder-decoders have a special temporal product structure which is intrinsically different
from other sequence modelling architectures.

3 Problem formulation

In this section, we precisely define the input space, output space, concept space and hypothesis space, respectively.

**Functional formulation of temporal modelling.** First, we define the input and output
space precisely. A temporal sequence can be viewed as a function of time t. The input space
is defined by X = C0((‚àí‚àû, 0], R[d]). This is the space of continuous functions from (‚àí‚àû, 0]
to R[d] vanishing at infinity, where d N+ is the dimension. Denote the element in by
_‚àà_ _X_
**_xtake the outputs space as = {xt ‚àà_** R[d] : t ‚â§ 0}, we equip Y = C Xb([0 with the supremum norm, ‚àû), R), the space of bounded continuous functions ‚à•x‚à•X := supt‚â§0 ‚à•xt‚à•‚àû. We
from [0, ‚àû) to R. We consider real-valued outputs, since each dimension can be handled
individually for vector-valued outputs.

The mapping between input and output sequences can be formulated as a sequence of
_functionals, i.e. yt = Ht(x), t_ 0. The output yt at the time step t depends on the input
_‚â•_
sequence x. The ground truth relation between inputs and outputs is formulated by the
sequence of functionals H := _Ht : t_ 0 .
_{_ _‚â•_ _}_

We provide an example to illustrate the above formulation. Given an input x, the output
**_y is a smoothed version of x, resulting from convolving x with the Gaussian kernel g(s) =_**
_‚àö12œÄ_ [exp(][‚àí] _[s]2[2]_ [). This relation can be formulated as][ y][t][ =][ H][t][(][x][) =] _‚àû0_ _g(t + s)x‚àísds._

R

**The RNN encoder-decoder model.** For the supervised learning problem, our goal is
to use a model to learn the target relationship H. First, we define the model. Among all
different variants of the encoder-decoder architectures, the RNN encoder-decoder introduced
in Cho et al. (2014b) can be considered as the most simple and representative model, where
the encoder and decoder are both RNNs. We study this particular model as we try to
eliminate other factors and only focus on the encoder-decoder architecture itself.

Under our setting, the simplified model of Cho et al. (2014b) with RNNs as both encoder
and decoder can be formulated as
_hs = œÉE(WEhs_ 1 + UExs + bE), _v = hœÑ_ _,_
_‚àí_
_gt = œÉD(WDgt_ 1 + bD), _g0 = v,_ (1)
_‚àí_
_ot = WOgt + bO,_

where ht, gt are hidden states of the encoder and decoder respectively. Recurrent activation
functions are denoted by œÉE and œÉD. Here, œÑ denotes the terminating time step of the
encoder, and v is the summary of the input sequence, which is called as the coding vector.
The model prediction is denoted asEquation (1) describes the following model dynamics. First, the encoder reads the entire ot ‚àà R. All the other notations are model parameters.
input x, and then summarises the input into a fixed size coding vector v, which is also the
last hidden state of the encoder. Next, the coding vector is passed into the decoder as the
initial state, and then the decoder produces an output at each time step. Note that the
encoder has a terminating time, and the decoder has a starting time. This is the reason
why we take the input and output as semi-infinite sequences.


-----

We study a linear, residual and continuous-time idealisation of the model dynamics (1):

_hÀô_ _s = Whs + Uxs,_ _v = Qh0,_ _s_ 0
_‚â§_
_gÀôt = V gt,_ _g0 = Pv,_ (2)

_ot = c[‚ä§]gt,_ _t_ 0,
_‚â•_

where W ‚àà R[m][E] _[√ó][m][E]_, U ‚àà R[m][E] _[√ó][d], Q ‚àà_ R[N] _[√ó][m][E]_, V ‚àà R[m][D][√ó][m][D], P ‚àà R[m][D][√ó][N] and c ‚àà R[m][D]
are parameters. mE and mD denote the width of encoder and decoder, respectively. The
coding vector v has dimension N, where we apply linear transformations to control it. We
assume h = 0, which is the usual choice for the initial condition of RNN hidden states.
_‚àí‚àû_

Since our goal is to investigate approximation problems over large time horizons, we are
supposed to consider the stable RNN encoder-decoders, where

_W_ _mE :=_ _W_ R[m][E] _[√ó][m][E]_ : eigenvalues of W have negative real parts _,_ (3)
_‚ààW_ _{_ _‚àà_ _}_

_V_ _mD :=_ _V_ R[m][D][√ó][m][D] : eigenvalues of V have negative real parts _._ (4)
_‚ààV_ _{_ _‚àà_ _}_

The hypothesis space of RNN encoder-decoder models with arbitrary widths and coding
vector dimension is defined as [ÀÜ] := _mE_ _,mD,N_ N+ [ÀÜ]mE _,mD,N_, where
_H_ _‚àà_ _H_

_‚àû_

## ÀÜmE,mD,N := HÀÜ := H t : t 0 [S] : [ÀÜ]H t(x) = c[‚ä§]e[V t]P Qe[W s]Ux sds, with
_H_ _{[ÀÜ]_ _‚â•_ _}_ 0 _‚àí_
 Z (5)

(W, U, Q, V, P, c) ‚ààWmE √ó R[m][E] _[√ó][d]_ _√ó R[N]_ _[√ó][m][E]_ _√ó VmD √ó R[m][D][√ó][N]_ _√ó R[m][D]_ _._


The widths mE, mD and the coding vector dimension N together control the capacity/complexity of the hypothesis space. Note that the assumptions on eigenvalues of W
and V ensure that the parameterized linear functionals are continuous.

Due to the mathematical form (5), not all functionals can be represented by RNN encoderdecoders. To achieve a good approximation, the target functionals must possess certain
structures. We introduce the following definitions to clarify these structures.
**Definition 3.1. Let H =** _Ht : t_ 0 _be a sequence of functionals._
_{_ _‚â•_ _}_

_1. For any t_ 0, the functional Ht is linear and continuous if for any Œª1, Œª2 R
_and x1, x2 ‚â•_ _, we have Ht(Œª1x1 + Œª2x2) = Œª1Ht(x1) + Œª2Ht(x2), and_ _H ‚ààt_ :=
supx _,_ **_x_** _‚ààX1_ _Ht(x)_ _<_ _, where_ _Ht_ _denotes the induced functional norm. ‚à•_ _‚à•_
_‚ààX_ _‚à•_ _‚à•X ‚â§_ _|_ _|_ _‚àû_ _‚à•_ _‚à•_

_2. For any t ‚â•_ 0, the functional Ht is regular if for any sequence {x[(][n][)]}n[‚àû]=1 _[‚äÇX]_
_such that limn‚Üí‚àû_ _x[(]s[n][)]_ = 0 for almost every s ‚â§ 0 (Lebesgue measure), we have
limn‚Üí‚àû _Ht(x[(][n][)]) = 0._

_‚àû_
_For a sequence of functionals H, we define its norm by_ **_H_** := _Ht_ _dt._
_‚à•_ _‚à•_ 0 _‚à•_ _‚à•_
Z

**Remark 3.1. The definitions of linear and continuous functionals are standard. One can**
_view regular functionals as those not determined by inputs on arbitrarily small time intervals,_
_e.g. an infinitely thin spike (i.e. Œ¥-functions)._

Given the above definitions, we immediately have the following observation.

**Proposition 3.1. Let** **_H[ÀÜ]_** _be a sequence of functionals in the RNN encoder-decoder_
_‚àà_ [ÀÜ]H
_hypothesis space (see (5)). Then for any t_ 0, [ÀÜ]H _t_ **_H is a linear, continuous and regular_**
_‚â•_ _‚àà_ [ÀÜ]
_functional. Furthermore,_ _H_ _t_ _decays exponentially as a function of t._
_‚à•[ÀÜ]_ _‚à•_

The proof is found in Appendix A. This proposition characterises properties of the encoderdecoder hypothesis space. In particular, it is different from the RNN hypothesis space
discussed in Li et al. (2021), since the encoder-decoder is not necessarily time-homogeneous.
**Remark 3.2. A sequence of functionals H is time-homogeneous if for any t, œÑ** 0, Ht(x) =
_‚â•_
_Ht+œÑ_ (x(œÑ )), with x(œÑ )s = xs‚àíœÑ for all s ‚àà R. That is, if the input is shifted to the right by


-----

_œÑ_ _, the output is also shifted by œÑ_ _. Temporal convolution is an example of time-homogeneous_
_operation (recall the Gaussian convolution discussed in Section 3._ _An example of time-_
_inhomogeneous relationship is video captioning: shifts in the sequence of input video frames_
_do not necessarily lead to corresponding shifts in the caption text sequence._

**Relation with RNNs.** Here, we emphasise the differences between the encoder-decoder
hypothesis space and the RNN hypothesis space discussed in Li et al. (2021), where

## ÀÜH [(RNN)]t (x) = ‚àû0 c[‚ä§]e[W][ (][t][+][s][)]Ux‚àísds. A key difference is that the encoder-decoder has a
structure involving two temporal parameters t and s, while the RNN only has one depending on t + s, due to the time-homogeneity.R Owing to this difference and the fact that
## ÀÜ, the encoder-decoder hypothesis space (5) is more general, with the extra ca_H[(RNN)]_ _‚äÇ_ [ÀÜ]H
pability to learn time-inhomogeneous relationships. Furthermore, e[V t] and e[W s] adapt to a
temporal product structure, which is an intrinsic difference between encoder-decoders and
other architectures. We will discuss this in detail in the next section.

4 Approximation results

One of the most fundamental problems for supervised learning is the approximation problem. It basically concerns the capacity of the hypothesis space to fit the concept space. In
general, there are two levels of approximation problems that can be discussed. The first is
known as the universal approximation, which considers the density of the hypothesis space
in the concept space. The second is the approximation rate, which aims to characterise
quantitatively the approximation accuracy concerning the capacity/complexity of the hypothesis space (e.g. the number of trainable parameters). In this section, both of them are
developed for RNN encoder-decoders.

4.1 Universal approximation

We first present the most basic density result, which states that any linear, continuous,
and regular temporal relationship can be approximated by RNN encoder-decoders up to
arbitrary accuracy. The proof is found in Appendix B.
**Theorem 4.1. Let H be a sequence of linear, continuous and regular functionals defined**
_on_ _, and satisfy_ **_H_** _<_ _. Then for any œµ > 0, there exists_ **_H[ÀÜ]_** _such that_
_X_ _‚à•_ _‚à•_ _‚àû_ _‚àà_ [ÀÜ]H

_‚àû_
**_H_** **_H_** _Ht_ _H_ _t_ _dt < œµ._ (6)
_‚à•_ _‚àí_ [ÀÜ]‚à•‚â° 0 _‚à•_ _‚àí_ [ÀÜ] _‚à•_
Z

Here, we highlight two important observations while deriving Theorem 4.1. First, one
can show that each sequence of functionals H can be associated with a unique twoparameter ‚Äúrepresentation‚Äù œÅ(t, s), such that H ‚ààHt(x) = _‚àû0_ _x[‚ä§]‚àís[œÅ][(][t, s][)][ds][. Recall the model]_
forming representation. The functional approximation is then reduced to function approximation[ÀÜ]H _t(x) =_ _‚àû0_ _x[‚ä§]‚àísœÅ[ÀÜ](t, s)ds, where ÀÜœÅ(t, s) := [c[‚ä§]e[V t]PQeR_ _[W s]U_ ][‚ä§] denotes the correspondR
in the sense of representations, i.e. ‚à•H ‚àíH[ÀÜ]‚à•‚â§‚à•œÅ‚àíœÅÀÜ‚à•L1([0,‚àû)2). It turns out that œÅ directly
affects the rate of approximation and gives rise to intrinsic properties. We will discuss this
in detail in Section 4.3. In addition, we again emphasise the differences between the present
work and Li et al. (2021). In Li et al. (2021), the target relationships are assumed to be
time-homogeneous with the representation Ht(x) = _‚àû0_ _œÅ(t + s)x‚àísds, which only depends_
on t + _s. However, the setting here does not assume time-homogeneity, hence implies a more_
general representation œÅ depending on the two temporal directionsR _t and s simultaneously._

4.2 General approximation rates

While the density result (Theorem 4.1) ensures the universal approximation property of the
RNN encoder-decoder, it does not identify targets that can be efficiently approximated. To
achieve this, we focus on approximation rates next. We characterise the temporal structure
of a target relationship by observing its responses to ‚Äúconstant‚Äù input signals. Here, we
consider the approximation rates for the model with ‚Äúlarge size‚Äù coding vector, where the


-----

dimension N _m¬Ø_ := min _mE, mD_ . This is the scenario where we fix the widths but take
_‚â•_ _{_ _}_
an oversized coding vector.
**Theorem 4.2. Let H be a sequence of linear, continuous and regular functionals defined**
_on_ _, and satisfy_ **_H_** _<_ _. Consider the output of piece-wise constant signals yi[c][(][t, s][) =]_
_X_ _‚à•_ _‚à•_ _‚àû_
_Ht(ei1(‚àí‚àû,‚àís]), t, s ‚â•_ 0, i = 1, 2, . . ., d, where {ei}i[d]=1 _[denotes the standard basis of][ R][d][.]_
_Assume that there exist Œ± ‚àà_ N+, Œ≤ > 0 such that for any i = 1, 2, . . ., d,

_yi[c]_ (7)

_[‚àà]_ _[C]_ _[Œ±][+1][([0][,][ ‚àû][)][2][)][,]_

_e[Œ≤][(][t][+][s][)][ ‚àÇ][k][+][l]_ _i_ [(][t, s][) =][ o][(1)][ as][ ‚à•][(][t, s][)][‚à•‚Üí‚àû][,] (k, l) N N+, k + l _Œ± + 1._ (8)

_‚àÇt[k]‚àÇs[l][ y][c]_ _‚àà_ _√ó_ _‚â§_

_Then for any mE, mD, N_ N+, there exists **_H[ÀÜ]_** _mE_ _,mD,N such that_
_‚àà_ _‚àà_ [ÀÜ]H

1 1

**_H_** **_H_** + _,_ (9)
_‚à•_ _‚àí_ [ÀÜ]‚à•‚â§ _[C][(]Œ≤[Œ±][2][)][Œ≥d]_ _m[Œ±]E_ _m[Œ±]D_

 

_where C(Œ±), Œ≥ > 0 are both universal constants with dependence only on Œ± and (Œ±, Œ≤),_

_respectively, and Œ≥ :=_ _i‚ààNmax+, i‚â§d_ _k,l‚ààNmax, k+l‚â§Œ±+1_ _t,s[sup]‚â•0_ _Œ≤[‚àí][(][k][+][l][)]e[Œ≤][(][t][+][s][)]_ _‚àÇt[‚àÇ][k][k][+]‚àÇs[l]_ _[l][ y]i[c][(][t, s][)]_ _< ‚àû. Here,_

_the number of trainable parameters is dN_ (mE + mD) with N _m¬Ø_ _._
_‚â•_

The proof is found in Appendix C. First, note that the error bound does not depend on the
coding vector size N, as long as N _m¬Ø_ . This is because further increasing N beyond ¬Øm
_‚â•_
only increases the number of trainable parameters, but does not increase the model capacity
(see Remark C.2). Only the model widths mE, mD affect the approximation capabilities.

Next, we focus on the classes of target relationships that can be well approximated. Here,
_Œ± characterises the smoothness of H, and Œ≤ characterises the temporal decay rates of the_
output responding to a constant signal under H. This is a notion of memory in the target
relationship. The error bound (9) indicates that a sequence of target functionals can be
efficiently approximated by the encoder-decoder if it is smooth (large Œ±), and has fast
decayed memory (large Œ≤).

The characterisation in smoothness and memory decay also appears in the approximation results of RNNs (Li et al., 2021), where the upper bound is _[C]Œ≤m[(][Œ±][)][Œ±][Œ≥d][ . However, our]_

results for encoder-decoders suggest extra structures, where the bound involves two (instead of one) temporal parameters together with smoothness and decay memories in both.
The two-parameter temporal dependence allows the encoder-decoder to approximate timeinhomogeneous relationships, which generalises the RNN. This two-parameter structure
further leads to adaptation to a specific low rank type of target relationships, resulting in
finer approximation rates as we discuss next.

4.3 Approximation rates via temporal product structure

**Motivation of temporal product structure.** In contrast with Theorem 4.2, we next
consider the model with N < ¬Øm = min _mE, mD_ . In this situation, the model has fewer
_{_ _}_
parameters, and we aim to characterise the target relationships by further exploiting the
structure of the two-parameter representation œÅ(t, s). This leads to a finer approximation
rate by considering mE, mD, N together.

We first motivate how the ‚Äútemporal product structure‚Äù arises, and how it relates to the
approximation. Detailed discussions and proofs are found in Appendix D. For the illustration
purpose, we set the input dimension d = 1. Recall Q ‚àà R[N] _[√ó][m][E]_, P ‚àà R[m][D][√ó][N], then the
representation ÀÜœÅ of the encoder-decoder functional can be rewritten as

_N_ _mD_ _mE_

_œÅÀÜ(t, s) = c[‚ä§]e[V t]P_ _Qe[W s]u =_ _ciPjn_ _e[V t][]ij_ _uiQnj_ _e[W s][]ji_
_¬∑_ _n=1_ _i,j=1_ ! _i,j=1_ !

X X  X 

_N_

= _œïÀÜn(t)œÜ[ÀÜ]n(s)._ (10)

_n=1_

X


-----

This is a tensor product structure over the (t, s) time domain (determined by the encoder
_œÜn_ and decoder _œï ÀÜn_ successively). We call it the temporal product structure. As is
_{_ [ÀÜ] _}_ _{_ _}_
shown later, this structure significantly affects approximation rates. When _œÜn_ and _œï ÀÜn_
_{_ [ÀÜ] _}_ _{_ _}_
are selected as the ‚Äúbases‚Äù along s, t direction, respectively, N is considered as the rank of
the temporal product. We also define N as the rank of the model, which is understood as
the maximum rank of temporal products that the encoder-decoder model can represent.

**The rank concept of temporal relationships.** Recall that the given number of trainable parameters is dN (mE + mD). Hence, a low rank model may achieve fewer trainable
parameters. When investigating relationships that can be well approximated by low rank
models, a natural conjecture would be ‚Äúlow rank‚Äù targets.

What is the meaning of ‚Äúlow rank‚Äù for a temporal relationship? It is well-known that in linear
algebra, an operator is low rank means that its range space is low-dimensional. This idea
can be also applied to temporal relationships. For a ‚Äúlow rank‚Äù temporal relationship, the
output sequence is more ‚Äúregular‚Äù, meaning that the output sequences (viewed as functions)
are in a low-dimensional function space. We provide an intuitive numerical illustration for
better understanding.

|Col1|Col2|
|---|---|
|||
|||
|||
|||
|||

|Col1|Col2|
|---|---|
|||
|||
|||
|||
|||
|||
|||

|Col1|Col2|
|---|---|
|||
|||
|||
|||
|||

|Col1|Col2|
|---|---|
|||
|||
|||
|||
|||
|||


_xt_ _Ht(x)_

2.0

2 1.5

1.0

1

0.5

0 0.0

0.5

1 _‚àí_
_‚àí_ _‚àí1.0_

_‚àí2_ _‚àí1.5_

_‚àí3_ 0 5 10 15 20 25 30 _‚àí2.0_ 0 5 10 15 20 25 30

_t_ _t_


_xt_ _Ht(x)_

40

2

30

1

20

0

10

_‚àí1_ 0

_‚àí2_ _‚àí10_

3
_‚àí_ 0 5 10 15 20 25 30 0 5 10 15 20 25 30

_t_ _t_


(a) high rank relationship


(b) low rank relationship


Figure 1: We construct a high rank and a low rank target from the temporal product. For
both (1a) and (1b), we plot the inputs xt together with the corresponding outputs Ht(x).
Detailed settings are found in Appendix E.1.

Figure 1 shows the outputs of a high rank (a) and a low rank (b) target relationship on
the same set of random input sequences. Different colours refer to different instances of
inputs. In the first case (high rank), the temporal structure of the outputs is very complex
and depends sensitively on the inputs. However, in the second case (low rank), the output
sequences are much more regular, and only macroscopic structures (e.g. scale/offset) appear.

**Remark 4.1. In the research of approximation theories for temporal sequences, prior works**
_also related a notion of rank to approximation properties of the dilated convolutional structure_
_(Jiang et al., 2021). Here, we emphasise that the notion of rank considered in our work is_
_very different from that in Jiang et al. (2021), which mainly concerns the tensorisation of_
_a discrete-time sequence according to the width of convolution filters._

**POD as an analogue of SVD.** Now, we characterise low rank and high rank temporal
relationships in a mathematical way. We will introduce the concepts informally, and rigorous
definitions and arguments can be found in Appendix D.

For a matrix, we can assess its rank by performing the singular value decomposition
(SVD). This method can be extended to the temporal relationships using proper orthogonal decomposition (POD; Liang et al. (2002), Berkooz et al. (1993), Chatterjee (2000)).
The basic insight is that the function œÅ can be decomposed into the following form:
_œÅ(t, s) =_ _n=1_ _[œÉ][n][œï][n][(][t][)][œÜ][n][(][s][), where][ N][0][ ‚â§‚àû][,][ {][œï][n][}][ and][ {][œÜ][n][}][ are orthonormal bases, and]_
ing SVD to an infinite-dimensional space (whenœÉ1 ‚â• _œÉ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•_ 0 denote the singular values. This procedure can be viewed as apply- N0 = ). An analogue of Eckart‚ÄìYoung

[P][N][0] _‚àû_
theorem (Eckart & Young, 1936), which characterises the best low rank approximation,
also holds for POD. It roughly states that inf rank(ÀÜœÅ)=N _[‚à•][œÅ][ ‚àí]_ _œÅ[ÀÜ]_ _L[2][ =][ P]n[N]=[0]_ _N_ +1 _[œÉ]n[2]_ [. That is,]
_‚à•[2]_


-----

any target œÅ has a rank-N best approximation, with error equalling to the tail sum of the
squared singular values. In other words, a target with fast decayed œÉn (low ‚Äúeffective rank‚Äù)
has smaller approximation errors. This forms the basis of our next result, which states that
if the target relationship possesses an effective low rank structure in terms of the decay
of singular values, then one can achieve an efficient approximation using encoder-decoder
structures by limiting the size of coding vectors. Detailed definitions for _œÉn_ and proofs
_{_ _}_
are found in Appendix D.

**Theorem 4.3. Assume the same conditions as in Theorem 4.2. Then for any mE, mD, N**
_‚àà_
N+ with N _m¬Ø_ _, there exists_ **_H[ÀÜ]_** _mE_ _,mD,N such that_
_‚â§_ _‚àà_ [ÀÜ]H

1 1 _m¬Ø_ 1/2

**_H_** **_H_** ‚â≤ _[C][(][Œ±][)][Œ≥d]_ 1 + _‚àöm¬Ø_ _N_ + + _œÉn[2]_
_‚à•_ _‚àí_ [ÀÜ]‚à• _Œ≤[2]_ (  _‚àí_  _¬∑_  _m[Œ±]E_ _m[Œ±]D_  _n=XN_ +1 !

_m¬Ø_ 1/2 1 1

+ _œÉn_ + _,_ (11)

_n=N_ +1 ! _¬∑_ _m[Œ±/]E_ [2] _m[Œ±/]D_ [2] ! )

X

_where ‚â≤_ _hides universal positive constants, and ¬Øm = min{mE, mD}. Here, the number of_
_trainable parameters is dN_ (mE + mD) with N _m¬Ø_ _._
_‚â§_

This is a finer approximation rate compared to Theorem 4.2, where both the widths mE, mD
and the coding vector size N affect the model capacity for approximation. Besides the
smoothness and memory decay, we have the additional rank structure of the target relationship, which is characterised by its singular values _œÉn_ . We again focus on the class of
_{_ _}_
functionals that can be well approximated. Smoothness Œ± and decay rate Œ≤ is the same as
Theorem 4.2. The difference lies in the rank structure indicated by _œÉn_ : the error bound is
_m_ _{_ _}_
small if _œÉn_ has a small tail _n=N_ +1 _[œÉ]n[2]_ [. It suggests that a target with fast decayed][ {][œÉ][n][}]
_{_ _}_
or low ‚Äúeffective rank‚Äù can be well approximated by the RNN encoder-decoder with fewer
parameters. Due to the Eckart‚ÄìYoung-like low rank approximation, we can appropriately

[P][ ¬Ø]
select N based on the decay rate of singular values.

Here, we emphasise that the temporal product is an intrinsic structure arising from the
encoder-decoder architecture. Recall the dynamics of the encoder-decoder: it first encodes
the input sequence into a coding vector, and then decodes an entire output sequence from
it. In this sense, the coding vector is the only interaction between the input and output.
Thus, the coding vector size N is an essential measure of the model capacity concerning
the dependence of outputs on inputs. Here, we show that this concept can be formalised
as a notion of rank, which can pinpoint the precise types of input-output relationships that
encoder-decoder architectures are well adapted to.

**Numerical illustrations.** Here, we utilise numerical examples to illustrate the above
discussions. We observe how the decay rate of singular values, the rank N0 of the target
relationships, and the model rank N affect the approximation error **_H_** **_H_** . However,
_‚à•_ _‚àí_ [ÀÜ]‚à•
it is not always possible to construct the best approximation. Instead, we perform some
training steps to achieve an upper bound of the approximation error, which is consistent
with our theoretical results.

In Figure 2, we train linear encoder-decoder models to learn three relationships of different
ranks determined by various decay patterns of singular values, given in (a), (b) and (c).
Different colours denote targets with different ranks. From Figure 2, we have the following
observations consistent with previous discussions. First, observe that increasing the model
rank N makes approximation errors smaller, as expected. Moreover, note that when increasing N, the speeds of error decrements are different. If the singular values decay fast, the
approximation errors also decay fast. This implies that a target with fast decayed singular
values can be approximated efficiently with fewer parameters (smaller N ). In addition, for
each experiment, we are able to achieve low approximation errors by choosing N _m. The_
_‚â™_
errors will remain unchanged or decrease much more slowly when further increasing N . This
suggests that in practice, one can choose N such that it covers the major singular values of
the target in order to improve the approximation efficiency.


-----

_√ó10[‚àí][3]_


2.8

2.3

1.8

1.3

0.8

0.0


1.1

0.9

0.7

0.5

0.3

0.0


1.0

0.8

0.6

0.4

0.2

0.0

|√ó10‚àí2|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||Target|Rank N0 = 2|
||||||N0 = 4 N0 = 6|
||||||N0 = 8|
|||||||
|||||||
|||||||

|√ó10‚àí3|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||Target|Rank N0 = 2|
||||||N0 = 4 N0 = 6|
||||||N0 = 8|
|||||||
|||||||
|||||||

|‚àí3|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||Targe|t Rank|
|||||N0 = ‚àû|
||||||
||||||
||||||
||||||


2 4 6 8 10
_N_

_n[‚àí]_ 8[1], _n_ _N0_
(a) œÉn = _‚â§_

0, _n > N0_




10


10 20 30

_N_

(c) œÉn = n[‚àí][2]


_n[‚àí][1],_ _n_ _N0_
(b) œÉn = _‚â§_
0, _n > N0_



Figure 2: In (a), (b), (c) we consider target relationships with different singular values
indicated in the respective caption. For (a), (b) we also consider targets with different rank,
where N0 = 2, 4, 6, 8. We use models with fixed width m = mE = mD = 128 and coding
vector size N . Detailed settings are found in Appendix E.2.

In Figure 3, we perform experiments on the forced Lorentz 96 system (Lorenz, 1996), which
parameterises a high-dimensional and nonlinear relationship between input forcing and
model states. The parameters K, J in the Lorenz 96 system control the overall complexity
of the target (see Appendix E.3 for details). We use the RNN encoder-decoder with tanh
activations to learn this target. Although our theories are developed in the linear regime,
the low rank approximation phenomenon also appears in this nonlinear setting. The error
decrements saturate when increasing the coding vector size N beyond a threshold, suggesting the existence of some implicit notion of ‚Äúrank‚Äù of the target nonlinear functional. This
‚Äúrank‚Äù increases with the target complexity (mainly K).


_√ó10[‚àí][4]_


|Col1|Col2|Ta|Col4|rget Parameters|Col6|
|---|---|---|---|---|---|
|||||K = 1, J = 6 K = 5, J = 6||
|||||||
|||||||
|||||K = 10, J = K = 20, J =|6 6|
|||||||
|||||||
|||||||
|||||||
|||||||

|√ó10‚àí4|Col2|Col3|Col4|
|---|---|---|---|
||Targ||et Parameters|
||||K = 5, J = 5 K = 5, J = 15 K = 5, J = 25|
||||K = 5, J = 100|
|||||
|||||
|||||


10 20 30


10 20 30


Figure 3: K, J are the parameters of the Lorenz 96 system. They describe the number
of independent and coupled variables in the system, which can be viewed as a complexity
measure. Detailed settings are found in Appendix E.3.

5 Conclusion


We theoretically study the approximation properties of the RNN encoder-decoder in a linear setting. We prove a universal approximation result for linear temporal relationships
utilising encoder-decoder architectures, and show that they generalise RNNs to the timeinhomogeneous setting. Moreover, we discover an important temporal product structure
that characterises the types of input-output relationships especially suited for the efficient
approximation using encoder-decoders. This elucidates the key differences between these
novel architectures and classical methods for temporal modelling, and forms a basic step
towards understanding the intricacies of modern deep learning.


-----

**Reproducibility statements.** Detailed proofs for theoretical results, and complete settings of numerical examples are found in the appendix. The source code for numerical tests
can be made available upon request.

Here is a quick reference:

Proposition 3.1 Properties of RNN encoder-decoder functionals Appendix A
Theorem 4.1 Universal approximation theorem Appendix B
Theorem 4.2 General approximation rates Appendix C
Theorem 4.3 Approximation rates concerning temporal product structure Appendix D
Figure 1 Illustration of high/low rank temporal relationships Appendix E.1
Figure 2 Numerical examples on singular values Appendix E.2
Figure 3 Numerical examples on Lorenz 96 systems Appendix E.3

Acknowledgements

ZL is supported by Peking University under BICMR mathematical scholarship. HJ is supported by National University of Singapore under PGF scholarship. QL is supported by the
National Research Foundation, Singapore, under the NRF fellowship (NRF-NRFF13-20210005).


-----

References

Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. Neural machine translation by
jointly learning to align and translate. In International Conference on Learning Repre_sentations, pp. 1‚Äì15, 2015._

Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

G. Berkooz, P. Holmes, and J. L. Lumley. The proper orthogonal decomposition in the
analysis of turbulent flows. _Annual Review of Fluid Mechanics, 25(1):539‚Äì575, 1993._
doi: 10.1146/annurev.fl.25.010193.002543. [URL https://doi.org/10.1146/annurev.](https://doi.org/10.1146/annurev.fl.25.010193.002543)
```
 fl.25.010193.002543.

```
Vladimir I. Bogachev. Measure theory, volume 1. Springer Science & Business Media, 2007.

Ching-Hua Chang and Chung-Wei Ha. On eigenvalues of differentiable positive definite
kernels. Integral Equations and Operator Theory, 33:1‚Äì7, 1999. doi: 10.1007/BF01203078.

Anindya Chatterjee. An introduction to the proper orthogonal decomposition. Current
_[Science, 78(7):808‚Äì817, 2000. ISSN 00113891. URL http://www.jstor.org/stable/](http://www.jstor.org/stable/24103957)_
```
 24103957.

```
Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen,
Z. Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Katya Gonina, Navdeep Jaitly,
Bo Li, Jan Chorowski, and Michiel Bacchiani. State-of-the-art speech recognition with
sequence-to-sequence models. In IEEE International Conference on Acoustics, Speech and
_Signal Processing, pp. 4774‚Äì4778, 2018._

Kyunghyun Cho, Bart van Merri¬®enboer, Dzmitry Bahdanau, and Yoshua Bengio. On
the properties of neural machine translation: Encoder‚Äìdecoder approaches. In Eighth
_Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103‚Äì111._
Association for Computational Linguistics, 2014a. doi: 10.3115/v1/W14-4012. URL
```
 https://aclanthology.org/W14-4012.

```
Kyunghyun Cho, Bart Van Merri¬®enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using
RNN encoder-decoder for statistical machine translation. Conference on Empirical Meth_ods in Natural Language Processing, pp. 1724‚Äì1734, 2014b. doi: 10.3115/v1/d14-1179._

Kenji Doya. Universality of fully-connected recurrent neural networks. IEEE Transactions
_on Neural Networks, 1993._

Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank.
_Psychometrika, 1(3):211‚Äì218, 1936._

Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179‚Äì211, 1990.

Ken-ichi Funahashi and Yuichi Nakamura. Approximation of dynamical systems by continuous time recurrent neural networks. Neural Networks, 6(6):801 ‚Äì 806, 1993. ISSN
0893-6080.

Sepp Hochreiter and J¬®urgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735‚Äì1780, 1997.

Haotian Jiang, Zhong Li, and Qianxiao Li. Approximation theory of convolutional architectures for time series modelling. In Proceedings of the 38th International Conference on
_Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 4961‚Äì_
[4970. PMLR, 2021. URL https://proceedings.mlr.press/v139/jiang21d.html.](https://proceedings.mlr.press/v139/jiang21d.html)

Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In Confer_ence on Empirical Methods in Natural Language Processing, pp. 1700‚Äì1709, 2013. ISBN_
9781937284978.


-----

Peter D. Lax. Functional Analysis. John Wiley & Sons, Inc., 2002.

Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis
with transformer network. In AAAI Conference on Artificial Intelligence, volume 33, pp.
6706‚Äì6713, 2019.

Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. On the curse of memory in recurrent neural networks: Approximation and optimization analysis. In International Conference on
_[Learning Representations, 2021. URL https://openreview.net/forum?id=8Sqhl-nF50.](https://openreview.net/forum?id=8Sqhl-nF50)_

Y. C. Liang, H. P. Lee, S. P. Lim, W. Z. Lin, K. H. Lee, and C. G. Wu. Proper orthogonal
decomposition and its applications‚ÄîPart I: Theory. Journal of Sound and Vibration, 252
(3):527‚Äì544, 2002. ISSN 0022-460X. doi: https://doi.org/10.1006/jsvi.2001.4041. URL
```
 https://www.sciencedirect.com/science/article/pii/S0022460X01940416.

```
G. G. Lorentz. Approximation of Functions. AMS Chelsea Publishing Series. Holt, Rinehart
[and Winston, 2005. ISBN 9780821840504. URL https://books.google.com.sg/books?](https://books.google.com.sg/books?id=8VMrOmTKSe0C)
```
 id=8VMrOmTKSe0C.

```
Edward N. Lorenz. Predictability: A problem partly solved. In Proc. Seminar on Pre_dictability, volume 1, pp. 40‚Äì58, 1996._

Wolfgang Maass, Prashant Joshi, and Eduardo D. Sontag. Computational aspects of feedback in neural circuits. PLOS Computational Biology, 3(1):e165, 2007.

Charles Bradfield Morrey. Multiple Integrals in the Calculus of Variations. Springer-Verlag,
1966.

Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine
_Learning, pp. 4055‚Äì4064. PMLR, 2018._

Walter Rudin. Real and Complex Analysis. Higher Mathematics Series. McGraw-Hill Edu[cation, 1987. ISBN 9780070542341. URL https://books.google.com.sg/books?id=Z_](https://books.google.com.sg/books?id=Z_fuAAAAMAAJ)
```
 fuAAAAMAAJ.

```
Anton Maximilian Sch¬®afer and Hans-Georg Zimmermann. Recurrent neural networks are
universal approximators. International Journal of Neural Systems, 17(4):253‚Äì263, 2007.

Martin H. Schultz. L[‚àû]-multivariate approximation theory. SIAM Journal on Numerical
_[Analysis, 6(2):161‚Äì183, 1969. doi: 10.1137/0706017. URL https://doi.org/10.1137/](https://doi.org/10.1137/0706017)_
```
 0706017.

```
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural
networks. In Advances in Neural Information Processing Systems, volume 4, pp. 3104‚Äì
3112, 2014.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances
_in Neural Information Processing Systems, pp. 5999‚Äì6009, 2017._

Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor
Darrell, and Kate Saenko. Sequence to sequence-video to text. In Proceedings of the
_IEEE International Conference on Computer Vision, pp. 4534‚Äì4542, 2015._

Jong Chul Ye and Woon Kyoung Sung. Understanding geometry of encoder-decoder
CNNs. In International Conference on Machine Learning, pp. 12245‚Äì12254, 2019. ISBN
9781510886988.

Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar.
Are transformers universal approximators of sequence-to-sequence functions? In Interna_[tional Conference on Learning Representations, 2020. URL https://openreview.net/](https://openreview.net/forum?id=ByxRM0Ntvr)_
```
 forum?id=ByxRM0Ntvr.

```

-----

A Properties of model functionals

In this section, we prove observations of the hypothesis space reported in Proposition 3.1.

_Proof of Proposition 3.1. Recall that_

_‚àû_
## ÀÜH t(x; Œ∏) = 0 c[‚ä§]e[V t]Me[W s]Ux‚àísds. (12)
Z

Fix any Œ∏ = (W, V, U, M, c). The linearity is obvious. Since both W _mE and V_ _mD_
_‚ààW_ _‚ààV_
have eigenvalues with negative real parts, there exist c1, c2, c[‚Ä≤]1[, c][‚Ä≤]2 _[>][ 0, such that][ ‚à•][e][V t][‚à•][‚àû]_ _[‚â§]_
_c1e[‚àí][c][2][t]_ and _e[W s]_ _c[‚Ä≤]1[e][‚àí][c]2[‚Ä≤]_ _[s]_ for any t, s 0, hence
_‚à•_ _‚à•‚àû_ _‚â§_ _‚â•_

_|c[‚ä§]e[V t]Me[W s]Ux‚àís| ‚â§‚à•c‚à•1‚à•e[V t]Me[W s]Ux‚àís‚à•‚àû_
_‚â§‚à•c‚à•1‚à•e[V t]‚à•‚àû‚à•M_ _‚à•‚àû‚à•e[W s]‚à•‚àû‚à•U_ _‚à•‚àû‚à•x‚àís‚à•‚àû_

‚â≤ _e[‚àí][c][2][t]e[‚àí][c]2[‚Ä≤]_ _[s]_ **_x_** _,_ (13)
_‚à•_ _‚à•X_

where ‚â≤ hides universal positive constants depending on parameters Œ∏. Therefore

_H_ _t(x; Œ∏)_ ‚â≤ _e[‚àí][c][2][t]_ **_x_** _H_ _t(_ ; Œ∏) ‚â≤ _e[‚àí][c][2][t]._ (14)
_|[ÀÜ]_ _|_ _‚à•_ _‚à•X ‚áí‚à•[ÀÜ]_ _¬∑_ _‚à•_

That is, the functional [ÀÜ]H _t(_ ; Œ∏) is bounded (i.e. continuous) with an exponentially-decayed

_¬∑_
norm (as a function of t). Finally, by (13) and Lebesgue‚Äôs dominated convergence theorem,
the functional [ÀÜ]H _t(_ ; Œ∏) is also regular. The proof is completed.

_¬∑_

B Universal approximation

In this section, we provide the proof of Theorem 4.1, i.e. the universal approximation
property of RNN encoder-decoders. As is stated in the main text, the key step is to utilise
the classical representation theorem, which helps us to reduce the approximation problem
of functionals to functions.

B.1 Preliminaries

First, we list the background definitions and notations used in the following theorems. Let
(X, ) be a measure space (with as the œÉ-algebra of subsets of X).
_A_ _A_

-  The space X is called locally compact if for any x _X, x has a compact neighbour-_
_‚àà_
hood.

-  X is a Hausdorff space if all distinct points in X are pair-wisely separable by neighbourhoods. That is, for any x, y _X, there exists a neighbourhood ‚àÜx of x and a_
_‚àà_
neighbourhood ‚àÜy of y, such that ‚àÜx ‚àÜy = ‚àÖ.
_‚à©_

-  The measure ¬µ is called a finite measure, if it satisfies ¬µ(X) < . The measure
_‚àû_
_ŒΩ is called a œÉ-finite measure, if X can be covered with at most countably many_
measurable sets with finite measure. That is, there are measurable sets with ŒΩ(An) < for all n N+, such that _n=1_ _[A][n][ =][ X][. Obviously, a finite] {An}n[‚àû]=1_ _[‚äÇ]_
_A_ _‚àû_ _‚àà_
measure is also œÉ-finite.

-  Let X = ( _, 0]. For a measure ¬µ on the measure space (([S][‚àû]_ _, 0],_ ), ¬µ is absolutely
_‚àí‚àû_ _‚àí‚àû_ _A_
_continuous with respect to the Lebesgue measure ŒΩ, if for every measurable set A,_
_ŒΩ(A) = 0 implies ¬µ(A) = 0, which is written as ¬µ_ _ŒΩ._
_‚â™_

Denote by C0(X) the linear space of continuous functions defined on X vanishing at infinity.
We have the following classical representation theorem.
**Theorem B.1 (Riesz-Markov-Kakutani representation theorem). Let X be a locally com-**
_pact Hausdorff space. For any continuous linear functional œà on C0(X), there is a unique,_
_regular, countably additive and signed measure ¬µ on X, such that_


_œà(f_ ) =


_f_ (x)d¬µ(x), _‚àÄf ‚àà_ _C0(X),_ (15)


-----

_with_ _œà_ = _¬µ_ (X). Here, _¬µ_ (X) denotes the total variation of (the signed measure) ¬µ,
_which is defined as ‚à•_ _‚à•_ _|_ _|_ _¬µ_ (X) := sup | _|_ _ki=1_ _i=1_ _[A][i][ is a partition over]_
_X with Ai_ _for all |_ _| i = 1, 2,_ _P, k._ _[|][¬µ][(][A][i][)][|][, where][ P][ :][ X][ =][ S][k]_
_‚ààA_ _¬∑ ¬∑ ¬∑_ P

_Proof. Well-known, see e.g. Bogachev (2007) (CH 7.10.4)._

**Remark B.1. It is straightforward to verify that |¬µ|(X) = supA‚ààA(|¬µ(A)| + |¬µ(A[c])|). Fur-**
_thermore, if ¬µ has a density d¬µ/dŒΩ with respect to a countably additive, nonnegative measure_
_ŒΩ, then we have |¬µ|(X) = ‚à•d¬µ/dŒΩ‚à•L1(ŒΩ)._

To handle signed measures, the following Jordan decomposition theorem (Bogachev, 2007)
is necessary.

**Theorem B.2. Let ¬µ be a signed measure on the measure space (X,** ). Then, there are two
_A_
_mutually singular (non-negative) measures ¬µ[+]_ _and ¬µ[‚àí]_ _on (X,_ ), such that ¬µ = ¬µ[+] _¬µ[‚àí]._
_A_ _‚àí_
_Moreover, such a pair (¬µ[+], ¬µ[‚àí]) is unique._

Based on this, we have the following proposition to characterise absolutely continuous signed
measures.

**Proposition B.1. If ¬µ and ŒΩ are signed measures, then we have ¬µ** _ŒΩ_ _¬µ[+]_ _ŒΩ and_
_‚â™_ _‚áî_ _‚â™_
_¬µ[‚àí]_ _ŒΩ._
_‚â™_

We also need the following Radon-Nikodym theorem (Bogachev, 2007).

**Theorem B.3. Let (X,** _, ŒΩ) be a œÉ-finite measure space, and let ¬µ be a œÉ-finite signed_
_A_
_measure, such that ¬µ_ _ŒΩ. Then there exists a unique measurable function f_ _, such that_
_‚â™_
_¬µ(A) =_ _A_ _[fdŒΩ][ for every measurable set][ A][.]_
R

B.2 Proofs

Before we prove the universal approximation theorem (Theorem 4.1), we need some lemmas.

**Lemma B.1. Let** _Ht : t_ 0 _be a family of linear, continuous and regular functionals_
_{_ _‚â•_ _}_
_defined on X_ _. Then there exists a integrable function œÅ : [0, ‚àû)[2]_ _‚Üí_ R[d], i.e.


_œÅ_ _L1([0,_ )2) :=
_‚à•_ _‚à•_ _‚àû_


_œÅi_ _L1([0,_ )2) < _,_ (16)
_‚à•_ _‚à•_ _‚àû_ _‚àû_
_i=1_

X


_such that_

_‚àû_
_Ht(x) =_ _x[‚ä§]s[œÅ][(][t, s][)][ds,]_ **_x_** _._ (17)

0 _‚àí_ _‚àÄ_ _‚ààX_

Z

_In particular, we have ‚à•H‚à•_ = _‚àû0_ _‚à•Ht‚à•dt = ‚à•œÅ‚à•L1([0,‚àû)2)._
R

_Proof. Obviously, (_ _, 0] is a locally compact Hausdorff space._ For any t 0, since
_‚àí‚àû_ _‚â•_
_Ht is linear continuous, according to the Riesz-Markov-Kakutani representation theorem_
(Theorem B.1), there exists a unique, regular, countably additive and signed measure ¬µt,
such that

0
_Ht(x) =_ _x[‚ä§]s_ _[d¬µ][t][(][s][)][,]_ _‚àÄx ‚ààX_ _,_ (18)
Z‚àí‚àû

absolutely continuous with respect towith _i=1_ _[|][¬µ][t,i][|][((][‚àí‚àû][,][ 0]) =][ ‚à•][H][t][‚à•][.]_ We show that for any ŒΩ (the Lebesgue measure), i.e. t ‚â• 0, i ¬µ = 1t,i _, 2, ¬∑ ¬∑ ¬∑ŒΩ. According, d, ¬µt,i is_
to Theorem B.2 and Proposition B.1, one can assume ¬µt,i to be non-negative without loss ‚â™

[P][d]
of generality. Take a measurable set A ( _, 0] with ŒΩ(A) = 0, the aim is to show_
_‚äÇ_ _‚àí‚àû_
_¬µt,i(A) = 0._ Let A[‚Ä≤] = ( _, 0]_ _A._ Since both A and A[‚Ä≤] are measurable, there exist
_‚àí‚àû_ _\_
_Kn ‚äÇ_ _A, Kn[‚Ä≤]_ _[‚äÇ]_ _[A][‚Ä≤][ with][ K][n][, K]n[‚Ä≤]_ [closed, such that][ ¬µ][t,i][(][A][ \][ K][n][)][ ‚â§] [1][/n][,][ ¬µ][t,i][(][A][‚Ä≤][ \][ K]n[‚Ä≤] [)][ ‚â§] [1][/n]


-----

and ŒΩ(A[‚Ä≤] _Kn[‚Ä≤]_ [)][ ‚â§] [1][/n][ for any][ n][ ‚àà] [N][+][. Fix any][ i][ ‚àà{][1][,][ 2][, . . ., d][}][, we construct the sequence]
_\_
of input signals **_x[(][n][)]_** _n=1_ [as]
_{_ _}[‚àû]_

0, _s_ 0, j = i,
_‚â§_ _Ã∏_

_x[(]s,j[n][)]_ [=] 0, _s_ _Kn[‚Ä≤]_ _[, j][ =][ i,]_ _j = 1, 2, . . ., d,_ (19)

Ô£± _‚àà_
Ô£≤1, _s_ _Kn, j = i,_

_‚àà_

which can then be continuously extended to (Ô£≥ _‚àí‚àû, 0] by defining x[(]s,i[n][)]_ [:=] _d(s,Kdn(s,K)+dn[‚Ä≤](s,K[)]_ _n[‚Ä≤]_ [)][ ‚àà]

[0, 1]. [1]

We deduce that limn‚Üí‚àû _x[(]s,i[n][)]_ [= 0 for][ ŒΩ][-a.e.][ s][ ‚â§] [0. In fact, let][ S][ :=][ {][s][ ‚â§] [0 : lim][n][‚Üí‚àû] _[x]s,i[(][n][)]_ [=]

0}, we have Kn[‚Ä≤] _[‚äÇ]_ _[S][ since for any][ s][ ‚àà]_ _[K]n[‚Ä≤]_ [,][ x][(]s,i[n][)] [= 0. Hence, (][‚àí‚àû][,][ 0]][ \][ S][ ‚äÇ] _[A][ ‚à™]_ [(][A][‚Ä≤][ \][ K]n[‚Ä≤] [),]
which gives ŒΩ(( _, 0]_ _S)_ _ŒΩ(A)+_ _ŒΩ(A[‚Ä≤]_ _Kn[‚Ä≤]_ [)][ ‚â§] [1][/n][ ‚Üí] [0 as][ n][ ‚Üí‚àû][. Due to the regularity]
_‚àí‚àû_ _\_ _‚â§_ _\_
of Ht, we get limn‚Üí‚àû _Ht(x[(][n][)]) = 0. By (18) and (19), we have_

_d_ 0 0

_Ht(x[(][n][)]) =_ _x[(]s,j[n][)][d¬µ][t,j][(][s][) =]_ _x[(]s,i[n][)][d¬µ][t,i][(][s][)]_

_j=1_ Z‚àí‚àû Z‚àí‚àû

X

= + + _x[(]s,i[n][)][d¬µ][t,i][(][s][) =][ ¬µ][t,i][(][K][n][) +][ I][1][,n][ +][ I][2][,n][,]_ (20)

_Kn_ _A_ _Kn_ _A[‚Ä≤]_ _Kn[‚Ä≤]_

Z Z _\_ Z _\_

where ¬µt,i(Kn) = _¬µt,i(A) ‚àí_ _¬µt,i(A \ Kn)_ _‚àà_ [¬µt,i(A) ‚àí 1/n, ¬µt,i(A)], and |I1,n| +
_|I2,n| ‚â§_ _A\Kn_ [+] _A[‚Ä≤]\Kn[‚Ä≤]_ [1][d¬µ][t,i][(][s][) =][ ¬µ][t,i][(][A][ \][ K][n][) +][ ¬µ][t,i][(][A][‚Ä≤][ \][ K]n[‚Ä≤] [)][ ‚â§] [2][/n][, which gives]

limn‚Üí‚àû _HRt(x[(][n][)]) =R ¬µt,i(A). Therefore, ¬µt,i(A) = 0._

Notice that _¬µt,i((_ _, 0])_ _¬µt,i_ (( _, 0])_ _Ht_ _<_ for a.e. t 0 (since **_H_** =
_|_ _‚àí‚àû_ _| ‚â§|_ _|_ _‚àí‚àû_ _‚â§‚à•_ _‚à•_ _‚àû_ _‚â•_ _‚à•_ _‚à•_
_‚àû_
0
Obviously, (([‚à•][H][t][‚à•][dt <][ ‚àû][), we get that ((], 0], _, ŒΩ) is a œÉ[‚àí‚àû]-finite measure space. According to the Radon-Nikodym[,][ 0]][,][ A][, ¬µ][t,i][) is a finite measure space, and hence][ œÉ][-finite.]_
R _‚àí‚àû_ _A_
theorem (Theorem B.3), there exists a unique measurable function œÅt,i : ( _, 0]_ R, such
_‚àí‚àû_ _‚Üí_
that ¬µt,i(A) = _A_ _[œÅ][t,i][(][s][)][dŒΩ][(][s][) for every measurable set][ A][. Hence, we have]_

0

R _‚àû_

_Ht(x) =_ _x[‚ä§]s_ _[œÅ][t][(][s][)][ds][ =]_ _x[‚ä§]s[œÅ][(][t, s][)][ds,]_ **_x_** _,_ (21)
Z‚àí‚àû Z0 _‚àí_ _‚àÄ_ _‚ààX_

with œÅ(t, s) := _œÅt(_ _s)._ In addition, by Remark B.1, we have _¬µt,i_ (( _, 0])_ =
0 _‚àí_ _|_ _|_ _‚àí‚àû_
_‚àí‚àû_ _[|][œÅ][t,i][(][s][)][|][ds][, which gives]_
R _d_ _d_

_‚àû_ _‚àû_
**_H_** = _Ht_ _dt =_ _¬µt,i_ (( _, 0])dt =_ _œÅi_ _L1([0,_ )2) = _œÅ_ _L1([0,_ )2). (22)
_‚à•_ _‚à•_ Z0 _‚à•_ _‚à•_ _i=1_ Z0 _|_ _|_ _‚àí‚àû_ _i=1_ _‚à•_ _‚à•_ _‚àû_ _‚à•_ _‚à•_ _‚àû_

X X

The proof is completed.

Based on this representation theorem, the problem of functional approximation is reduced
as function approximation. That is,

_‚àû_ _‚àû_
**_H_** **_H_** = _Ht_ _H_ _t_ _dt =_ sup _Ht(x)_ ÀÜH _t(x; Œ∏)_ _dt_
_‚à•_ _‚àí_ [ÀÜ]‚à• 0 _‚à•_ _‚àí_ [ÀÜ] _‚à•_ 0 **_x_** 1 _‚àí_
Z Z _‚à•_ _‚à•X ‚â§_

_‚àû_ _‚àû_
= sup _x[‚ä§]s[(][œÅ][(][t, s][)][ ‚àí]_ _œÅ[ÀÜ](t, s))ds_

0 **_x_** 1 0 _‚àí_

Z _‚à•_ _‚à•X ‚â§_ Z

_‚àû_ _‚àû_
_‚â§_ 0 **_xsup_** 1 0 _‚à•x‚àís‚à•‚àû‚à•œÅ(t, s) ‚àí_ _œÅÀÜ(t, s)[dt]‚à•1dsdt_
Z _‚à•_ _‚à•X ‚â§_ Z

_d_ _‚àû_ _‚àû_

_œÅi(t, s)_ _œÅÀÜi(t, s)_ _dsdt,_ (23)

_‚â§_ _i=1_ Z0 Z0 _|_ _‚àí_ _|_

X

1
Here, d(s, B) := inf _s_ _a_ : a _B_ is the distance between a point s and a set B.
_{|_ _‚àí_ _|_ _‚àà_ _}_


-----

i.e.


**_H_** **_H_** _œÅ_ _œÅÀÜ_ _L1([0,_ )2) :=
_‚à•_ _‚àí_ [ÀÜ]‚à•‚â§‚à• _‚àí_ _‚à•_ _‚àû_


_i=1_ _‚à•œÅi ‚àí_ _œÅÀÜi‚à•L1([0,‚àû)2)._ (24)

X


**Lemma B.2. Let œÅ(t, s) : [0,** )[2] R with _œÅ_ _L1([0,_ )2) < _. Then for any œµ > 0, there_
_‚àû_ _‚Üí_ _‚à•_ _‚à•_ _‚àû_ _‚àû_
_exists a polynomial p(u, v) =_ _j=1,k=1_ _[c][jk][u][j][v][k][, such that]_
_‚àû_ _‚àû_

[P][m] _œÅ(t, s)_ _p(e[‚àí][t], e[‚àí][s])_ _dtds < œµ._ (25)

0 0 _|_ _‚àí_ _|_

Z Z

_Proof. Fix any œµ > 0. Consider the following transformation_

1

_uv_ _[œÅ][(][‚àí]_ [ln][ u,][ ‚àí] [ln][ v][)][,] _u, v_ (0, 1],

_R(u, v) =_ _‚àà_ (26)

0, _uv = 0._



This transformation preserves the norm with _œÅ_ _L1([0,_ )2) = _R_ _L1([0,1]2)._
_‚à•_ _‚à•_ _‚àû_ _‚à•_ _‚à•_

First, according to the density of continuous functions in L[p] space (Rudin, 1987, Theorem
3.14), there exists R[Àú] ‚àà _C([0, 1][2]), such that ‚à•R ‚àí_ _R[Àú]‚à•L1([0,1]2) < œµ/2. Next, by the density of_
polynomials in the space of continuous functions (Lorentz, 2005, Theorem 6), there exists
a polynomial q(u, v) = _j,k=0_ _[c][jk][u][j][v][k][, such that][ ‚à•]R[Àú]_ _q_ _L‚àû([0,1]2) < œµ/2._ Finally, let
_‚àí_ _‚à•_
_p(u, v) = uvq(u, v), we have_

[P][m] 1 1
_‚àû_ _‚àû_

_œÅ(t, s)_ _p(e[‚àí][t], e[‚àí][s])_ _dtds =_

0 0 _|_ _‚àí_ _|_ 0 0 _uv [p][(][u, v][)]_

Z Z Z Z

= ‚à•R ‚àí _q‚à•[R]L1[(]([0[u, v],1][)]2[ ‚àí])_ [1] _[dudv]_
_R_ _R_ _L1([0,1]2) +_ _R_ _q_ _L‚àû([0,1]2)_
_‚â§‚à•_ _‚àí_ [Àú]‚à• _‚à•_ [Àú] ‚àí _‚à•_
_< œµ/2 + œµ/2 = œµ,_ (27)

which completes the proof.

Now we are ready to prove the universal approximation theorem.

_Proof of Theorem 4.1. According to the representation theorem (Lemma B.1), we have_

_‚àû_
_Ht(x) =_ _x[‚ä§]s[œÅ][(][t, s][)][ds,]_ **_x_** _,_ (28)

0 _‚àí_ _‚àÄ_ _‚ààX_

Z

where _œÅ_ _L1([0,_ )2) = **_H_** _<_ . Therefore, by Lemma B.2, there exists pi(u, v) =
_m_ _‚à•_ _‚à•_ _‚àû_ _‚à•_ _‚à•_ _‚àû_
_j,k=1_ _[c]jk[(][i][)][u][j][v][k][,][ i][ = 1][,][ 2][, . . ., d][, where][ m][ is the maximal degree of][ {][p][i][}]i[d]=1[, such that]_
P _d_

_‚àû_ _‚àû_

_œÅi(t, s)_ _pi(e[‚àí][t], e[‚àí][s])_ _dtds < œµ._ (29)

_i=1_ Z0 Z0 _|_ _‚àí_ _|_

X

Let

_c = u = 1m, V = W[Àú]_ = diag (1, 2, . . ., m),
_‚àí_

_W = diag( W,[Àú]_ _W,[Àú]_ _¬∑ ¬∑ ¬∑,_ _W[Àú]_ ) ‚àà R[dm][√ó][dm], U = diag(u, u, ¬∑ ¬∑ ¬∑, u) ‚àà R[dm][√ó][d], (30)

_M = PQ = (M1, M2, ¬∑ ¬∑ ¬∑, Md) ‚àà_ R[m][√ó][dm], [Mi]jk = c[(]jk[i][)][,]

we get

_œÅÀÜ(t, s)[‚ä§]_ = c[‚ä§]e[V t]PQe[W s]U = c[‚ä§]e[V t]Me[W s]U


= c[‚ä§]e[V t] _¬∑ (M1, M2, ¬∑ ¬∑ ¬∑, Md) ¬∑ diag(e[W s], e[W s], ¬∑ ¬∑ ¬∑, e[W s]) ¬∑ diag(u, u, ¬∑ ¬∑ ¬∑, u)_

= (c[‚ä§]e[V t]M1, c[‚ä§]e[V t]M2, ¬∑ ¬∑ ¬∑, c[‚ä§]e[V t]Md) ¬∑ diag(e[W s]u, e[W s]u, ¬∑ ¬∑ ¬∑, e[W s]u)

= (c[‚ä§]e[V t]M1e[W s]u, c[‚ä§]e[V t]M2e[W s]u, ¬∑ ¬∑ ¬∑, c[‚ä§]e[V t]Mde[W s]u), (31)


-----

with

_œÅÀÜi(t, s) = c[‚ä§]e[V t]Mie[W s]u = pi(e[‚àí][t], e[‚àí][s]),_ _i = 1, 2, . . ., d._ (32)

Therefore, by (24) and (29), we have


_‚à•H ‚àí_ **_H[ÀÜ]‚à•‚â§_**

=

which completes the proof.


_i=1_ _‚à•œÅi ‚àí_ _œÅÀÜi‚à•L1([0,‚àû)2)_

X

_d_ _‚àû_ _‚àû_

_œÅi(t, s)_ _pi(e[‚àí][t], e[‚àí][s])_ _dsdt < œµ,_ (33)

_i=1_ Z0 Z0 _‚àí_

X


C General approximation rates

In this section, the proof of Theorem 4.2 is given. Again, by (24), the aim now is to
investigate the function approximation _œÅ_ _œÅÀÜ_ . Since one can handle each spatial dimension
_‚à•_ _‚àí_ _‚à•_
separately (similarly with (30) and (31)), we firstly derive the estimates by assuming d = 1,
and then extend the obtained results to the case of multi-dimensional inputs (for general
_d ‚àà_ N+).

**Conditions on representation.** To characterise the accuracy of using the model
_c[‚ä§]e[V t]Me[W s]u (with M := PQ) to approximate the target œÅ(t, s), the first stuff is to trans-_
late the conditions on the output (of piece-wise constant signals) to the representation.
Recall that y[c](t, s) = Ht(1( _,_ _s]), t, s_ 0, we get œÅ(t, s) = _ds_ _[H][t][(][1][(][‚àí‚àû][,][‚àí][s][]][). Hence, the]_
_‚àí‚àû_ _‚àí_ _‚â•_ _‚àí_ _[d]_

assumptions on y[c] in Theorem 4.2 is equivalent to the following smoothness and exponential
decay conditions on œÅ. That is, there exist Œ± ‚àà N+, Œ≤ > 0 such that

_œÅ_ _C_ _[Œ±]([0, +_ )[2]), (34)
_‚àà_ _‚àû_

_e[Œ≤][(][t][+][s][)][ ‚àÇ][k][+][l]_ _k, l_ N, k + l _Œ±._ (35)

_‚àÇt[k]‚àÇs[l][ œÅ][(][t, s][) =][ o][(1) as][ ‚à•][(][t, s][)][‚à•‚Üí‚àû][,]_ _‚àà_ _‚â§_

Note that the last decay condition implies


_‚àÇ[k][+][l]_
sup _Œ≤[‚àí][(][k][+][l][)]e[Œ≤][(][t][+][s][)]_ _k, l_ N, k + l _Œ±_ (36)
_t,s‚â•0_ _‚àÇt[k]‚àÇs[l][ œÅ][(][t, s][)]_ _[‚â§]_ _[Œ≥,]_ _‚àà_ _‚â§_


for some Œ≥ > 0.

C.1 Basics


Let ‚Ñ¶ _‚àà_ R[d] be a bounded set. Define the spaces

_C_ _[Œ±](‚Ñ¶) := {f ‚àà_ _C(‚Ñ¶) :[¬Ø]_ _D[i]f ‚àà_ _C(‚Ñ¶) for all[¬Ø]_ _|i| ‚â§_ _Œ±},_ _Œ± ‚àà_ N, (37)

_C_ _[Œ±,¬µ](‚Ñ¶) := {f ‚àà_ _C_ _[Œ±](‚Ñ¶) : |D[i]f_ (x) ‚àí _D[i]f_ (y)| ‚â§ _K‚à•x ‚àí_ _y‚à•2[¬µ]_ [for some][ K >][ 0][,]
for all x, y ‚Ñ¶and **_i_** = Œ± _,_ (38)
_‚àà_ _|_ _|_ _}_

and the ‚Äúnorm‚Äù

_D[i]f_ (x) _D[i]f_ (y)

_f_ _Œ±,¬µ,‚Ñ¶_ := sup sup _|_ _‚àí_ _|_ _,_ _f_ _C_ _[Œ±,¬µ](‚Ñ¶),_ (39)
_|_ _|_ _|i|=Œ±_ _x,y‚àà‚Ñ¶_ _‚à•x ‚àí_ _y‚à•2[¬µ]_ _‚àÄ_ _‚àà_

with the shorthand ‚à•¬∑ ‚à•‚Ñ¶ := | ¬∑ |0,0,‚Ñ¶.

**Theorem C.1 (Multivariate Jackson‚Äôs theorem (Schultz, 1969), Theorem 4.10). Let ‚Ñ¶** _‚àà_ R[d]

_be a regular,_ [2] _bounded and open set, and f ‚àà_ _C_ _[Œ±,¬µ](‚Ñ¶)[¬Ø]_ _for some Œ± ‚àà_ N, ¬µ ‚àà [0, 1]. Then for
_any n ‚àà_ N+, we have

inf _f_ _p_ ‚Ñ¶¬Ø _[‚â§]_ _[C][(][Œ±, ¬µ][)]_ ‚Ñ¶[,] (40)
_p_ _n_ _‚à•_ _‚àí_ _‚à•_ _n[Œ±][+][¬µ][ |][f]_ _[|][Œ±,¬µ,]_ [¬Ø]
_‚ààP_ _[d]_

2It is proved that every bounded, open and convex set is regular. See Morrey (1966) (Lemma
3.4.1).


-----

_where_ _n_ _[denotes the set of all polynomials with the degree of no more than][ n][ in each]_
_P_ _[d]_
_variable, C(Œ±, ¬µ) > 0 is a universal constant only depending on Œ±, ¬µ and ‚Ñ¶._

A commonly used case is when ¬µ = 0. That is, for f _C_ _[Œ±](‚Ñ¶), we get[¬Ø]_
_‚àà_

_f_ _Œ±,0,‚Ñ¶¬Ø_ _[‚â§]_ [2 max] ‚Ñ¶) _[<][ ‚àû][.]_ (41)
_|_ _|_ **_i_** =Œ±
_|_ _|_ _[‚à•][D][i][f]_ _[‚à•][L][‚àû][(¬Ø]_

For any x, x0 ‚Ñ¶, let Àúp(x) := p(x) + f (x0) _p(x0), and Àúe(x) := f_ (x) _pÀú(x). Then Àúp_ _n[,]_
and _‚àà_ [¬Ø] _‚àí_ _‚àí_ _‚ààP_ _[d]_

_|f_ (x) ‚àí _pÀú(x)| ‚â§|eÀú(x0)| + |eÀú(x) ‚àí_ _eÀú(x0)|_

_‚â§_ _x,ysup‚àà‚Ñ¶[¬Ø]_ _|eÀú(x) ‚àí_ _eÀú(y)| = |eÀú|0,0,‚Ñ¶¬Ø_ [=][ ‚à•][f][ ‚àí] _p[Àú]‚à•‚Ñ¶¬Ø_ _[,]_ _‚àÄx ‚àà_ ‚Ñ¶[¬Ø] _._ (42)


This gives the following convenient corollary.

**Corollary C.1. Let ‚Ñ¶** _‚àà_ R[d] _be a regular, bounded and open set, and f ‚àà_ _C_ _[Œ±](‚Ñ¶)[¬Ø]_ _for some_
_Œ±_ N. Then for any n N+, there exists p _n_ _[such that]_
_‚àà_ _‚àà_ _‚ààP_ _[d]_

_f_ _p_ _L‚àû(¬Ø‚Ñ¶)_ ‚Ñ¶)[,] (43)
_‚à•_ _‚àí_ _‚à•_ _[‚â§]_ _[C]n[Œ±][Œ±][ max]|i|=Œ±_ _[‚à•][D][i][f]_ _[‚à•][L][‚àû][(¬Ø]_

_where_ _n_ _[denotes the set of all polynomials with the degree of no more than][ n][ in each]_
_P_ _[d]_
_variable, CŒ± > 0 is a universal constant only depending on Œ± and ‚Ñ¶._

C.2 Proofs

Now we are ready to present the proof.

_Proof of Theorem 4.2. Step 1: domain transform. Consider the transform from the infinite_
domain [0, )[2] to the compact one [0, 1][2]:
_‚àû_

1

_uv_ _[œÅ][(][‚àí][c][0][ ln][ u,][ ‚àí][c][0][ ln][ v][)][,]_ _u, v_ (0, 1],

_R(u, v) =_ _‚àà_ (44)

0, _uv = 0,_



where c0 := (Œ± + 1)/Œ≤ > 0 is a fixed constant. A straightforward computation by induction
shows that, for any k, l ‚àà N, k + l ‚â§ _Œ±, and any u, v ‚àà_ (0, 1],


_‚àÇ[k][+][l]_

_‚àÇu[k]‚àÇv[l][ R][(][u, v][) = (]u[k][‚àí][+1][1)]v[k][l][+][+1][l]_


_l_

_‚àÇ[i][+][j]_
_C(k, i)C_ _[‚Ä≤](l, j)c[i]0[+][j]_ (45)

_‚àÇt[i]‚àÇs[j][ œÅ][(][‚àí][c][0][ ln][ u,][ ‚àí][c][0][ ln][ v][)][,]_

_j=0_

X


_i=0_


where C(k, i), C _[‚Ä≤](l, j) are some integer constants, and (t, s) = (_ _c0 ln u,_ _c0 ln v) is a one-_
_‚àí_ _‚àí_
to-one mapping between (0, 1][2] and [0, )[2]. By (36), we get
_‚àû_


_‚àÇ[k][+][l]_

_‚àÇu[k]‚àÇv[l][ R][(][u, v][)]_


_‚àÇ[i][+][j]_

_‚àÇt[i]‚àÇs[j][ œÅ][(][‚àí][c][0][ ln][ u,][ ‚àí][c][0][ ln][ v][)]_


_|C(k, i)||C_ _[‚Ä≤](l, j)|c[i]0[+][j]_
_j=0_

X


_u[k][+1]v[l][+1]_


_i=0_


_‚àÇ[k][+][l]_

_c0, e[‚àí]_ _c[s]0 )_

_‚àÇu[k]‚àÇv[l][ R][(][e][‚àí]_ _[t]_

_[‚â§]_ _[e]_


_‚àÇ[i][+][j]_

_‚àÇt[i]‚àÇs[j][ œÅ][(][t, s][)]_


(k+1) (l+1)

_c0_ _te_ _c0_


_|C(k, i)||C_ _[‚Ä≤](l, j)|c[i]0[+][j]_
_j=0_

X


_i=0_


_l_

_‚àÇ[i][+][j]_
_C(k, i)_ _C_ _[‚Ä≤](l, j)_ (Œ± + 1)[i][+][j] _Œ≤[‚àí][(][i][+][j][)]e[Œ≤][(][t][+][s][)]_
_|_ _||_ _|_ _¬∑_ _‚àÇt[i]‚àÇs[j][ œÅ][(][t, s][)]_
_j=0_

X

_l_

_C(k, i)_ _C_ _[‚Ä≤](l, j)_ (Œ± + 1)[i][+][j]Œ≥ _C(Œ±)Œ≥,_ (46)
_|_ _||_ _|_ _‚â§_
_j=0_

X


_i=0_

_k_

_i=0_

X


-----

where C(Œ±) > 0 is a universal constant only depending on Œ±. According to the decay
condition (35), we get


_‚àÇ[i][+][j]_

_‚àÇt[i]‚àÇs[j][ œÅ][(][‚àí][c][0][ ln][ u,][ ‚àí][c][0][ ln][ v][)]_

= lim
_u[k][+1]v[l][+1]_ (t,s) (+ _,+_ ) _[e]_
_‚Üí_ _‚àû_ _‚àû_


(k+1) (l+1)

_c0_ _te_ _c0_ _s_ _[‚àÇ][i][+][j]_

_‚àÇt[i]‚àÇs[j][ œÅ][(][t, s][)]_


lim
(u,v)‚Üí(0,0)


= lim (kŒ±‚àí+1Œ±) _[Œ≤t]e_ (Œ±l‚àí+1Œ±) _[Œ≤s]_ _e[Œ≤][(][t][+][s][)][ ‚àÇ][i][+][j]_
(t,s) (+ _,+_ ) _[e]_ _¬∑_ _‚àÇt[i]‚àÇs[j][ œÅ][(][t, s][)]_
_‚Üí_ _‚àû_ _‚àû_

= 0, (47)


(kŒ±‚àí+1Œ±) _[Œ≤t]e_


and similarly for u0, v0 ‚àà (0, 1],

_‚àÇ[i][+][j]_

_‚àÇt[i]‚àÇs[j][ œÅ][(][‚àí][c][0][ ln][ u,][ ‚àí][c][0][ ln][ v][)]_

lim = lim
(u,v)‚Üí(u0,0) _u[k][+1]v[l][+1]_ (t,s)‚Üí(‚àíc0 ln u0,+‚àû) _[e]_


(l+1)

_c0_ _s_ _[‚àÇ][i][+][j]_

_‚àÇt[i]‚àÇs[j][ œÅ][(][t, s][)]_


(k+1)

_c0_ _te_


= lim (kŒ±‚àí+1Œ±) _[Œ≤t]e_ (Œ±l‚àí+1Œ±) _[Œ≤s]_ _e[Œ≤][(][t][+][s][)][ ‚àÇ][i][+][j]_
(t,s) + _¬∑_ _‚àÇt[i]‚àÇs[j][ œÅ][(][t, s][)]_
_‚à•_ _‚à•‚Üí_ _‚àû_ _[e]_

= 0, (48)


(kŒ±‚àí+1Œ±) _[Œ≤t]e_


_‚àÇ[i][+][j]_

_‚àÇt[i]‚àÇs[j][ œÅ][(][‚àí][c][0][ ln][ u,][ ‚àí][c][0][ ln][ v][)]_

= 0. (49)
_u[k][+1]v[l][+1]_

_‚àÇ[k][+][l]_

(u, v) [0, 1] 0 0 [0, 1], (50)
_‚àÇu[k]‚àÇv[l][ R][(][u, v][) = 0][,]_ _‚àà_ _√ó {_ _} ‚à™{_ _} √ó_


lim
(u,v)‚Üí(0,v0)

This gives

and


_‚àÇ[k][+][l]_

_M0 :=_ max max (51)
_k,l_ N, k+l _Œ±_ (u,v) [0,1][2] _‚àÇu[k]‚àÇv[l][ R][(][u, v][)]_
_‚àà_ _‚â§_ _‚àà_ _[‚â§]_ _[C][(][Œ±][)][Œ≥]_

by (46) and (50). Hence, R(u, v) _C_ _[Œ±]([0, 1][2]) with bounded derivatives._
_‚àà_

_Step 2: polynomial approximation. According to Corollary C.1 and (51), we obtain that_
there exists R[Àú]n _n[, such that]_
_‚ààP_ [2]

_‚àÇ[k][+][l]_

_R_ _Rn_ _L‚àû([0,1]2)_ max
_‚à•_ _‚àí_ [Àú] _‚à•_ _‚â§_ _[C]n[Œ±][Œ±]_ _k,l‚ààN, k+l=Œ±_ _‚àÇu[k]‚àÇv[l][ R][(][u, v][)]_ _L[‚àû]([0,1][2])_

_,_ _n_ N+, (52)

_‚â§_ _[C][(]n[Œ±][Œ±][)][Œ≥]_ _‚àÄ_ _‚àà_

whereRÀún(u, v C) (Œ±R) >n( 0 is a universal constant only related tou, 0) _Rn(0, v) + R[Àú]n(0, 0), we get R[ÀÜ]n_ _n Œ±[with ÀÜ]. Furthermore, let Rn(u, 0) = R[ÀÜ]n(0R[ÀÜ], vn() = 0 foru, v) :=_
any u, v _‚àí[0[Àú], 1]. By (50), we have ‚àí_ [Àú] _R(u, v) = 0 for any ( ‚ààPu, v[2]_ ) [0, 1] 0 0 [0, 1], then
_‚àà_ _‚àà_ _√ó {_ _} ‚à™{_ _} √ó_

_R_ _Rn_ _L‚àû([0,1]2)_ _R_ _Rn_ _L‚àû([0,1]2) +_ _Rn_ _Rn_ _L‚àû([0,1]2)_
_‚à•_ _‚àí_ [ÀÜ] _‚à•_ _‚â§‚à•_ _‚àí_ [Àú] _‚à•_ _‚à•_ [Àú] _‚àí_ [ÀÜ] _‚à•_
_R_ _Rn_ _L‚àû([0,1]2) +_ _Rn(u, 0)_ _R(u, 0)_ _L‚àû([0,1]2)_
_‚â§‚à•_ _‚àí_ [Àú] _‚à•_ _‚à•_ [Àú] _‚àí_ _‚à•_
+ _Rn(0, v)_ _R(0, v)_ _L‚àû([0,1]2) +_ _Rn(0, 0)_ _R(0, 0)_ _L‚àû([0,1]2)_
_‚à•_ [Àú] _‚àí_ _‚à•_ _‚à•_ [Àú] _‚àí_ _‚à•_
‚â≤ _‚à•R ‚àí_ _R[Àú]n‚à•L‚àû([0,1]2),_ (53)

i.e. we can further require the approximator satisfying the zero half-boundary condition
(i.e. vanishing on (u, v) [0, 1] 0 0 [0, 1]) without effecting the approximation
_‚àà_ _√ó {_ _} ‚à™{_ _} √ó_
accuracy. Let ¬Øm := min _mE, mD_, we get
_{_ _}_

1 1

_R_ _R_ _L‚àû([0,1]2)_ _C(Œ±)Œ≥_ + _,_ (54)
_‚à•_ _‚àí_ [Àú]‚à• _‚â§_ _[C]m[(]¬Ø[Œ±][Œ±][)][Œ≥]_ _‚â§_ _m[Œ±]E_ _m[Œ±]D_

 

where


_RÀú := R[Àú] ¬Øm_ _[‚ààP]m[2]¬Ø_ _[,]_ _RÀú(u, v) :=_


_rÀúiju[i]v[j]._ (55)
_j=1_

X


_i=1_


-----

Let

_c = 1 ¬Øm[,]_ _u = 1m ¬Ø_ _[,]_ _M = [Àúrij] ‚àà_ Rm[¬Ø] _√óm ¬Ø_ _,_ (56)
_V = ‚àídiag(2, 3, ¬∑ ¬∑ ¬∑, ¬Øm + 1)/c0,_ _W = ‚àídiag(2, 3, ¬∑ ¬∑ ¬∑, ¬Øm + 1)/c0,_ (57)

then by (24) and (54), we have

_‚à•H ‚àí_ **_H[ÀÜ]‚à•‚â§‚à•œÅ ‚àí_** _œÅÀÜ‚à•L1([0,‚àû)2)_ (58)

= _œÅ(t, s)_ _c[‚ä§]e[V t]Me[W s]u_ _L[1]([0,_ )[2])
_‚àí_ _‚àû_

= _œÅ(t, s)_ _e[‚àí]_ _c[t]0 e[‚àí]_ _c[s]0 ÀúR(e[‚àí]_ _c[t]0, e[‚àí]_ _c[s]0 )_
_‚àí_ _L[1]([0,_ )[2])

_‚àû_
= c[2]0 _R_ _RÀú_ _L[1]([0,1][2])_ (59)
_‚àí_

1 1

_c[2]0_ _R_ _RÀú_ _L[‚àû]([0,1][2])_ + _._ (60)
_‚â§_ _‚àí_ _[‚â§]_ _[C]Œ≤[2][(][Œ±]m¬Ø[)][Œ±][Œ≥][ ‚â§]_ _[C][(]Œ≤[Œ±][2][)][Œ≥]_  _m[Œ±]E_ _m[Œ±]D_ 

That is, one can achieve an approximation accuracy scaling like (1/ ¬Øm)[Œ±] with ¬Øm[2] parameters.
The proof is completed.

**Remark C.1. The extension to multi-dimensional inputs (general d ‚àà** N+) is found in the
_last paragraph of Appendix D.2._

**Remark C.2. Recall M = PQ ‚àà** R[m][D][√ó][m][E] _with P ‚àà_ R[m][D][√ó][N] _, Q ‚àà_ R[N] _[√ó][m][E]_ _, we only need_
_to investigate the case of N_ min _mE, mD_ = ¬Øm, since rank(M ) _m¬Ø_ _._
_‚â§_ _{_ _}_ _‚â§_


D Approximation rates via temporal product structure

In this section, the proof of Theorem 4.3 is provided.

D.1 Proper Orthogonal Decomposition

Proper orthogonal decomposition (POD; (Liang et al., 2002), (Berkooz et al., 1993), (Chatterjee, 2000)) is a method for model reduction, which is commonly applied to numerical
PDEs and fluids mechanics. It can be viewed as an extension of singular value decomposition (SVD) and principal component analysis (PCA) to infinite-dimensional spaces.

Fix any R _L[‚àû]([0, 1][2])._ [3] Define the POD operator
_‚àà_

1 1
: œÜ(v) _R(u, v)œÜ(v)dv_ _R(u, v)du,_ _œÜ(v)_ _L[2][0, 1]._ (61)
_K_ _7‚Üí_ 0 0 _¬∑_ _‚àà_
Z Z

**Proposition D.1. The operator** _is linear, bounded, compact, self-adjoint and non-_
_K_
_negative._


_Proof. (i) The linearity is obvious._

(ii) Let


1
: œÜ(v)
_R_ _7‚Üí_ 0
Z


_R(u, v)œÜ(v)dv,_ _œÜ(v)_ _L[2][0, 1],_ (62)
_‚àà_


then


1
( _œÜ)(v) =_ _R(u, v)(_ _œÜ)(u)du,_ _œÜ(v)_ _L[2][0, 1]._ (63)
_K_ 0 _R_ _‚àà_
Z

By the Cauchy-Schwartz inequality, we get


_‚à•RœÜ‚à•L2[0,1] ‚â§‚à•R‚à•L2([0,1]2)‚à•œÜ‚à•L2[0,1],_ (64)

3Here, we use the same notation as (44), since the function defined there is also bounded.


-----

which gives

_‚à•KœÜ‚à•L2[0,1] ‚â§‚à•R‚à•L2([0,1]2)‚à•RœÜ‚à•L2[0,1] ‚â§‚à•R‚à•L[2]_ [2]([0,1][2])[‚à•][œÜ][‚à•][L][2][[0][,][1]][.] (65)

Note that R(u, v) _L[‚àû]([0, 1][2])_ _L[2]([0, 1][2]), hence both_ and are bounded operator
_‚àà_ _‚äÇ_ _K_ _R_
from L[2][0, 1] to itself.

(iii) It is well-known that the Hilbert‚ÄìSchmidt integral operator

1
( _œà)(v) =_ _R(u, v)œà(u)du,_ _œà(u)_ _L[2][0, 1]_ (66)
_C_ 0 _‚àà_
Z

is a compact operator from L[2][0, 1] to itself. Therefore

_œÜ =_ _œÜ,_ _œÜ_ _L[2][0, 1],_ (67)
_K_ _CR_ _‚àà_

which gives = . Since is bounded and is compact, we get is also compact.
_K_ _CR_ _R_ _C_ _K_

(iv) By Fubini‚Äôs theorem, it is straightforward to verify that

1 1 1
_œÜ, œà_ _L2[0,1] =_ _R(u, w)_ _R(u, v)œÜ(v)dv_ _du_ _œà(w)dw_
_‚ü®K_ _‚ü©_ 0 0 0 _¬∑_
Z Z Z 

1 1 1
= _R(u, w)R(u, v)œÜ(v)œà(w)dvdudw_

0 0 0

Z Z Z

1 1 1
= _R(u, v)_ _R(u, w)œà(w)dw_ _du_ _œÜ(v)dv_

0 0 0 _¬∑_

Z Z Z 

= ‚ü®œÜ, Kœà‚ü©L2[0,1], _œÜ, œà ‚àà_ _L[2][0, 1]._ (68)

(v) By Fubini‚Äôs theorem, it is straightforward to verify that

1 1
_œÜ, œÜ_ _L2[0,1] =_ ( _œÜ)(u)R(u, w)du_ _œÜ(w)dw_
_‚ü®K_ _‚ü©_ 0 0 _R_ _¬∑_
Z Z

1 1
= ( _œÜ)(u)R(u, w)œÜ(w)dudw_

0 0 _R_

Z Z

1 1
= ( _œÜ)(u)_ _R(u, w)œÜ(w)dw_ _du_

0 _R_ 0

Z Z 

1
= ( _œÜ)[2](u)du_ 0, _œÜ_ _L[2][0, 1]._ (69)

0 _R_ _‚â•_ _‚àà_

Z

The proof is completed.

Combining (i)‚Äî(iv) and applying Hilbert‚ÄìSchmidt‚Äôs expansion theorem, we obtain that
_L[2][0, 1] has an orthonormal basis {œÜn}n‚ààN_ _{œàŒæ}Œæ‚ààŒû, such that_

-  _œÜn = ŒªnœÜn, Œªn_ = 0 for n, andS _œàŒæ = 0 for Œæ_ Œû, where is a finite or
_Kcountable set. If_ _Ã∏_ is not finite, we have lim ‚ààN _K_ _n_ _Œªn = 0; ‚àà_ _N_
_N_ _‚Üí‚àû_

-  For any œà _L[2][0, 1], we have_
_‚àà_

_œà =_ _œà, œÜn_ _L2[0,1]œÜn +_ _œà, œàŒæ_ _L2[0,1]œàŒæ,_ (70)

_‚ü®_ _‚ü©_ _‚ü®_ _‚ü©_
_nX‚ààN_ _ŒæX‚ààŒû_

where the second summation has at most countable non-zero terms, and


_œà =_
_K_


_Œªn‚ü®œà, œÜn‚ü©L2[0,1]œÜn._ (71)
_nX‚ààN_


Here, all the series converge under the norm ‚à•¬∑ ‚à•L2[0,1]. Without loss of generality, N =
1, 2, _, N0_ for N0 N+ or N0 = + (i.e. = N+). By (69), we get
_{_ _¬∑ ¬∑ ¬∑_ _}_ _‚àà_ _‚àû_ _N_

0 ‚â§‚ü®KœÜn, œÜn‚ü©L2[0,1] = Œªn‚ü®œÜn, œÜn‚ü©L[2] [2][0,1] [=][ Œª][n][,] _‚àÄn ‚ààN_ _,_ (72)


-----

i.e.n all the eigenvalues of. In addition, limn K are non-negative, andŒªn = 0 implies that one can index all the eigenvalues in a Œªn Ã∏= 0 for n ‚ààN implies Œªn > 0,
_‚àÄ_ _‚ààN_ _‚Üí‚àû_
non-increasing sequence: Œª1 ‚â• _Œª2 ‚â•¬∑ ¬∑ ¬∑ ‚â•_ _Œªn ‚â•¬∑ ¬∑ ¬∑ ‚â•_ 0.

Then, we can present the POD estimate.
**Theorem D.1. For any R** _L[‚àû]([0, 1][2]), we have_
_‚àà_

1 _N_ 2 _N0_
Z0 _n=1‚ü®R(u, v), œÜn(v)‚ü©L2[0,1]œÜn(v)_ _L[2][0,1]_ _du =_ _n=N_ +1 _Œªn,_ _‚àÄN ‚àà_ N. (73)

X X

_[R][(][u, v][)][ ‚àí]_

_Proof. Combining (69) and (72) gives_


1
_Œªn =_ _œÜn, œÜn_ _L2[0,1] =_
_‚ü®K_ _‚ü©_ 0
Z


( _œÜn)[2](u)du,_ _n_ _._ (74)
_R_ _‚àÄ_ _‚ààN_


Similarly,
1

( _œàŒæ)[2](u)du =_ _œàŒæ, œàŒæ_ _L2[0,1] =_
0 _R_ _‚ü®K_ _‚ü©_

Z

which gives


_Œªn‚ü®œàŒæ, œÜn‚ü©L[2]_ [2][0,1] [= 0][,] _‚àÄŒæ ‚àà_ Œû, (75)
_nX‚ààN_


1
( _œàŒæ)(u) =_ _R(u, v)œàŒæ(v)dv = 0,_ _a.e. u_ [0, 1]. (76)
_R_ 0 _‚àà_
Z

Notice that Ru(v) := R(u, v) _C_ _[Œ±][0, 1]_ _L[2][0, 1] for any u_ [0, 1]. By (70) and (76), we
_‚àà_ _‚äÇ_ _‚àà_
get

_Ru_ _L[2][0,1]_ [=] _Ru, œÜn_ _L2[0,1]œÜn +_ _Ru, œàŒæ_ _L2[0,1]œàŒæ,_
_‚à•_ _‚à•[2]_ _‚ü®_ _‚ü©_ _‚ü®_ _‚ü©_

-  Xn‚ààN _ŒæX‚ààŒû_

_Ru, œÜn_ _L2[0,1]œÜn +_ _Ru, œàŒæ_ _L2[0,1]œàŒæ_
_nX‚ààN_ _‚ü®_ _‚ü©_ _ŒæX‚ààŒû‚ü®_ _‚ü©_ +L[2][0,1]

= _Ru, œÜn_ _L[2][0,1]_ [+] _Ru, œàŒæ_ _L[2][0,1]_

_‚ü®_ _‚ü©[2]_ _‚ü®_ _‚ü©[2]_
_nX‚ààN_ _ŒæX‚ààŒû_

= ( _œÜn)[2](u),_ _a.e. u_ [0, 1]. (77)

_R_ _‚àà_
_nX‚ààN_

Hence for any N ‚àà N, we have

2

_N_ _N_

_n=1‚ü®Ru, œÜn‚ü©L2[0,1]œÜn_ _L[2][0,1]_ = ‚à•Ru‚à•L[2] [2][0,1] _[‚àí]_ _n=1‚ü®Ru, œÜn‚ü©L[2]_ [2][0,1] (78)

X X

_N_

_[R][u][ ‚àí]_

= ( _œÜn)[2](u)_ ( _œÜn)[2](u)_

_R_ _‚àí_ _R_
_nX‚ààN_ _nX=1_

_N0_

= ( _œÜn)[2](u),_ (79)

_R_
_n=N_ +1

X

where the summation is zero by convention if the subscript is larger than the superscript.
This by (74) implies


1

0

Z


_N0_

_Œªn._ (80)
_n=N_ +1

X


Z0 _n=1‚ü®R(u, v), œÜn(v)‚ü©L2[0,1]œÜn(v)_ _L[2][0,1]_ _du =_ _n=N_ +1 _Œªn._ (80)

X X

Here, the equality holds as a consequence of Beppo Levi‚Äôs monotone convergence lemma

_[R][(][u, v][)][ ‚àí]_

and Lebesgue‚Äôs dominated convergence theorem, and one has _n_ _[Œª][n][ <][ +][‚àû][. In fact, for]_
_‚ààN_

[P]


-----

_N0 = +_, let Sn = _k=1_ _[Œª][k][, we get][ S][n][ increasing (since][ Œª][k][ ‚â•]_ [0 for all][ k][ ‚ààN] [). By (74)]
_‚àû_
and (78), we have

_n_ 1 1 _n_ 1

[P][n]

_Sn =_ _k=1_ Z0 (RœÜk)[2](u)du = Z0 _k=1‚ü®Ru, œÜk‚ü©L[2]_ [2][0,1][du][ ‚â§] Z0 _‚à•Ru‚à•L[2]_ [2][0,1][du][ =][ ‚à•][R][‚à•]L[2] [2]([0,1][2])[,]

X X

(81)

which gives that Sn converges as n . The proof is completed.
_‚Üí‚àû_

**Remark D.1. Let œïn(u) := ‚ü®R(u, v), œÜn(v)‚ü©L2[0,1], then we have the POD estimate**
_R(u, v)_ _n_ _[œï][n][(][u][)][œÜ][n][(][v][)][, where the error is characterised by the tail sum of eigenvalues of]_
_‚âà_ [P]

_the POD operator._

Recall the POD operator defined in (61). We similarly define

1 1
Àú : œÜ(v) _RÀú(u, v)œÜ(v)dv_ _R[Àú](u, v)du,_ _œÜ(v)_ _L[2][0, 1],_ (82)
_K_ _7‚Üí_ 0 0 _¬∑_ _‚àà_
Z Z

where R[Àú] is defined as (55), i.e. the approximator constructed in the general approximation
theorem before. Obviously, as a polynomial, R[Àú] _L[‚àû]([0, 1][2]). Hence, by Proposition D.1,_
_‚àà_
Àú is also linear, bounded, compact, self-adjoint and non-negative. In addition, according to
_K_
Theorem D.1, we have the following POD estimate
1 _N_ 2 _NÀú0_
Z0 _RÀú(u, v) ‚àí_ _n=1_ ÀúR(u, v), _œÜ[Àú]n(v)_ _L[2][0,1]_ _œÜ[Àú]n(v)_ _L[2][0,1]_ _du =_ _n=N_ +1 _ŒªÀún,_ (83)

X X

where _Œªn_ _NnÀú=10_ [are eigenvalues of Àú] satisfying Œª[Àú]1 _Œª2_ _Œªn_ 0, and _œÜn_ _NnÀú=10_
_L[2][0, 1] are the corresponding orthonormal eigenfunctions, i.e. {[Àú]_ _}_ _K_ _‚â•_ [Àú] _‚â•¬∑ ¬∑ ¬∑ ‚â•_ [Àú] _‚â•¬∑ ¬∑ ¬∑ ‚â•ÀúœÜn = Œª[Àú]nœÜ[Àú]n, {Œª[Àú]n[Àú] >} 0 for[‚äÇ]_
_K_ [Àú]
_n ‚àà{1, 2, ¬∑ ¬∑ ¬∑,_ _N[Àú]0}._

**Lemma D.1.** _K[Àú] is a finite-rank operator. That is,_ _N[Àú]0 ‚â§_ _m¬Ø_ = min{mE, mD} < ‚àû.

_Proof. Let Àúœïn(u) :=_ ÀúR(u, v), _œÜ[Àú]n(v)_ _L[2][0,1][. We first show that both Àú]œïn(u) and œÜ[Àú]n(v) are_

_m_ _m ¬Ø_
polynomials. In fact, since R[Àú](u, v) = _i=1_ _j=1_ _r[Àú]iju[i]v[j], we have_

_m¬Ø_ _m¬Ø_ P

_‚àÇ[k]_ _i!_

[P][ ¬Ø]

_R(u, v)_ _rÀúij_
_‚àÇu[k][ Àú]_ [=] (i _k)!_ _[u][i][‚àí][k][v][j]_

_i=k_ _j=1_ _‚àí_

X X

_m¬Ø_ _m¬Ø_

_i!_
_rÀúij_ _m),_ _k = 1, 2,_ _,_ (84)

_‚â§_ _i=k_ _j=1_ _|_ _|_ (i ‚àí _k)!_ [‚âú] _[C][1][(][k,][ ¬Ø]_ _¬∑ ¬∑ ¬∑_

X X

with the convention that the summation is zero if the subscript is larger than the superscript,
i.e. C1(k, ¬Øm) = 0 for any k > ¬Øm. Let C1( ¬Øm) := max{‚à•R[Àú]‚à•L‚àû([0,1]2), max1‚â§k‚â§m¬Ø _[C]1[(][k,][ ¬Ø]m)},_
then we have
_‚àÇ[k]_

_R(u, v)œÜ[Àú]n(v)_ _m)_ _œÜn(v)_ _L[2][0, 1]_ _L[1][0, 1],_ _k = 0, 1,_ _._ (85)
_‚àÇu[k][ Àú]_ _[‚â§]_ _[C][1][( ¬Ø]_ _|_ [Àú] _| ‚àà_ _‚äÇ_ _¬∑ ¬∑ ¬∑_

According to Lebesgue‚Äôs dominated convergence theorem, we get by induction that

1 1

_d[k]_ _œïn(u) =_ _[d][k]_ _RÀú(u, v)œÜ[Àú]n(v)dv =_ _‚àÇ[k]_ _R(u, v)œÜ[Àú]n(v)dv,_ _k = 0, 1,_ _._ (86)

_du[k][ Àú]_ _du[k]_ 0 0 _‚àÇu[k][ Àú]_ _¬∑ ¬∑ ¬∑_

Z Z

Similarly, we get

_m¬Ø_ _m¬Ø_

_‚àÇ[k]_ _j!_

_R(u, v)_ _rÀúiju[i]_
_‚àÇv[k][ Àú]_ [=] (j _k)!_ _[v][j][‚àí][k]_

_i=1_ _j=k_ _‚àí_

X X


_m¬Ø_

_j!_
_rÀúij_ _m),_ _k = 1, 2,_ _,_ (87)
_j=k_ _|_ _|_ (j ‚àí _k)!_ [‚âú] _[C][2][(][k,][ ¬Ø]_ _¬∑ ¬∑ ¬∑_

X


_i=1_


-----

and C2(k, ¬Øm) = 0 for any k > ¬Øm. Let C2( ¬Øm) := max{‚à•R[Àú]‚à•L‚àû([0,1]2), max1‚â§k‚â§m¬Ø _[C]2[(][k,][ ¬Ø]m)},_
then for k = 0, 1,, we have
_¬∑ ¬∑ ¬∑_

1

_‚àÇ[k]_ _R(u, v)_ _RÀú(u, v)œÜ[Àú]n(v)dv_ _m)_ _R_ _L‚àû([0,1]2)_ _œÜn_ _L1[0,1]_

_‚àÇv[k][ Àú]_ Z0 _[‚â§]_ _[C][2][( ¬Ø]_ _‚à•_ [Àú]‚à• _‚à•_ [Àú] _‚à•_

_C2[2][( ¬Ø]m)_ _œÜn_ _L2[0,1] = C2[2][( ¬Ø]m)_ _L[1][0, 1]._ (88)
_‚â§_ _‚à•_ [Àú] _‚à•_ _‚äÇ_

According to Lebesgue‚Äôs dominated convergence theorem, we get by induction that

1 1

_dvd[k][k][ Àú]œÜn(v) = ŒªÀú[1]n_ 0 _‚àÇv‚àÇ[k][k][ Àú]R(u, v)_ 0 _RÀú(u, v)œÜ[Àú]n(v)dvdu,_ _k = 0, 1, ¬∑ ¬∑ ¬∑ ._ (89)

Z Z

That is, Àúœïn, _œÜ[Àú]n ‚àà_ _C_ _[‚àû][0, 1] for any n = 1, 2, ¬∑ ¬∑ ¬∑,_ _N[Àú]0._ Since R[Àú]d([k]u, 0) = R[Àú](0d, v[k] ) = 0 for
_u, v_ [0, 1], we get Àúœïn(0) = œÜ[Àú]n(0) = 0. Furthermore, we have _du[k][ Àú]œïn(u) =_ _dv[k][ Àú]œÜn(v) = 0_

forNÀú0 k > ‚â§ ‚ààm <¬Ø ¬Øm ‚àû, hence Àú. [4] The proof is completed.œïn, _œÜ[Àú]n ‚ààPm ¬Ø_ [. Since][ {]œÜ[Àú]n}NnÀú=10 _[‚äÇ]_ _[L][2][[0][,][ 1] are orthonormal, we must have]_

D.2 Approximation rates


**Perturbation of eigenvalues.** First, we need to bound the gap between the eigenvalues
_Œªn_ _NnÀú=10_ [and][ {][Œª][n][}]n[N]=1[0] [(corresponding to the function][ R][ defined in (44)). The following]
_{[Àú]_ _}_
theorem is necessary.
**Theorem D.2 (Courant‚ÄìFischer‚ÄìWeyl min-max principle; Lax (2002) (Chapter 28, The-**
orem 4)). Let _be a compact, self-adjoint operator on a Hilbert space_ _, whose positive_
_B_ _H_
_eigenvalues are listed in a decreasing order ¬µ1 ‚â•_ _¬µ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•_ _¬µk ‚â•¬∑ ¬∑ ¬∑ > 0. Then_

max min (90)
_Sk_ _x‚ààSk, ‚à•x‚à•H=1[‚ü®B][x, x][‚ü©][H][ =][ ¬µ][k][,]_

_where_ _k_ _is any k-dimensional linear subspace._
_S_ _‚äÇH_

Based on it, we have the following lemma to characterise the perturbation of singular values.
**Lemma D.2. For any R1, R2 ‚àà** _L[‚àû]([0, 1][2]), we have the estimate_

_Œª[R]k_ [1] _Œª[R]k_ [2] (91)
_‚àí_ _[‚â§‚à•][R][1][ ‚àí]_ _[R][2][‚à•][L][2][([0][,][1]][2][)][.]_

q q

_Proof. According to Theorem D.2 and by (69), we have_

_Œª[R]k_ [= max] min min _L[2][0,1][.]_ (92)
_Sk_ _œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1[‚ü®K][R][œÜ, œÜ][‚ü©][L][2][[0][,][1]][ = max]Sk_ _œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1_ _[‚à•R][R][œÜ][‚à•][2]_

Note that RR1 ‚àíRR2 = RR1‚àíR2 and by (64), we have

_Œª[R]k_ [1] = max min

q _Sk_ _œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1_ _[‚à•R][R][1][œÜ][‚à•][L][2][[0][,][1]]_

max min ( _R1_ _R2)œÜ_ _L2[0,1] +_ _R2œÜ_ _L2[0,1]_
_‚â§_ _Sk_ _œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1_ _‚à•_ _R_ _‚àíR_ _‚à•_ _‚à•R_ _‚à•_

  

_‚â§_ maxSk _œÜ‚ààSk, ‚à•minœÜ‚à•L2[0,1]=1_ _‚à•RR1‚àíR2‚à•_ + ‚à•RR2œÜ‚à•L2[0,1]

  

max min
_‚â§_ _Sk_ _œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1_ _[‚à•R][R][2][œÜ][‚à•][L][2][[0][,][1]][ +][ ‚à•][R][1][ ‚àí]_ _[R][2][‚à•][L][2][([0][,][1]][2][)]_

= _Œª[R]k_ [2] + _R1_ _R2_ _L2([0,1]2),_ (93)

_‚à•_ _‚àí_ _‚à•_

4 q _m_
In fact, for any p _m ¬Ø_ [with][ p][(0) = 0, we have][ p][ ‚àà] [span][{][v, v][2][,][ ¬∑ ¬∑ ¬∑][, v][ ¬Ø] . Through a standard
Schmidt-orthogonalization, we can get ‚ààP _ek(v)_ _k, k = 1, 2,_ _, ¬Øm, such that}_ _ei, ej_ _L2[0,1] =_
_‚ààP_ _¬∑ ¬∑ ¬∑_ _‚ü®_ _‚ü©_
_Œ¥pij(0) = 0, and p, q ‚àà(0) = 0, then their coordinates under the basisspan{e1(v), e2(v), ¬∑ ¬∑ ¬∑, em ¬Ø_ [(][v][)][}][. That is, if][ ‚ü®][p, q]e[‚ü©]kL[2]mk[0[¬Ø]=1,1] [are orthogonal. Hence, the Àú][= 0 for some][ p, q][ ‚ààP]m¬Ø [with]N0
_{_ _}_
orthogonal ¬Øm-dimensional coordinates here leads to at most ¬Øm non-zeros.


-----

and similarly,

_Œª[R]k_ [2] = max min

q _Sk_ _œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1_ _[‚à•R][R][2][œÜ][‚à•][L][2][[0][,][1]]_

max min ( _R2_ _R1)œÜ_ _L2[0,1] +_ _R1œÜ_ _L2[0,1]_
_‚â§_ _Sk_ _œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1_ _‚à•_ _R_ _‚àíR_ _‚à•_ _‚à•R_ _‚à•_

 

_‚â§_ maxSk _œÜ‚ààSk, ‚à•minœÜ‚à•L2[0,1]=1_ _‚à•RR2‚àíR1‚à•_ + ‚à•RR1œÜ‚à•L2[0,1]

  

max min
_‚â§_ _Sk_ _œÜ‚ààSk, ‚à•œÜ‚à•L2[0,1]=1_ _[‚à•R][R][1][œÜ][‚à•][L][2][[0][,][1]][ +][ ‚à•][R][2][ ‚àí]_ _[R][1][‚à•][L][2][([0][,][1]][2][)]_


= _Œª[R]k_ [1] + _R1_ _R2_ _L2([0,1]2),_ (94)

_‚à•_ _‚àí_ _‚à•_

q

which completes the proof.

**Proofs.** Now we are ready to derive the final estimate.

_Proof of Theorem 4.3. By Lemma D.2 and (54), we get_


_Œªk_ _ŒªÀúk_ _R_ _L2([0,1]2)_ _R_ _R_ _L‚àû([0,1]2)_ _._ (95)
_‚àí_ _[‚â§‚à•][R][ ‚àí]_ [Àú]‚à• _‚â§‚à•_ _‚àí_ [Àú]‚à• _‚â§_ _[C]m[(]¬Ø[Œ±][Œ±][)][Œ≥]_
q

p

Combining (54), (83) and (95) gives that

1 _N_ _e[‚àí]_ _c[t]0 Àúœïn(e[‚àí]_ _c[t]0 )_ _e[‚àí]_ _c[s]0 ÀúœÜn(e[‚àí]_ _c[s]0 )_

_c[2]0_ _nX=1_ _¬∑_ _L[1]([0,‚àû)[2])_

_N_

_[œÅ][(][t, s][)][ ‚àí]_

= _œïÀún(u)œÜ[Àú]n(v)_

_n=1_ _L[1]([0,1][2])_

X

_N_

_‚â§_ _R[R]([(]u, v[u, v])[)] ‚àí[ ‚àí]_ _RÀú(u, v)_ _L[1]([0,1][2])_ [+] _RÀú(u, v) ‚àí_ _œïÀún(u)œÜ[Àú]n(v)_

_n=1_ _L[1]([0,1][2])_

X

_N_

_‚â§_ _R(u, v) ‚àí_ _RÀú(u, v)_ _L[‚àû]([0,1][2])_ [+] _RÀú(u, v) ‚àí_ _œïÀún(u)œÜ[Àú]n(v)_

_n=1_ _L[2]([0,1][2])_

X

_NÀú0_ _NÀú0_ _NÀú0_

+ _ŒªÀún_ + _Œªn_ _Œªn_ + _Œªn_

_‚â§_ _[C]m[(]¬Ø[Œ±][Œ±][)][Œ≥]_ vun=N +1 _‚â§_ _[C]m[(]¬Ø[Œ±][Œ±][)][Œ≥]_ vun=N +1 _|[Àú]_ _‚àí_ _|_ _n=N_ +1

u X u X X
t t


_‚â§_ _[C]m[(]¬Ø[Œ±][Œ±][)][Œ≥]_

_‚â§_ _[C]m[(]¬Ø[Œ±][Œ±][)][Œ≥]_

‚â≤ _C(Œ±)Œ≥_


_NÀú0_

_Œªn +_
_n=N_ +1

X

_NÀú0_

_Œªn +_
_n=N_ +1

X


_NÀú0_

_n=N_ +1

X

_NÀú0_

_n=N_ +1

X

1

_¬∑_ _m¬Ø_ _[Œ±][ +]_


_ŒªÀún +_

_‚àö2_

v
u
u

_NÀút0_


_ŒªÀún_
_‚àí_

_ŒªÀún_
_‚àí_


_Œªn_

_Œªn_


_Œªn_

p

_NÀú0_

_n=N_ +1

X


_Œªn_

1

_m¬Ø_ _[Œ±/][2]_


_ŒªÀún_
_‚àí_


_Œªn_


_NÀú0_

_Œªn +_
_n=N_ +1

X


1 1

‚â≤ _C(Œ±)Œ≥_ 1 + _NÀú0_ _N_ _Œªn +_ _Œªn_ (96)

Ô£±Ô£≤ q _‚àí_  _¬∑_ _m¬Ø_ _[Œ±][ +]_ vuun=XN +1 vuun=XN +1 p _¬∑_ _m¬Ø_ _[Œ±/][2]_ Ô£ºÔ£Ω

t t

where ‚â≤ hides universal positive constants.Ô£≥ Ô£æ _[,]_

For the corresponding parameters, recall that Àúœïn, _œÜ[Àú]n_ _m ¬Ø_ [with Àú]œïn(0) = œÜ[Àú]n(0) = 0, we can
write _‚ààP_


1 +


_œïÀún(u) =_


_Pinu[i],_ _œÜÀún(v) =_
_i=1_

X


_Qnjv[j]_ (97)
_j=1_

X


-----

for any n = 1, 2, ¬∑ ¬∑ ¬∑, _N[Àú]0. Let_

_c = 1 ¬Øm[,]_ _u = 1m ¬Ø_ _[,]_ (98)
_V = ‚àídiag(2, 3, ¬∑ ¬∑ ¬∑, ¬Øm + 1)/c0,_ _W = ‚àídiag(2, 3, ¬∑ ¬∑ ¬∑, ¬Øm + 1)/c0,_ (99)

_M = PQ with P = [Pin] ‚àà_ Rm[¬Ø] _√óN_ _, Q = [Qnj] ‚àà_ RN _√óm ¬Ø_ _,_ (100)

then we have


_e[‚àí]_ _[i]c[+1]0_ _[t]Pin_
_i=1_ _¬∑_

X


_Qnje[‚àí]_ _[j]c[+1]0_ _[s]_
_j=1_

X


_c[‚ä§]e[V t]Me[W s]u = c[‚ä§]e[V t]P_ _Qe[W s]u =_
_¬∑_


_n=1_


_N_

= _e[‚àí]_ _c[t]0 Àúœïn(e[‚àí]_ _c[t]0 )_ _e[‚àí]_ _c[s]0 ÀúœÜn(e[‚àí]_ _c[s]0 )._ (101)

_¬∑_

_n=1_

X

Plugging this into (96) gives


_‚à•H ‚àí_ **_H[ÀÜ]‚à•‚â§‚à•œÅ ‚àí_** _œÅÀÜ‚à•L1([0,‚àû)2)_ (102)

= _œÅ(t, s)_ _c[‚ä§]e[V t]Me[W s]u_ _L[1]([0,_ )[2])
_‚àí_ _‚àû_

1 _NÀú0_ _NÀú0_ 1

‚â≤ _[C][(][Œ±][)][Œ≥]_ 1 + _NÀú0_ _N_ _Œªn +_ _Œªn_

_Œ≤[2]_ Ô£±Ô£≤ q _‚àí_  _¬∑_ _m¬Ø_ _[Œ±][ +]_ vuun=XN +1 vuun=XN +1 p _¬∑_ _m¬Ø_ _[Œ±/][2]_ Ô£ºÔ£Ω

t t (103)

Ô£≥ Ô£æ _[,]_

and the number of trainable parameters is 2N ¬Øm. Together with Lemma D.1, the proof is
completed.

**Extension to multi-dimensional inputs.** The above results can be naturally extended
to the general case where a d-dimensional input is given (‚àÄd ‚àà N+). In fact, let
_œÅi(t, s) ‚àí_ _c[‚ä§]e[V t]Mie[W s]u_ _L[1]([0,‚àû)[2])_ [‚â≤] _[œµ,]_ _i = 1, 2, ¬∑ ¬∑ ¬∑, d,_ (104)

for some 0 < œµ 1, then we take
_‚â™_

_M = (M1, M2, ¬∑ ¬∑ ¬∑, Md) ‚àà_ Rm[¬Ø] _√ódm ¬Ø_ _,_ (105)

_m_ _dm ¬Ø_ _m_ _d_
_W = diag(W, W,_ _, W_ ) R[d][ ¬Ø] _√ó_ _,_ _U = diag(u, u,_ _, u)_ R[d][ ¬Ø] _√ó_ _,_ (106)
_¬∑ ¬∑ ¬∑_ _‚àà_ _¬∑ ¬∑ ¬∑_ _‚àà_

and have

_c[‚ä§]e[V t]Me[W s]U = c[‚ä§]e[V t]_ _¬∑ (M1, M2, ¬∑ ¬∑ ¬∑, Md) ¬∑ diag(e[W s], e[W s], ¬∑ ¬∑ ¬∑, e[W s]) ¬∑ diag(u, u, ¬∑ ¬∑ ¬∑, u)_

= (c[‚ä§]e[V t]M1, c[‚ä§]e[V t]M2, ¬∑ ¬∑ ¬∑, c[‚ä§]e[V t]Md) ¬∑ diag(e[W s]u, e[W s]u, ¬∑ ¬∑ ¬∑, e[W s]u)

= (c[‚ä§]e[V t]M1e[W s]u, c[‚ä§]e[V t]M2e[W s]u, ¬∑ ¬∑ ¬∑, c[‚ä§]e[V t]Mde[W s]u). (107)

If the form PQ(= M ) is required, it is sufficient to take Mi = PiQi, i = 1, 2, _, d, and_
_¬∑ ¬∑ ¬∑_

_P = (P1, P2, ¬∑ ¬∑ ¬∑, Pd) ‚àà_ Rm[¬Ø] _√ódN_ _, Q = diag(Q1, Q2, ¬∑ ¬∑ ¬∑, Qd) ‚àà_ RdN _√ódm ¬Ø_ _._ (108)

Therefore, we obtain

_d_

_i=1_ _œÅi(t, s) ‚àí_ _c[‚ä§]e[V t]Me[W s]U_ _i_ _L[1]([0,‚àû)[2])_ [‚â≤] _[dœµ,]_ (109)

X  

with the number of parameters increased by d-times compared to the corresponding onedimensional setting.

D.3 Case analysis

**Bounds under different cases.** Now we make the comparison between (102) and (58).
Recall that {Œªn}n[N]=1[0] [is a positive decreased sequence (with lim][n][‚Üí‚àû] _[Œª][n][ = 0 and][ P]n[‚àû]=1_ _[Œª][n][ ‚â§]_
_R_ _L[2]([0,1][2])_ [by (81), if][ N][0][ = +][‚àû][), and Àú]N0 _m¬Ø_, we have the following cases.
_‚à•_ _‚à•[2]_ _‚â§_


-----

-  if N[Àú]0 = o( ¬Øm), we set N = N[Àú]0 in (102) and get the same bound as (58), but the
number of parameters is only ( N[Àú]0 ¬Øm) = o( ¬Øm[2]);
_O_

-  if N[Àú]0 = ( ¬Øm), then (102) implies that
_O_
_œÅ(t, s)_ _c[‚ä§]e[V t]Me[W s]u_ _L[1]([0,_ )[2])
_‚àí_ _‚àû_

‚â≤ _[C][(][Œ±][)][Œ≥]_ 1 + _‚àöm¬Ø_ _N_ 1 _m¬Ø_ _Œªn +_ _m¬Ø_ _Œªn_ 1

_Œ≤[2]_ Ô£± _‚àí_ _¬∑_ _m¬Ø_ _[Œ±][ +]_ vun=N +1 vun=N +1 _¬∑_ _m¬Ø_ _[Œ±/][2]_ Ô£º

Ô£≤  u X u X p Ô£Ω

t t


‚â≤ _[C][(][Œ±][)][Œ≥]_

_Œ≤[2]_


1

_m¬Ø_ _[Œ±][‚àí]_ 2[1] [+]


_Œªn +_
_n=N_ +1

X


(110)

Ô£º
Ô£Ω

Ô£æ _[.]_

1

_m¬Ø_ _[Œ±/][2]_ Ô£º

Ô£Ω


_Œªn_
_¬∑_


_m¬Ø_ _[Œ±/][2]_

_Œªn_
_¬∑_

p


_n=N_ +1


We are supposed to require that

Ô£≥

1

_m¬Ø_ _[Œ±][‚àí]_ [1]2 [‚â≥] [max] Ô£±

Ô£≤


_Œªn,_
_n=N_ +1

X


_n=N_ +1


1

(111)
_m¬Ø_ _[Œ±][‚àí][1][ .]_


_Œªn ‚â≤_
_n=N_ +1

X


_Œªn ‚â≤_


_m¬Ø_ [2][Œ±][‚àí][1][,]


_n=N_ +1


We give the following typical examples to illustrate sufficient conditions to guarantee
(111).

**‚Äì If Œªn =** (n[‚àí][r]) with r > 2Œ± + 1 3 (Œ± N+), since
_O_ _‚â•_ _‚àà_


_n[‚àí][r]_ _‚â§_
_n=N_ +1 Z

X

we get by (111) that


_x[‚àí][r]dx =_


_r > 1,_ (112)
_‚àÄ_

2rŒ±‚àí‚àí11, (113)


_N_ _[r][‚àí][1][ ‚àí]_


_r_ 1
_‚àí_


_m¬Ø_ _[r][‚àí][1]_


_m_

_n[‚àí][r]_ ‚â≤
_n=N_ +1

X


1

_m_
_m¬Ø_ [2][Œ±][‚àí][1][ ‚áî] _[N][ ‚â≥]_ [¬Ø]


_Œªn ‚â≤_
_n=N_ +1

X


1

_N_ _[r][‚àí][1][ ‚â≤]_


_m¬Ø_ _m¬Ø_ 1 1 _Œ±r_ _‚àí1_

_n=N_ +1 _Œªn ‚â≤_ _n=N_ +1 _n[‚àí]_ _[r]2 ‚â≤_ _N_ _r2_ _[‚àí][1][ ‚â≤]_ _m¬Ø_ _[Œ±][‚àí][1][ ‚áî]_ _[N][ ‚â≥]_ _m[¬Ø]_ 2 _[‚àí][1]_ _._ (114)

X p X

Assume that N _m¬Ø_ _[Œ¥]_ with Œ¥ [0, 1), then by (113) and (114), we require
_‚àº_ _‚àà_
_Œ¥ ‚â•_ max{ [2]r[Œ±]‚àí[‚àí]1[1] _[,][ Œ±]r2_ _[‚àí][‚àí][1][1]_ _[}][, i.e.]_

2Œ± 1 _Œ±_ 1
_r_ max _‚àí_ + 1, 2 _‚àí_ + 1 = [2][Œ±][ ‚àí] [1] + 1. (115)
_‚â•_ _Œ¥_ _Œ¥_ _Œ¥_
  

Meanwhile, the POD-estimate (110) achieves an accuracy scaling like (1/ ¬Øm)[Œ±][‚àí] 2[1]

with ( ¬Øm[1+][Œ¥]) parameters, while under the same capacity, the accuracy of
_O_ _Œ±(1+Œ¥)_
(58) scales like (1/ ¬Øm) 2 . The former beats the latter if Œ± 2 _[>][ Œ±][(1+]2_ _[Œ¥][)]_,

_‚àí_ [1]

i.e. _Œ¥ < 1_ 1/Œ± (Œ± 2). When Œ¥ = 1 1/Œ±, (115) becomes r
_‚àí_ _‚â•_ _‚àí_ _‚â•_
max [2]Œ±[Œ±][2][‚àí]1[1] _[,][ 2(][Œ±][ + 1)][}][ =][ 2]Œ±[Œ±][2][‚àí]1[1]_ [. That is to say,][ r][‚àó] [:=][ 2]Œ±[Œ±][2][‚àí]1[1] can be viewed

as an upper bound of the critical point where the two estimates are compa-{ _‚àí_ _‚àí_ _‚àí_
rable. When r > r[‚àó], the POD-estimate outperforms the non-POD-estimate,
and this effect gets more notable with r increasing. In fact, when r > r[‚àó], we

take N = ¬Øm 2rŒ±‚àí‚àí11 > ¬Øm _Œ±r2_ _‚àí[‚àí]1[1]_ (hence satisfying (113) and (114)), which gives an

((1/ ¬Øm)[Œ±][‚àí] [1]2 ) approximation error with ( ¬Øm[1+][ 2]r[Œ±]‚àí[‚àí]1[1] ) parameters for the POD_O_ _O_

estimate, while ( ¬Øm[2][‚àí] _Œ±[1] ) trainable parameters are needed to achieve the same_
_O_

accuracy using the non-POD-estimate. Since [2]r[Œ±][‚àí]1[1] _[<][ 1][ ‚àí]_ _Œ±[1]_ [, we get that the]

_‚àí_

POD-estimate outperforms the non-POD-estimate. When r +, the num_‚Üí_ _‚àû_
ber of trainable parameters for the POD-estimate is ( ¬Øm), much better than
_O_
the non-POD-estimate.


-----

**Remark D.2. Let**

1
_K(v, w) :=_

0

Z

_we get_


_R(u, v)R(u, w)du,_ _v, w_ [0, 1], (116)
_‚àà_


1
( _œÜ)(w) =_ _K(v, w)œÜ(v)dv,_ _œÜ_ _L[2][0, 1]._ (117)
_K_ 0 _‚àà_
Z

_Recall that R_ _C_ _[Œ±]([0, 1][2])_ _L[‚àû]([0, 1][2]), we get K_ _L[‚àû]([0, 1][2])_ _L[2]([0, 1][2]),_
_‚àà_ _‚äÇ_ _‚àà_ _‚äÇ_
_and hence_ _can be also viewed as a Hilbert-Schmidt integral operator with the_
_K_
_kernel K, where K is obviously symmetric as K(v, w) = K(w, v), and positive_
1 1
_definite since 0_ _œÜ, œÜ_ _L2[0,1] =_ 0 0 _[K][(][v, w][)][œÜ][(][v][)][œÜ][(][w][)][dvdw][,][ œÜ][ ‚àà]_ _[L][2][[0][,][ 1]]_
_‚â§‚ü®K_ _‚ü©_
_by (69). Since the derivatives of R (up to Œ±-order) are continuous on [0, 1][2]_
R R
_(hence bounded and integrable), we get K_ _C_ _[Œ±,Œ±]([0, 1][2]) (Œ±-differentiable for_
_‚àà_
_both arguments). According to Chang & Ha (1999) (Theorem 1), we have_


_‚àû_

_n=N_ +2Œ±+1 _Œªn ‚â≤_ _N_ _[‚àí][Œ±],_ _‚àÄN ‚àà_ N+ ‚áí _Œªn = O(n[‚àí][(][Œ±][+1)]),_ _n ‚Üí‚àû._ (118)

X

_That is to say, this general setting (only assume smoothness of the kernel)_
_can not guarantee the sufficient condition provided here (Œªn =_ (n[‚àí][r]) with
_O_
_r >_ [2]Œ±[Œ±][2][‚àí]1[1] _[), i.e. the point that the POD-estimate outperforms the non-POD-]_

_estimate requires a faster decay of the eigenvalues.‚àí_

**‚Äì If Œªn =** (e[‚àí][œân]) with œâ > 0, we get by (111) that
_O_

_m¬Ø_ _‚àû_ 1 ¬Øm[2][Œ±][‚àí][1]

_Œªn ‚â≤_ _e[‚àí][œân]_ = _[e][‚àí][œâ][(][N]_ [+1)] 1,

1 _e[‚àí][œâ][ ‚â≤]_ _m¬Ø_ [2][Œ±][‚àí][1][ ‚áî] _[N][ ‚â≥]_ _œâ[1]_ [ln] 1 _e[‚àí][œâ]_ _‚àí_

_n=N_ +1 _n=N_ +1 _‚àí_  _‚àí_ 

X X

(119)

_m¬Ø_

_‚àû_ 2 [(][N] [+1)] 1 _m¬Ø_ _[Œ±][‚àí][1]_

_Œªn ‚â≤_ _e[‚àí]_ _[œâ]2_ _[n]_ = _[e]1[‚àí]_ _[œâ]_ _e[‚àí]_ _[œâ]2_ ‚â≤ _m¬Ø_ _[Œ±][‚àí][1][ ‚áî]_ _[N][ ‚â≥]_ _œâ[2]_ [ln] 1 _e[‚àí]_ _[œâ]2_ _‚àí_ 1.

_n=N_ +1 _n=N_ +1 _‚àí_  _‚àí_ 

X p X

(120)


That is to say, for any Œ± ‚àà N+, œâ > 0, we have N ‚àº (2Œ± ‚àí 1) ln ¬Øm. This
implies an ((1/ ¬Øm)[Œ±][‚àí] 2[1] ) approximation error with ( ¬Øm ln ¬Øm) parameters for
_O_ _O_

the POD-estimate, while ( ¬Øm[2][‚àí] _Œ±[1] ) parameters are needed to achieve the same_
_O_

accuracy using the non-POD-estimate.

**‚Äì If N0 < +‚àû** (i.e. K is a finite rank operator by (71)), one can just take N = N0
and get by (110) an ((1/m ¬Ø )[Œ±][‚àí] 2[1] ) approximation error. For ¬Øm N+ sufficiently
_O_ _‚àà_

large such that ¬Øm _N0[Œ∫]_ [for some][ Œ∫][ ‚â´] [1, the number of trainable parameters for]

the POD-estimate is ‚àº _O(N ¬Øm) = O(N0[Œ∫][+1]), while O_ _N0Œ∫(2‚àí_ _Œ±[1]_ [)] parameters are

needed to achieve the same accuracy using the non-POD-estimate. Obviously, 
if Œ± 2, we get Œ∫(2 _Œ±_ [)][ ‚â•] [3]2 _[Œ∫][ ‚â´]_ _[Œ∫][ + 1.]_
_‚â•_ _‚àí_ [1]

**Eigenvalue approximation.** One can apply (91) in Lemma D.2 to estimate the eigenvalues _Œªn_ _n=1[, where][ R][1][ =][ R][ and][ R][2][ is taken as some approximator of][ R][, say ÀÜ]R. Let_
_ŒªÀÜ := Œª {R[ÀÜ], we recall (91) as}[N][0]_

_Œªk ‚àí_ qŒªÀÜk _[‚â§‚à•][R][ ‚àí]_ _R[ÀÜ]‚à•L2([0,1]2)._ (121)

p

We provide a naive method here. That is, one can take R[ÀÜ] as a piece-wise constant approximation of R. Fix any n ‚àà N+. Let I0 := {0}, Ii := ( _[i][‚àí]n[1]_ _[,][ i]n_ [] for][ i][ = 1][,][ 2][,][ ¬∑ ¬∑ ¬∑][, n][, then]

_i_ _j_ _i,j=0_ [is the uniform partition over [0][,][ 1]][2][. Let ÀÜ]R(u, v) := _i,j=0_ _[R][(][ i]n_ _[,][ j]n_ [)][1][I][i][√óI][j] [(][u, v][).]
_{I_ _√óI_ _}[n]_

[P][n]


-----

Then for any œÜ _L[2][0, 1], if w_ _k, k = 0, 1,_ _, n, we have_
_‚àà_ _‚ààI_ _¬∑ ¬∑ ¬∑_

1 1
( _R ÀÜ[œÜ][)(][w][) =]_ _RÀÜ(u, v)œÜ(v)dv_ _R[ÀÜ](u, w)du_
_K_ 0 0 _¬∑_
Zn Zn _n_ _n_ _i_ _i‚Ä≤_ 1 1

= _i=0_ _j=0_ _i[‚Ä≤]=0_ _j[‚Ä≤]=0_ _R_  _n_ _[, j]n_  _R_  _n_ _[, j]n[‚Ä≤]_  Z0 Z0 **1Ii√óIj** (u, v)œÜ(v)dv ¬∑ 1Ii‚Ä≤ _√óIj‚Ä≤_ (u, w)du

X X X X

_n_ _n_

_i_ _i_

= _i=0_ _j=0_ _R_  _n_ _[, j]n_  _R_  _n_ _[, k]n_  ZIi ZIj **1Ii√óIj** (u, v)œÜ(v)dv ¬∑ 1Ii√óIk (u, w)du

X X

_n_ _n_

_i_ _i_

= _R_ _R_ [1] _œÜ(v)dv,_ (122)

_i=0_ _j=0_  _n_ _[, j]n_   _n_ _[, k]n_  _¬∑_ _n_ ZIj

X X

which is a constant only related to k. That is, _R ÀÜ[œÜ][ is a piece-wise constant func-]_
_K_
tion, i.e. _KR ÀÜ[œÜ][ ‚àà]_ [span][ {][1][I][k] _[}]k[n]=0[, or range(][K]R[ ÀÜ][)][ ‚äÇ]_ [span][ {][1][I][k] _[}]k[n]=0[.]_ Obviously, {1Ik _}k[n]=0_
is an orthogonal set, which implies that the operator _R ÀÜ_ [is of finite rank at most]
1 _K_
_n + 1. Let R[ÀÜ]ij :=_ _n_ _[R][(][ i][‚àí]n[1]_ _[,][ j][‚àí]n[1]_ [),][ i, j][ = 1][,][ 2][,][ ¬∑ ¬∑ ¬∑][, n][ + 1,][ {][œÉ][k][}]k[n]=1[+1] [be the singular value of]

_RÀÜ := [ R[ÀÜ]ij]_ R[(][n][+1)][√ó][(][n][+1)] and V R[(][n][+1)][√ó][(][n][+1)] with columns as the corresponding right
_‚àà_ _‚àà_
singular vectors. Set [e1, e2, ¬∑ ¬∑ ¬∑, en+1] := [1I0, 1I1, ¬∑ ¬∑ ¬∑, 1In] V, then by (122), we get for
_l = 1, 2,_ _, n + 1,_
_¬∑ ¬∑ ¬∑_ _n_ _n_

_i_ _i_

( _R ÀÜ[e][l][)(][w][) =]_ _R_ _R_ [1]
_K_ _n_ _[, j]n_ _n_ _[, k]n_ _¬∑_ _n[2][ V][j][+1][,l]_

_i=0_ _j=0_    

X X

_n+1_ _n+1_

= _RÀÜijR[ÀÜ]i,k+1Vjl =_ _RÀÜ[‚ä§]RV[ÀÜ]_ :,l (123)

_k+1_ _[,]_

_i=1_ _j=1_

X X h i

which gives


_RÀÜ[‚ä§]RV[ÀÜ]_ :,l


_KR ÀÜ[e][l][ =]_ _RÀÜ[‚ä§]RV[ÀÜ]_ :,l _k+1_ **[1][I][k][ =][ œÉ]l[2]** _Vk+1,l1Ik = œÉl[2][e][l][,]_ (124)

_k=0_ _k=0_

X h i X

i.e. the set {œÉk[2][}][n]k=1[+1] [collects the all the eigenvalues of][ K]R[ ÀÜ][, which can be obtained by the]
SVD of R[ÀÜ].

For the error estimate, it is straightforward to have
2
_R(u, v)_ _R(u, v)_
_‚àí_ [ÀÜ] _L[2]([0,1][2])_

_n_ _n_ _ni_ _nj_ 2

_i_

= _dudv_

_i=1_ _j=1_ Z _i‚àín1_ Z _j‚àín1_  _n_ _[, j]n_ 

Xn Xn _ni_ _nj_ 2

_[R][(][u, v][)][ ‚àí]_ _[R]_

_‚â§_ _i=1_ _j=1_ Z _i‚àín1_ Z _j‚àín1_ (u,vmax)‚àà[0,1][2][ ‚à•‚àá][R][(][u, v][)][‚à•]2[2] _[¬∑]_ u ‚àí _n[i]_ _[, v][ ‚àí]_ _n[j]_ 2 _dudv_

Xn Xn _i_ _j_

_n_ _n_

2C [2](Œ±)Œ≥[2] [2] _,_ (125)

_‚â§_ _i=1_ _j=1_ Z _i‚àín1_ Z _j‚àín1_ _¬∑_ _n[2][ dudv][ = 4][C]_ [2]n[(][Œ±][2] [)][Œ≥][2]

X X

hence by (121), _Œªk_ _œÉk_ _n_ for any n N+.
_‚àí_ _‚â§_ [2][C][(][Œ±][)][Œ≥] _‚àà_

_[‚àö]_

E Numerical settings

According to Lemma B.1, the target input-output temporal relationship has a Riesz representation form. Under the discrete-time regime, we are supposed to set


_l_
_k+1_ **[1][I][k][ =][ œÉ][2]**


_R ÀÜ[e][l][ =]_
_K_


_Ht(x) =_


_œÅ(t, s)xs,_ (126)
_s=1_

X


where T N+ is the path length.
_‚àà_


-----

E.1 Settings of Figure 1

For the input, we generate 6 sequences using Gaussian i.i.d. random variables with the path
length T = 30. Hence, the output (target) is Ht(x) = _s=1_ _[œÅ][(][t, s][)][x][s][.]_

The high rank target has the representation

[P][30]

cos(t), _t = s,_
_œÅ[high](t, s) =_ (127)
0, _t_ = s,
 _Ã∏_

while the low rank target has the representation


99

_n=0_

X


_œÅ[low](t, s) =_

E.2 Settings of Figure 2


1

(128)
_n + 1 [cos(][nœÄt][) cos(][nœÄs][)][.]_


Consider the target with the following representation

_œÅ(t, s) = e[‚àí]_ _c[t]0 e[‚àí]_ _c[s]0 R(e[‚àí]_ _c[t]0, e[‚àí]_ _c[s]0 ),_ (129)


where


_R(u, v) =_


_‚àû_

_œÉnœïn(u)œÜn(v),_ (130)
_n=1_

X


with both _œïk_ and _œÜk_ as orthonormal bases. In this way, we construct a target with
_{_ _}_ _{_ _}_
singular values _œÉk_ . Under the discrete-time setting, we are supposed to use the following
_{_ _}_
linear, width-m RNN encoder-decoder

_œÑ_

## ÀÜH t(x) = c[‚ä§]V [t]PQW [s][‚àí][1]Ux(œÑ s), (131)

_‚àí_
_s=1_

X

where P R[m][√ó][N], Q R[N] _[√ó][m]_ with m = mD = mE.
_‚àà_ _‚àà_

Recall that the approximation error is derived as

_‚à•H ‚àí_ **_H[ÀÜ]‚à•_** ‚â≤ _‚à•R ‚àí_ _R[ÀÜ]‚à•L1([0,1]2) ‚â§‚à•R ‚àí_ _R[ÀÜ]‚à•L2([0,1]2),_ (132)

where R[ÀÜ](u, v) := _i=1_ _œï[Àú]i(u)œÜ[Àú]i(v)_ _m_ [with Àú]œïn, _œÜ[Àú]n_ _m. In the numerical experiments,_
_‚ààP_ [2] _‚ààP_
we first construct an R(u, v), and then fit it with the polynomial R[ÀÜ](u, v) using the method
of least squares. The norm[P][N][0] _R_ _R_ _L2([0,1]2) is used to evaluate the approximation error._
_‚à•_ _‚àí_ [ÀÜ]‚à•

In the particular example reported in Figure 2, we set m = 128, œïn(u) = _‚àö2 sin(nœÄu)_

1
_n‚àí_ 8, _n_ _N0_

and œÜn(v) = _‚àö2 sin(nœÄv). The singular values are taken as: (a) œÉn =_ _‚â§_ ;

0, _n > N0_



1
_n‚àí_ _,_ _n_ _N0_
(b) œÉn = _‚â§_ ; (c) œÉn = n[‚àí][2], with N0 = 2, 4, 6, 8. As an infinite sum, R is
0, _n > N0_


constructed under a finite truncation with the first 50 terms.

E.3 Settings of Figure 3

We perform experiments on nonlinear targets to show that the insight of low rank approximation also holds in the nonlinear case.

**Nonlinear target.** We consider the forced Lorenz 96 system (Lorenz, 1996), which is an
important example of reduced order modelling for convection dynamics, with applications
in weather forecasting.

Mathematically, the system has K output variables _yk_ and JK hidden variables _zj,k_
_{_ _}_ _{_ _}_
with k = 1, 2, . . ., K and j = 1, 2, . . ., J. The parameters K, J control the number of


-----

variables in the system, and can be viewed as a complexity measure. The input _xk_ is an
_{_ _}_
external temporal forcing. The system satisfies the following dynamics


_dyk_

_dt_ [=][ ‚àí][y][k][‚àí][1][(][y][k][‚àí][2][ ‚àí] _[y][k][+1][)][ ‚àí]_ _[y][k][ +][ x][k][ ‚àí]_ _J[1]_


_zj,k,_ (133)
_j=1_

X


_dzj,k_

= _zj+1,k(zj+2,k_ _zj_ 1,k) _zj,k + yk,_ (134)
_dt_ _‚àí_ _‚àí_ _‚àí_ _‚àí_

with cyclic indices yk+K = yk, zj,k+K = zj,k and zj+J,k = zj,k. Here, we take _xk_ as
_{_ _}_
randomly generated input sequences with the path length 64. We have tested for several
cases with different parameters: i) J = 6 with K = 1, 5, 10, 20; ii) K = 5 with J =
5, 15, 25, 100.

Note that the forced Lorenz 96 system parameterizes a highly nonlinear functional.

**Nonlinear model.** We learn the above system using RNN encoder-decoders with nonlinear activations, i.e.

_hs = œÉ(WEhs_ 1 + UExs + bE), _v = œÉ(QhœÑ + b1),_
_‚àí_
_gt = œÉ(WDgt_ 1 + bD), _g0 = œÉ(Pv + b2),_ (135)
_‚àí_
_ot = WOgt + bO,_

where œÉ is the element-wise tanh activation. Let m = 128 be the hidden dimension, N =
1Wthe model with a fixed hidden dimension, 2E, . . .,, WD 32 be the size of the coding vector ‚àà R[m][√ó][m], b1 ‚àà R[N], WO ‚àà R[m][√ó][K] m, Q v but different ‚àà, we haveR[m][√ó][N], x Ps ‚àà, o NtR, b, thus only sizes of[N]O[√ó] ‚àà[m]. Note that we constructR[K], hs, bE, bD Q, P, b, b2 ‚àà 1R, b[m]2,
are varying, while sizes of other parameters remain unchanged.

**Training and initialisation.** We denote the model with the coding vector size N as
EncDec[(][N] [)]. We utilise the Adam optimiser and train from EncDec[(1)] to EncDec[(32)]. For
EncDec[(1)], we use a normal random initialisation, and train for 3000 epoches until a stable
error. For EncDec[(][N] [)] with N > 1, we use the parameters trained from EncDec[(][N] _[‚àí][1)]_ as the
initialisation. For the parameters Q, P, b1, b2, we pad them to match the size of EncDec[(][N] [)]
with normal distributions as initialisations.

It is shown that the low rank approximation phenomena discovered in the linear setting also
appears in this nonlinear case.


-----

